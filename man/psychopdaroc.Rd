% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/psychopdaROC.h.R
\name{psychopdaROC}
\alias{psychopdaROC}
\title{Advanced ROC Analysis}
\usage{
psychopdaROC(
  clinicalMode = "basic",
  data,
  dependentVars,
  classVar,
  positiveClass,
  subGroup = NULL,
  clinicalPreset = "none",
  method = "maximize_metric",
  metric = "youden",
  direction = ">=",
  specifyCutScore = "",
  tol_metric = 0.05,
  break_ties = "mean",
  allObserved = FALSE,
  boot_runs = 0,
  seed = 123,
  usePriorPrev = FALSE,
  priorPrev = 0.5,
  costratioFP = 1,
  sensSpecTable = FALSE,
  showThresholdTable = FALSE,
  maxThresholds = 20,
  delongTest = FALSE,
  plotROC = TRUE,
  combinePlots = TRUE,
  cleanPlot = FALSE,
  showOptimalPoint = TRUE,
  displaySE = FALSE,
  smoothing = FALSE,
  showConfidenceBands = FALSE,
  legendPosition = "right",
  directLabel = FALSE,
  interactiveROC = FALSE,
  showCriterionPlot = FALSE,
  showPrevalencePlot = FALSE,
  showDotPlot = FALSE,
  precisionRecallCurve = FALSE,
  partialAUC = FALSE,
  partialAUCfrom = 0.8,
  partialAUCto = 1,
  rocSmoothingMethod = "none",
  bootstrapCI = FALSE,
  bootstrapReps = 2000,
  quantileCIs = FALSE,
  quantiles = "0.1,0.25,0.5,0.75,0.9",
  compareClassifiers = FALSE,
  calculateIDI = FALSE,
  calculateNRI = FALSE,
  refVar,
  nriThresholds = "",
  idiNriBootRuns = 1000,
  effectSizeAnalysis = FALSE,
  powerAnalysis = FALSE,
  powerAnalysisType = "post_hoc",
  expectedAUCDifference = 0.1,
  targetPower = 0.8,
  significanceLevel = 0.05,
  correlationROCs = 0.5,
  bayesianAnalysis = FALSE,
  priorAUC = 0.7,
  priorPrecision = 10,
  clinicalUtilityAnalysis = FALSE,
  treatmentThreshold = "0.05,0.5,0.05",
  harmBenefitRatio = 0.25,
  interventionCost = FALSE,
  fixedSensSpecAnalysis = FALSE,
  fixedAnalysisType = "sensitivity",
  fixedSensitivityValue = 0.9,
  fixedSpecificityValue = 0.9,
  showFixedROC = TRUE,
  fixedInterpolation = "linear",
  showFixedExplanation = TRUE,
  metaAnalysis = FALSE,
  metaAnalysisMethod = "both",
  heterogeneityTest = TRUE,
  forestPlot = FALSE,
  overrideMetaAnalysisWarning = FALSE
)
}
\arguments{
\item{clinicalMode}{Select the complexity level of analysis: Basic -
Essential ROC metrics for clinical decision making Advanced - Additional
statistical comparisons and metrics Comprehensive - Full research-grade
analysis with all options}

\item{data}{The data as a data frame.}

\item{dependentVars}{Test variable(s) to be evaluated for classification
performance. Multiple variables can be selected for comparison.}

\item{classVar}{Binary classification variable representing the true class
(gold standard). Must have exactly two levels.}

\item{positiveClass}{Specifies which level of the class variable should be
treated as the positive class.}

\item{subGroup}{Optional grouping variable for stratified analysis. ROC
curves will be calculated separately for each group.}

\item{clinicalPreset}{Choose a preset configuration optimized for specific
clinical scenarios: Screening - High sensitivity to avoid missing cases
Confirmation - High specificity to avoid false positives Balanced - Equal
weight to sensitivity and specificity Research - Comprehensive analysis for
publication}

\item{method}{Method for determining the optimal cutpoint. Different
methods optimize different aspects of classifier performance.}

\item{metric}{Metric to optimize when determining the cutpoint.  Only
applies to maximize/minimize methods.}

\item{direction}{Direction of classification relative to the cutpoint. Use
'>=' when higher test values indicate the positive class.}

\item{specifyCutScore}{Specific cutpoint value to use when method is set to
'Manual cutpoint'.}

\item{tol_metric}{Tolerance for the metric value when multiple cutpoints
yield similar performance. Cutpoints within this tolerance are considered
equivalent.}

\item{break_ties}{Method for handling ties when multiple cutpoints achieve
the same metric value.}

\item{allObserved}{Display performance metrics for all observed test values
as potential cutpoints, not just the optimal cutpoint.}

\item{boot_runs}{Number of bootstrap iterations for methods using
bootstrapping. Set to 0 to disable bootstrapping.}

\item{seed}{Random seed for reproducibility of bootstrap and permutation
tests.}

\item{usePriorPrev}{Use a specified prior prevalence instead of the sample
prevalence for calculating predictive values.}

\item{priorPrev}{Population prevalence to use for predictive value
calculations. Only used when 'Use Prior Prevalence' is checked.}

\item{costratioFP}{Relative cost of false positives compared to false
negatives. Values > 1 penalize false positives more heavily.}

\item{sensSpecTable}{Display detailed confusion matrices at optimal
cutpoints.}

\item{showThresholdTable}{Display detailed table with performance metrics
at multiple thresholds.}

\item{maxThresholds}{Maximum number of threshold values to show in the
threshold table.}

\item{delongTest}{Test whether the diagnostic performance differs
significantly between multiple tests. Uses DeLong's method to compare Area
Under the Curve (AUC) values. Requires at least two test variables.}

\item{plotROC}{Display ROC curves for visual assessment of classifier
performance.}

\item{combinePlots}{When multiple test variables are selected, combine all
ROC curves in a single plot.}

\item{cleanPlot}{Create clean ROC curves without annotations, suitable for
publications.}

\item{showOptimalPoint}{Display the optimal cutpoint on the ROC curve.}

\item{displaySE}{Display standard error bands on ROC curves (when LOESS
smoothing is applied).}

\item{smoothing}{Apply LOESS smoothing to ROC curves for visualization.}

\item{showConfidenceBands}{Display confidence bands around the ROC curve.}

\item{legendPosition}{Position of the legend in plots with multiple ROC
curves.}

\item{directLabel}{Label curves directly on the plot instead of using a
legend.}

\item{interactiveROC}{Create an interactive HTML ROC plot (requires plotROC
package).}

\item{showCriterionPlot}{Plot showing how sensitivity and specificity
change across different thresholds.}

\item{showPrevalencePlot}{Plot showing how PPV and NPV change with disease
prevalence.}

\item{showDotPlot}{Dot plot showing the distribution of test values by
class.}

\item{precisionRecallCurve}{Display precision-recall curves alongside ROC
curves.}

\item{partialAUC}{Calculate AUC for a specific region of the ROC curve.}

\item{partialAUCfrom}{Lower bound of specificity range for partial AUC
calculation.}

\item{partialAUCto}{Upper bound of specificity range for partial AUC
calculation.}

\item{rocSmoothingMethod}{Method for smoothing the ROC curve (requires pROC
package).}

\item{bootstrapCI}{Calculate bootstrap confidence intervals for AUC and
optimal cutpoints.}

\item{bootstrapReps}{Number of bootstrap replications for confidence
interval calculation.}

\item{quantileCIs}{Display confidence intervals at specific quantiles of
the test variable.}

\item{quantiles}{Comma-separated list of quantiles (0-1) at which to
display confidence intervals.}

\item{compareClassifiers}{Perform comprehensive comparison of classifier
performance metrics.}

\item{calculateIDI}{Calculate how much better one test is at discriminating
between diseased and healthy patients. IDI (Integrated Discrimination
Improvement) measures the average improvement in predicted probabilities.}

\item{calculateNRI}{Calculate how many patients are correctly reclassified
when using a new test. NRI (Net Reclassification Index) measures the net
improvement in patient classification.}

\item{refVar}{Reference test variable for IDI and NRI calculations. Other
variables will be compared against this reference.}

\item{nriThresholds}{Comma-separated probability thresholds (0-1) defining
risk categories for NRI. Leave empty for continuous NRI.}

\item{idiNriBootRuns}{Number of bootstrap iterations for IDI and NRI
confidence intervals.}

\item{effectSizeAnalysis}{Calculate effect sizes for ROC curve differences
using Cohen's conventions and standardized mean differences between AUC
values.}

\item{powerAnalysis}{Perform statistical power analysis for ROC curve
comparisons including sample size estimation and power calculations for
detecting AUC differences.}

\item{powerAnalysisType}{Type of power analysis to perform.}

\item{expectedAUCDifference}{Expected difference in AUC values for power
calculations and sample size estimation.}

\item{targetPower}{Target statistical power for sample size calculations
(typically 0.8 or 0.9).}

\item{significanceLevel}{Type I error rate for power calculations
(typically 0.05).}

\item{correlationROCs}{Expected correlation between paired ROC curves for
power calculations. Use 0.5 for moderate correlation, 0.0 for independent
samples.}

\item{bayesianAnalysis}{Perform bootstrap-based ROC analysis with optional
prior weighting to estimate uncertainty in AUC. Uses bootstrap resampling
to create an empirical distribution. NOTE: This is NOT full Bayesian MCMC
inference; it uses bootstrap simulation with prior parameters as weights.
Interpret "credible intervals" as bootstrap percentile confidence
intervals.}

\item{priorAUC}{Prior belief about AUC value for Bayesian analysis (center
of prior distribution).}

\item{priorPrecision}{Precision of prior belief (higher values = more
confident prior).}

\item{clinicalUtilityAnalysis}{Perform clinical utility analysis including
net benefit curves, decision curve analysis, and clinical impact
assessment.}

\item{treatmentThreshold}{Treatment threshold range for decision curve
analysis (min,max,step). Example: "0.05,0.5,0.05" creates thresholds from
5\\% to 50\\% in 5\\% steps.}

\item{harmBenefitRatio}{Ratio of harm from unnecessary treatment to benefit
from necessary treatment. Lower values favor more aggressive treatment
policies.}

\item{interventionCost}{Include cost-effectiveness considerations in
clinical utility analysis.}

\item{fixedSensSpecAnalysis}{Determine cutoffs based on fixed sensitivity
or specificity values and display corresponding performance metrics.}

\item{fixedAnalysisType}{Choose whether to fix sensitivity or specificity
value for cutoff determination.}

\item{fixedSensitivityValue}{Target sensitivity value (0-1) for determining
the corresponding cutoff and specificity.}

\item{fixedSpecificityValue}{Target specificity value (0-1) for determining
the corresponding cutoff and sensitivity.}

\item{showFixedROC}{Display separate ROC curve highlighting the fixed
sensitivity/specificity point.}

\item{fixedInterpolation}{Method for interpolating between observed points
to achieve target sensitivity/specificity.}

\item{showFixedExplanation}{Display explanatory guide for fixed
sensitivity/specificity analysis including  clinical interpretation,
interpolation methods, and usage recommendations.}

\item{metaAnalysis}{Perform meta-analysis of AUC values across multiple
test variables. Requires at least 3 test variables to enable pooled effect
estimation.}

\item{metaAnalysisMethod}{Statistical method for combining AUC estimates
across studies/variables.}

\item{heterogeneityTest}{Perform Cochran's Q test and calculate IÂ²
statistic to assess heterogeneity between AUC estimates.}

\item{forestPlot}{Create forest plot visualization of individual and pooled
AUC estimates with confidence intervals.}

\item{overrideMetaAnalysisWarning}{ADVANCED OPTION: Bypass the independence
assumption check for meta-analysis. WARNING - Only use if you fully
understand the statistical implications. Meta-analysis on non-independent
data produces invalid results and should NOT be used for formal inference.
Use DeLong's test instead for within-study comparisons of multiple markers.}
}
\value{
A results object containing:
\tabular{llllll}{
\code{results$instructions} \tab \tab \tab \tab \tab a html \cr
\code{results$procedureNotes} \tab \tab \tab \tab \tab a html \cr
\code{results$runSummary} \tab \tab \tab \tab \tab a html \cr
\code{results$simpleResultsTable} \tab \tab \tab \tab \tab a table \cr
\code{results$clinicalInterpretationTable} \tab \tab \tab \tab \tab a table \cr
\code{results$resultsTable} \tab \tab \tab \tab \tab an array of tables \cr
\code{results$sensSpecTable} \tab \tab \tab \tab \tab an array of htmls \cr
\code{results$thresholdTable} \tab \tab \tab \tab \tab a table \cr
\code{results$fixedSensSpecTable} \tab \tab \tab \tab \tab a table \cr
\code{results$fixedSensSpecExplanation} \tab \tab \tab \tab \tab a html \cr
\code{results$aucSummaryTable} \tab \tab \tab \tab \tab a table \cr
\code{results$delongComparisonTable} \tab \tab \tab \tab \tab a table \cr
\code{results$delongTest} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$plotROC} \tab \tab \tab \tab \tab an array of images \cr
\code{results$interactivePlot} \tab \tab \tab \tab \tab an image \cr
\code{results$fixedSensSpecROC} \tab \tab \tab \tab \tab an array of images \cr
\code{results$criterionPlot} \tab \tab \tab \tab \tab an array of images \cr
\code{results$prevalencePlot} \tab \tab \tab \tab \tab an array of images \cr
\code{results$dotPlot} \tab \tab \tab \tab \tab an array of images \cr
\code{results$dotPlotMessage} \tab \tab \tab \tab \tab a html \cr
\code{results$precisionRecallPlot} \tab \tab \tab \tab \tab an array of images \cr
\code{results$idiTable} \tab \tab \tab \tab \tab a table \cr
\code{results$nriTable} \tab \tab \tab \tab \tab a table \cr
\code{results$effectSizeTable} \tab \tab \tab \tab \tab a table \cr
\code{results$powerAnalysisTable} \tab \tab \tab \tab \tab a table \cr
\code{results$bayesianROCTable} \tab \tab \tab \tab \tab a table \cr
\code{results$clinicalUtilityTable} \tab \tab \tab \tab \tab a table \cr
\code{results$metaAnalysisWarning} \tab \tab \tab \tab \tab a html \cr
\code{results$metaAnalysisTable} \tab \tab \tab \tab \tab a table \cr
\code{results$sensitivityAnalysisTable} \tab \tab \tab \tab \tab a table \cr
\code{results$decisionCurveTable} \tab \tab \tab \tab \tab a table \cr
\code{results$partialAUCTable} \tab \tab \tab \tab \tab a table \cr
\code{results$bootstrapCITable} \tab \tab \tab \tab \tab a table \cr
\code{results$rocComparisonTable} \tab \tab \tab \tab \tab a table \cr
\code{results$effectSizePlot} \tab \tab \tab \tab \tab an array of images \cr
\code{results$powerCurvePlot} \tab \tab \tab \tab \tab an array of images \cr
\code{results$bayesianTracePlot} \tab \tab \tab \tab \tab an array of images \cr
\code{results$decisionCurvePlot} \tab \tab \tab \tab \tab an array of images \cr
\code{results$metaAnalysisForestPlot} \tab \tab \tab \tab \tab an array of images \cr
\code{results$sensitivityAnalysisPlot} \tab \tab \tab \tab \tab an array of images \cr
}

Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:

\code{results$simpleResultsTable$asDF}

\code{as.data.frame(results$simpleResultsTable)}
}
\description{
Receiver Operating Characteristic (ROC) curve analysis with optimal
cutpoint determination.
}
