% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pathagreement.h.R
\name{pathagreement}
\alias{pathagreement}
\title{Pathology Interrater Reliability}
\usage{
pathagreement(
  data,
  vars,
  sft = FALSE,
  heatmap = FALSE,
  heatmapDetails = FALSE,
  heatmapTheme = "viridis",
  wght = "unweighted",
  exct = FALSE,
  multiraterMethod = "auto",
  fleissCI = TRUE,
  kripp = FALSE,
  krippMethod = "nominal",
  consensus = FALSE,
  consensus_method = "majority",
  tie_breaking = "exclude",
  show_consensus_table = FALSE,
  showClinicalSummary = FALSE,
  showAboutAnalysis = FALSE,
  showAssumptions = FALSE,
  showWeightedKappaGuide = FALSE,
  showStatisticalGlossary = FALSE,
  styleDistanceMetric = "agreement",
  raterCharacteristics = FALSE,
  experienceVar = NULL,
  trainingVar = NULL,
  institutionVar = NULL,
  specialtyVar = NULL,
  identifyDiscordantCases = FALSE,
  caseID = NULL,
  icc = FALSE,
  bootstrap = FALSE,
  bootstrapSamples = 1000,
  pairwiseAnalysis = FALSE,
  categoryAnalysis = FALSE,
  outlierAnalysis = FALSE,
  pathologyContext = FALSE,
  gwetAC = FALSE,
  pabak = FALSE,
  sampleSizePlanning = FALSE,
  targetKappa = 0.8,
  targetPrecision = 0.1,
  raterBiasAnalysis = FALSE,
  agreementTrendAnalysis = FALSE,
  caseDifficultyScoring = FALSE,
  agreementStabilityAnalysis = FALSE,
  performClustering = FALSE,
  clusteringMethod = "ward",
  nStyleGroups = 3,
  autoSelectGroups = FALSE,
  showClusteringHeatmap = TRUE,
  heatmapColorScheme = "diagnostic",
  identifyDiscordant = FALSE,
  discordantThreshold = 0.5,
  raterExperience = NULL,
  raterSpecialty = NULL,
  raterInstitution = NULL,
  raterVolume = NULL,
  referenceStandard = NULL,
  useMetadataRows = FALSE,
  showInlineComments = FALSE,
  showClusteringInterpretation = FALSE,
  enhancedErrorGuidance = TRUE,
  showProgressIndicators = TRUE
)
}
\arguments{
\item{data}{The data as a data frame. Each row represents a case/subject,
and columns represent different raters/observers.}

\item{vars}{Variables representing different raters/observers. Each
variable should contain the ratings/diagnoses  given by each observer for
the same set of cases.}

\item{sft}{Show frequency tables for each rater and cross-tabulation tables
for pairwise comparisons.}

\item{heatmap}{Show agreement heatmap visualization with color-coded
agreement levels.}

\item{heatmapDetails}{Show detailed heatmap with kappa values and
confidence intervals for all rater pairs.}

\item{heatmapTheme}{Choose color scheme for the agreement heatmap
visualization.}

\item{wght}{Weighting scheme for kappa analysis. Use 'squared' or 'equal'
only with ordinal variables. Weighted kappa accounts for the degree of
disagreement.}

\item{exct}{Use exact method for Fleiss' kappa calculation with 3 or more
raters. More accurate but computationally intensive.}

\item{multiraterMethod}{Choose specific method for multi-rater agreement
analysis or use automatic selection.}

\item{fleissCI}{Calculate 95\\% confidence intervals for Fleiss' kappa using
asymptotic standard errors.}

\item{kripp}{Calculate Krippendorff's alpha, a generalized measure of
reliability for any number of observers and data types.}

\item{krippMethod}{Measurement level for Krippendorff's alpha calculation.
Choose based on your data type.}

\item{consensus}{Perform consensus scoring analysis to determine
agreed-upon ratings from multiple raters.}

\item{consensus_method}{Method for determining consensus scores from
multiple raters.}

\item{tie_breaking}{How to handle cases where no consensus can be reached
using the selected method.}

\item{show_consensus_table}{Display detailed consensus scoring results
including individual rater scores and consensus outcomes.}

\item{showClinicalSummary}{Show clinical summary with plain-language
interpretation of agreement statistics and their practical implications.}

\item{showAboutAnalysis}{Show educational information about inter-rater
reliability analysis, when to use it, and what the outputs mean.}

\item{showAssumptions}{Show important assumptions, data requirements,
common pitfalls, and interpretation guidelines for the analysis.}

\item{showWeightedKappaGuide}{Show explanatory guide for weighted kappa
options, including when to use linear vs quadratic weighting schemes.}

\item{showStatisticalGlossary}{Show glossary of statistical terms (kappa,
ICC, alpha, etc.) with clinical interpretations and usage guidelines.}

\item{styleDistanceMetric}{Distance metric for measuring diagnostic
similarity between raters for style clustering.}

\item{raterCharacteristics}{Include rater background characteristics
(experience, training, institution) in style analysis.}

\item{experienceVar}{Optional variable containing rater experience
information (years of experience, level of training, etc.)}

\item{trainingVar}{Optional variable containing rater training institution
or background information}

\item{institutionVar}{Optional variable containing rater current
institution or location information}

\item{specialtyVar}{Optional variable containing rater medical specialty or
subspecialty information}

\item{identifyDiscordantCases}{Identify cases that distinguish different
diagnostic styles - useful for training and consensus development.}

\item{caseID}{Optional variable containing case identifiers. If not
specified, cases will be numbered automatically.}

\item{icc}{Calculate ICC for continuous or ordinal data. Provides
additional reliability measures beyond kappa.}

\item{bootstrap}{Calculate bootstrap confidence intervals for
Krippendorff's alpha and other statistics.}

\item{bootstrapSamples}{Number of bootstrap samples for confidence interval
calculation.}

\item{pairwiseAnalysis}{Detailed analysis of agreement between each pair of
raters.}

\item{categoryAnalysis}{Agreement analysis for each diagnostic category
separately.}

\item{outlierAnalysis}{Identify cases with unusually poor agreement across
raters.}

\item{pathologyContext}{Calculate pathology-specific metrics including
diagnostic accuracy, sensitivity, and specificity when gold standard is
available.}

\item{gwetAC}{Calculate Gwet's AC1 and AC2 coefficients, which are more
robust than kappa for high agreement scenarios and less affected by
prevalence.}

\item{pabak}{Calculate Prevalence-Adjusted Bias-Adjusted Kappa (PABAK) to
address prevalence and bias issues in agreement studies.}

\item{sampleSizePlanning}{Perform sample size planning calculations for
agreement studies with specified precision requirements.}

\item{targetKappa}{Target kappa value for sample size planning
calculations.}

\item{targetPrecision}{Target precision for confidence interval width in
sample size planning.}

\item{raterBiasAnalysis}{Analyze systematic tendencies and biases for each
rater compared to the consensus or average ratings.}

\item{agreementTrendAnalysis}{Analyze how agreement changes over time or
case sequence, useful for training effect assessment.}

\item{caseDifficultyScoring}{Quantify inherent case difficulty based on
inter-rater disagreement patterns and provide difficulty scores.}

\item{agreementStabilityAnalysis}{Bootstrap-based stability measures to
assess the consistency of agreement statistics across different samples.}

\item{performClustering}{Identify diagnostic style groups among raters
using hierarchical clustering. Implements methodology from Usubutun et al.
(2012) Modern Pathology. Clusters raters based on diagnosis pattern
similarity to reveal systematic differences in diagnostic approach.}

\item{clusteringMethod}{Hierarchical clustering linkage method. Ward's
method minimizes within-group variance and is recommended for identifying
distinct diagnostic styles (Usubutun 2012).}

\item{nStyleGroups}{Number of diagnostic style groups to identify. Original
study found 3 groups: conservative (under-diagnosis), moderate (majority),
and sensitive (aligns with expert).}

\item{autoSelectGroups}{Use silhouette method or within-cluster sum of
squares to automatically determine optimal number of style groups.}

\item{showClusteringHeatmap}{Display Cases Ã— Raters heatmap with
hierarchical dendrograms showing diagnostic patterns and style groups.}

\item{heatmapColorScheme}{Color scheme for clustering heatmap. Diagnostic
uses distinct colors per category as in Usubutun (2012).}

\item{identifyDiscordant}{Flag cases with high inter-rater disagreement
that distinguish diagnostic style groups. These cases are useful for
training and quality assurance discussions.}

\item{discordantThreshold}{Minimum disagreement proportion for flagging
discordant cases. 0.5 means at least 50\\% of raters disagreed with majority
diagnosis.}

\item{raterExperience}{Years of experience for each rater. Will be tested
for association with style group membership.}

\item{raterSpecialty}{Specialty or practice type (e.g., specialist vs
generalist, subspecialty). Will be tested for association with style group
membership.}

\item{raterInstitution}{Training or current practice institution. Will be
tested for association with style group membership. Usubutun (2012) found
no association, suggesting diagnostic style is personal rather than
institutional.}

\item{raterVolume}{Number of cases seen per month or year. Will be tested
for association with style group membership.}

\item{referenceStandard}{Expert consensus or reference standard diagnosis.
Used to compare style groups and identify which group aligns most closely
with expert judgment.}

\item{useMetadataRows}{Enable extraction of rater characteristics from
special metadata rows in the dataset. Metadata rows should have case_id
starting with "META_" (e.g., META_experience, META_specialty). Values in
rater columns will be extracted as characteristics for association testing.}

\item{showInlineComments}{Show detailed statistical explanations and
interpretations inline with results for educational purposes.}

\item{showClusteringInterpretation}{Display explanatory guide for
interpreting clustering results, diagnostic style groups, and discordant
cases. Useful for understanding clinical implications.}

\item{enhancedErrorGuidance}{Provide detailed error messages and
suggestions for resolving common issues in agreement analysis.}

\item{showProgressIndicators}{Display progress indicators for
computationally intensive operations like bootstrap calculations.}
}
\value{
A results object containing:
\tabular{llllll}{
\code{results$todo} \tab \tab \tab \tab \tab a html \cr
\code{results$warnings} \tab \tab \tab \tab \tab a html \cr
\code{results$overviewTable} \tab \tab \tab \tab \tab a table \cr
\code{results$kappaTable} \tab \tab \tab \tab \tab a table \cr
\code{results$iccTable} \tab \tab \tab \tab \tab a table \cr
\code{results$pairwiseTable} \tab \tab \tab \tab \tab a table \cr
\code{results$categoryTable} \tab \tab \tab \tab \tab a table \cr
\code{results$outlierTable} \tab \tab \tab \tab \tab a table \cr
\code{results$diagnosticAccuracyTable} \tab \tab \tab \tab \tab a table \cr
\code{results$diagnosticStyleTable} \tab \tab \tab \tab \tab a table \cr
\code{results$styleSummaryTable} \tab \tab \tab \tab \tab a table \cr
\code{results$discordantCasesTable} \tab \tab \tab \tab \tab a table \cr
\code{results$krippTable} \tab \tab \tab \tab \tab a table \cr
\code{results$consensusTable} \tab \tab \tab \tab \tab a table \cr
\code{results$consensusSummary} \tab \tab \tab \tab \tab a table \cr
\code{results$heatmapPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$pairwisePlot} \tab \tab \tab \tab \tab an image \cr
\code{results$categoryPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$confusionMatrixPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$diagnosticStyleDendrogram} \tab \tab \tab \tab \tab an image \cr
\code{results$diagnosticStyleHeatmap} \tab \tab \tab \tab \tab an image \cr
\code{results$diagnosticStyleCombined} \tab \tab \tab \tab \tab an image \cr
\code{results$raterFrequencyTables$frequencyTable} \tab \tab \tab \tab \tab a table \cr
\code{results$crosstabTable} \tab \tab \tab \tab \tab a table \cr
\code{results$clinicalSummary} \tab \tab \tab \tab \tab a html \cr
\code{results$reportTemplate} \tab \tab \tab \tab \tab a html \cr
\code{results$aboutAnalysis} \tab \tab \tab \tab \tab a html \cr
\code{results$assumptions} \tab \tab \tab \tab \tab a html \cr
\code{results$weightedKappaGuide} \tab \tab \tab \tab \tab a html \cr
\code{results$statisticalGlossary} \tab \tab \tab \tab \tab a html \cr
\code{results$gwetACTable} \tab \tab \tab \tab \tab a table \cr
\code{results$pabakTable} \tab \tab \tab \tab \tab a table \cr
\code{results$sampleSizeTable} \tab \tab \tab \tab \tab a table \cr
\code{results$raterBiasTable} \tab \tab \tab \tab \tab a table \cr
\code{results$agreementTrendTable} \tab \tab \tab \tab \tab a table \cr
\code{results$caseDifficultyTable} \tab \tab \tab \tab \tab a table \cr
\code{results$stabilityTable} \tab \tab \tab \tab \tab a table \cr
\code{results$trendPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$biasPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$difficultyPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$inlineComments} \tab \tab \tab \tab \tab a html \cr
\code{results$styleGroupSummary} \tab \tab \tab \tab \tab a table \cr
\code{results$styleGroupProfiles} \tab \tab \tab \tab \tab a table \cr
\code{results$discordantCasesCluster} \tab \tab \tab \tab \tab a table \cr
\code{results$characteristicAssociations} \tab \tab \tab \tab \tab a table \cr
\code{results$referenceComparison} \tab \tab \tab \tab \tab a table \cr
\code{results$clusteringHeatmap} \tab \tab \tab \tab \tab Heatmap showing diagnostic patterns with dual dendrograms (raters and cases) \cr
\code{results$clusterDendrogram} \tab \tab \tab \tab \tab Dendrogram showing hierarchical relationships between raters \cr
\code{results$silhouettePlot} \tab \tab \tab \tab \tab Silhouette plot showing cluster separation and cohesion \cr
\code{results$clusteringInterpretation} \tab \tab \tab \tab \tab Explanatory guide for understanding diagnostic style groups and cluster results \cr
}

Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:

\code{results$overviewTable$asDF}

\code{as.data.frame(results$overviewTable)}
}
\description{
Comprehensive interrater reliability analysis including Cohen's kappa (2
raters),  Fleiss' kappa (3+ raters), Krippendorff's alpha, and consensus
analysis. Provides agreement statistics, visualization, and clinical
interpretation for categorical rating data.
}
\examples{
# Load example data
data('pathology_ratings', package = 'ClinicoPath')

# Basic agreement analysis with 2 raters
pathagreement(pathology_ratings,
          vars = c('rater1', 'rater2'))

# Advanced analysis with 3+ raters including visualization
pathagreement(pathology_ratings,
          vars = c('rater1', 'rater2', 'rater3'),
          multiraterMethod = 'fleiss',
          fleissCI = TRUE,
          heatmap = TRUE,
          heatmapDetails = TRUE,
          sft = TRUE)

# Krippendorff's alpha for ordinal data
agreement(pathology_ratings,
          vars = c('rater1', 'rater2', 'rater3'),
          multiraterMethod = 'krippendorff',
          kripp = TRUE,
          krippMethod = 'ordinal')

# Consensus analysis
agreement(pathology_ratings,
          vars = c('rater1', 'rater2', 'rater3'),
          consensus = TRUE,
          consensus_method = 'majority',
          tie_breaking = 'arbitration',
          show_consensus_table = TRUE)

}
