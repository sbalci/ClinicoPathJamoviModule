% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/entropyanalysis.h.R
\name{entropyanalysis}
\alias{entropyanalysis}
\title{Entropy and Mutual Information Analysis}
\usage{
entropyanalysis(
  data,
  outcome,
  probability_vars,
  predictor_var,
  calculate_entropy = TRUE,
  calculate_conditional_entropy = TRUE,
  calculate_mutual_information = TRUE,
  calculate_kl_divergence = FALSE,
  uncertainty_threshold = 0.5,
  normalize_entropy = TRUE,
  binning_method = "equal_width",
  n_bins = 10,
  show_case_level = FALSE,
  flag_uncertain = TRUE,
  plot_entropy_distribution = TRUE,
  plot_uncertainty_by_class = TRUE,
  plot_mi_heatmap = FALSE,
  random_seed = 42
)
}
\arguments{
\item{data}{the data as a data frame}

\item{outcome}{a string naming the true outcome/class variable}

\item{probability_vars}{vector of predicted probability variables (one per
class)}

\item{predictor_var}{optional single predictor for mutual information
calculation}

\item{calculate_entropy}{calculate Shannon entropy for each prediction}

\item{calculate_conditional_entropy}{calculate conditional entropy H(Y|X)}

\item{calculate_mutual_information}{calculate mutual information I(X;Y)}

\item{calculate_kl_divergence}{calculate Kullback-Leibler divergence from
uniform distribution}

\item{uncertainty_threshold}{entropy threshold for flagging
high-uncertainty predictions}

\item{normalize_entropy}{normalize entropy to \link{0,1} scale (divide by
log(n_classes))}

\item{binning_method}{binning method for continuous variables in MI
calculation}

\item{n_bins}{number of bins for discretizing continuous variables}

\item{show_case_level}{show entropy for each individual case}

\item{flag_uncertain}{identify cases exceeding uncertainty threshold}

\item{plot_entropy_distribution}{plot histogram of entropy values}

\item{plot_uncertainty_by_class}{plot entropy distribution for each true
class}

\item{plot_mi_heatmap}{plot heatmap of pairwise mutual information (for
multiple features)}

\item{random_seed}{random seed for reproducibility}
}
\value{
A results object containing:
\tabular{llllll}{
\code{results$instructionsText} \tab \tab \tab \tab \tab a html \cr
\code{results$summaryTable} \tab \tab \tab \tab \tab a table \cr
\code{results$entropyTable} \tab \tab \tab \tab \tab a table \cr
\code{results$mutualInfoTable} \tab \tab \tab \tab \tab a table \cr
\code{results$conditionalEntropyTable} \tab \tab \tab \tab \tab a table \cr
\code{results$klDivergenceTable} \tab \tab \tab \tab \tab a table \cr
\code{results$caseLevelTable} \tab \tab \tab \tab \tab a table \cr
\code{results$entropyDistPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$uncertaintyByClassPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$interpretationText} \tab \tab \tab \tab \tab a html \cr
}

Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:

\code{results$summaryTable$asDF}

\code{as.data.frame(results$summaryTable)}
}
\description{
Entropy and Mutual Information analysis quantifies uncertainty in AI
predictions
and measures information gain from diagnostic tests or features.
}
\details{
Shannon Entropy quantifies prediction uncertainty (higher = more
uncertain).
Mutual Information measures how much knowing one variable reduces
uncertainty
about another.

Applications: AI triage systems ("defer to pathologist" decisions), feature
selection, test ordering optimization, multi-class uncertainty
quantification.
}
\examples{
# Example with AI prediction probabilities
data <- data.frame(
  true_class = factor(sample(c("A", "B", "C"), 100, replace=TRUE)),
  prob_A = runif(100),
  prob_B = runif(100),
  prob_C = runif(100)
)

entropyanalysis(
  data = data,
  outcome = 'true_class',
  probability_vars = c('prob_A', 'prob_B', 'prob_C'),
  uncertainty_threshold = 0.5
)

}
