% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/aivalidation.h.R
\name{aivalidation}
\alias{aivalidation}
\title{AI Model Validation}
\usage{
aivalidation(
  data,
  predictorVars,
  outcomeVar,
  positiveLevel,
  compareModels = FALSE,
  youdensJ = FALSE,
  matthewsCC = FALSE,
  bootstrapCI = FALSE,
  nBootstrap = 1000,
  rocPlot = FALSE,
  crossValidation = "none",
  stratified = TRUE,
  randomSeed = 42,
  showExplanations = FALSE,
  showSummaries = FALSE
)
}
\arguments{
\item{data}{the data as a data frame}

\item{predictorVars}{a vector of strings naming the predictor variables (AI
scores, human scores, biomarkers, etc.) from \code{data}. Limited to first
5 for pairwise comparisons.}

\item{outcomeVar}{a string naming the binary outcome variable (gold
standard) from \code{data}}

\item{positiveLevel}{the level of the outcome variable which represents the
positive case}

\item{compareModels}{perform statistical comparison between models using
DeLong test for AUC comparison}

\item{youdensJ}{calculate and display Youden's J statistic (Sensitivity +
Specificity - 1)}

\item{matthewsCC}{calculate and display Matthews Correlation Coefficient
(MCC)}

\item{bootstrapCI}{use bootstrap resampling for confidence intervals (more
robust for small samples)}

\item{nBootstrap}{number of bootstrap iterations (higher values are more
accurate but slower)}

\item{rocPlot}{generate ROC curves for all predictor variables}

\item{crossValidation}{cross-validation method for model validation
(simplified to avoid resource limits)}

\item{stratified}{maintain outcome variable proportions across folds}

\item{randomSeed}{random seed for reproducible cross-validation results}

\item{showExplanations}{show detailed methodology explanations}

\item{showSummaries}{show interpretation summaries of results}
}
\value{
A results object containing:
\tabular{llllll}{
\code{results$instructions} \tab \tab \tab \tab \tab a html \cr
\code{results$performanceTable} \tab \tab \tab \tab \tab Performance metrics for each predictor variable \cr
\code{results$comparisonTable} \tab \tab \tab \tab \tab Statistical comparison between predictor models using DeLong test \cr
\code{results$cvPerformanceTable} \tab \tab \tab \tab \tab Cross-validated performance metrics for each predictor \cr
\code{results$rocPlot} \tab \tab \tab \tab \tab ROC curves for all predictor models \cr
\code{results$methodologyExplanation} \tab \tab \tab \tab \tab a html \cr
\code{results$resultsInterpretation} \tab \tab \tab \tab \tab a html \cr
}

Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:

\code{results$performanceTable$asDF}

\code{as.data.frame(results$performanceTable)}
}
\description{
Simplified AI model validation tool for comparing diagnostic performance.
Calculates AUC, sensitivity, and specificity for predictor variables and
performs statistical comparison using DeLong test.
}
\examples{
\donttest{
data('medical_ai_data', package='ClinicoPath')

aivalidation(data = medical_ai_data,
            predictorVars = c('AI_score', 'human_score', 'biomarker1'),
            outcomeVar = 'diagnosis',
            positiveLevel = 'positive',
            compareModels = TRUE)
}
}
