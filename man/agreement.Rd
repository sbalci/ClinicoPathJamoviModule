% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/agreement.h.R
\name{agreement}
\alias{agreement}
\title{Interrater Reliability}
\usage{
agreement(
  data,
  vars,
  sft = FALSE,
  showText = FALSE,
  wght = "unweighted",
  exct = FALSE,
  kripp = FALSE,
  krippMethod = "nominal",
  bootstrap = FALSE,
  gwet = FALSE,
  gwetWeights = "unweighted",
  ccc = FALSE,
  blandAltman = FALSE,
  blandAltmanPlot = FALSE,
  proportionalBias = FALSE,
  baConfidenceLevel = 0.95,
  hierarchicalKappa = FALSE,
  clusterVariable,
  randomEffectsRater = FALSE,
  randomEffectsCluster = FALSE,
  iccHierarchical = FALSE,
  clusterSpecificKappa = FALSE,
  betweenClusterVariance = FALSE,
  withinClusterVariance = FALSE,
  shrinkageEstimates = FALSE,
  testClusterHomogeneity = FALSE,
  clusterRankings = FALSE,
  showSummary = FALSE,
  showAbout = FALSE,
  consensusVar = FALSE,
  consensusRule = "majority",
  tieBreaker = "exclude",
  consensusName = "consensus_score",
  referenceRater,
  rankRaters = TRUE,
  loaVariable = FALSE,
  loaThresholds = "custom",
  loaHighThreshold = 75,
  loaLowThreshold = 56,
  loaVarName = "loa_category"
)
}
\arguments{
\item{data}{The data as a data frame. The data should be in long format,
where each row is a unique observation.}

\item{vars}{A string naming the variable from \code{data} that contains the
diagnosis given by the observer, variable can be categorical or ordinal.}

\item{sft}{Display frequency tables showing the distribution of ratings for
each rater. Useful for understanding rating patterns and identifying
potential biases.}

\item{showText}{Display simple preformatted text version of frequency
tables. Provides a plain-text alternative to the HTML formatted tables.}

\item{wght}{For ordinal variables (e.g., tumor grade G1/G2/G3), weighted
kappa accounts for degree of disagreement. Linear weights: Adjacent
disagreements (G1 vs G2) receive partial credit. Squared weights: Larger
disagreements (G1 vs G3) are penalized more heavily. Use 'Unweighted' for
nominal categories with no inherent order.}

\item{exct}{Use exact p-value calculation instead of normal approximation.
Recommended for small sample sizes (< 30 cases) with 3 or more raters.
Note: Not applicable for 2-rater analysis (use Cohen's kappa).}

\item{kripp}{Alternative reliability measure that handles missing data and
supports various data types. Useful when raters didn't rate all cases or
when comparing different measurement levels.}

\item{krippMethod}{Specifies the measurement level for Krippendorff's alpha
calculation.}

\item{bootstrap}{Calculate bootstrap confidence intervals for
Krippendorff's alpha.}

\item{gwet}{Alternative agreement coefficient that is more stable than
Cohen's kappa when dealing with high agreement rates or unbalanced marginal
distributions (e.g., rare tumor subtypes). Gwet's AC corrects for the
paradoxical behavior of kappa in extreme cases.}

\item{gwetWeights}{Unweighted (AC1) for nominal categories. Linear or
Quadratic weights (AC2) for ordinal data.}

\item{ccc}{Calculate Lin's Concordance Correlation Coefficient for
continuous measurement agreement. CCC assesses the agreement between two
continuous measurements or raters, combining measures of both precision and
accuracy. Particularly useful for method comparison studies, inter-rater
reliability of continuous scores (e.g., quantitative pathology
measurements, biomarker levels), and validation of new measurement
techniques. CCC ranges from -1 to +1, with +1 indicating perfect
concordance.}

\item{blandAltman}{Perform Bland-Altman analysis for assessing agreement
between two continuous measurement methods. Calculates mean difference
(bias) and limits of agreement (LOA = mean ± 1.96×SD). Essential for method
comparison and validation studies in clinical chemistry, pathology, and
diagnostic imaging. Particularly useful for comparing a new measurement
method against a reference standard, or assessing inter-rater reliability
for continuous measurements.}

\item{blandAltmanPlot}{Generate Bland-Altman plot showing difference vs.
average of two measurements. Displays mean bias line and limits of
agreement (±1.96 SD).}

\item{proportionalBias}{Test whether the difference between methods is
related to the magnitude of measurements (proportional bias). Uses linear
regression of difference on average values.}

\item{baConfidenceLevel}{Confidence level for limits of agreement (default
95\\%).}

\item{hierarchicalKappa}{Enable hierarchical (multilevel) kappa analysis
for nested data structures (e.g., pathologists nested within institutions,
readers nested within centers). Accounts for clustering effects and
provides institution/cluster-specific agreement estimates. Essential for
multi-center reliability studies.}

\item{clusterVariable}{Variable defining clusters/institutions/centers. For
example, hospital ID, institution name, or scanner ID. Raters are nested
within these clusters.}

\item{randomEffectsRater}{Model rater effects as random (raters are a
random sample from population of potential raters). If false, raters are
treated as fixed effects. Random effects recommended when generalizing
beyond specific raters.}

\item{randomEffectsCluster}{Model cluster/institution effects as random
(institutions are random sample). Allows generalization beyond specific
institutions in the study.}

\item{iccHierarchical}{Calculate intraclass correlation coefficients for
hierarchical data. ICC(1): between-cluster agreement, ICC(2): reliability
of cluster means, ICC(3): within-cluster agreement. Decomposes variance
into cluster-level and rater-level components.}

\item{clusterSpecificKappa}{Calculate kappa separately for each
cluster/institution to identify sites with poor agreement. Useful for
quality control in multi-center studies.}

\item{betweenClusterVariance}{Estimate variance component for
between-cluster differences in agreement. Large between-cluster variance
indicates institutional heterogeneity requiring investigation (different
protocols, training, etc.).}

\item{withinClusterVariance}{Estimate variance component for within-cluster
rater disagreement. Comparison of within vs between variance informs
whether issues are local (within institutions) or systematic (between
institutions).}

\item{shrinkageEstimates}{Calculate shrinkage estimates for
cluster-specific kappas. Shrinks extreme estimates toward overall mean,
providing more stable estimates for small clusters.}

\item{testClusterHomogeneity}{Test whether agreement is homogeneous across
clusters (null hypothesis: all clusters have equal kappa). Significant
result indicates heterogeneity requiring investigation.}

\item{clusterRankings}{Rank clusters/institutions by agreement performance
with confidence intervals. Identifies best and worst performing sites. Use
cautiously to avoid unfair comparisons when cluster sizes differ
substantially.}

\item{showSummary}{Display a natural-language interpretation of results
with color-coded agreement levels and clinical guidance. Recommended for
reports and presentations.}

\item{showAbout}{Display an explanatory panel describing what this analysis
does, when to use it, and how to interpret results.}

\item{consensusVar}{Calculate consensus (modal) score across raters and add
as new computed column. Essential for multi-rater studies requiring a
reference standard.}

\item{consensusRule}{Rule for defining consensus. Simple majority = modal
category with >50\\% of votes. Supermajority requires ≥75\\% agreement.
Unanimous requires 100\\% agreement.}

\item{tieBreaker}{How to handle ties (e.g., 50-50 split). Exclude = set
consensus to NA for tied cases. First = use first category that appears.
Lowest/Highest = use min/max of tied categories.}

\item{consensusName}{Name of the new computed variable to be added to the
dataset.}

\item{referenceRater}{Select a reference rater (e.g., consensus score, gold
standard, senior pathologist). Each selected rater will be compared
pairwise with this reference, producing individual kappas. Essential for
training assessment, rater certification, and performance monitoring.}

\item{rankRaters}{Rank raters from highest to lowest kappa (relative to
reference). Shows best and worst performing raters for quality control and
training needs.}

\item{loaVariable}{Calculate level of agreement (LoA) for each case and add
as new computed column. Categorizes cases as
Absolute/High/Moderate/Low/Poor agreement based on proportion of raters
agreeing. Useful for identifying difficult cases and quality control.}

\item{loaThresholds}{How to define LoA categories. Custom = user-defined
cutpoints. Quartiles/Tertiles = data-driven splits.}

\item{loaHighThreshold}{Minimum \\% agreement for "High" LoA (e.g., 75\\% =
≥12/16 raters for N=16).}

\item{loaLowThreshold}{Minimum \\% agreement for "Low" LoA (e.g., 56\\% =
≥9/16 raters for N=16).}

\item{loaVarName}{Name of the new computed variable for level of agreement.}
}
\value{
A results object containing:
\tabular{llllll}{
\code{results$welcome} \tab \tab \tab \tab \tab a html \cr
\code{results$headingIRR} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$irrtable} \tab \tab \tab \tab \tab a table \cr
\code{results$text2} \tab \tab \tab \tab \tab a html \cr
\code{results$text} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$headingKripp} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$krippTable} \tab \tab \tab \tab \tab a table \cr
\code{results$headingGwet} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$gwetTable} \tab \tab \tab \tab \tab a table \cr
\code{results$headingCCC} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$cccTable} \tab \tab \tab \tab \tab a table \cr
\code{results$headingBlandAltman} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$blandAltmanTable} \tab \tab \tab \tab \tab a table \cr
\code{results$proportionalBiasTable} \tab \tab \tab \tab \tab a table \cr
\code{results$blandAltmanPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$weightedKappaGuide} \tab \tab \tab \tab \tab a html \cr
\code{results$hierarchicalICCTable} \tab \tab \tab \tab \tab a table \cr
\code{results$varianceComponents} \tab \tab \tab \tab \tab a table \cr
\code{results$clusterSpecificKappaTable} \tab \tab \tab \tab \tab a table \cr
\code{results$clusterHomogeneityTest} \tab \tab \tab \tab \tab a table \cr
\code{results$clusterRankingsTable} \tab \tab \tab \tab \tab a table \cr
\code{results$consensusTable} \tab \tab \tab \tab \tab a table \cr
\code{results$headingPairwise} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$pairwiseKappaTable} \tab \tab \tab \tab \tab a table \cr
\code{results$headingLoA} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$loaTable} \tab \tab \tab \tab \tab a table \cr
\code{results$summary} \tab \tab \tab \tab \tab a html \cr
\code{results$about} \tab \tab \tab \tab \tab a html \cr
}

Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:

\code{results$irrtable$asDF}

\code{as.data.frame(results$irrtable)}
}
\description{
Function for Interrater Reliability.
}
\examples{
\donttest{
# example will be added
}
}
