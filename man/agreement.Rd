% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/agreement.h.R
\name{agreement}
\alias{agreement}
\title{Interrater Reliability}
\usage{
agreement(
  data,
  vars,
  sft = FALSE,
  heatmap = FALSE,
  heatmapDetails = FALSE,
  wght = "unweighted",
  exct = FALSE,
  multiraterMethod = "auto",
  fleissCI = TRUE,
  kripp = FALSE,
  krippMethod = "nominal",
  consensus = FALSE,
  consensus_method = "majority",
  tie_breaking = "exclude",
  show_consensus_table = TRUE
)
}
\arguments{
\item{data}{The data as a data frame. Each row represents a case/subject,
and columns represent different raters/observers.}

\item{vars}{Variables representing different raters/observers. Each
variable should contain the ratings/diagnoses  given by each observer for
the same set of cases.}

\item{sft}{Show frequency tables for each rater and cross-tabulation tables
for pairwise comparisons.}

\item{heatmap}{Show agreement heatmap visualization with color-coded
agreement levels.}

\item{heatmapDetails}{Show detailed heatmap with kappa values and
confidence intervals for all rater pairs.}

\item{wght}{Weighting scheme for kappa analysis. Use 'squared' or 'equal'
only with ordinal variables. Weighted kappa accounts for the degree of
disagreement.}

\item{exct}{Use exact method for Fleiss' kappa calculation with 3 or more
raters. More accurate but computationally intensive.}

\item{multiraterMethod}{Choose specific method for multi-rater agreement
analysis or use automatic selection.}

\item{fleissCI}{Calculate 95\\% confidence intervals for Fleiss' kappa using
asymptotic standard errors.}

\item{kripp}{Calculate Krippendorff's alpha, a generalized measure of
reliability for any number of observers and data types.}

\item{krippMethod}{Measurement level for Krippendorff's alpha calculation.
Choose based on your data type.}

\item{consensus}{Perform consensus scoring analysis to determine
agreed-upon ratings from multiple raters.}

\item{consensus_method}{Method for determining consensus scores from
multiple raters.}

\item{tie_breaking}{How to handle cases where no consensus can be reached
using the selected method.}

\item{show_consensus_table}{Display detailed consensus scoring results
including individual rater scores and consensus outcomes.}
}
\value{
A results object containing:
\tabular{llllll}{
\code{results$todo} \tab \tab \tab \tab \tab a html \cr
\code{results$overviewTable} \tab \tab \tab \tab \tab a table \cr
\code{results$kappaTable} \tab \tab \tab \tab \tab a table \cr
\code{results$krippTable} \tab \tab \tab \tab \tab a table \cr
\code{results$consensusTable} \tab \tab \tab \tab \tab a table \cr
\code{results$consensusSummary} \tab \tab \tab \tab \tab a table \cr
\code{results$heatmapPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$frequencyTables} \tab \tab \tab \tab \tab a html \cr
}

Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:

\code{results$overviewTable$asDF}

\code{as.data.frame(results$overviewTable)}
}
\description{
Function for Interrater Reliability.
}
\examples{
\donttest{
# example will be added
}
}
