% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/agreement.h.R
\name{agreement}
\alias{agreement}
\title{Interrater Reliability}
\usage{
agreement(
  data,
  vars,
  baConfidenceLevel = 0.95,
  proportionalBias = FALSE,
  blandAltmanPlot = FALSE,
  sft = FALSE,
  showText = FALSE,
  wght = "unweighted",
  exct = FALSE,
  kripp = FALSE,
  krippMethod = "nominal",
  bootstrap = FALSE,
  gwet = FALSE,
  gwetWeights = "unweighted",
  showLevelInfo = FALSE,
  hierarchicalKappa = FALSE,
  clusterVariable = NULL,
  iccHierarchical = FALSE,
  clusterSpecificKappa = FALSE,
  betweenClusterVariance = FALSE,
  withinClusterVariance = FALSE,
  shrinkageEstimates = FALSE,
  testClusterHomogeneity = FALSE,
  clusterRankings = FALSE,
  showSummary = FALSE,
  showAbout = FALSE,
  consensusVar = FALSE,
  consensusRule = "majority",
  tieBreaker = "exclude",
  consensusName = "consensus_score",
  referenceRater = NULL,
  rankRaters = FALSE,
  loaVariable = FALSE,
  loaThresholds = "custom",
  loaHighThreshold = 75,
  loaLowThreshold = 56
)
}
\arguments{
\item{data}{The data as a data frame. The data should be in long format,
where each row is a unique observation.}

\item{vars}{A string naming the variable from \code{data} that contains the
diagnosis given by the observer, variable can be categorical or ordinal.}

\item{baConfidenceLevel}{.}

\item{proportionalBias}{.}

\item{blandAltmanPlot}{Generate Bland-Altman plot for continuous agreement
analysis. Displays mean difference and limits of agreement between the
first two raters. Only applicable when raters provide continuous
measurements (e.g., tumor size in mm).}

\item{sft}{Display frequency tables showing the distribution of ratings for
each rater. Useful for understanding rating patterns and identifying
potential biases.}

\item{showText}{Display simple preformatted text version of frequency
tables. Provides a plain-text alternative to the HTML formatted tables.}

\item{wght}{For ordinal variables (e.g., tumor grade G1/G2/G3), weighted
kappa accounts for degree of disagreement. Linear weights: Adjacent
disagreements (G1 vs G2) receive partial credit. Squared weights: Larger
disagreements (G1 vs G3) are penalized more heavily. Use 'Unweighted' for
nominal categories with no inherent order.}

\item{exct}{Use exact p-value calculation instead of normal approximation.
Recommended for small sample sizes (< 30 cases) with 3 or more raters.
Note: Not applicable for 2-rater analysis (use Cohen's kappa).}

\item{kripp}{Alternative reliability measure that handles missing data and
supports various data types. Useful when raters didn't rate all cases or
when comparing different measurement levels.}

\item{krippMethod}{Specifies the measurement level for Krippendorff's alpha
calculation.}

\item{bootstrap}{Calculate bootstrap confidence intervals for
Krippendorff's alpha.}

\item{gwet}{Alternative agreement coefficient that is more stable than
Cohen's kappa when dealing with high agreement rates or unbalanced marginal
distributions (e.g., rare tumor subtypes). Gwet's AC corrects for the
paradoxical behavior of kappa in extreme cases.}

\item{gwetWeights}{Unweighted (AC1) for nominal categories. Linear or
Quadratic weights (AC2) for ordinal data.}

\item{showLevelInfo}{Display information about how categorical levels are
currently ordered in your variables. Essential for weighted kappa analysis
to ensure ordinal levels are properly ordered (e.g., G1 → G2 → G3 for tumor
grades).}

\item{hierarchicalKappa}{Enable hierarchical (multilevel) kappa analysis
for nested data structures (e.g., pathologists nested within institutions,
readers nested within centers). Accounts for clustering effects and
provides institution/cluster-specific agreement estimates. Essential for
multi-center reliability studies.}

\item{clusterVariable}{Variable defining clusters/institutions/centers. For
example, hospital ID, institution name, or scanner ID. Raters are nested
within these clusters.}

\item{iccHierarchical}{Calculate intraclass correlation coefficients for
hierarchical data. ICC(1): between-cluster agreement, ICC(2): reliability
of cluster means, ICC(3): within-cluster agreement. Decomposes variance
into cluster-level and rater-level components.}

\item{clusterSpecificKappa}{Calculate kappa separately for each
cluster/institution to identify sites with poor agreement. Useful for
quality control in multi-center studies.}

\item{betweenClusterVariance}{Estimate variance component for
between-cluster differences in agreement. Large between-cluster variance
indicates institutional heterogeneity requiring investigation (different
protocols, training, etc.).}

\item{withinClusterVariance}{Estimate variance component for within-cluster
rater disagreement. Comparison of within vs between variance informs
whether issues are local (within institutions) or systematic (between
institutions).}

\item{shrinkageEstimates}{Calculate shrinkage estimates for
cluster-specific kappas. Shrinks extreme estimates toward overall mean,
providing more stable estimates for small clusters.}

\item{testClusterHomogeneity}{Test whether agreement is homogeneous across
clusters (null hypothesis: all clusters have equal kappa). Significant
result indicates heterogeneity requiring investigation.}

\item{clusterRankings}{Rank clusters/institutions by agreement performance
with confidence intervals. Identifies best and worst performing sites. Use
cautiously to avoid unfair comparisons when cluster sizes differ
substantially.}

\item{showSummary}{Display a natural-language interpretation of results
with color-coded agreement levels and clinical guidance. Recommended for
reports and presentations.}

\item{showAbout}{Display an explanatory panel describing what this analysis
does, when to use it, and how to interpret results.}

\item{consensusVar}{Calculate consensus (modal) score across raters and add
as new computed column. Essential for multi-rater studies requiring a
reference standard.}

\item{consensusRule}{Rule for defining consensus. Simple majority = modal
category with >50\\% of votes. Supermajority requires ≥75\\% agreement.
Unanimous requires 100\\% agreement.}

\item{tieBreaker}{How to handle ties (e.g., 50-50 split). Exclude = set
consensus to NA for tied cases. First = use first category that appears.
Lowest/Highest = use min/max of tied categories.}

\item{consensusName}{Name of the new computed variable to be added to the
dataset.}

\item{referenceRater}{Select a reference rater (e.g., consensus score, gold
standard, senior pathologist). Each selected rater will be compared
pairwise with this reference, producing individual kappas. Essential for
training assessment, rater certification, and performance monitoring.}

\item{rankRaters}{Rank raters from highest to lowest kappa (relative to
reference). Shows best and worst performing raters for quality control and
training needs.}

\item{loaVariable}{Calculate level of agreement (LoA) for each case and add
as new computed column. Categorizes cases as
Absolute/High/Moderate/Low/Poor agreement based on proportion of raters
agreeing. Useful for identifying difficult cases and quality control.}

\item{loaThresholds}{How to define LoA categories. Custom = user-defined
cutpoints. Quartiles/Tertiles = data-driven splits.}

\item{loaHighThreshold}{Minimum \\% agreement for "High" LoA (e.g., 75\\% =
≥12/16 raters for N=16).}

\item{loaLowThreshold}{Minimum \\% agreement for "Low" LoA (e.g., 56\\% =
≥9/16 raters for N=16).}
}
\value{
A results object containing:
\tabular{llllll}{
\code{results$welcome} \tab \tab \tab \tab \tab a html \cr
\code{results$headingIRR} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$irrtable} \tab \tab \tab \tab \tab a table \cr
\code{results$text2} \tab \tab \tab \tab \tab a html \cr
\code{results$text} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$headingKripp} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$krippTable} \tab \tab \tab \tab \tab a table \cr
\code{results$headingGwet} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$gwetTable} \tab \tab \tab \tab \tab a table \cr
\code{results$levelInfo} \tab \tab \tab \tab \tab a html \cr
\code{results$hierarchicalICCTable} \tab \tab \tab \tab \tab a table \cr
\code{results$varianceComponents} \tab \tab \tab \tab \tab a table \cr
\code{results$clusterSpecificKappaTable} \tab \tab \tab \tab \tab a table \cr
\code{results$clusterHomogeneityTest} \tab \tab \tab \tab \tab a table \cr
\code{results$clusterRankingsTable} \tab \tab \tab \tab \tab a table \cr
\code{results$consensusTable} \tab \tab \tab \tab \tab a table \cr
\code{results$consensusVar} \tab \tab \tab \tab \tab an output \cr
\code{results$headingPairwise} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$pairwiseKappaTable} \tab \tab \tab \tab \tab a table \cr
\code{results$headingLoA} \tab \tab \tab \tab \tab a preformatted \cr
\code{results$loaTable} \tab \tab \tab \tab \tab a table \cr
\code{results$loaVar} \tab \tab \tab \tab \tab an output \cr
\code{results$weightedKappaGuide} \tab \tab \tab \tab \tab a html \cr
\code{results$summary} \tab \tab \tab \tab \tab a html \cr
\code{results$reportText} \tab \tab \tab \tab \tab a html \cr
\code{results$about} \tab \tab \tab \tab \tab a html \cr
\code{results$blandAltmanPlot} \tab \tab \tab \tab \tab an image \cr
}

Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:

\code{results$irrtable$asDF}

\code{as.data.frame(results$irrtable)}
}
\description{
Function for Interrater Reliability.
}
\examples{
\donttest{
# example will be added
}
}
