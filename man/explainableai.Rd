% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/explainableai.h.R
\name{explainableai}
\alias{explainableai}
\title{Explainable AI Analysis}
\usage{
explainableai(
  data,
  analysis_type = "feature_importance",
  features,
  target_var,
  model_predictions,
  image_paths,
  attention_maps,
  shap_method = "kernel_explainer",
  lime_method = "tabular",
  n_samples = 100,
  n_features = 20,
  plot_type = "summary",
  overlay_original = TRUE,
  confidence_level = 0.95,
  background_samples = 100,
  perturbation_method = "random",
  clustering_method = "none",
  interaction_analysis = FALSE,
  local_explanations = TRUE,
  global_explanations = TRUE,
  save_explanations = FALSE,
  explanation_path = "",
  attention_threshold = 0.1,
  colormap = "viridis"
)
}
\arguments{
\item{data}{the data as a data frame}

\item{analysis_type}{type of explainability analysis to perform}

\item{features}{features/variables for importance analysis}

\item{target_var}{target variable or model predictions}

\item{model_predictions}{variable containing model predictions or
probabilities}

\item{image_paths}{variable containing paths to image files}

\item{attention_maps}{variable containing attention map data or file paths}

\item{shap_method}{SHAP explainer method based on model type}

\item{lime_method}{LIME explanation method for different data types}

\item{n_samples}{number of samples to use for explanation analysis}

\item{n_features}{number of top important features to display}

\item{plot_type}{type of visualization for explanations}

\item{overlay_original}{overlay attention maps on original images}

\item{confidence_level}{confidence level for statistical intervals}

\item{background_samples}{number of background samples for SHAP baseline}

\item{perturbation_method}{method for perturbing features in permutation
importance}

\item{clustering_method}{method for grouping similar features}

\item{interaction_analysis}{analyze pairwise feature interactions}

\item{local_explanations}{create individual sample explanations}

\item{global_explanations}{create overall model explanations}

\item{save_explanations}{save explanation data for external use}

\item{explanation_path}{file path to save explanation results}

\item{attention_threshold}{minimum attention value to display in
visualizations}

\item{colormap}{color palette for explanation visualizations}
}
\value{
A results object containing:
\tabular{llllll}{
\code{results$overview} \tab \tab \tab \tab \tab a html \cr
\code{results$featureimportance$importancetable} \tab \tab \tab \tab \tab Ranked list of feature importance scores \cr
\code{results$featureimportance$importanceplot} \tab \tab \tab \tab \tab Bar plot of feature importance scores \cr
\code{results$shapanalysis$shapvaluestable} \tab \tab \tab \tab \tab Mean absolute SHAP values per feature \cr
\code{results$shapanalysis$shapwaterfalltable} \tab \tab \tab \tab \tab Individual prediction explanations \cr
\code{results$shapanalysis$shapinteractiontable} \tab \tab \tab \tab \tab Feature interaction effects \cr
\code{results$shapanalysis$shapsummaryplot} \tab \tab \tab \tab \tab Overview of SHAP values for all features \cr
\code{results$shapanalysis$shapwaterfallplot} \tab \tab \tab \tab \tab Individual prediction explanation \cr
\code{results$shapanalysis$shapinteractionplot} \tab \tab \tab \tab \tab Feature interaction heatmap \cr
\code{results$limeanalysis$limeexplanationtable} \tab \tab \tab \tab \tab Local explanations for individual predictions \cr
\code{results$limeanalysis$limeplot} \tab \tab \tab \tab \tab Local explanation visualization \cr
\code{results$attentionanalysis$attentionstatstable} \tab \tab \tab \tab \tab Summary statistics of attention patterns \cr
\code{results$attentionanalysis$attentionpeakstable} \tab \tab \tab \tab \tab Highest attention regions across samples \cr
\code{results$attentionanalysis$attentionheatmapplot} \tab \tab \tab \tab \tab Attention map overlays on original images \cr
\code{results$attentionanalysis$attentiondistributionplot} \tab \tab \tab \tab \tab Distribution of attention values \cr
\code{results$partialdependence$pdptable} \tab \tab \tab \tab \tab Partial dependence effect sizes \cr
\code{results$partialdependence$pdpplot} \tab \tab \tab \tab \tab Effect of individual features on predictions \cr
\code{results$partialdependence$iceplot} \tab \tab \tab \tab \tab Individual conditional expectation curves \cr
\code{results$globalexplanations$modelinsightstable} \tab \tab \tab \tab \tab Overall model behavior insights \cr
\code{results$globalexplanations$featureclusteringtable} \tab \tab \tab \tab \tab Groups of similar features \cr
\code{results$globalexplanations$globalinsightplot} \tab \tab \tab \tab \tab Overall model behavior visualization \cr
\code{results$globalexplanations$featureclusterplot} \tab \tab \tab \tab \tab Dendrogram or cluster plot of features \cr
\code{results$localexplanations$samplewiseexplanationtable} \tab \tab \tab \tab \tab Detailed explanations for individual samples \cr
\code{results$localexplanations$localexplanationplot} \tab \tab \tab \tab \tab Individual sample explanation examples \cr
\code{results$validationmetrics$validationtable} \tab \tab \tab \tab \tab Quality metrics for explanations \cr
\code{results$validationmetrics$stabilityanalysistable} \tab \tab \tab \tab \tab Consistency of explanations across perturbations \cr
\code{results$validationmetrics$validationplot} \tab \tab \tab \tab \tab Validation metrics visualization \cr
}
}
\description{
Comprehensive explainable AI toolkit for interpreting machine learning
models
in medical and clinical applications. Provides attention maps, feature
importance,
SHAP values, and other interpretability methods.
}
\examples{
# SHAP analysis for feature importance
explainableai(
    data = model_data,
    model_predictions = "predictions_var",
    features = c("feature1", "feature2", "feature3"),
    method = "shap",
    plot_type = "summary"
)

# Attention map analysis for image models
explainableai(
    data = image_data,
    image_paths = "image_path_var",
    attention_maps = "attention_var",
    method = "attention_analysis",
    overlay_original = TRUE
)

}
