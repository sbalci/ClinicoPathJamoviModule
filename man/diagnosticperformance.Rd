% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/diagnosticperformance.h.R
\name{diagnosticperformance}
\alias{diagnosticperformance}
\title{Diagnostic Performance Analysis}
\usage{
diagnosticperformance(
  data,
  outcome,
  outcomeLevel,
  predictors,
  validationMethod = "none",
  cvFolds = 10,
  cvRepeats = 1,
  bootstrapN = 1000,
  holdoutProp = 0.3,
  comparisonMethod = "delong",
  optimalCutoff = "youden",
  costRatio = 1,
  confidenceLevel = 0.95,
  plotROC = TRUE,
  plotPR = FALSE,
  plotCalibration = FALSE,
  showMetrics = TRUE,
  showCutoffAnalysis = TRUE,
  stratifyBy
)
}
\arguments{
\item{data}{.}

\item{outcome}{Binary outcome variable indicating disease/condition status
(0/1 or factor).}

\item{outcomeLevel}{Level of outcome variable representing positive cases
(events).}

\item{predictors}{Continuous predictor variables (biomarkers, risk scores)
to evaluate.}

\item{validationMethod}{Method for validating diagnostic performance to
avoid overfitting.}

\item{cvFolds}{Number of folds for cross-validation (typically 5 or 10).}

\item{cvRepeats}{Number of times to repeat cross-validation for more stable
results.}

\item{bootstrapN}{Number of bootstrap samples for bootstrap validation and
confidence intervals.}

\item{holdoutProp}{Proportion of data to hold out for validation (0.1 to
0.5).}

\item{comparisonMethod}{Statistical method for comparing multiple ROC
curves.}

\item{optimalCutoff}{Method for determining optimal diagnostic cutoff
point.}

\item{costRatio}{Ratio of cost of false positive to false negative (for
cost-weighted cutoff).}

\item{confidenceLevel}{Confidence level for confidence intervals (0.80 to
0.99).}

\item{plotROC}{.}

\item{plotPR}{Generate precision-recall curves (useful for imbalanced
datasets).}

\item{plotCalibration}{Generate calibration plots to assess prediction
calibration.}

\item{showMetrics}{Show comprehensive diagnostic performance metrics table.}

\item{showCutoffAnalysis}{Show optimal cutoff analysis with
sensitivity/specificity trade-offs.}

\item{stratifyBy}{Optional variable for stratified analysis (subgroup
analysis).}
}
\value{
A results object containing:
\tabular{llllll}{
\code{results$text} \tab \tab \tab \tab \tab a html \cr
\code{results$performanceTable} \tab \tab \tab \tab \tab a table \cr
\code{results$cutoffTable} \tab \tab \tab \tab \tab a table \cr
\code{results$comparisonTable} \tab \tab \tab \tab \tab a table \cr
\code{results$validationTable} \tab \tab \tab \tab \tab a table \cr
\code{results$rocPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$prPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$calibrationPlot} \tab \tab \tab \tab \tab an image \cr
\code{results$stratifiedResults} \tab \tab \tab \tab \tab a table \cr
\code{results$interpretation} \tab \tab \tab \tab \tab a html \cr
\code{results$recommendations} \tab \tab \tab \tab \tab a html \cr
}

Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:

\code{results$performanceTable$asDF}

\code{as.data.frame(results$performanceTable)}
}
\description{
Comprehensive diagnostic performance analysis including ROC curves,
cross-validation,
and statistical comparison of diagnostic tests. Supports both single and
multiple
biomarker evaluation with robust validation methods.
}
\examples{
# Single biomarker diagnostic performance
diagnosticperformance(
    data = dataset,
    outcome = "disease_status",
    predictors = "biomarker_level",
    validation_method = "crossval",
    cv_folds = 10
)

# Multiple biomarker comparison
diagnosticperformance(
    data = dataset,
    outcome = "disease_status",
    predictors = c("marker1", "marker2", "marker3"),
    comparison_method = "delong",
    validation_method = "bootstrap",
    bootstrap_n = 1000
)

}
