
# This file is automatically generated, you probably don't want to edit this

praucClass <- R6::R6Class(
    "praucClass",
    inherit = praucBase,
    private = list(
        .init = function() {
            private$.initInstructions()
        },

        .run = function() {
            # Check for required inputs
            if (is.null(self$options$outcome) || is.null(self$options$predictor)) {
                return()
            }

            # Get data
            data <- self$data
            outcome <- data[[self$options$outcome]]
            predictor <- as.numeric(data[[self$options$predictor]])

            # Remove missing values
            complete_cases <- complete.cases(outcome, predictor)
            outcome <- outcome[complete_cases]
            predictor <- predictor[complete_cases]

            if (length(outcome) < 10) {
                stop("Insufficient data for PR analysis. Need at least 10 complete observations.")
            }

            # Validate binary outcome
            outcome_levels <- levels(outcome)
            if (length(outcome_levels) != 2) {
                stop("Outcome variable must have exactly 2 levels for binary classification.")
            }

            # Determine positive level
            if (self$options$positive_level == "") {
                positive_level <- outcome_levels[2]  # Default to second level
            } else {
                positive_level <- self$options$positive_level
                if (!(positive_level %in% outcome_levels)) {
                    stop(paste0("Positive level '", positive_level, "' not found in outcome variable."))
                }
            }

            # Convert to binary (1 = positive, 0 = negative)
            y_true <- as.integer(outcome == positive_level)

            # Set random seed
            set.seed(self$options$random_seed)

            # Perform PR analysis
            tryCatch({
                # Calculate PR curve
                pr_results <- private$.calculatePRCurve(y_true, predictor)

                # Populate summary table
                private$.populateSummary(y_true, predictor, pr_results)

                # Find optimal thresholds
                if (self$options$calculate_fscore) {
                    private$.findOptimalThresholds(y_true, predictor, pr_results)
                }

                # Populate performance at key thresholds
                private$.populateKeyThresholds(y_true, predictor, pr_results)

                # Populate interpretation
                private$.populateInterpretation(y_true, predictor, pr_results)

            }, error = function(e) {
                stop(paste0("Error in PR analysis: ", e$message))
            })
        },

        .initInstructions = function() {
            instructions <- self$results$instructions
            html <- "<h3>Precision-Recall Analysis for Imbalanced Datasets</h3>
            <p><b>Purpose:</b> Precision-Recall (PR) analysis is superior to ROC analysis for highly imbalanced datasets.</p>
            <ul>
              <li><b>Precision (PPV):</b> TP / (TP + FP) - What proportion of positive predictions are correct?</li>
              <li><b>Recall (Sensitivity):</b> TP / (TP + FN) - What proportion of actual positives are detected?</li>
              <li><b>PR-AUC:</b> Area under precision-recall curve (Average Precision)</li>
              <li><b>F1-Score:</b> Harmonic mean of precision and recall (2 × Precision × Recall / (Precision + Recall))</li>
              <li><b>Baseline PR-AUC:</b> Equals the prevalence (random classifier performance)</li>
            </ul>
            <p><b>Clinical Applications:</b></p>
            <ul>
              <li>Rare event detection: mitotic figures, micrometastases</li>
              <li>Cancer screening with low prevalence</li>
              <li>Digital pathology quality control</li>
              <li>AI triage where abnormal cases are infrequent</li>
            </ul>
            <p><b>Interpretation:</b> PR-AUC ranges from 0 to 1. Higher values indicate better performance.
            The baseline (random classifier) PR-AUC equals the prevalence. Improvement over baseline indicates
            useful predictive ability.</p>"
            instructions$setContent(html)
        },

        .calculatePRCurve = function(y_true, predictor) {
            # Get unique thresholds
            thresholds <- sort(unique(predictor), decreasing = TRUE)

            # Add extreme thresholds
            thresholds <- c(max(predictor) + 0.001, thresholds, min(predictor) - 0.001)

            # Calculate precision and recall at each threshold
            n_thresholds <- length(thresholds)
            precision <- numeric(n_thresholds)
            recall <- numeric(n_thresholds)
            f1_score <- numeric(n_thresholds)

            n_positive <- sum(y_true == 1)
            n_negative <- sum(y_true == 0)

            for (i in seq_along(thresholds)) {
                thresh <- thresholds[i]

                # Predictions at this threshold
                y_pred <- as.integer(predictor >= thresh)

                # Confusion matrix
                tp <- sum(y_pred == 1 & y_true == 1)
                fp <- sum(y_pred == 1 & y_true == 0)
                fn <- sum(y_pred == 0 & y_true == 1)
                tn <- sum(y_pred == 0 & y_true == 0)

                # Precision and recall
                if (tp + fp > 0) {
                    precision[i] <- tp / (tp + fp)
                } else {
                    precision[i] <- 1  # No positive predictions
                }

                if (tp + fn > 0) {
                    recall[i] <- tp / (tp + fn)
                } else {
                    recall[i] <- 0
                }

                # F1-score
                if (precision[i] + recall[i] > 0) {
                    f1_score[i] <- 2 * precision[i] * recall[i] / (precision[i] + recall[i])
                } else {
                    f1_score[i] <- 0
                }
            }

            # Calculate PR-AUC using trapezoidal integration
            # Sort by recall (ascending)
            order_idx <- order(recall)
            recall_sorted <- recall[order_idx]
            precision_sorted <- precision[order_idx]

            # Trapezoidal integration
            prauc <- 0
            for (i in 2:length(recall_sorted)) {
                width <- recall_sorted[i] - recall_sorted[i-1]
                height <- (precision_sorted[i] + precision_sorted[i-1]) / 2
                prauc <- prauc + (width * height)
            }

            # Calculate ROC-AUC if requested
            rocauc <- NULL
            if (self$options$compare_to_roc) {
                rocauc <- private$.calculateROCAUC(y_true, predictor)
            }

            list(
                thresholds = thresholds,
                precision = precision,
                recall = recall,
                f1_score = f1_score,
                prauc = prauc,
                rocauc = rocauc,
                n_positive = n_positive,
                n_negative = n_negative
            )
        },

        .calculateROCAUC = function(y_true, predictor) {
            # Simple ROC-AUC calculation using Mann-Whitney U
            n_pos <- sum(y_true == 1)
            n_neg <- sum(y_true == 0)

            pred_pos <- predictor[y_true == 1]
            pred_neg <- predictor[y_true == 0]

            # Count concordant pairs
            u_stat <- 0
            for (i in seq_along(pred_pos)) {
                u_stat <- u_stat + sum(pred_pos[i] > pred_neg)
                u_stat <- u_stat + 0.5 * sum(pred_pos[i] == pred_neg)
            }

            rocauc <- u_stat / (n_pos * n_neg)
            return(rocauc)
        },

        .populateSummary = function(y_true, predictor, pr_results) {
            table <- self$results$prSummary

            n <- length(y_true)
            n_positive <- pr_results$n_positive
            n_negative <- pr_results$n_negative
            prevalence <- n_positive / n
            prauc <- pr_results$prauc

            # Bootstrap CI if requested
            ci_lower <- NA
            ci_upper <- NA
            prauc_se <- NA

            if (self$options$confidence_intervals) {
                boot_results <- private$.bootstrapPRAUC(y_true, predictor)
                prauc_se <- sd(boot_results)

                if (self$options$ci_method == "bootstrap") {
                    ci_level <- self$options$confidence_level
                    ci_lower <- quantile(boot_results, (1 - ci_level) / 2)
                    ci_upper <- quantile(boot_results, (1 + ci_level) / 2)
                } else {  # BCa
                    # Simplified BCa (full implementation would need acceleration constant)
                    ci_level <- self$options$confidence_level
                    ci_lower <- quantile(boot_results, (1 - ci_level) / 2)
                    ci_upper <- quantile(boot_results, (1 + ci_level) / 2)
                }
            }

            # Baseline and improvement
            baseline_prauc <- prevalence  # Random classifier
            improvement <- prauc - baseline_prauc

            values <- list(
                n = n,
                n_positive = n_positive,
                n_negative = n_negative,
                prevalence = prevalence,
                prauc = prauc,
                prauc_se = prauc_se,
                ci_lower = ci_lower,
                ci_upper = ci_upper,
                baseline_prauc = baseline_prauc,
                improvement = improvement,
                rocauc = pr_results$rocauc
            )

            table$setRow(rowNo = 1, values = values)
        },

        .bootstrapPRAUC = function(y_true, predictor) {
            n_boot <- self$options$bootstrap_samples
            n <- length(y_true)
            boot_prauc <- numeric(n_boot)

            for (b in 1:n_boot) {
                boot_idx <- sample(n, replace = TRUE)
                y_boot <- y_true[boot_idx]
                pred_boot <- predictor[boot_idx]

                # Calculate PR-AUC for bootstrap sample
                pr_boot <- private$.calculatePRCurve(y_boot, pred_boot)
                boot_prauc[b] <- pr_boot$prauc
            }

            return(boot_prauc)
        },

        .findOptimalThresholds = function(y_true, predictor, pr_results) {
            table <- self$results$optimalThresholds

            # Parse beta weights
            beta_str <- self$options$beta_weights
            beta_values <- as.numeric(unlist(strsplit(beta_str, ",")))
            beta_values <- beta_values[!is.na(beta_values)]

            if (length(beta_values) == 0) {
                beta_values <- c(1)  # Default F1
            }

            # For each beta value, find optimal threshold
            for (beta in beta_values) {
                best_idx <- private$.findOptimalFScore(y_true, predictor, beta)

                thresh <- pr_results$thresholds[best_idx]
                prec <- pr_results$precision[best_idx]
                rec <- pr_results$recall[best_idx]

                # Calculate F-score
                if (prec + rec > 0) {
                    fscore <- (1 + beta^2) * prec * rec / ((beta^2 * prec) + rec)
                } else {
                    fscore <- 0
                }

                # Confusion matrix at this threshold
                y_pred <- as.integer(predictor >= thresh)
                tp <- sum(y_pred == 1 & y_true == 1)
                fp <- sum(y_pred == 1 & y_true == 0)
                fn <- sum(y_pred == 0 & y_true == 1)
                tn <- sum(y_pred == 0 & y_true == 0)

                metric_name <- if (beta == 1) {
                    "F1-Score (Balanced)"
                } else if (beta == 2) {
                    "F2-Score (Emphasize Recall)"
                } else if (beta == 0.5) {
                    "F0.5-Score (Emphasize Precision)"
                } else {
                    paste0("F", beta, "-Score")
                }

                table$addRow(rowKey = metric_name, values = list(
                    metric = metric_name,
                    optimal_threshold = thresh,
                    precision = prec,
                    recall = rec,
                    fscore = fscore,
                    tp = tp,
                    fp = fp,
                    fn = fn,
                    tn = tn
                ))
            }
        },

        .findOptimalFScore = function(y_true, predictor, beta) {
            # Find threshold that maximizes F-beta score
            thresholds <- sort(unique(predictor), decreasing = TRUE)
            best_fscore <- -Inf
            best_idx <- 1

            for (i in seq_along(thresholds)) {
                y_pred <- as.integer(predictor >= thresholds[i])

                tp <- sum(y_pred == 1 & y_true == 1)
                fp <- sum(y_pred == 1 & y_true == 0)
                fn <- sum(y_pred == 0 & y_true == 1)

                if (tp + fp > 0 && tp + fn > 0) {
                    prec <- tp / (tp + fp)
                    rec <- tp / (tp + fn)

                    if (prec + rec > 0) {
                        fscore <- (1 + beta^2) * prec * rec / ((beta^2 * prec) + rec)

                        if (fscore > best_fscore) {
                            best_fscore <- fscore
                            best_idx <- i
                        }
                    }
                }
            }

            return(best_idx)
        },

        .populateKeyThresholds = function(y_true, predictor, pr_results) {
            table <- self$results$performanceAtKey

            # Key thresholds to evaluate
            key_thresholds <- c(0.1, 0.25, 0.5, 0.75, 0.9)

            for (thresh in key_thresholds) {
                y_pred <- as.integer(predictor >= thresh)

                tp <- sum(y_pred == 1 & y_true == 1)
                fp <- sum(y_pred == 1 & y_true == 0)
                fn <- sum(y_pred == 0 & y_true == 1)
                tn <- sum(y_pred == 0 & y_true == 0)

                if (tp + fp > 0) {
                    prec <- tp / (tp + fp)
                } else {
                    prec <- 0
                }

                if (tp + fn > 0) {
                    rec <- tp / (tp + fn)
                } else {
                    rec <- 0
                }

                if (prec + rec > 0) {
                    f1 <- 2 * prec * rec / (prec + rec)
                } else {
                    f1 <- 0
                }

                ppv_interpretation <- if (prec >= 0.9) {
                    "Excellent precision"
                } else if (prec >= 0.8) {
                    "Good precision"
                } else if (prec >= 0.7) {
                    "Moderate precision"
                } else {
                    "Low precision"
                }

                table$addRow(rowKey = as.character(thresh), values = list(
                    threshold = thresh,
                    precision = prec,
                    recall = rec,
                    f1_score = f1,
                    positive_predictions = tp + fp,
                    negative_predictions = tn + fn,
                    ppv_interpretation = ppv_interpretation
                ))
            }
        },

        .populateInterpretation = function(y_true, predictor, pr_results) {
            interpretation <- self$results$interpretation

            prauc <- pr_results$prauc
            prevalence <- pr_results$n_positive / length(y_true)
            improvement <- prauc - prevalence

            html <- "<h3>Interpretation of Precision-Recall Analysis</h3>"

            # PR-AUC interpretation
            html <- paste0(html, "<h4>PR-AUC Performance:</h4>")
            html <- paste0(html, "<p><b>PR-AUC:</b> ", round(prauc, 3), "</p>")
            html <- paste0(html, "<p><b>Baseline (Random):</b> ", round(prevalence, 3), " (prevalence)</p>")
            html <- paste0(html, "<p><b>Improvement:</b> ", round(improvement, 3), "</p>")

            if (improvement < 0.05) {
                perf_text <- "Minimal improvement over random classifier. Model provides little predictive value."
            } else if (improvement < 0.15) {
                perf_text <- "Modest improvement over baseline. Model shows some predictive ability."
            } else if (improvement < 0.30) {
                perf_text <- "Good improvement over baseline. Model has useful predictive ability."
            } else {
                perf_text <- "Excellent improvement over baseline. Model has strong predictive ability."
            }

            html <- paste0(html, "<p>", perf_text, "</p>")

            # ROC comparison if available
            if (!is.null(pr_results$rocauc)) {
                html <- paste0(html, "<h4>ROC vs PR Comparison:</h4>")
                html <- paste0(html, "<p><b>ROC-AUC:</b> ", round(pr_results$rocauc, 3), "</p>")

                if (pr_results$rocauc - prauc > 0.2) {
                    html <- paste0(html, "<p><b>Important:</b> ROC-AUC is much higher than PR-AUC, which is expected for imbalanced datasets. ",
                                  "PR-AUC provides a more realistic assessment of model performance when positives are rare.</p>")
                }
            }

            # Clinical application
            html <- paste0(html, "<h4>Clinical Application:</h4>")
            html <- paste0(html, "<p>For imbalanced datasets (prevalence = ", round(prevalence * 100, 1), "%): </p>")
            html <- paste0(html, "<ul>")
            html <- paste0(html, "<li>PR analysis focuses on performance in detecting the minority (positive) class</li>")
            html <- paste0(html, "<li>High precision = Few false positives (important for costly confirmatory tests)</li>")
            html <- paste0(html, "<li>High recall = Few missed positives (important for serious diseases)</li>")
            html <- paste0(html, "<li>F1-score balances precision and recall for overall performance</li>")
            html <- paste0(html, "</ul>")

            interpretation$setContent(html)
        },

        .plotPRCurve = function(image, ...) {
            if (is.null(self$options$outcome) || is.null(self$options$predictor)) {
                return(FALSE)
            }

            # Get data
            data <- self$data
            outcome <- data[[self$options$outcome]]
            predictor <- as.numeric(data[[self$options$predictor]])

            complete_cases <- complete.cases(outcome, predictor)
            outcome <- outcome[complete_cases]
            predictor <- predictor[complete_cases]

            # Determine positive level
            outcome_levels <- levels(outcome)
            if (self$options$positive_level == "") {
                positive_level <- outcome_levels[2]
            } else {
                positive_level <- self$options$positive_level
            }

            y_true <- as.integer(outcome == positive_level)

            # Calculate PR curve
            pr_results <- private$.calculatePRCurve(y_true, predictor)

            # Plot
            plot(pr_results$recall, pr_results$precision,
                 type = "l", lwd = 2, col = "darkblue",
                 xlim = c(0, 1), ylim = c(0, 1),
                 xlab = "Recall (Sensitivity)",
                 ylab = "Precision (PPV)",
                 main = paste0("Precision-Recall Curve (AUC = ", round(pr_results$prauc, 3), ")"))

            # Add baseline
            prevalence <- pr_results$n_positive / length(y_true)
            abline(h = prevalence, lty = 2, col = "red", lwd = 1.5)

            # Add optimal F1 point if calculated
            if (self$options$calculate_fscore) {
                best_idx <- which.max(pr_results$f1_score)
                points(pr_results$recall[best_idx], pr_results$precision[best_idx],
                       pch = 19, col = "darkgreen", cex = 1.5)
                text(pr_results$recall[best_idx], pr_results$precision[best_idx],
                     labels = "  Optimal F1", pos = 4, col = "darkgreen")
            }

            # Add legend
            legend("topright",
                   legend = c("PR Curve", paste0("Baseline (", round(prevalence, 3), ")"), "Optimal F1"),
                   col = c("darkblue", "red", "darkgreen"),
                   lty = c(1, 2, NA),
                   pch = c(NA, NA, 19),
                   lwd = c(2, 1.5, NA))

            grid(col = "gray80")

            TRUE
        },

        .plotComparison = function(image, ...) {
            # Implementation for ROC vs PR comparison plot
            TRUE
        },

        .plotFScore = function(image, ...) {
            # Implementation for F-score vs threshold plot
            TRUE
        }
    )
)
