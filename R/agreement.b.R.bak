#' @title Interrater Reliability Analysis
#' @return Table
#' @importFrom R6 R6Class
#' @import jmvcore
#' @import magrittr
#' @importFrom irr kappa2 kappam.fleiss agree
#' @importFrom dplyr select group_by count
#' @importFrom htmlTable htmlTable
#' @importFrom glue glue
#'
#' @description This function calculates interrater reliability for ordinal or categorical data.
#'
#' @details The function calculates Cohen's kappa for two raters and Fleiss' kappa for three or more raters.
#'
#'


# See
# \url{http://www.cookbook-r.com/Statistical_analysis/Inter-rater_reliability/#ordinal-data-weighted-kappa}


agreementClass <- if (requireNamespace("jmvcore")) R6::R6Class("agreementClass",
    inherit = agreementBase, private = list(

        .init = function() {
            # Initialize plot render functions
            if (self$options$blandAltmanPlot) {
                self$results$blandAltman$setRenderFun(private$.blandAltman)
            }
        },

        # Helper function to escape variable names for R code generation
        # Variable names with spaces or special characters are wrapped in backticks
        .escapeVariableName = function(varName) {
            if (is.null(varName) || length(varName) == 0) {
                return(NULL)
            }

            # Check if variable name needs escaping
            # If make.names() would change it, it needs backticks
            if (!identical(make.names(varName), varName)) {
                return(paste0('`', varName, '`'))
            }

            return(varName)
        },

        .createSummary = function(result1, result2, wght, exct) {
            # Create plain-language summary of agreement results

            # Extract values with safety checks
            n_subjects <- result1[["subjects"]]
            n_raters <- result1[["raters"]]
            perc_agree <- round(result1[["value"]], 1)
            kappa_val <- round(result2[["value"]], 3)
            p_val <- result2[["p.value"]]
            method <- result2[["method"]]

            # Safety check for p_val
            if (is.null(p_val) || length(p_val) == 0 || is.na(p_val)) {
                p_val <- NA
            }

            # Interpret kappa (Landis & Koch, 1977)
            if (is.na(kappa_val) || kappa_val < 0) {
                interp <- "poor agreement (worse than chance)"
            } else if (kappa_val < 0.20) {
                interp <- "slight agreement"
            } else if (kappa_val < 0.40) {
                interp <- "fair agreement"
            } else if (kappa_val < 0.60) {
                interp <- "moderate agreement"
            } else if (kappa_val < 0.80) {
                interp <- "substantial agreement"
            } else {
                interp <- "almost perfect agreement"
            }

            # Statistical significance
            if (is.na(p_val)) {
                sig_text <- "p-value not available"
            } else if (p_val < 0.001) {
                sig_text <- "p < .001, highly statistically significant"
            } else if (p_val < 0.01) {
                sig_text <- sprintf("p = %.3f, statistically significant", p_val)
            } else if (p_val < 0.05) {
                sig_text <- sprintf("p = %.3f, statistically significant", p_val)
            } else {
                sig_text <- sprintf("p = %.3f, not statistically significant", p_val)
            }

            # Weight description
            if (wght == "equal") {
                weight_desc <- " with linear weights"
            } else if (wght == "squared") {
                weight_desc <- " with squared weights"
            } else {
                weight_desc <- ""
            }

            # Exact kappa note
            exact_note <- if (exct) " using exact calculation" else ""

            # Build summary with consistent styling
            html_output <- paste0("
            <div style='font-family: Arial, sans-serif; max-width: 800px; line-height: 1.4;'>
                <div style='background: #f5f5f5; border: 2px solid #333; padding: 15px; margin-bottom: 15px;'>
                <h3 style='margin: 0 0 5px 0; font-size: 16px; color: #333;'>Agreement Analysis Summary</h3>
                <p style='margin: 0; font-size: 14px; color: #666;'>", n_subjects, " cases rated by ", n_raters, " raters</p>
                </div>

                <div style='font-size: 14px; color: #333;'>
                    <table style='width: 100%; border-collapse: collapse; margin-bottom: 15px;'>
                    <tr>
                        <td style='border: 1px solid #ccc; padding: 10px; background: #f9f9f9;'>
                        <strong>Raw Agreement</strong><br>
                        <span style='font-size: 18px;'>", perc_agree, "%</span>
                        </td>
                        <td style='border: 1px solid #ccc; padding: 10px; background: #f9f9f9;'>
                        <strong>Kappa (Œ∫)</strong><br>
                        <span style='font-size: 18px;'>", kappa_val, "</span>
                        </td>
                    </tr>
                    </table>

                    <p style='margin: 10px 0;'><strong>Method:</strong> ", method, exact_note, weight_desc, "</p>
                    <p style='margin: 10px 0;'><strong>Statistical test:</strong> ", sig_text, "</p>
                    <p style='margin: 10px 0;'><strong>Interpretation:</strong> ", interp, "</p>

                    <div style='background: #f9f9f9; border: 1px solid #ccc; padding: 12px; margin: 15px 0;'>
                        <p style='margin: 0 0 8px 0; font-weight: bold;'>Clinical Meaning</p>
                        <p style='margin: 0; font-size: 14px;'>",
                        if (kappa_val >= 0.60) {
                            "The raters show good consistency, suggesting reliable measurements for clinical decisions or research."
                        } else if (kappa_val >= 0.40) {
                            "The raters show moderate consistency. Consider additional training or clearer protocols."
                        } else {
                            "The raters show low consistency. Measurement reliability is questionable - review criteria and provide training."
                        }, "</p>
                    </div>

                    <div style='background: #f9f9f9; border: 1px solid #ccc; padding: 12px; margin: 15px 0;'>
                        <p style='margin: 0 0 8px 0; font-weight: bold;'>Key Assumptions</p>
                        <ul style='margin: 0; font-size: 13px; padding-left: 20px;'>
                            <li>All raters scored the same cases independently</li>
                            <li>Categories are mutually exclusive and consistent</li>",
                            if (wght != "unweighted") "
                            <li>Ordinal scale with meaningful distances between categories</li>" else "", "
                            <li>Kappa sensitive to category prevalence</li>
                        </ul>
                    </div>

                    <p style='font-size: 12px; color: #666; margin-top: 15px;'>
                    Reference: Landis & Koch (1977). Biometrics, 33, 159-174.</p>
                </div>
            </div>
            ")

            return(html_output)
        },

        .createAboutPanel = function() {
            # Create explanatory panel about the analysis

            html_output <- paste0("
            <div style='font-family: Arial, sans-serif; max-width: 800px; line-height: 1.4;'>
                <div style='background: #f5f5f5; border: 2px solid #333; padding: 15px; margin-bottom: 15px;'>
                <h3 style='margin: 0 0 5px 0; font-size: 16px; color: #333;'>About This Analysis</h3>
                <p style='margin: 0; font-size: 14px; color: #666;'>Understanding interrater reliability methods and applications</p>
                </div>

                <div style='font-size: 14px; color: #333;'>
                    <p style='margin: 10px 0;'><strong>What this analysis does:</strong></p>
                    <ul style='margin: 5px 0 15px 20px;'>
                        <li>Measures consistency between multiple raters</li>
                        <li>Accounts for chance agreement</li>
                        <li>Provides statistical measures with significance tests</li>
                    </ul>

                    <p style='margin: 10px 0;'><strong>When to use:</strong></p>
                    <ul style='margin: 5px 0 15px 20px;'>
                        <li>Quality assurance (inter-pathologist agreement)</li>
                        <li>Method validation (rating scales, classifications)</li>
                        <li>Training assessment (trainee vs expert concordance)</li>
                        <li>Research (multi-rater study reliability)</li>
                    </ul>

                    <p style='margin: 10px 0;'><strong>Data requirements:</strong></p>
                    <ul style='margin: 5px 0 15px 20px;'>
                        <li>At least 2 rater variables (columns)</li>
                        <li>Same cases rated by all raters</li>
                        <li>Categorical or ordinal data</li>
                        <li>Matching category levels across raters</li>
                    </ul>

                    <table style='width: 100%; border-collapse: collapse; margin: 15px 0;'>
                    <tr>
                        <td style='border: 1px solid #ccc; padding: 10px; vertical-align: top;'>
                        <strong>Cohen's kappa</strong><br>
                        <span style='font-size: 13px; color: #666;'>For 2 raters. Supports weighted analysis for ordinal data.</span>
                        </td>
                    </tr>
                    <tr>
                        <td style='border: 1px solid #ccc; padding: 10px; vertical-align: top;'>
                        <strong>Fleiss' kappa</strong><br>
                        <span style='font-size: 13px; color: #666;'>For 3+ raters. Fixed marginal probabilities.</span>
                        </td>
                    </tr>
                    <tr>
                        <td style='border: 1px solid #ccc; padding: 10px; vertical-align: top;'>
                        <strong>Krippendorff's alpha</strong><br>
                        <span style='font-size: 13px; color: #666;'>Alternative measure. Handles missing data and various data types.</span>
                        </td>
                    </tr>
                    </table>

                    <div style='background: #f9f9f9; border: 1px solid #ccc; padding: 10px; margin-top: 15px;'>
                        <p style='margin: 0; font-size: 13px;'><strong>Tip:</strong> For ordinal data (e.g., tumor grades), use weighted kappa to account for degree of disagreement.</p>
                    </div>
                </div>
            </div>
            ")

            return(html_output)
        },

        .createWeightedKappaGuide = function(weight_type) {
            # Create HTML guide for weighted kappa interpretation

            if (weight_type == "equal") {
                weight_name <- "Linear (Equal) Weights"
                weight_desc <- "Disagreements are weighted proportionally to their distance on the ordinal scale."
                formula <- "Weight = 1 - |i - j| / (k - 1)"
                formula_desc <- "where i and j are category positions, k is number of categories"

                example_table <- "
                <table style='margin: 15px auto; border-collapse: collapse; font-size: 12px;'>
                <tr style='background: #f0f0f0;'>
                    <th style='padding: 6px 12px; border: 1px solid #ddd;'>Distance</th>
                    <th style='padding: 6px 12px; border: 1px solid #ddd;'>Example</th>
                    <th style='padding: 6px 12px; border: 1px solid #ddd;'>Weight</th>
                    <th style='padding: 6px 12px; border: 1px solid #ddd;'>Penalty</th>
                </tr>
                <tr>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>0 (agreement)</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>1 vs 1</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>1.00</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>None</td>
                </tr>
                <tr>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>1 category</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>1 vs 2</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>0.75</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>Small</td>
                </tr>
                <tr>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>2 categories</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>1 vs 3</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>0.50</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>Moderate</td>
                </tr>
                <tr>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>Maximum</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>1 vs 5</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>0.00</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>Full</td>
                </tr>
                </table>"

                use_case <- "Use linear weights when each step on your ordinal scale represents equal incremental difference (e.g., Likert scales with equal intervals)."

                interpretation <- "
                <ul style='font-size: 13px; line-height: 1.8; margin: 10px 0;'>
                <li>Adjacent disagreements (e.g., 'Agree' vs 'Strongly Agree') receive <b>partial credit</b></li>
                <li>Distant disagreements (e.g., 'Strongly Disagree' vs 'Strongly Agree') receive <b>no credit</b></li>
                <li>Penalty increases <b>linearly</b> with distance</li>
                <li>Weighted kappa will be <b>higher</b> than unweighted kappa when most disagreements are adjacent</li>
                </ul>"

            } else if (weight_type == "squared") {
                weight_name <- "Squared (Quadratic) Weights"
                weight_desc <- "Disagreements are weighted by the squared distance, penalizing larger disagreements more heavily."
                formula <- "Weight = 1 - [(i - j) / (k - 1)]¬≤"
                formula_desc <- "where i and j are category positions, k is number of categories"

                example_table <- "
                <table style='margin: 15px auto; border-collapse: collapse; font-size: 12px;'>
                <tr style='background: #f0f0f0;'>
                    <th style='padding: 6px 12px; border: 1px solid #ddd;'>Distance</th>
                    <th style='padding: 6px 12px; border: 1px solid #ddd;'>Example</th>
                    <th style='padding: 6px 12px; border: 1px solid #ddd;'>Weight</th>
                    <th style='padding: 6px 12px; border: 1px solid #ddd;'>Penalty</th>
                </tr>
                <tr>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>0 (agreement)</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>1 vs 1</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>1.00</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>None</td>
                </tr>
                <tr>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>1 category</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>1 vs 2</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>0.94</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>Very small</td>
                </tr>
                <tr>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>2 categories</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>1 vs 3</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>0.75</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>Moderate</td>
                </tr>
                <tr>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>Maximum</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd;'>1 vs 5</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>0.00</td>
                    <td style='padding: 6px 12px; border: 1px solid #ddd; text-align: center;'>Full</td>
                </tr>
                </table>"

                use_case <- "Use squared weights when larger disagreements are disproportionately more serious than small ones (e.g., clinical severity scales, diagnostic accuracy)."

                interpretation <- "
                <ul style='font-size: 13px; line-height: 1.8; margin: 10px 0;'>
                <li>Adjacent disagreements receive <b>much more credit</b> than linear (0.94 vs 0.75)</li>
                <li>Distant disagreements are <b>heavily penalized</b></li>
                <li>Penalty increases <b>exponentially</b> with distance</li>
                <li>Weighted kappa will be <b>substantially higher</b> than unweighted when disagreements cluster near the diagonal</li>
                <li><b>Special property:</b> Squared weights equal the intraclass correlation coefficient under certain conditions</li>
                </ul>"
            }

            html_output <- paste0("
            <div style='font-family: Arial, sans-serif; max-width: 800px; line-height: 1.4;'>
                <div style='background: #f5f5f5; border: 2px solid #333; padding: 15px; margin-bottom: 15px;'>
                <h3 style='margin: 0 0 5px 0; font-size: 16px; color: #333;'>", weight_name, "</h3>
                <p style='margin: 0; font-size: 14px; color: #666;'>", weight_desc, "</p>
                </div>

                <div style='font-size: 14px; color: #333;'>
                    <div style='background: #f9f9f9; border: 1px solid #ccc; padding: 12px; margin: 15px 0;'>
                        <p style='margin: 0 0 8px 0; font-weight: bold;'>Formula</p>
                        <p style='font-family: monospace; font-size: 13px; margin: 5px 0; background: white; padding: 8px; border: 1px solid #ccc;'>",
                        formula, "</p>
                        <p style='font-size: 13px; margin: 5px 0; color: #666;'>", formula_desc, "</p>
                    </div>

                    <p style='margin: 15px 0; font-weight: bold;'>Example (5-point scale):</p>",
                    example_table, "

                    <div style='background: #f9f9f9; border: 1px solid #ccc; padding: 12px; margin: 15px 0;'>
                        <p style='margin: 0 0 8px 0; font-weight: bold;'>When to Use</p>
                        <p style='margin: 0; font-size: 14px;'>", use_case, "</p>
                    </div>

                    <div style='background: #f9f9f9; border: 1px solid #ccc; padding: 12px; margin: 15px 0;'>
                        <p style='margin: 0 0 8px 0; font-weight: bold;'>Key Points</p>",
                        interpretation, "
                    </div>

                    <div style='background: #f9f9f9; border: 1px solid #ccc; padding: 12px; margin: 15px 0;'>
                        <p style='margin: 0 0 8px 0; font-weight: bold;'>Important Note</p>
                        <p style='margin: 0; font-size: 14px;'>Weighted kappa values cannot be directly compared to unweighted kappa. They measure different aspects of agreement.</p>
                    </div>

                    <div style='background: #f9f9f9; border: 1px solid #ccc; padding: 12px; margin: 15px 0;'>
                        <p style='margin: 0 0 8px 0; font-weight: bold;'>Interpretation Guidelines (Landis & Koch, 1977)</p>
                        <table style='width: 100%; font-size: 13px;'>
                        <tr><td style='padding: 2px 0;'>&lt; 0.00</td><td style='padding: 2px 0;'>Poor agreement</td></tr>
                        <tr><td style='padding: 2px 0;'>0.00 - 0.20</td><td style='padding: 2px 0;'>Slight agreement</td></tr>
                        <tr><td style='padding: 2px 0;'>0.21 - 0.40</td><td style='padding: 2px 0;'>Fair agreement</td></tr>
                        <tr><td style='padding: 2px 0;'>0.41 - 0.60</td><td style='padding: 2px 0;'>Moderate agreement</td></tr>
                        <tr><td style='padding: 2px 0;'>0.61 - 0.80</td><td style='padding: 2px 0;'>Substantial agreement</td></tr>
                        <tr><td style='padding: 2px 0;'>0.81 - 1.00</td><td style='padding: 2px 0;'>Almost perfect agreement</td></tr>
                        </table>
                    </div>
                </div>
            </div>
            ")

            return(html_output)
        },

        .blandAltman = function(image, ...) {
            # Render Bland-Altman plot from stored state

            plotState <- image$state

            if (is.null(plotState)) {
                return(FALSE)
            }

            # Extract data from state
            diff <- plotState$diff
            avg <- plotState$avg
            mean_diff <- plotState$mean_diff
            lower_loa <- plotState$lower_loa
            upper_loa <- plotState$upper_loa
            conf_level <- plotState$conf_level
            prop_bias <- plotState$prop_bias
            rater_names <- plotState$rater_names

            # Create plot
            plot_data <- data.frame(
                avg = avg,
                diff = diff
            )

            p <- ggplot2::ggplot(plot_data, ggplot2::aes(x = avg, y = diff)) +
                ggplot2::geom_point(alpha = 0.6, size = 2) +
                ggplot2::geom_hline(yintercept = mean_diff, linetype = "solid", color = "blue", size = 1) +
                ggplot2::geom_hline(yintercept = lower_loa, linetype = "dashed", color = "red", size = 0.8) +
                ggplot2::geom_hline(yintercept = upper_loa, linetype = "dashed", color = "red", size = 0.8) +
                ggplot2::labs(
                    title = "Bland-Altman Plot",
                    subtitle = sprintf("Mean Difference and %g%% Limits of Agreement", conf_level * 100),
                    x = sprintf("Mean of %s and %s", rater_names[1], rater_names[2]),
                    y = sprintf("Difference (%s - %s)", rater_names[1], rater_names[2])
                ) +
                ggplot2::annotate("text", x = max(avg) * 0.95, y = mean_diff,
                                label = sprintf("Mean: %.2f", mean_diff),
                                hjust = 1, vjust = -0.5, color = "blue") +
                ggplot2::annotate("text", x = max(avg) * 0.95, y = lower_loa,
                                label = sprintf("Lower LoA: %.2f", lower_loa),
                                hjust = 1, vjust = 1.2, color = "red") +
                ggplot2::annotate("text", x = max(avg) * 0.95, y = upper_loa,
                                label = sprintf("Upper LoA: %.2f", upper_loa),
                                hjust = 1, vjust = -0.2, color = "red") +
                ggplot2::theme_minimal() +
                ggplot2::theme(
                    plot.title = ggplot2::element_text(hjust = 0.5, face = "bold"),
                    plot.subtitle = ggplot2::element_text(hjust = 0.5)
                )

            # Add proportional bias trend line if requested
            if (prop_bias) {
                p <- p + ggplot2::geom_smooth(method = "lm", se = TRUE, color = "darkgreen", size = 0.8)
            }

            print(p)

            return(TRUE)
        },

        .populateGwetExplanation = function() {
            # Provide educational content about Gwet's AC coefficient

            html_content <- "
            <div style='font-family: Arial, sans-serif; max-width: 800px; line-height: 1.6;'>
                <div style='background: #f0f8ff; border-left: 4px solid #2196F3; padding: 15px; margin-bottom: 20px;'>
                    <h3 style='margin: 0 0 10px 0; color: #1976D2;'>What is Gwet's AC Coefficient?</h3>
                    <p style='margin: 0; color: #333;'>
                        Gwet's AC coefficient solves the <strong>kappa paradox</strong> where Cohen's/Fleiss' kappa
                        can be artificially low despite high observed agreement.
                    </p>
                </div>

                <div style='background: #fff3e0; border-left: 4px solid #FF9800; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #F57C00;'>When to Use Gwet's AC</h4>
                    <p style='margin: 0 0 10px 0;'><strong>This method is particularly valuable when you have:</strong></p>
                    <ul style='margin: 0; padding-left: 20px;'>
                        <li><strong>Rare tumor subtypes</strong> ‚Äì Unbalanced categories where some diagnoses are very uncommon</li>
                        <li><strong>High agreement rates</strong> ‚Äì Easy-to-diagnose cases where raters agree >90% of the time</li>
                        <li><strong>Skewed marginal distributions</strong> ‚Äì When one category dominates the data</li>
                    </ul>
                </div>

                <div style='background: #e8f5e9; border-left: 4px solid #4CAF50; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #388E3C;'>AC1 vs AC2</h4>
                    <table style='width: 100%; border-collapse: collapse;'>
                        <tr style='background: #f5f5f5;'>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Method</th>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Use Case</th>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>AC1 (unweighted)</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Nominal categories with no inherent order</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>AC2 (linear weights)</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Ordinal data where adjacent disagreements are less severe</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px;'><strong>AC2 (quadratic weights)</strong></td>
                            <td style='padding: 8px;'>Ordinal data where larger disagreements should be heavily penalized</td>
                        </tr>
                    </table>
                </div>

                <div style='background: #fce4ec; border-left: 4px solid #E91E63; padding: 15px;'>
                    <h4 style='margin: 0 0 10px 0; color: #C2185B;'>Clinical Example</h4>
                    <p style='margin: 0; font-style: italic;'>
                        When diagnosing a rare tumor subtype that appears in only 2% of cases, kappa may be low (e.g., 0.40)
                        even when pathologists agree 98% of the time. Gwet's AC provides a more accurate measure
                        (e.g., 0.92) that reflects the true level of agreement.
                    </p>
                </div>

                <div style='margin-top: 15px; padding: 10px; background: #f5f5f5; border-radius: 4px;'>
                    <p style='margin: 0; font-size: 12px; color: #666;'>
                        <strong>Reference:</strong> Gwet, K. L. (2008). Computing inter-rater reliability and its variance
                        in the presence of high agreement. <em>British Journal of Mathematical and Statistical Psychology</em>, 61(1), 29-48.
                    </p>
                </div>
            </div>
            "

            self$results$gwetExplanation$setContent(html_content)
        },

        .populateClinicalUseCases = function() {
            # Provide comprehensive clinical use cases and method selection guide

            html_content <- "
            <div style='font-family: Arial, sans-serif; max-width: 900px; line-height: 1.6;'>
                <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; margin-bottom: 20px; border-radius: 8px;'>
                    <h2 style='margin: 0 0 10px 0;'>Clinical Use Cases & Method Selection Guide</h2>
                    <p style='margin: 0; opacity: 0.9;'>Choose the right agreement measure for your pathology research</p>
                </div>

                <!-- Categorical Data Methods -->
                <div style='background: #f8f9fa; border-left: 5px solid #6c757d; padding: 20px; margin-bottom: 20px;'>
                    <h3 style='margin: 0 0 15px 0; color: #343a40;'>üìä Categorical/Ordinal Data</h3>

                    <div style='background: white; padding: 15px; margin-bottom: 15px; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>
                        <h4 style='margin: 0 0 10px 0; color: #495057;'>Cohen's/Fleiss' Kappa (Standard Method)</h4>
                        <p style='margin: 0 0 10px 0;'><strong>Use for:</strong></p>
                        <ul style='margin: 0 0 10px 0; padding-left: 20px;'>
                            <li><strong>Tumor grading</strong> - G1, G2, G3 classification</li>
                            <li><strong>Histologic type</strong> - Adenocarcinoma, squamous cell carcinoma, etc.</li>
                            <li><strong>Margin status</strong> - Positive, negative, close</li>
                            <li><strong>TNM staging</strong> - T1, T2, T3, T4 categories</li>
                            <li><strong>Biomarker scoring</strong> - Negative (0), 1+, 2+, 3+</li>
                        </ul>
                        <p style='margin: 0; font-size: 13px; color: #6c757d;'><em>Note: May be problematic with high agreement or rare categories</em></p>
                    </div>

                    <div style='background: white; padding: 15px; margin-bottom: 15px; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>
                        <h4 style='margin: 0 0 10px 0; color: #495057;'>Weighted Kappa (Ordinal Data)</h4>
                        <p style='margin: 0 0 10px 0;'><strong>Use for ordered categories where partial credit matters:</strong></p>
                        <ul style='margin: 0 0 10px 0; padding-left: 20px;'>
                            <li><strong>Dysplasia grading</strong> - None, low-grade, high-grade</li>
                            <li><strong>Mitotic count categories</strong> - Low (1-9), intermediate (10-19), high (‚â•20)</li>
                            <li><strong>Inflammation severity</strong> - Absent, mild, moderate, severe</li>
                            <li><strong>Fibrosis stage</strong> - F0, F1, F2, F3, F4</li>
                        </ul>
                        <p style='margin: 0; padding: 10px; background: #fff3cd; border-left: 3px solid #ffc107; font-size: 13px;'>
                            <strong>‚ö†Ô∏è Important:</strong> Use \"Show Level Ordering Information\" to verify proper ordering (e.g., F0 ‚Üí F1 ‚Üí F2 ‚Üí F3 ‚Üí F4)
                        </p>
                    </div>

                    <div style='background: white; padding: 15px; margin-bottom: 15px; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>
                        <h4 style='margin: 0 0 10px 0; color: #495057;'>Gwet's AC1/AC2 (Kappa Paradox Solution)</h4>
                        <p style='margin: 0 0 10px 0;'><strong>Choose when you have:</strong></p>
                        <ul style='margin: 0 0 10px 0; padding-left: 20px;'>
                            <li><strong>Rare tumor subtypes</strong> - Neuroendocrine carcinoma in colorectal specimens (2% prevalence)</li>
                            <li><strong>High agreement rates</strong> - Benign vs malignant with 95%+ agreement</li>
                            <li><strong>Unbalanced categories</strong> - Metastasis (5% positive) vs primary (95%)</li>
                            <li><strong>Quality control studies</strong> - Most cases straightforward, few difficult</li>
                        </ul>
                        <p style='margin: 0; padding: 10px; background: #d1ecf1; border-left: 3px solid #17a2b8; font-size: 13px;'>
                            <strong>üí° Tip:</strong> Run both Kappa and Gwet's AC - if they differ substantially, Gwet's AC is more reliable
                        </p>
                    </div>

                    <div style='background: white; padding: 15px; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>
                        <h4 style='margin: 0 0 10px 0; color: #495057;'>Krippendorff's Alpha (Missing Data/Flexible)</h4>
                        <p style='margin: 0 0 10px 0;'><strong>Use when:</strong></p>
                        <ul style='margin: 0; padding-left: 20px;'>
                            <li><strong>Incomplete ratings</strong> - Not all pathologists rated all cases</li>
                            <li><strong>Different measurement levels</strong> - Comparing nominal, ordinal, interval data</li>
                            <li><strong>Large panels</strong> - Many raters (>5) with varying participation</li>
                        </ul>
                    </div>
                </div>

                <!-- Continuous Data Methods -->
                <div style='background: #f8f9fa; border-left: 5px solid #007bff; padding: 20px; margin-bottom: 20px;'>
                    <h3 style='margin: 0 0 15px 0; color: #343a40;'>üìè Continuous/Numeric Data</h3>

                    <div style='background: white; padding: 15px; margin-bottom: 15px; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>
                        <h4 style='margin: 0 0 10px 0; color: #495057;'>ICC (Intraclass Correlation Coefficient)</h4>
                        <p style='margin: 0 0 10px 0;'><strong>Use for:</strong></p>
                        <ul style='margin: 0 0 10px 0; padding-left: 20px;'>
                            <li><strong>Tumor size</strong> - Diameter measurements (mm or cm)</li>
                            <li><strong>Biomarker quantification</strong> - Ki-67 proliferation index (%)</li>
                            <li><strong>Cell counts</strong> - Mitotic figures per 10 HPF</li>
                            <li><strong>Morphometric analysis</strong> - Nuclear area (Œºm¬≤), gland perimeter</li>
                            <li><strong>Digital pathology</strong> - Automated vs manual measurements</li>
                            <li><strong>Continuous scores</strong> - Visual analog scales (0-100)</li>
                        </ul>
                        <p style='margin: 0; padding: 10px; background: #e7f3ff; border-left: 3px solid #007bff; font-size: 13px;'>
                            <strong>üìã Model selection:</strong> Use ICC(2,1) or ICC(3,1) for most pathology studies where all cases are rated by the same pathologists
                        </p>
                    </div>

                    <div style='background: white; padding: 15px; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>
                        <h4 style='margin: 0 0 10px 0; color: #495057;'>Bland-Altman Plot (Visual Assessment)</h4>
                        <p style='margin: 0 0 10px 0;'><strong>Use alongside ICC to:</strong></p>
                        <ul style='margin: 0 0 10px 0; padding-left: 20px;'>
                            <li><strong>Detect systematic bias</strong> - Does one rater consistently measure higher/lower?</li>
                            <li><strong>Identify outliers</strong> - Cases with unusually large disagreement</li>
                            <li><strong>Check proportional bias</strong> - Does disagreement increase with measurement size?</li>
                            <li><strong>Define acceptable limits</strong> - What range of disagreement is clinically acceptable?</li>
                        </ul>
                        <p style='margin: 0; padding: 10px; background: #d4edda; border-left: 3px solid #28a745; font-size: 13px;'>
                            <strong>‚úÖ Best practice:</strong> Always use ICC + Bland-Altman together for continuous data
                        </p>
                    </div>
                </div>

                <!-- Quick Decision Guide -->
                <div style='background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; padding: 20px; border-radius: 8px;'>
                    <h3 style='margin: 0 0 15px 0;'>üéØ Quick Decision Guide</h3>
                    <table style='width: 100%; background: white; color: #333; border-radius: 4px; overflow: hidden;'>
                        <tr style='background: #f8f9fa;'>
                            <th style='padding: 12px; text-align: left; border-bottom: 2px solid #dee2e6;'>Your Data Type</th>
                            <th style='padding: 12px; text-align: left; border-bottom: 2px solid #dee2e6;'>Recommended Method</th>
                        </tr>
                        <tr>
                            <td style='padding: 10px; border-bottom: 1px solid #dee2e6;'>Nominal categories (no order)</td>
                            <td style='padding: 10px; border-bottom: 1px solid #dee2e6;'><strong>Kappa</strong> or <strong>Gwet's AC1</strong></td>
                        </tr>
                        <tr>
                            <td style='padding: 10px; border-bottom: 1px solid #dee2e6;'>Ordinal categories (ordered)</td>
                            <td style='padding: 10px; border-bottom: 1px solid #dee2e6;'><strong>Weighted Kappa</strong> or <strong>Gwet's AC2</strong></td>
                        </tr>
                        <tr>
                            <td style='padding: 10px; border-bottom: 1px solid #dee2e6;'>Continuous measurements</td>
                            <td style='padding: 10px; border-bottom: 1px solid #dee2e6;'><strong>ICC + Bland-Altman</strong></td>
                        </tr>
                        <tr>
                            <td style='padding: 10px; border-bottom: 1px solid #dee2e6;'>Rare categories (< 10%)</td>
                            <td style='padding: 10px; border-bottom: 1px solid #dee2e6;'><strong>Gwet's AC</strong> (not Kappa)</td>
                        </tr>
                        <tr>
                            <td style='padding: 10px;'>Missing data/incomplete ratings</td>
                            <td style='padding: 10px;'><strong>Krippendorff's Alpha</strong></td>
                        </tr>
                    </table>
                </div>

                <!-- Sample Size Recommendations -->
                <div style='margin-top: 20px; padding: 15px; background: #fff3e0; border-left: 4px solid #ff9800; border-radius: 4px;'>
                    <h4 style='margin: 0 0 10px 0; color: #e65100;'>üìä Sample Size Recommendations</h4>
                    <ul style='margin: 0; padding-left: 20px; font-size: 14px;'>
                        <li><strong>Minimum:</strong> 30 cases for stable kappa/ICC estimates</li>
                        <li><strong>Recommended:</strong> 50-100 cases for categorical data, 30-50 for continuous</li>
                        <li><strong>Rare categories:</strong> Include ‚â•20 positive cases if possible</li>
                        <li><strong>Pilot studies:</strong> 20 cases minimum, but interpret cautiously</li>
                    </ul>
                </div>
            </div>
            "

            self$results$clinicalUseCases$setContent(html_content)
        },

        .populateLightKappaExplanation = function() {
            # Provide educational content about Light's Kappa

            html_content <- "
            <div style='font-family: Arial, sans-serif; max-width: 800px; line-height: 1.6;'>
                <div style='background: #f0f8ff; border-left: 4px solid #2196F3; padding: 15px; margin-bottom: 20px;'>
                    <h3 style='margin: 0 0 10px 0; color: #1976D2;'>What is Light's Kappa?</h3>
                    <p style='margin: 0; color: #333;'>
                        Light's kappa is an alternative agreement measure for <strong>3 or more raters</strong>.
                        It calculates the <strong>average of all pairwise kappas</strong> between raters.
                    </p>
                </div>

                <div style='background: #fff3e0; border-left: 4px solid #FF9800; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #F57C00;'>When to Use Light's Kappa</h4>
                    <p style='margin: 0 0 10px 0;'><strong>Choose Light's kappa instead of Fleiss' kappa when:</strong></p>
                    <ul style='margin: 0; padding-left: 20px;'>
                        <li><strong>Raters have different marginal distributions</strong> - Some pathologists may be more conservative/liberal than others</li>
                        <li><strong>Unequal rater characteristics</strong> - Mix of senior and junior pathologists</li>
                        <li><strong>Assumptions questionable</strong> - When Fleiss' kappa assumptions don't hold</li>
                        <li><strong>Pairwise comparisons matter</strong> - Want to understand individual rater-to-rater agreement</li>
                    </ul>
                </div>

                <div style='background: #e8f5e9; border-left: 4px solid #4CAF50; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #388E3C;'>Light's Kappa vs Fleiss' Kappa</h4>
                    <table style='width: 100%; border-collapse: collapse;'>
                        <tr style='background: #f5f5f5;'>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Feature</th>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Light's Kappa</th>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Fleiss' Kappa</th>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>Method</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Average of pairwise kappas</td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Overall agreement across all raters</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>Assumptions</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Fewer, more flexible</td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Assumes similar marginal distributions</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>Use when</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Raters differ systematically</td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Raters are similar/interchangeable</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px;'><strong>Typical value</strong></td>
                            <td style='padding: 8px;'>Often slightly lower</td>
                            <td style='padding: 8px;'>Often slightly higher</td>
                        </tr>
                    </table>
                </div>

                <div style='background: #fce4ec; border-left: 4px solid #E91E63; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #C2185B;'>Clinical Example</h4>
                    <p style='margin: 0; font-style: italic;'>
                        In a study with 3 pathologists grading dysplasia (none, low-grade, high-grade):
                        Pathologist A is conservative (more often grades as \"none\"), Pathologist B is moderate,
                        and Pathologist C is more aggressive (more often grades as \"high-grade\").
                        Light's kappa would be more appropriate than Fleiss' kappa because the raters have
                        systematically different rating patterns.
                    </p>
                </div>

                <div style='background: #e3f2fd; border-left: 4px solid #1976D2; padding: 15px;'>
                    <h4 style='margin: 0 0 10px 0; color: #1565C0;'>Interpretation</h4>
                    <p style='margin: 0 0 5px 0;'>Use the same interpretation guidelines as Cohen's kappa:</p>
                    <table style='width: 100%; border-collapse: collapse; font-size: 14px;'>
                        <tr>
                            <td style='padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;'>< 0.20</td>
                            <td style='padding: 5px; border-bottom: 1px solid #ddd;'>Slight agreement</td>
                        </tr>
                        <tr>
                            <td style='padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;'>0.20 ‚Äì 0.40</td>
                            <td style='padding: 5px; border-bottom: 1px solid #ddd;'>Fair agreement</td>
                        </tr>
                        <tr>
                            <td style='padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;'>0.40 ‚Äì 0.60</td>
                            <td style='padding: 5px; border-bottom: 1px solid #ddd;'>Moderate agreement</td>
                        </tr>
                        <tr>
                            <td style='padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;'>0.60 ‚Äì 0.80</td>
                            <td style='padding: 5px; border-bottom: 1px solid #ddd;'>Substantial agreement</td>
                        </tr>
                        <tr>
                            <td style='padding: 5px; font-weight: bold;'>0.80 ‚Äì 1.00</td>
                            <td style='padding: 5px;'>Almost perfect agreement</td>
                        </tr>
                    </table>
                </div>

                <div style='margin-top: 15px; padding: 10px; background: #f5f5f5; border-radius: 4px;'>
                    <p style='margin: 0; font-size: 12px; color: #666;'>
                        <strong>Reference:</strong> Light, R. J. (1971). Measures of response agreement for qualitative data: Some generalizations and alternatives.
                        <em>Psychological Bulletin</em>, 76(5), 365-377.
                    </p>
                </div>
            </div>
            "

            self$results$lightKappaExplanation$setContent(html_content)
        },

        .calculateLightKappa = function(ratings) {
            # Calculate Light's kappa for 3+ raters

            # Validate minimum number of raters
            if (ncol(ratings) < 3) {
                self$results$lightKappaTable$setNote(
                    "error",
                    "Light's kappa requires at least 3 raters. You have selected only 2 raters. Use Cohen's kappa instead."
                )
                return()
            }

            # Validate data is categorical
            if (any(sapply(ratings, is.numeric)) && !any(sapply(ratings, is.factor))) {
                self$results$lightKappaTable$setNote(
                    "error",
                    "Light's kappa requires categorical data. Your data appears to be continuous. Use ICC instead."
                )
                return()
            }

            # Prepare data - remove rows with all missing values
            complete_idx <- rowSums(!is.na(ratings)) > 0
            ratings_clean <- ratings[complete_idx, , drop = FALSE]

            if (nrow(ratings_clean) < 2) {
                self$results$lightKappaTable$setNote(
                    "error",
                    "Insufficient complete cases for Light's kappa calculation. At least 2 cases required."
                )
                return()
            }

            # Calculate Light's kappa
            tryCatch({
                light_result <- irr::kappam.light(ratings_clean)

                # Populate table
                self$results$lightKappaTable$setRow(rowNo = 1, values = list(
                    method = "Light's Kappa (average of pairwise kappas)",
                    subjects = light_result$subjects,
                    raters = light_result$raters,
                    kappa = light_result$value,
                    p = light_result$p.value
                ))

                # Add interpretation note
                kappa_val <- light_result$value
                if (kappa_val < 0.20) {
                    interp <- "Slight agreement"
                } else if (kappa_val < 0.40) {
                    interp <- "Fair agreement"
                } else if (kappa_val < 0.60) {
                    interp <- "Moderate agreement"
                } else if (kappa_val < 0.80) {
                    interp <- "Substantial agreement"
                } else {
                    interp <- "Almost perfect agreement"
                }

                self$results$lightKappaTable$setNote(
                    "interpretation",
                    sprintf("Interpretation: %s (Landis & Koch, 1977). Light's kappa averages %d pairwise kappas between raters.",
                            interp, choose(light_result$raters, 2))
                )

            }, error = function(e) {
                self$results$lightKappaTable$setNote(
                    "error",
                    sprintf("Error calculating Light's kappa: %s", e$message)
                )
            })
        },

        .populateKendallWExplanation = function() {
            # Provide educational content about Kendall's W

            html_content <- "
            <div style='font-family: Arial, sans-serif; max-width: 800px; line-height: 1.6;'>
                <div style='background: #f0f8ff; border-left: 4px solid #2196F3; padding: 15px; margin-bottom: 20px;'>
                    <h3 style='margin: 0 0 10px 0; color: #1976D2;'>What is Kendall's W?</h3>
                    <p style='margin: 0; color: #333;'>
                        Kendall's coefficient of concordance (W) measures the <strong>agreement among multiple raters</strong>
                        when ranking or rating ordinal data. W ranges from <strong>0 (no agreement)</strong> to
                        <strong>1 (perfect agreement)</strong>.
                    </p>
                </div>

                <div style='background: #fff3e0; border-left: 4px solid #FF9800; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #F57C00;'>When to Use Kendall's W</h4>
                    <p style='margin: 0 0 10px 0;'><strong>Particularly useful for:</strong></p>
                    <ul style='margin: 0; padding-left: 20px;'>
                        <li><strong>Rankings</strong> - When raters rank cases from best to worst (e.g., ranking diagnostic difficulty)</li>
                        <li><strong>Ordinal scales</strong> - Tumor grade (G1/G2/G3), severity scores, stage classifications</li>
                        <li><strong>Agreement on order</strong> - When you care whether raters rank cases in similar order</li>
                        <li><strong>Multiple raters (‚â•3)</strong> - Works best with 3 or more independent raters</li>
                    </ul>
                </div>

                <div style='background: #e8f5e9; border-left: 4px solid #4CAF50; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #388E3C;'>Interpreting Kendall's W</h4>
                    <table style='width: 100%; border-collapse: collapse;'>
                        <tr>
                            <td style='padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;'>W = 0.00 - 0.20</td>
                            <td style='padding: 5px; border-bottom: 1px solid #ddd;'>Very weak agreement (essentially random)</td>
                        </tr>
                        <tr>
                            <td style='padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;'>W = 0.21 - 0.40</td>
                            <td style='padding: 5px; border-bottom: 1px solid #ddd;'>Weak agreement</td>
                        </tr>
                        <tr>
                            <td style='padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;'>W = 0.41 - 0.60</td>
                            <td style='padding: 5px; border-bottom: 1px solid #ddd;'>Moderate agreement</td>
                        </tr>
                        <tr>
                            <td style='padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;'>W = 0.61 - 0.80</td>
                            <td style='padding: 5px; border-bottom: 1px solid #ddd;'>Strong agreement</td>
                        </tr>
                        <tr>
                            <td style='padding: 5px; font-weight: bold;'>W = 0.81 - 1.00</td>
                            <td style='padding: 5px;'>Very strong agreement (nearly unanimous)</td>
                        </tr>
                    </table>
                </div>

                <div style='background: #fce4ec; border-left: 4px solid #E91E63; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #C2185B;'>Kendall's W vs Other Methods</h4>
                    <table style='width: 100%; border-collapse: collapse; font-size: 13px;'>
                        <tr style='background: #f5f5f5;'>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #C2185B;'>Method</th>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #C2185B;'>Best For</th>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>Kendall's W</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Ranking agreement, ordinal scales, consensus on ordering</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>Weighted Kappa</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Exact category matching with partial credit for near-misses</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>ICC</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Continuous measurements (tumor size, biomarker levels)</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px;'><strong>Fleiss'/Light's Kappa</strong></td>
                            <td style='padding: 8px;'>Categorical agreement (nominal or ordinal categories)</td>
                        </tr>
                    </table>
                </div>

                <div style='background: #e3f2fd; border-left: 4px solid #1976D2; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #1565C0;'>Clinical Example</h4>
                    <p style='margin: 0; padding: 10px; background: white; border-radius: 4px; font-size: 13px;'>
                        <strong>Scenario:</strong> Four pathologists rank 20 tumor samples by diagnostic difficulty
                        (1 = easiest to 20 = most difficult).<br><br>
                        <strong>W = 0.75</strong> indicates <strong>strong agreement</strong> - the pathologists generally
                        agree on which cases are easy vs difficult, even if they don't assign identical ranks.<br><br>
                        This helps identify challenging cases that may need additional review or contribute to training materials.
                    </p>
                </div>

                <div style='background: #fff9c4; border-left: 4px solid #FBC02D; padding: 15px;'>
                    <h4 style='margin: 0 0 10px 0; color: #F57F17;'>Statistical Significance</h4>
                    <p style='margin: 0; font-size: 13px;'>
                        The chi-square test evaluates whether the observed agreement (W) is statistically different from
                        random chance. A significant p-value (< 0.05) indicates that raters agree more than expected by chance.
                        However, even statistically significant agreement may not be clinically meaningful if W is low.
                    </p>
                </div>

                <div style='margin-top: 15px; padding: 10px; background: #f5f5f5; border-radius: 4px;'>
                    <p style='margin: 0; font-size: 12px; color: #666;'>
                        <strong>Reference:</strong> Kendall, M. G., & Babington Smith, B. (1939). The problem of m rankings.
                        <em>The Annals of Mathematical Statistics</em>, 10(3), 275-287.
                    </p>
                </div>
            </div>
            "

            self$results$kendallWExplanation$setContent(html_content)
        },

        .calculateKendallW = function(ratings) {
            # Calculate Kendall's coefficient of concordance (W) for ordinal/ranking data

            # Validate data is numeric or can be converted to ranks
            # Kendall's W requires either numeric data or ordinal factors
            if (!all(sapply(ratings, function(x) is.numeric(x) || is.ordered(x) || is.factor(x)))) {
                self$results$kendallWTable$setNote(
                    "error",
                    "Kendall's W requires numeric, ordinal, or factor data that can be ranked."
                )
                return()
            }

            # Prepare data - remove rows with all missing values
            complete_idx <- rowSums(!is.na(ratings)) > 0
            ratings_clean <- ratings[complete_idx, , drop = FALSE]

            if (nrow(ratings_clean) < 2) {
                self$results$kendallWTable$setNote(
                    "error",
                    "Insufficient complete cases for Kendall's W calculation. At least 2 cases required."
                )
                return()
            }

            # Convert factors to numeric if needed (preserving ordinal structure)
            ratings_matrix <- as.matrix(ratings_clean)
            for (i in 1:ncol(ratings_matrix)) {
                if (is.factor(ratings_clean[[i]]) || is.ordered(ratings_clean[[i]])) {
                    ratings_matrix[, i] <- as.numeric(ratings_clean[[i]])
                }
            }

            # Calculate Kendall's W
            tryCatch({
                # irr::kendall returns W coefficient, Chisq, p.value
                kendall_result <- irr::kendall(ratings_matrix, correct = TRUE)

                # Populate table
                self$results$kendallWTable$setRow(rowNo = 1, values = list(
                    method = "Kendall's W (coefficient of concordance)",
                    subjects = kendall_result$subjects,
                    raters = kendall_result$raters,
                    w = kendall_result$value,
                    chisq = kendall_result$statistic,
                    df = nrow(ratings_matrix) - 1,
                    p = kendall_result$p.value
                ))

                # Add interpretation note
                w_val <- kendall_result$value
                if (w_val <= 0.20) {
                    interp <- "Very weak agreement (essentially random)"
                } else if (w_val <= 0.40) {
                    interp <- "Weak agreement"
                } else if (w_val <= 0.60) {
                    interp <- "Moderate agreement"
                } else if (w_val <= 0.80) {
                    interp <- "Strong agreement"
                } else {
                    interp <- "Very strong agreement"
                }

                # Format p-value for note
                if (kendall_result$p.value < 0.001) {
                    p_text <- "p < .001"
                } else {
                    p_text <- sprintf("p = %.3f", kendall_result$p.value)
                }

                self$results$kendallWTable$setNote(
                    "interpretation",
                    sprintf("Interpretation: %s. Chi-square test: %s, indicating agreement %s significantly different from chance.",
                            interp, p_text,
                            if(kendall_result$p.value < 0.05) "is" else "is not")
                )

            }, error = function(e) {
                self$results$kendallWTable$setNote(
                    "error",
                    sprintf("Error calculating Kendall's W: %s", e$message)
                )
            })
        },

        .populateRaterBiasExplanation = function() {
            # Provide educational content about Rater Bias Test

            html_content <- "
            <div style='font-family: Arial, sans-serif; max-width: 800px; line-height: 1.6;'>
                <div style='background: #f0f8ff; border-left: 4px solid #2196F3; padding: 15px; margin-bottom: 20px;'>
                    <h3 style='margin: 0 0 10px 0; color: #1976D2;'>What is the Rater Bias Test?</h3>
                    <p style='margin: 0; color: #333;'>
                        The Rater Bias Test uses a <strong>chi-square test</strong> to detect whether raters have
                        <strong>systematically different rating patterns</strong>. It tests the null hypothesis that
                        all raters use the rating categories with equal frequency.
                    </p>
                </div>

                <div style='background: #fff3e0; border-left: 4px solid #FF9800; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #F57C00;'>When to Use This Test</h4>
                    <p style='margin: 0 0 10px 0;'><strong>Essential for quality control when:</strong></p>
                    <ul style='margin: 0; padding-left: 20px;'>
                        <li><strong>Rater training</strong> - Identifying trainees who are systematically too lenient or strict</li>
                        <li><strong>Performance monitoring</strong> - Detecting drift in rater behavior over time</li>
                        <li><strong>Multi-center studies</strong> - Ensuring consistent grading across sites</li>
                        <li><strong>Certification</strong> - Verifying raters use categories appropriately before certification</li>
                        <li><strong>Detecting systematic errors</strong> - Finding raters who consistently over-diagnose or under-diagnose</li>
                    </ul>
                </div>

                <div style='background: #e8f5e9; border-left: 4px solid #4CAF50; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #388E3C;'>Interpreting Results</h4>
                    <table style='width: 100%; border-collapse: collapse;'>
                        <tr style='background: #f5f5f5;'>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Result</th>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Interpretation</th>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>p ‚â• 0.05</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>No significant bias detected - raters use categories similarly</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>p < 0.05</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Significant bias detected - raters have systematically different patterns</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px;'><strong>p < 0.001</strong></td>
                            <td style='padding: 8px;'>Strong evidence of bias - investigate individual rater frequencies</td>
                        </tr>
                    </table>
                </div>

                <div style='background: #fce4ec; border-left: 4px solid #E91E63; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #C2185B;'>Common Bias Patterns in Pathology</h4>
                    <table style='width: 100%; border-collapse: collapse; font-size: 13px;'>
                        <tr style='background: #f5f5f5;'>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #C2185B;'>Pattern</th>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #C2185B;'>Clinical Impact</th>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>Lenient rater</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Systematically assigns lower grades (potential under-diagnosis)</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>Strict rater</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Systematically assigns higher grades (potential over-diagnosis)</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>Central tendency bias</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Avoids extreme categories, clusters ratings in middle</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px;'><strong>Extreme response bias</strong></td>
                            <td style='padding: 8px;'>Overuses extreme categories, avoids middle grades</td>
                        </tr>
                    </table>
                </div>

                <div style='background: #e3f2fd; border-left: 4px solid #1976D2; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #1565C0;'>Clinical Example</h4>
                    <p style='margin: 0; padding: 10px; background: white; border-radius: 4px; font-size: 13px;'>
                        <strong>Scenario:</strong> Three pathologists grade 50 tumor samples as G1/G2/G3.<br><br>
                        <strong>Pathologist A:</strong> 45% G1, 40% G2, 15% G3<br>
                        <strong>Pathologist B:</strong> 20% G1, 60% G2, 20% G3<br>
                        <strong>Pathologist C:</strong> 15% G1, 40% G2, 45% G3<br><br>
                        <strong>Result: p < 0.001</strong> - Strong evidence of systematic bias. Pathologist A is lenient
                        (favors low grades), Pathologist C is strict (favors high grades). This requires investigation
                        and potentially retraining before they can reliably grade cases.
                    </p>
                </div>

                <div style='background: #fff9c4; border-left: 4px solid #FBC02D; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #F57F17;'>Important Considerations</h4>
                    <ul style='margin: 0; padding-left: 20px; font-size: 13px;'>
                        <li><strong>Sample size matters:</strong> Test has limited power with small samples (< 30 cases)</li>
                        <li><strong>Bias ‚â† Poor agreement:</strong> Raters can be biased but still agree (all systematically lenient)</li>
                        <li><strong>Clinical context:</strong> Some bias may be acceptable (e.g., erring on side of caution)</li>
                        <li><strong>Training intervention:</strong> Significant bias often correctable with feedback and training</li>
                        <li><strong>Follow-up:</strong> After detecting bias, examine individual rater frequency distributions</li>
                    </ul>
                </div>

                <div style='background: #e8f5e9; border-left: 4px solid #4CAF50; padding: 15px;'>
                    <h4 style='margin: 0 0 10px 0; color: #388E3C;'>Relationship to Agreement Measures</h4>
                    <p style='margin: 0; font-size: 13px;'>
                        Rater bias is <strong>independent from agreement</strong>. You can have:<br><br>
                        ‚úÖ <strong>High agreement + No bias:</strong> Ideal situation<br>
                        ‚ö†Ô∏è <strong>High agreement + Bias:</strong> All raters systematically lenient/strict together<br>
                        ‚ö†Ô∏è <strong>Low agreement + No bias:</strong> Random disagreement but no systematic patterns<br>
                        ‚ùå <strong>Low agreement + Bias:</strong> Both random and systematic errors present
                    </p>
                </div>

                <div style='margin-top: 15px; padding: 10px; background: #f5f5f5; border-radius: 4px;'>
                    <p style='margin: 0; font-size: 12px; color: #666;'>
                        <strong>Reference:</strong> Stuart, A. A. (1955). A test for homogeneity of the marginal distributions
                        in a two-way classification. <em>Biometrika</em>, 42(3/4), 412-416.
                    </p>
                </div>
            </div>
            "

            self$results$raterBiasExplanation$setContent(html_content)
        },

        .calculateRaterBias = function(ratings) {
            # Test for systematic rater bias using chi-square test
            # Detects if raters have different marginal distributions

            # Validate data is categorical
            if (any(sapply(ratings, is.numeric)) && !any(sapply(ratings, is.factor))) {
                self$results$raterBiasTable$setNote(
                    "error",
                    "Rater Bias Test requires categorical data. Your data appears to be continuous."
                )
                return()
            }

            # Prepare data - remove rows with all missing values
            complete_idx <- rowSums(!is.na(ratings)) > 0
            ratings_clean <- ratings[complete_idx, , drop = FALSE]

            if (nrow(ratings_clean) < 2) {
                self$results$raterBiasTable$setNote(
                    "error",
                    "Insufficient complete cases for Rater Bias Test. At least 2 cases required."
                )
                return()
            }

            # Calculate Rater Bias Test
            tryCatch({
                # irr::rater.bias returns chi-square statistic, df, p-value
                bias_result <- irr::rater.bias(ratings_clean)

                # Populate table
                self$results$raterBiasTable$setRow(rowNo = 1, values = list(
                    method = "Chi-square test for marginal homogeneity",
                    chisq = bias_result$statistic,
                    df = bias_result$parameter,
                    p = bias_result$p.value
                ))

                # Add interpretation note
                if (bias_result$p.value >= 0.05) {
                    interp <- "No significant bias detected"
                    detail <- "Raters use rating categories with similar frequencies. No evidence of systematic lenient/strict patterns."
                } else if (bias_result$p.value >= 0.01) {
                    interp <- "Significant bias detected"
                    detail <- "Raters have systematically different rating patterns (p < .05). Examine individual rater frequency distributions to identify which raters are lenient/strict."
                } else {
                    interp <- "Strong evidence of bias"
                    detail <- "Raters have markedly different rating patterns (p < .01). This indicates some raters systematically over-diagnose or under-diagnose. Recommend retraining or excluding biased raters."
                }

                self$results$raterBiasTable$setNote(
                    "interpretation",
                    sprintf("%s: %s", interp, detail)
                )

                # Add sample size note if small
                if (nrow(ratings_clean) < 30) {
                    self$results$raterBiasTable$setNote(
                        "power",
                        sprintf("Note: Small sample size (n=%d). Test may have limited power to detect bias. Results should be interpreted cautiously.",
                                nrow(ratings_clean))
                    )
                }

            }, error = function(e) {
                self$results$raterBiasTable$setNote(
                    "error",
                    sprintf("Error calculating Rater Bias Test: %s", e$message)
                )
            })
        },

        .populatePairwiseKappaExplanation = function() {
            # Provide educational content about Pairwise Kappa Analysis

            html_content <- "
            <div style='font-family: Arial, sans-serif; max-width: 800px; line-height: 1.6;'>
                <div style='background: #f0f8ff; border-left: 4px solid #2196F3; padding: 15px; margin-bottom: 20px;'>
                    <h3 style='margin: 0 0 10px 0; color: #1976D2;'>What is Pairwise Kappa Analysis?</h3>
                    <p style='margin: 0; color: #333;'>
                        Pairwise Kappa Analysis compares <strong>each rater individually</strong> against a
                        <strong>reference rater</strong> (gold standard, consensus, or senior expert).
                        Each comparison produces a separate Cohen's kappa measuring agreement between that
                        rater and the reference.
                    </p>
                </div>

                <div style='background: #fff3e0; border-left: 4px solid #FF9800; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #F57C00;'>When to Use Pairwise Analysis</h4>
                    <p style='margin: 0 0 10px 0;'><strong>Essential for:</strong></p>
                    <ul style='margin: 0; padding-left: 20px;'>
                        <li><strong>Training assessment</strong> - Compare trainees vs expert to measure learning progress</li>
                        <li><strong>Rater certification</strong> - Verify raters meet minimum kappa threshold vs gold standard</li>
                        <li><strong>Performance monitoring</strong> - Track individual rater agreement with reference over time</li>
                        <li><strong>Quality control</strong> - Identify specific raters who need retraining</li>
                        <li><strong>Competency evaluation</strong> - Rank raters by performance for advancement decisions</li>
                    </ul>
                </div>

                <div style='background: #e8f5e9; border-left: 4px solid #4CAF50; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #388E3C;'>Interpreting Kappa Values</h4>
                    <table style='width: 100%; border-collapse: collapse;'>
                        <tr style='background: #f5f5f5;'>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Kappa</th>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Agreement Level</th>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Training Status</th>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>Œ∫ < 0.40</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Poor to fair</td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>‚ö†Ô∏è Needs significant training</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>0.40-0.60</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Moderate</td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>‚ö†Ô∏è Additional training recommended</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>0.60-0.75</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Substantial</td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>‚úÖ Acceptable performance</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px;'><strong>Œ∫ > 0.75</strong></td>
                            <td style='padding: 8px;'>Excellent</td>
                            <td style='padding: 8px;'>‚úÖ Certified/Expert level</td>
                        </tr>
                    </table>
                </div>

                <div style='background: #fce4ec; border-left: 4px solid #E91E63; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #C2185B;'>Choosing the Reference Rater</h4>
                    <table style='width: 100%; border-collapse: collapse; font-size: 13px;'>
                        <tr style='background: #f5f5f5;'>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #C2185B;'>Reference Type</th>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #C2185B;'>Use Case</th>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>Gold Standard</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Verified diagnosis (biopsy, surgery, outcome data)</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>Consensus Score</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Modal rating from expert panel (create using Consensus Variable)</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>Senior Expert</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Most experienced pathologist/clinician</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px;'><strong>External Reviewer</strong></td>
                            <td style='padding: 8px;'>Independent expert from another institution</td>
                        </tr>
                    </table>
                </div>

                <div style='background: #e3f2fd; border-left: 4px solid #1976D2; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #1565C0;'>Clinical Example: Trainee Certification</h4>
                    <p style='margin: 0; padding: 10px; background: white; border-radius: 4px; font-size: 13px;'>
                        <strong>Scenario:</strong> Five pathology residents (Raters 1-5) grade 100 tumor samples.
                        A senior pathologist provides reference diagnoses.<br><br>
                        <strong>Results:</strong><br>
                        ‚Ä¢ Resident 1: Œ∫ = 0.82 (Excellent) ‚úÖ Ready for certification<br>
                        ‚Ä¢ Resident 2: Œ∫ = 0.68 (Substantial) ‚úÖ Acceptable, continue monitoring<br>
                        ‚Ä¢ Resident 3: Œ∫ = 0.52 (Moderate) ‚ö†Ô∏è Additional training needed<br>
                        ‚Ä¢ Resident 4: Œ∫ = 0.45 (Moderate) ‚ö†Ô∏è Review difficult cases with expert<br>
                        ‚Ä¢ Resident 5: Œ∫ = 0.28 (Poor) ‚ùå Requires intensive retraining<br><br>
                        <strong>Action:</strong> Residents 1-2 certified. Residents 3-5 receive targeted training
                        based on specific error patterns, then retest after 3 months.
                    </p>
                </div>

                <div style='background: #fff9c4; border-left: 4px solid #FBC02D; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #F57F17;'>Ranking Raters</h4>
                    <p style='margin: 0; font-size: 13px;'>
                        When <strong>Rank Raters by Performance</strong> is enabled, raters are sorted from highest
                        to lowest kappa. This identifies:<br><br>
                        ‚úÖ <strong>Top performers:</strong> Candidates for senior roles, teaching positions, or gold standard rating<br>
                        ‚ö†Ô∏è <strong>Middle performers:</strong> Acceptable but could benefit from continued training<br>
                        ‚ùå <strong>Low performers:</strong> Require immediate intervention or exclusion from study
                    </p>
                </div>

                <div style='background: #e8f5e9; border-left: 4px solid #4CAF50; padding: 15px;'>
                    <h4 style='margin: 0 0 10px 0; color: #388E3C;'>Pairwise vs Overall Agreement</h4>
                    <p style='margin: 0; font-size: 13px;'>
                        <strong>Pairwise Kappa (vs Reference):</strong> Measures each rater's agreement with gold standard<br>
                        ‚Üí <em>Focus: Individual performance assessment</em><br><br>
                        <strong>Fleiss'/Light's Kappa:</strong> Measures overall agreement among all raters<br>
                        ‚Üí <em>Focus: General reliability of rating system</em><br><br>
                        <strong>Recommendation:</strong> Use both! Overall kappa shows if your rating system is reliable.
                        Pairwise kappa identifies which specific raters need training.
                    </p>
                </div>

                <div style='margin-top: 15px; padding: 10px; background: #f5f5f5; border-radius: 4px;'>
                    <p style='margin: 0; font-size: 12px; color: #666;'>
                        <strong>Reference:</strong> Cohen, J. (1960). A coefficient of agreement for nominal scales.
                        <em>Educational and Psychological Measurement</em>, 20(1), 37-46.
                    </p>
                </div>
            </div>
            "

            self$results$pairwiseKappaExplanation$setContent(html_content)
        },

        .calculatePairwiseKappa = function(ratings, reference_ratings) {
            # Calculate Cohen's kappa for each rater vs reference

            # Validate reference rater is provided
            if (is.null(reference_ratings) || length(reference_ratings) == 0) {
                self$results$pairwiseKappaTable$setNote(
                    "error",
                    "Please select a reference rater variable."
                )
                return()
            }

            # Ensure reference_ratings is a vector
            if (is.data.frame(reference_ratings)) {
                reference_ratings <- reference_ratings[[1]]
            }

            # Validate data is categorical
            if (any(sapply(ratings, is.numeric)) && !any(sapply(ratings, is.factor))) {
                self$results$pairwiseKappaTable$setNote(
                    "warning",
                    "Pairwise kappa is designed for categorical data. Your data appears to be continuous."
                )
            }

            # Get rater names
            rater_names <- names(ratings)
            n_raters <- length(rater_names)

            # Store results for ranking
            kappa_results <- list()

            # Calculate kappa for each rater vs reference
            for (i in 1:n_raters) {
                rater_data <- ratings[[i]]
                rater_name <- rater_names[i]

                # Create pairwise data frame
                pairwise_data <- data.frame(
                    rater = rater_data,
                    reference = reference_ratings
                )

                # Remove rows with any missing values
                complete_idx <- complete.cases(pairwise_data)
                pairwise_clean <- pairwise_data[complete_idx, ]

                if (nrow(pairwise_clean) < 2) {
                    # Insufficient data for this rater
                    kappa_results[[rater_name]] <- list(
                        rater = rater_name,
                        subjects = 0,
                        kappa = NA,
                        z = NA,
                        p = NA,
                        error = "Insufficient complete cases"
                    )
                    next
                }

                # Calculate Cohen's kappa
                tryCatch({
                    kappa_result <- irr::kappa2(pairwise_clean)

                    kappa_results[[rater_name]] <- list(
                        rater = rater_name,
                        subjects = kappa_result$subjects,
                        kappa = kappa_result$value,
                        z = kappa_result$statistic,
                        p = kappa_result$p.value,
                        error = NULL
                    )

                }, error = function(e) {
                    kappa_results[[rater_name]] <- list(
                        rater = rater_name,
                        subjects = nrow(pairwise_clean),
                        kappa = NA,
                        z = NA,
                        p = NA,
                        error = e$message
                    )
                })
            }

            # Rank raters if requested
            if (self$options$rankRaters) {
                # Sort by kappa (descending), handling NAs
                valid_kappas <- sapply(kappa_results, function(x) if(is.null(x$error)) x$kappa else NA)
                rank_order <- order(valid_kappas, decreasing = TRUE, na.last = TRUE)
                kappa_results <- kappa_results[rank_order]

                # Assign ranks
                rank <- 1
                for (i in seq_along(kappa_results)) {
                    if (!is.na(kappa_results[[i]]$kappa)) {
                        kappa_results[[i]]$rank <- rank
                        rank <- rank + 1
                    } else {
                        kappa_results[[i]]$rank <- NA
                    }
                }
            }

            # Populate table
            pairwise_table <- self$results$pairwiseKappaTable

            for (result in kappa_results) {
                row_values <- list(
                    rater = result$rater,
                    subjects = result$subjects,
                    kappa = result$kappa,
                    z = result$z,
                    p = result$p
                )

                if (self$options$rankRaters) {
                    row_values$rank <- result$rank
                }

                pairwise_table$addRow(rowKey = result$rater, values = row_values)

                # Add interpretation or error note
                if (!is.null(result$error)) {
                    pairwise_table$addFootnote(rowKey = result$rater, col = "kappa",
                                               sprintf("Error: %s", result$error))
                } else if (!is.na(result$kappa)) {
                    # Add interpretation
                    kappa_val <- result$kappa
                    if (kappa_val < 0.40) {
                        interp <- "Poor to fair - needs training"
                    } else if (kappa_val < 0.60) {
                        interp <- "Moderate - additional training recommended"
                    } else if (kappa_val < 0.75) {
                        interp <- "Substantial - acceptable"
                    } else {
                        interp <- "Excellent - certified/expert level"
                    }

                    pairwise_table$addFootnote(rowKey = result$rater, col = "kappa", interp)
                }
            }

            # Add overall summary note
            valid_kappas <- sapply(kappa_results, function(x) if(is.null(x$error)) x$kappa else NA)
            mean_kappa <- mean(valid_kappas, na.rm = TRUE)
            n_valid <- sum(!is.na(valid_kappas))

            if (n_valid > 0) {
                pairwise_table$setNote(
                    "summary",
                    sprintf("Average kappa across %d raters: %.3f. Raters are compared individually against the reference rater using Cohen's kappa.",
                            n_valid, mean_kappa)
                )
            }
        },

        .populateICCExplanation = function() {
            # Provide educational content about ICC

            html_content <- "
            <div style='font-family: Arial, sans-serif; max-width: 800px; line-height: 1.6;'>
                <div style='background: #f0f8ff; border-left: 4px solid #2196F3; padding: 15px; margin-bottom: 20px;'>
                    <h3 style='margin: 0 0 10px 0; color: #1976D2;'>What is ICC (Intraclass Correlation Coefficient)?</h3>
                    <p style='margin: 0; color: #333;'>
                        ICC measures the reliability and agreement of <strong>continuous measurements</strong>
                        between raters. It's the gold standard for assessing inter-rater reliability with numeric data.
                    </p>
                </div>

                <div style='background: #fff3e0; border-left: 4px solid #FF9800; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #F57C00;'>When to Use ICC</h4>
                    <p style='margin: 0 0 10px 0;'><strong>Use ICC for continuous measurements:</strong></p>
                    <ul style='margin: 0; padding-left: 20px;'>
                        <li><strong>Tumor size</strong> ‚Äì Diameter measurements in mm or cm</li>
                        <li><strong>Biomarker quantification</strong> ‚Äì Protein expression levels, cell counts</li>
                        <li><strong>Morphometric analysis</strong> ‚Äì Nuclear size, gland area measurements</li>
                        <li><strong>Scoring systems</strong> ‚Äì Continuous scores (e.g., 0-100 scale)</li>
                    </ul>
                </div>

                <div style='background: #e8f5e9; border-left: 4px solid #4CAF50; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #388E3C;'>ICC Model Selection Guide</h4>
                    <table style='width: 100%; border-collapse: collapse; font-size: 13px;'>
                        <tr style='background: #f5f5f5;'>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Model</th>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Design</th>
                            <th style='padding: 8px; text-align: left; border-bottom: 2px solid #388E3C;'>Use Case</th>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>ICC(1,1)</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>One-way random</td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Each case rated by different raters</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>ICC(2,1)</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Two-way random</td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>All cases rated by same raters (random sample)</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>ICC(3,1)</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Two-way mixed</td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>All cases rated by same raters (fixed panel)</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>ICC(1,k)</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>One-way, average</td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Reliability of mean of k different raters</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'><strong>ICC(2,k)</strong></td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Two-way random, average</td>
                            <td style='padding: 8px; border-bottom: 1px solid #ddd;'>Reliability of mean rating (random raters)</td>
                        </tr>
                        <tr>
                            <td style='padding: 8px;'><strong>ICC(3,k)</strong></td>
                            <td style='padding: 8px;'>Two-way mixed, average</td>
                            <td style='padding: 8px;'>Reliability of mean rating (fixed panel)</td>
                        </tr>
                    </table>
                    <p style='margin: 10px 0 0 0; font-size: 12px; color: #555;'>
                        <strong>Tip:</strong> Use ICC(2,1) or ICC(3,1) for most pathology studies where all cases are rated by the same pathologists.
                    </p>
                </div>

                <div style='background: #fce4ec; border-left: 4px solid #E91E63; padding: 15px; margin-bottom: 20px;'>
                    <h4 style='margin: 0 0 10px 0; color: #C2185B;'>Interpreting ICC Values</h4>
                    <table style='width: 100%; border-collapse: collapse;'>
                        <tr>
                            <td style='padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;'>ICC < 0.50</td>
                            <td style='padding: 5px; border-bottom: 1px solid #ddd;'>Poor reliability</td>
                        </tr>
                        <tr>
                            <td style='padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;'>0.50 ‚Äì 0.75</td>
                            <td style='padding: 5px; border-bottom: 1px solid #ddd;'>Moderate reliability</td>
                        </tr>
                        <tr>
                            <td style='padding: 5px; font-weight: bold; border-bottom: 1px solid #ddd;'>0.75 ‚Äì 0.90</td>
                            <td style='padding: 5px; border-bottom: 1px solid #ddd;'>Good reliability</td>
                        </tr>
                        <tr>
                            <td style='padding: 5px; font-weight: bold;'>ICC > 0.90</td>
                            <td style='padding: 5px;'>Excellent reliability</td>
                        </tr>
                    </table>
                </div>

                <div style='background: #e3f2fd; border-left: 4px solid #1976D2; padding: 15px;'>
                    <h4 style='margin: 0 0 10px 0; color: #1565C0;'>ICC vs Bland-Altman</h4>
                    <p style='margin: 0;'>
                        <strong>ICC</strong> quantifies reliability with a single coefficient (0-1).<br>
                        <strong>Bland-Altman</strong> visualizes agreement and detects systematic bias.<br>
                        <em>Best practice: Use both methods together for comprehensive assessment.</em>
                    </p>
                </div>

                <div style='margin-top: 15px; padding: 10px; background: #f5f5f5; border-radius: 4px;'>
                    <p style='margin: 0; font-size: 12px; color: #666;'>
                        <strong>Reference:</strong> Shrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater reliability.
                        <em>Psychological Bulletin</em>, 86(2), 420-428.
                    </p>
                </div>
            </div>
            "

            self$results$iccExplanation$setContent(html_content)
        },

        .calculateICC = function(ratings) {
            # Calculate Intraclass Correlation Coefficient for continuous data

            # Validate that data is numeric
            if (!all(sapply(ratings, is.numeric))) {
                self$results$iccTable$setNote(
                    "error",
                    "ICC requires continuous (numeric) data. Your data appears to be categorical. Use kappa instead."
                )
                return()
            }

            # Get ICC type
            icc_type <- self$options$iccType

            # Map user-friendly names to irr package format
            type_mapping <- list(
                icc11 = "oneway",
                icc21 = "agreement",
                icc31 = "consistency",
                icc1k = "oneway",
                icc21 = "agreement",
                icc3k = "consistency"
            )

            unit_mapping <- list(
                icc11 = "single",
                icc21 = "single",
                icc31 = "single",
                icc1k = "average",
                icc2k = "average",
                icc3k = "average"
            )

            # Get ICC type and unit parameters
            type_param <- type_mapping[[icc_type]]
            unit_param <- unit_mapping[[icc_type]]

            # Prepare data - remove rows with any missing values
            complete_idx <- complete.cases(ratings)
            ratings_clean <- ratings[complete_idx, , drop = FALSE]

            if (nrow(ratings_clean) < 2) {
                self$results$iccTable$setNote(
                    "error",
                    "Insufficient complete cases for ICC calculation. At least 2 complete cases required."
                )
                return()
            }

            # Calculate ICC
            tryCatch({
                icc_result <- irr::icc(
                    ratings_clean,
                    model = type_param,
                    type = unit_param,
                    unit = "single",
                    conf.level = 0.95
                )

                # Determine model name for display
                model_names <- list(
                    icc11 = "ICC(1,1) - One-way random, single",
                    icc21 = "ICC(2,1) - Two-way random, single",
                    icc31 = "ICC(3,1) - Two-way mixed, single",
                    icc1k = "ICC(1,k) - One-way random, average",
                    icc2k = "ICC(2,k) - Two-way random, average",
                    icc3k = "ICC(3,k) - Two-way mixed, average"
                )

                # Populate table
                self$results$iccTable$setRow(rowNo = 1, values = list(
                    model = model_names[[icc_type]],
                    subjects = nrow(ratings_clean),
                    raters = ncol(ratings_clean),
                    icc_value = icc_result$value,
                    ci_lower = icc_result$lbound,
                    ci_upper = icc_result$ubound,
                    f_value = icc_result$Fvalue,
                    df1 = icc_result$df1,
                    df2 = icc_result$df2,
                    p = icc_result$p.value
                ))

                # Add interpretation note
                icc_val <- icc_result$value
                if (icc_val < 0.50) {
                    interp <- "Poor reliability"
                } else if (icc_val < 0.75) {
                    interp <- "Moderate reliability"
                } else if (icc_val < 0.90) {
                    interp <- "Good reliability"
                } else {
                    interp <- "Excellent reliability"
                }

                self$results$iccTable$setNote(
                    "interpretation",
                    sprintf("Interpretation: %s (Koo & Li, 2016). Consider using Bland-Altman plot for visual assessment of agreement.", interp)
                )

            }, error = function(e) {
                self$results$iccTable$setNote(
                    "error",
                    sprintf("Error calculating ICC: %s", e$message)
                )
            })
        },

        .calculateGwetAC = function(ratings) {
            # Calculate Gwet's AC1 (unweighted) or AC2 (weighted) coefficient
            # More stable than kappa for high agreement or rare categories

            # Check if irrCAC package is available
            if (!requireNamespace("irrCAC", quietly = TRUE)) {
                self$results$gwetTable$setNote(
                    "error",
                    "The 'irrCAC' package is required for Gwet's AC calculation but is not installed. Please install it with: install.packages('irrCAC')"
                )
                return()
            }

            # Get weights option
            weights <- self$options$gwetWeights

            # Prepare data - irrCAC expects subjects in rows, raters in columns
            # Convert to matrix format, handling missing data
            n_subjects <- nrow(ratings)
            n_raters <- ncol(ratings)

            # Remove rows with all missing values
            complete_idx <- rowSums(!is.na(ratings)) > 0
            ratings_clean <- ratings[complete_idx, , drop = FALSE]

            if (nrow(ratings_clean) == 0) {
                self$results$gwetTable$setNote(
                    "error",
                    "No complete cases available for Gwet's AC calculation."
                )
                return()
            }

            # Convert to format expected by irrCAC
            # For categorical data, need to convert factors to character/numeric
            ratings_matrix <- as.matrix(ratings_clean)

            # Determine method name
            if (weights == "unweighted") {
                method_name <- "Gwet's AC1 (unweighted)"
                weight_param <- "unweighted"
            } else if (weights == "linear") {
                method_name <- "Gwet's AC2 (linear weights)"
                weight_param <- "linear"
            } else {
                method_name <- "Gwet's AC2 (quadratic weights)"
                weight_param <- "quadratic"
            }

            # Calculate Gwet's AC
            tryCatch({
                # Use gwet.ac.raw for raw agreement data
                result <- irrCAC::gwet.ac.raw(
                    ratings = ratings_matrix,
                    weights = weight_param,
                    conflev = 0.95,
                    N = Inf,  # Assume infinite population
                    print = FALSE
                )

                # Extract results
                coef <- result$est$coefficient
                se <- result$est$se
                ci_lower <- result$est$conf.int[1]
                ci_upper <- result$est$conf.int[2]

                # Calculate p-value (two-tailed test against H0: AC = 0)
                z_stat <- coef / se
                p_value <- 2 * (1 - pnorm(abs(z_stat)))

                # Populate table
                self$results$gwetTable$setRow(rowNo = 1, values = list(
                    method = method_name,
                    subjects = nrow(ratings_clean),
                    raters = n_raters,
                    coefficient = coef,
                    se = se,
                    ci_lower = ci_lower,
                    ci_upper = ci_upper,
                    p = p_value
                ))

                # Add interpretation note
                if (coef < 0) {
                    interp <- "Poor agreement (less than chance)"
                } else if (coef < 0.20) {
                    interp <- "Slight agreement"
                } else if (coef < 0.40) {
                    interp <- "Fair agreement"
                } else if (coef < 0.60) {
                    interp <- "Moderate agreement"
                } else if (coef < 0.80) {
                    interp <- "Substantial agreement"
                } else {
                    interp <- "Almost perfect agreement"
                }

                self$results$gwetTable$setNote(
                    "interpretation",
                    sprintf("Interpretation: %s. Gwet's AC is more robust than kappa when dealing with high agreement rates or unbalanced categories.", interp)
                )

            }, error = function(e) {
                self$results$gwetTable$setNote(
                    "error",
                    sprintf("Error calculating Gwet's AC: %s", e$message)
                )
            })
        },

        .populateLevelInfo = function(ratings) {
            # Display level ordering information for categorical variables
            # Essential for weighted kappa to verify ordinal levels are properly ordered

            table <- self$results$levelInfoTable

            # Process each rater variable
            for (i in seq_along(self$options$vars)) {
                var_name <- self$options$vars[i]
                var_data <- ratings[[i]]

                # Determine variable type and levels
                is_factor <- is.factor(var_data)
                is_ordered <- is.ordered(var_data)

                if (is_factor) {
                    levels_list <- levels(var_data)
                    n_levels <- length(levels_list)
                    levels_str <- paste(levels_list, collapse = " ‚Üí ")

                    if (is_ordered) {
                        data_type <- "Ordinal (ordered factor)"
                        note <- "‚úì Suitable for weighted kappa"
                    } else {
                        data_type <- "Nominal (unordered factor)"
                        note <- "Use unweighted kappa or convert to ordinal"
                    }
                } else {
                    # Numeric or other types
                    unique_vals <- sort(unique(var_data[!is.na(var_data)]))
                    n_levels <- length(unique_vals)

                    if (n_levels <= 20) {
                        levels_str <- paste(unique_vals, collapse = " ‚Üí ")
                    } else {
                        levels_str <- paste("Range:", min(unique_vals), "to", max(unique_vals))
                    }

                    if (is.numeric(var_data)) {
                        data_type <- "Numeric (continuous)"
                        note <- "Consider Bland-Altman instead of kappa"
                    } else {
                        data_type <- "Character/Other"
                        note <- "Convert to factor for kappa analysis"
                    }
                }

                # Add row to table
                table$addRow(rowKey = i, values = list(
                    variable = var_name,
                    levels = levels_str,
                    n_levels = n_levels,
                    data_type = data_type,
                    note = note
                ))
            }

            # Add informational note about weighted kappa
            if (any(sapply(ratings, is.ordered))) {
                table$setNote(
                    "weighted_info",
                    "For weighted kappa, verify that ordinal levels are in the correct order (e.g., G1 ‚Üí G2 ‚Üí G3 for tumor grades). Use jamovi's Data ‚Üí Setup tab to reorder levels if needed."
                )
            }
        },

        .populateBlandAltman = function(ratings) {
            # Generate Bland-Altman plot for continuous agreement analysis
            # Only applicable for 2 raters with numeric/continuous data

            # Validate inputs
            if (ncol(ratings) != 2) {
                self$results$blandAltman$setVisible(FALSE)
                self$results$blandAltmanStats$setVisible(FALSE)
                self$results$blandAltmanStats$setNote(
                    "error",
                    "Bland-Altman analysis requires exactly 2 raters. Please select only 2 variables."
                )
                return()
            }

            # Check if data is numeric/continuous
            if (!is.numeric(ratings[[1]]) || !is.numeric(ratings[[2]])) {
                self$results$blandAltman$setVisible(FALSE)
                self$results$blandAltmanStats$setVisible(FALSE)
                self$results$blandAltmanStats$setNote(
                    "error",
                    "Bland-Altman analysis requires continuous (numeric) data. Your data appears to be categorical."
                )
                return()
            }

            # Calculate Bland-Altman statistics
            rater1 <- ratings[[1]]
            rater2 <- ratings[[2]]

            # Remove missing values pairwise
            complete_idx <- complete.cases(ratings)
            rater1 <- rater1[complete_idx]
            rater2 <- rater2[complete_idx]

            # Calculate difference and mean
            diff <- rater1 - rater2
            avg <- (rater1 + rater2) / 2

            # Statistics
            mean_diff <- mean(diff, na.rm = TRUE)
            sd_diff <- sd(diff, na.rm = TRUE)

            # Limits of Agreement (LoA)
            conf_level <- self$options$baConfidenceLevel
            z_value <- qnorm((1 + conf_level) / 2)
            lower_loa <- mean_diff - z_value * sd_diff
            upper_loa <- mean_diff + z_value * sd_diff

            # Test for proportional bias (if requested)
            prop_bias_p <- NA
            if (self$options$proportionalBias) {
                tryCatch({
                    lm_result <- lm(diff ~ avg)
                    prop_bias_p <- summary(lm_result)$coefficients[2, 4]  # p-value for slope
                }, error = function(e) {
                    prop_bias_p <- NA
                })
            }

            # Populate statistics table
            self$results$blandAltmanStats$setRow(rowNo = 1, values = list(
                meanDiff = mean_diff,
                sdDiff = sd_diff,
                lowerLoA = lower_loa,
                upperLoA = upper_loa,
                propBiasP = prop_bias_p
            ))

            # Generate plot
            plot <- self$results$blandAltman
            plot$setState(list(
                diff = diff,
                avg = avg,
                mean_diff = mean_diff,
                lower_loa = lower_loa,
                upper_loa = upper_loa,
                conf_level = conf_level,
                prop_bias = self$options$proportionalBias,
                rater_names = names(ratings)
            ))
        },

        .calculateAgreementStatus = function(ratings) {
            # Calculate agreement status for each case
            # Categorizes as: All Agreed, Majority Agreed, No Agreement

            threshold <- self$options$agreementThreshold
            n_raters <- ncol(ratings)
            n_cases <- nrow(ratings)

            # Initialize results storage
            agreement_results <- data.frame(
                case_id = 1:n_cases,
                agreement_status = character(n_cases),
                agreement_percent = numeric(n_cases),
                modal_category = character(n_cases),
                n_agreeing = integer(n_cases),
                stringsAsFactors = FALSE
            )

            # Calculate for each case
            for (i in 1:n_cases) {
                row_ratings <- unlist(ratings[i, ])
                row_ratings <- row_ratings[!is.na(row_ratings)]

                if (length(row_ratings) == 0) {
                    agreement_results$agreement_status[i] <- "Missing Data"
                    agreement_results$agreement_percent[i] <- NA
                    agreement_results$modal_category[i] <- NA
                    agreement_results$n_agreeing[i] <- 0
                    next
                }

                # Find mode (most common rating)
                freq_table <- table(row_ratings)
                max_freq <- max(freq_table)
                modal_value <- names(freq_table)[which.max(freq_table)]

                # Calculate agreement percentage
                agree_pct <- (max_freq / length(row_ratings)) * 100

                # Categorize agreement status
                if (max_freq == length(row_ratings)) {
                    status <- "All Agreed"
                } else if (agree_pct >= threshold) {
                    status <- "Majority Agreed"
                } else {
                    status <- "No Agreement"
                }

                # Store results
                agreement_results$agreement_status[i] <- status
                agreement_results$agreement_percent[i] <- agree_pct / 100  # Convert to proportion for format: pc
                agreement_results$modal_category[i] <- modal_value
                agreement_results$n_agreeing[i] <- max_freq
            }

            # Populate case-level detail table
            detail_table <- self$results$agreementStatusDetail
            for (i in 1:n_cases) {
                detail_table$addRow(rowKey = i, values = list(
                    case_id = agreement_results$case_id[i],
                    agreement_status = agreement_results$agreement_status[i],
                    agreement_percent = agreement_results$agreement_percent[i],
                    modal_category = agreement_results$modal_category[i],
                    n_agreeing = agreement_results$n_agreeing[i]
                ))
            }

            # Calculate distribution summary (if requested)
            if (self$options$showAgreementTable) {
                status_counts <- table(agreement_results$agreement_status)
                status_table <- self$results$agreementStatusTable

                status_order <- c("All Agreed", "Majority Agreed", "No Agreement", "Missing Data")
                for (status in status_order) {
                    if (status %in% names(status_counts)) {
                        count <- as.integer(status_counts[status])
                        pct <- count / n_cases

                        # Interpretation text
                        interpretation <- switch(status,
                            "All Agreed" = "Complete consensus across all raters",
                            "Majority Agreed" = sprintf("‚â•%.0f%% of raters agree", threshold),
                            "No Agreement" = sprintf("<%.0f%% agreement - review recommended", threshold),
                            "Missing Data" = "Insufficient data for classification"
                        )

                        status_table$addRow(rowKey = status, values = list(
                            status = status,
                            count = count,
                            percentage = pct,
                            interpretation = interpretation
                        ))
                    }
                }
            }

            # Add agreement status variable to dataset (if requested)
            if (self$options$addAgreementStatus) {
                self$results$addAgreementStatus$setRowNums(1:n_cases)
                self$results$addAgreementStatus$setValues(agreement_results$agreement_status)
            }

            # Update computed variables info
            if (self$options$consensusVar || self$options$addAgreementStatus) {
                private$.updateComputedVariablesInfo()
            }
        },

        .createConsensusVariable = function(ratings) {
            # Create consensus (modal) rating variable based on selected rule

            consensus_rule <- self$options$consensusRule
            tie_breaker <- self$options$tieBreaker
            n_cases <- nrow(ratings)
            n_raters <- ncol(ratings)

            # Initialize consensus vector
            consensus <- character(n_cases)

            # Calculate threshold based on rule
            threshold <- switch(consensus_rule,
                "majority" = 0.5,        # >50%
                "supermajority" = 0.75,  # ‚â•75%
                "unanimous" = 1.0        # 100%
            )

            # Calculate for each case
            for (i in 1:n_cases) {
                row_ratings <- unlist(ratings[i, ])
                row_ratings <- row_ratings[!is.na(row_ratings)]

                if (length(row_ratings) == 0) {
                    consensus[i] <- NA
                    next
                }

                # Find mode and frequency
                freq_table <- table(row_ratings)
                max_freq <- max(freq_table)
                modes <- names(freq_table)[freq_table == max_freq]

                # Calculate agreement proportion
                agree_prop <- max_freq / length(row_ratings)

                # Check if consensus meets threshold
                if (agree_prop >= threshold) {
                    if (length(modes) == 1) {
                        # Single mode - use it
                        consensus[i] <- modes[1]
                    } else {
                        # Tie - apply tie breaker
                        consensus[i] <- switch(tie_breaker,
                            "exclude" = NA,
                            "first" = modes[1],
                            "lowest" = {
                                # Convert to numeric if possible for comparison
                                numeric_modes <- suppressWarnings(as.numeric(modes))
                                if (all(!is.na(numeric_modes))) {
                                    modes[which.min(numeric_modes)]
                                } else {
                                    # For character data, use alphabetical order
                                    modes[order(modes)[1]]
                                }
                            },
                            "highest" = {
                                # Convert to numeric if possible for comparison
                                numeric_modes <- suppressWarnings(as.numeric(modes))
                                if (all(!is.na(numeric_modes))) {
                                    modes[which.max(numeric_modes)]
                                } else {
                                    # For character data, use alphabetical order
                                    modes[order(modes, decreasing = TRUE)[1]]
                                }
                            },
                            NA  # Default: exclude
                        )
                    }
                } else {
                    # Does not meet threshold
                    consensus[i] <- NA
                }
            }

            # Populate consensus summary table
            consensus_no_na <- consensus[!is.na(consensus)]
            if (length(consensus_no_na) > 0) {
                freq_table <- table(consensus_no_na)
                consensus_table <- self$results$consensusTable

                # Calculate average agreement for each consensus category
                for (category in names(freq_table)) {
                    # Find cases with this consensus category
                    matching_cases <- which(consensus == category)

                    # Calculate average agreement percentage for these cases
                    avg_agreement <- mean(sapply(matching_cases, function(case_idx) {
                        row_ratings <- unlist(ratings[case_idx, ])
                        row_ratings <- row_ratings[!is.na(row_ratings)]
                        if (length(row_ratings) == 0) return(NA)
                        max_freq <- max(table(row_ratings))
                        return(max_freq / length(row_ratings))
                    }), na.rm = TRUE)

                    consensus_table$addRow(rowKey = category, values = list(
                        category = category,
                        consensus_count = as.integer(freq_table[category]),
                        percentage = as.integer(freq_table[category]) / n_cases,
                        avg_agreement = avg_agreement
                    ))
                }
            }

            # Add consensus variable to dataset (if requested)
            if (self$options$consensusVar) {
                self$results$consensusVar$setRowNums(1:n_cases)
                self$results$consensusVar$setValues(consensus)
            }

            # Update computed variables info
            if (self$options$consensusVar || self$options$addAgreementStatus) {
                private$.updateComputedVariablesInfo()
            }
        },

        .updateComputedVariablesInfo = function() {
            # Generate HTML info about computed variables added to dataset

            info_html <- "<div style='font-family: Arial, sans-serif; padding: 10px;'>"
            info_html <- paste0(info_html, "<h4 style='margin-top: 0;'>Computed Variables Added:</h4><ul>")

            if (self$options$consensusVar) {
                var_name <- self$options$consensusName
                rule_text <- switch(self$options$consensusRule,
                    "majority" = "Simple Majority (>50%)",
                    "supermajority" = "Supermajority (‚â•75%)",
                    "unanimous" = "Unanimous (100%)"
                )
                info_html <- paste0(info_html,
                    "<li><strong>", var_name, "</strong>: Consensus rating (", rule_text, ")</li>")
            }

            if (self$options$addAgreementStatus) {
                var_name <- self$options$agreementStatusName
                threshold <- self$options$agreementThreshold
                info_html <- paste0(info_html,
                    "<li><strong>", var_name, "</strong>: Agreement status (threshold: ", threshold, "%)</li>")
            }

            if (self$options$loaVariable) {
                var_name <- self$options$loaVariableName
                method_text <- switch(self$options$loaThresholds,
                    "custom" = paste0("Custom (High: ", self$options$loaHighThreshold, "%, Low: ", self$options$loaLowThreshold, "%)"),
                    "quartiles" = "Quartile-based (data-driven)",
                    "tertiles" = "Tertile-based (data-driven)"
                )
                info_html <- paste0(info_html,
                    "<li><strong>", var_name, "</strong>: Level of Agreement (", method_text, ")</li>")
            }

            info_html <- paste0(info_html, "</ul>")
            info_html <- paste0(info_html,
                "<p style='color: #666; font-size: 12px;'>",
                "These variables are now available in your dataset for further analysis.",
                "</p></div>")

            self$results$computedVariablesInfo$setContent(info_html)
        },

        .calculateLevelOfAgreement = function(ratings) {
            # Calculate level of agreement (LoA) for each case
            # Categorizes cases as Absolute/High/Moderate/Low/Poor based on agreement proportion

            n_cases <- nrow(ratings)
            n_raters <- ncol(ratings)

            # Get thresholds
            method <- self$options$loaThresholds

            # Initialize LoA categories vector
            loa_categories <- character(n_cases)
            agreement_pcts <- numeric(n_cases)
            modal_ratings <- character(n_cases)
            n_agreeing_vec <- integer(n_cases)

            # Calculate agreement proportion for each case
            for (i in 1:n_cases) {
                row_ratings <- unlist(ratings[i, ])
                row_ratings <- row_ratings[!is.na(row_ratings)]

                if (length(row_ratings) == 0) {
                    loa_categories[i] <- NA
                    agreement_pcts[i] <- NA
                    modal_ratings[i] <- NA
                    n_agreeing_vec[i] <- NA
                    next
                }

                # Find mode and frequency
                freq_table <- table(row_ratings)
                max_freq <- max(freq_table)
                modes <- names(freq_table)[freq_table == max_freq]

                # Calculate agreement proportion
                agree_pct <- max_freq / length(row_ratings)
                agreement_pcts[i] <- agree_pct
                n_agreeing_vec[i] <- max_freq
                modal_ratings[i] <- modes[1]  # Use first mode if tie
            }

            # Determine thresholds based on method
            if (method == "custom") {
                high_threshold <- self$options$loaHighThreshold / 100
                low_threshold <- self$options$loaLowThreshold / 100
                moderate_threshold <- (high_threshold + low_threshold) / 2
            } else if (method == "quartiles") {
                # Data-driven quartile approach
                valid_pcts <- agreement_pcts[!is.na(agreement_pcts)]
                quartiles <- quantile(valid_pcts, probs = c(0.25, 0.50, 0.75), na.rm = TRUE)
                low_threshold <- quartiles[1]
                moderate_threshold <- quartiles[2]
                high_threshold <- quartiles[3]
            } else if (method == "tertiles") {
                # Data-driven tertile approach
                valid_pcts <- agreement_pcts[!is.na(agreement_pcts)]
                tertiles <- quantile(valid_pcts, probs = c(1/3, 2/3), na.rm = TRUE)
                low_threshold <- tertiles[1]
                moderate_threshold <- tertiles[1]  # Same as low for tertiles
                high_threshold <- tertiles[2]
            }

            # Categorize each case
            for (i in 1:n_cases) {
                if (is.na(agreement_pcts[i])) {
                    loa_categories[i] <- NA
                } else if (agreement_pcts[i] == 1.0) {
                    loa_categories[i] <- "Absolute"
                } else if (agreement_pcts[i] >= high_threshold) {
                    loa_categories[i] <- "High"
                } else if (agreement_pcts[i] >= moderate_threshold) {
                    loa_categories[i] <- "Moderate"
                } else if (agreement_pcts[i] >= low_threshold) {
                    loa_categories[i] <- "Low"
                } else {
                    loa_categories[i] <- "Poor"
                }
            }

            # Populate LoA distribution table (if requested)
            if (self$options$showLoaTable) {
                loa_table <- self$results$loaTable

                # Count cases in each category
                category_counts <- table(loa_categories, useNA = "no")
                category_order <- c("Absolute", "High", "Moderate", "Low", "Poor")

                for (category in category_order) {
                    if (category %in% names(category_counts)) {
                        count <- as.integer(category_counts[category])
                        pct <- count / n_cases

                        # Interpretation text
                        interpretation <- switch(category,
                            "Absolute" = "Perfect agreement (100%) - all raters concur",
                            "High" = "Strong agreement - most raters concur, minimal disagreement",
                            "Moderate" = "Moderate agreement - majority concur, some disagreement",
                            "Low" = "Weak agreement - considerable disagreement present",
                            "Poor" = "Poor agreement - substantial disagreement, difficult case"
                        )

                        loa_table$addRow(rowKey = category, values = list(
                            loa_category = category,
                            count = count,
                            percentage = pct,
                            interpretation = interpretation
                        ))
                    }
                }
            }

            # Populate case-level detail table
            detail_table <- self$results$loaDetailTable
            for (i in 1:n_cases) {
                detail_table$addRow(rowKey = i, values = list(
                    case_id = i,
                    loa_category = if (!is.na(loa_categories[i])) loa_categories[i] else "N/A",
                    agreement_pct = agreement_pcts[i],
                    modal_rating = if (!is.na(modal_ratings[i])) modal_ratings[i] else "N/A",
                    n_agreeing = n_agreeing_vec[i]
                ))
            }

            # Add LoA variable to dataset (if requested)
            if (self$options$loaVariable) {
                self$results$loaOutput$setRowNums(1:n_cases)
                self$results$loaOutput$setValues(loa_categories)
            }

            # Update computed variables info
            if (self$options$consensusVar || self$options$addAgreementStatus || self$options$loaVariable) {
                private$.updateComputedVariablesInfo()
            }
        },

        .calculateHierarchicalKappa = function(ratings, cluster_data) {
            # Hierarchical/Multilevel Kappa Analysis for nested data structures
            # TODO: Full implementation requires multilevel modeling (lme4/nlme packages)

            # Validate cluster variable
            if (is.null(cluster_data) || length(cluster_data) == 0) {
                self$results$hierarchicalOverallTable$setNote(
                    "error",
                    "Please select a cluster/institution variable to perform hierarchical analysis."
                )
                return()
            }

            # For now, provide informative placeholder
            self$results$hierarchicalOverallTable$setNote(
                "info",
                "Hierarchical/Multilevel Kappa analysis is currently under development. This advanced feature will provide: (1) Overall kappa accounting for clustering, (2) Cluster-specific estimates with shrinkage, (3) Variance decomposition (between/within clusters), (4) ICC decomposition, (5) Homogeneity testing across sites. Full implementation coming in next release."
            )

            # TODO: Implement the following components when complete:
            # - Overall hierarchical kappa using multilevel model
            # - Cluster-specific kappa estimates
            # - Variance component decomposition (between-cluster, within-cluster)
            # - Hierarchical ICC (ICC1, ICC2, ICC3)
            # - Homogeneity test (Breslow-Day or Q-statistic)
            # - Empirical Bayes shrinkage estimates
            # - Cluster rankings with confidence intervals
        },

        .populateHierarchicalExplanation = function() {
            # Generate comprehensive HTML explanation for hierarchical kappa

            html <- "<div style='font-family: Arial, sans-serif; padding: 15px; line-height: 1.6;'>"

            html <- paste0(html, "
                <h3 style='color: #2E5090; margin-top: 0;'>Hierarchical/Multilevel Kappa Analysis</h3>

                <div style='background-color: #F0F7FF; padding: 12px; border-left: 4px solid #2E5090; margin-bottom: 15px;'>
                    <strong>What is it?</strong><br/>
                    Hierarchical kappa extends standard kappa to account for nested data structures where raters
                    are grouped within clusters (institutions, centers, scanners). It decomposes agreement into
                    between-cluster and within-cluster components, providing more accurate estimates when clustering exists.
                </div>

                <h4 style='color: #2E5090; margin-top: 20px;'>When to Use</h4>
                <ul>
                    <li><strong>Multi-center trials</strong>: Pathologists nested within hospitals</li>
                    <li><strong>Multi-scanner studies</strong>: Radiologists nested within imaging centers</li>
                    <li><strong>Training programs</strong>: Residents nested within training sites</li>
                    <li><strong>Quality control</strong>: Identifying institutions with poor agreement</li>
                </ul>

                <h4 style='color: #2E5090; margin-top: 20px;'>Key Components</h4>
                <ul>
                    <li><strong>Overall Hierarchical Kappa</strong>: Population-level agreement accounting for clustering</li>
                    <li><strong>Cluster-Specific Estimates</strong>: Kappa for each institution/center</li>
                    <li><strong>Variance Decomposition</strong>: Between-cluster vs within-cluster variance</li>
                    <li><strong>Homogeneity Testing</strong>: Are all clusters performing equally?</li>
                    <li><strong>Shrinkage Estimates</strong>: Stabilized estimates for small clusters</li>
                </ul>

                <h4 style='color: #2E5090; margin-top: 20px;'>Interpreting Variance Components</h4>
                <ul>
                    <li><strong>High between-cluster variance</strong>: Institutional differences (protocols, training)</li>
                    <li><strong>High within-cluster variance</strong>: Local rater disagreement</li>
                    <li><strong>Shrinkage</strong>: Pulls extreme cluster estimates toward overall mean</li>
                </ul>

                <div style='background-color: #FFF9E6; padding: 12px; border-left: 4px solid #FFA500; margin-top: 15px;'>
                    <strong>Note:</strong> Full hierarchical analysis is computationally intensive and requires sufficient
                    data within each cluster (typically ‚â•10 cases per cluster recommended).
                </div>
            </div>")

            self$results$hierarchicalExplanation$setContent(html)
        },

        .run = function() {

        # Validate input ----
        if (is.null(self$options$vars) || length(self$options$vars) < 2) {
            self$results$welcome$setVisible(TRUE)
            self$results$welcome$setContent(
                "<div style='font-family: Arial, sans-serif; max-width: 800px; line-height: 1.4;'>
                <div style='background: #f5f5f5; border: 2px solid #333; padding: 20px; margin-bottom: 20px;'>
                <h2 style='margin: 0 0 10px 0; font-size: 18px; color: #333;'>Interrater Reliability Analysis</h2>
                <p style='margin: 0; font-size: 14px; color: #666;'>Measure agreement between multiple raters scoring the same cases</p>
                </div>

                <div style='background: #f9f9f9; border-left: 4px solid #333; padding: 15px; margin-bottom: 20px;'>
                <h3 style='margin: 0 0 10px 0; color: #333; font-size: 16px;'>Setup Progress</h3>
                <div style='margin-bottom: 10px; font-size: 14px;'>
                [ ] Raters: 0/2 minimum - Select at least 2 rater variables to begin
                </div>
                </div>

                <table style='width: 100%; border-collapse: collapse; margin-bottom: 20px;'>
                <tr>
                <td style='width: 50%; border: 1px solid #ccc; padding: 15px; vertical-align: top;'>
                <h4 style='margin: 0 0 10px 0; font-size: 15px;'>Quick Start</h4>
                <ol style='margin: 0; padding-left: 20px; font-size: 14px;'>
                <li>Select 2+ rater variables</li>
                <li>For ordinal data, choose weighted kappa</li>
                <li>Enable optional outputs as needed</li>
                </ol>
                </td>
                <td style='width: 50%; border: 1px solid #ccc; padding: 15px; vertical-align: top;'>
                <h4 style='margin: 0 0 10px 0; font-size: 15px;'>Available Methods</h4>
                <ul style='margin: 0; padding-left: 20px; font-size: 14px;'>
                <li><strong>Cohen's kappa:</strong> 2 raters</li>
                <li><strong>Fleiss' kappa:</strong> 3+ raters</li>
                <li><strong>Krippendorff's alpha:</strong> Alternative measure</li>
                </ul>
                </td>
                </tr>
                </table>
                </div>"
            )
            return()
        } else {
            self$results$welcome$setVisible(FALSE)
            if (nrow(self$data) == 0) stop("Data contains no (complete) rows")

            # Data preparation ----
            exct <- self$options$exct
            wght <- self$options$wght
            mydata <- self$data

            # Safe variable selection - ensure proper data frame structure
            # Note: Variable names with spaces (e.g., "Rater 1") are handled correctly
            # by R's bracket notation. No special escaping needed for data access.
            ratings <- mydata[, self$options$vars, drop = FALSE]

            # Ensure it's a proper data frame
            if (!is.data.frame(ratings)) {
                ratings <- as.data.frame(ratings)
            }

            # Preserve jamovi attributes if they exist
            if (!is.null(attr(mydata, "jmv-desc"))) {
                # Only copy attributes for selected columns
                desc_attr <- attr(mydata, "jmv-desc")
                if (length(desc_attr) > 0) {
                    selected_desc <- desc_attr[names(desc_attr) %in% self$options$vars]
                    if (length(selected_desc) > 0) {
                        attr(ratings, "jmv-desc") <- selected_desc
                    }
                }
            }

            # Check for missing values and notify user ----
            if (any(is.na(ratings))) {
                n_total <- nrow(ratings)
                n_complete <- sum(complete.cases(ratings))
                n_missing <- n_total - n_complete
                pct_missing <- round(100 * n_missing / n_total, 1)

                self$results$irrtable$setNote(
                    "missing",
                    sprintf("Note: %d of %d cases excluded due to missing values (%.1f%%). Analysis based on %d complete cases.",
                            n_missing, n_total, pct_missing, n_complete)
                )
            }

            # Level Ordering Information (if requested) ----
            if (self$options$showLevelInfo) {
                private$.populateLevelInfo(ratings)
            }

            # 2 raters: Cohen's kappa ----
            if (length(self$options$vars) == 2) {
                xorder <- unlist(lapply(ratings, is.ordered))

                if (wght %in% c("equal", "squared") && !all(xorder == TRUE)) {
                    stop("Weighted kappa requires ordinal variables. Please select ordinal data or choose 'Unweighted'.")
                }

                if (exct == TRUE) {
                    stop("Exact kappa requires at least 3 raters. Please add more rater variables or disable 'Exact Kappa'.")
                }


                # irr::kappa2 ----
                result2 <- irr::kappa2(ratings = ratings, weight = wght)

            # >=3 raters: Fleiss kappa ----
            } else if (length(self$options$vars) >= 3) {
                result2 <- irr::kappam.fleiss(ratings = ratings, exact = exct, detail = TRUE)
            }


            # Percentage agreement ----
            result1 <- irr::agree(ratings)
            if (result1[["value"]] > 100) {
                result1[["value"]] <- "Please check the data. It seems that observers do not agree on any cases"
            }

            # Populate main results table ----
            table2 <- self$results$irrtable

            # Note: Exact kappa (Conger's) does not provide z-statistic or p-value
            # Only Fleiss' formulation allows testing H0: Kappa=0
            z_stat <- if (!is.null(result2[["statistic"]])) result2[["statistic"]] else NA
            p_val <- if (!is.null(result2[["p.value"]])) result2[["p.value"]] else NA

            table2$setRow(rowNo = 1, values = list(
                method = result2[["method"]],
                subjects = result1[["subjects"]],
                raters = result1[["raters"]],
                peragree = result1[["value"]],
                kappa = result2[["value"]],
                z = z_stat,
                p = p_val
            ))

            # Add note if exact kappa was used (no statistical test)
            if (exct && length(self$options$vars) >= 3) {
                table2$setNote(
                    "exact_note",
                    "Note: Exact Kappa (Conger, 1980) does not provide statistical test. Use Fleiss' Kappa for hypothesis testing (H0: Kappa=0)."
                )
            }

            # Frequency tables (if requested) ----
            # Control visibility based on number of raters and sft option
            num_raters <- length(self$options$vars)

            if (self$options$sft) {
                # Show contingency table only for 2 raters
                self$results$contingencyTable$setVisible(num_raters == 2)

                # Show rating combinations table only for 3+ raters
                self$results$ratingCombinationsTable$setVisible(num_raters >= 3)

                # For 2 raters, create a 2x2 contingency table
                if (num_raters == 2) {
                    # Create contingency table
                    cont_table <- table(ratings[[1]], ratings[[2]])

                    # Get row and column names
                    row_names <- rownames(cont_table)
                    col_names <- colnames(cont_table)

                    # Create jamovi table with dynamic columns
                    contTable <- self$results$contingencyTable

                    # Add row name column
                    # Note: names(ratings)[1] preserves original variable name with spaces
                    contTable$addColumn(
                        name = 'rater1',
                        title = names(ratings)[1],  # Original name (e.g., "Rater 1")
                        type = 'text'
                    )

                    # Add columns for each category of rater 2
                    # Note: use make.names() for safe column IDs, display original labels
                    for (col_name in col_names) {
                        col_id <- make.names(col_name)
                        contTable$addColumn(
                            name = col_id,
                            title = as.character(col_name),  # Original category label
                            type = 'integer'
                        )
                    }

                    # Add row total column
                    contTable$addColumn(
                        name = 'row_total',
                        title = 'Total',
                        type = 'integer'
                    )

                    # Populate data rows
                    for (i in seq_along(row_names)) {
                        row_data <- list(rater1 = as.character(row_names[i]))

                        for (j in seq_along(col_names)) {
                            col_id <- make.names(col_names[j])
                            row_data[[col_id]] <- as.integer(cont_table[i, j])
                        }

                        row_data$row_total <- sum(cont_table[i, ])
                        contTable$addRow(rowKey = i, values = row_data)
                    }

                    # Add total row
                    total_row <- list(rater1 = 'Total')
                    for (j in seq_along(col_names)) {
                        col_id <- make.names(col_names[j])
                        total_row[[col_id]] <- sum(cont_table[, j])
                    }
                    total_row$row_total <- sum(cont_table)
                    contTable$addRow(rowKey = 'total', values = total_row)

                    # Add note about percentages
                    contTable$setNote(
                        'interpretation',
                        sprintf('N = %d cases. Cell counts show frequency of agreement/disagreement patterns.',
                                sum(cont_table))
                    )

                } else {
                    # For 3+ raters, show combination counts
                    freq_table <- ratings %>%
                        dplyr::group_by_all() %>%
                        dplyr::count() %>%
                        as.data.frame()

                    # Create jamovi table with dynamic columns
                    combTable <- self$results$ratingCombinationsTable

                    # Add columns for each rater
                    # Note: use make.names() for jamovi column IDs, but keep original
                    # variable names (with spaces) as titles for display
                    for (var_name in self$options$vars) {
                        col_id <- make.names(var_name)
                        combTable$addColumn(
                            name = col_id,
                            title = var_name,  # Original name with spaces preserved
                            type = 'text'
                        )
                    }

                    # Add count column
                    combTable$addColumn(
                        name = 'count',
                        title = 'Count',
                        type = 'integer'
                    )

                    # Populate rows (limit to 100 most frequent combinations)
                    max_display <- min(100, nrow(freq_table))

                    for (i in 1:max_display) {
                        row_data <- list()

                        # Add rater values
                        # Note: freq_table column names preserve spaces from original variables
                        for (var_name in self$options$vars) {
                            col_id <- make.names(var_name)
                            value <- freq_table[i, var_name, drop = TRUE]  # Explicit drop for clarity
                            row_data[[col_id]] <- as.character(value)
                        }

                        # Add count
                        row_data$count <- as.integer(freq_table$n[i])

                        combTable$addRow(rowKey = i, values = row_data)
                    }

                    # Add note
                    if (nrow(freq_table) > max_display) {
                        combTable$setNote(
                            'truncated',
                            sprintf('Showing %d of %d unique rating combinations. Total: %d ratings.',
                                    max_display, nrow(freq_table), sum(freq_table$n))
                        )
                    } else {
                        combTable$setNote(
                            'complete',
                            sprintf('%d unique rating combinations. Total: %d ratings.',
                                    nrow(freq_table), sum(freq_table$n))
                        )
                    }
                }
            } else {
                # Hide both frequency tables when sft is disabled
                self$results$contingencyTable$setVisible(FALSE)
                self$results$ratingCombinationsTable$setVisible(FALSE)
            }

            # Weighted Kappa Guide (if using weights) ----
            if (wght != "unweighted") {
                weight_guide <- private$.createWeightedKappaGuide(wght)
                self$results$weightedKappaGuide$setContent(weight_guide)
            }

            # Natural-language summary (if requested) ----
            if (self$options$showSummary) {
                summary_text <- private$.createSummary(result1, result2, wght, exct)
                self$results$summary$setContent(summary_text)
            }

            # About panel (if requested) ----
            if (self$options$showAbout) {
                about_text <- private$.createAboutPanel()
                self$results$about$setContent(about_text)

                # Clinical use cases guide
                private$.populateClinicalUseCases()
            }
        }



        # Krippendorff's Alpha (if requested) ----
        if (self$options$kripp) {
            # Convert ratings data frame to matrix
            ratings_matrix <- as.matrix(ratings)

            # Ensure numeric conversion if needed
            if (!is.numeric(ratings_matrix)) {
                # If categorical/factor data, convert to numeric codes
                ratings_matrix <- matrix(
                    as.numeric(factor(ratings_matrix)),
                    nrow = nrow(ratings_matrix),
                    ncol = ncol(ratings_matrix)
                )
            }

            # Add error handling
            tryCatch({
                # Calculate Krippendorff's alpha
                kripp_result <- irr::kripp.alpha(
                    ratings_matrix,
                    method = self$options$krippMethod
                )

                # Initialize values list for table
                values_list <- list(
                    method = paste0("Krippendorff's Alpha (", self$options$krippMethod, ")"),
                    subjects = nrow(ratings_matrix),
                    raters = ncol(ratings_matrix),
                    alpha = kripp_result$value
                )

                # Calculate bootstrap CI if requested
                if (self$options$bootstrap) {
                    set.seed(123) # for reproducibility
                    n_boot <- 1000
                    alpha_boots <- numeric(n_boot)

                    for(i in 1:n_boot) {
                        boot_indices <- sample(1:nrow(ratings_matrix), replace = TRUE)
                        boot_data <- ratings_matrix[boot_indices,]

                        boot_alpha <- try(irr::kripp.alpha(boot_data,
                                                           method = self$options$krippMethod)$value,
                                          silent = TRUE)

                        if(!inherits(boot_alpha, "try-error")) {
                            alpha_boots[i] <- boot_alpha
                        }
                    }

                    # Calculate 95% confidence intervals
                    ci <- quantile(alpha_boots, c(0.025, 0.975), na.rm = TRUE)

                    # Add CI values to list
                    values_list$ci_lower <- ci[1]
                    values_list$ci_upper <- ci[2]
                }

                # Populate results table
                krippTable <- self$results$krippTable
                krippTable$setRow(rowNo = 1, values = values_list)

            }, error = function(e) {
                # Handle any errors that occur during calculation
                errorMessage <- paste("Error calculating Krippendorff's alpha:", e$message)
                warning(errorMessage)

                # Initialize values list for error case
                values_list <- list(
                    method = paste0("Krippendorff's Alpha (", self$options$krippMethod, ")"),
                    subjects = nrow(ratings_matrix),
                    raters = ncol(ratings_matrix),
                    alpha = NA
                )

                if (self$options$bootstrap) {
                    values_list$ci_lower <- NA
                    values_list$ci_upper <- NA
                }

                # Populate table with NA values
                krippTable <- self$results$krippTable
                krippTable$setRow(rowNo = 1, values = values_list)

                # Add error message as footnote
                krippTable$addFootnote(rowNo = 1, col = "alpha", paste0("Error calculating Krippendorff's alpha: ", e$message))
            })
        }

        # Light's Kappa (if requested) ----
        if (self$options$lightKappa) {
            private$.populateLightKappaExplanation()
            private$.calculateLightKappa(ratings)
        }

        # Kendall's W (if requested) ----
        if (self$options$kendallW) {
            private$.populateKendallWExplanation()
            private$.calculateKendallW(ratings)
        }

        # Rater Bias Test (if requested) ----
        if (self$options$raterBias) {
            private$.populateRaterBiasExplanation()
            private$.calculateRaterBias(ratings)
        }

        # Pairwise Kappa Analysis (if requested) ----
        if (self$options$pairwiseKappa) {
            private$.populatePairwiseKappaExplanation()

            # Get reference rater data
            if (!is.null(self$options$referenceRater)) {
                reference_ratings <- jmvcore::select(mydata, self$options$referenceRater)
                private$.calculatePairwiseKappa(ratings, reference_ratings)
            } else {
                self$results$pairwiseKappaTable$setNote(
                    "error",
                    "Please select a reference rater variable to perform pairwise kappa analysis."
                )
            }
        }

        # Gwet's AC1/AC2 (if requested) ----
        if (self$options$gwet) {
            private$.populateGwetExplanation()
            private$.calculateGwetAC(ratings)
        }

        # ICC (if requested) ----
        if (self$options$icc) {
            private$.populateICCExplanation()
            private$.calculateICC(ratings)
        }

        # Agreement Status Calculation (if requested) ----
        if (self$options$agreementStatus) {
            private$.calculateAgreementStatus(ratings)
        }

        # Consensus Variable Calculation (if requested) ----
        if (self$options$consensusVar) {
            private$.createConsensusVariable(ratings)
        }

        # Level of Agreement Variable (if requested) ----
        if (self$options$loaVariable) {
            private$.calculateLevelOfAgreement(ratings)
        }

        # Hierarchical/Multilevel Kappa (if requested) ----
        if (self$options$hierarchicalKappa) {
            # Populate explanation if requested
            if (self$options$showAbout) {
                private$.populateHierarchicalExplanation()
            }

            # Get cluster variable data
            cluster_data <- NULL
            if (!is.null(self$options$clusterVariable)) {
                cluster_data <- jmvcore::select(mydata, self$options$clusterVariable)
            }

            private$.calculateHierarchicalKappa(ratings, cluster_data)
        }

        # Bland-Altman analysis (if requested) ----
        if (self$options$blandAltmanPlot) {
            private$.populateBlandAltman(ratings)
        }

        }  # End of .run function

    ),  # End of private list

    public = list(
        #' @description
        #' Generate R source code for Interrater Reliability analysis
        #' @return Character string with R syntax for reproducible analysis outside jamovi
        # TEMPORARILY COMMENTED OUT TO PREVENT ERRORS
        # asSource = function() {
        #     vars <- self$options$vars
        #
        #     # Return empty string if insufficient variables
        #     if (is.null(vars) || length(vars) < 2) {
        #         return('')
        #     }

            # Escape variable names that need backticks
            vars_escaped <- sapply(vars, function(v) {
                private$.escapeVariableName(v)
            })

            # Build vars argument for function call
            # Each variable name is quoted, with backticks if needed
            vars_arg <- paste0('vars = c(',
                             paste(sapply(vars_escaped, function(v) {
                                 # If already has backticks, preserve them in quotes
                                 if (grepl("^`.*`$", v)) {
                                     paste0('"', v, '"')
                                 } else {
                                     paste0('"', v, '"')
                                 }
                             }), collapse = ', '),
                             ')')

            # Build other arguments
            args_list <- c(vars_arg)

            # Add weighted kappa option if not default
            if (self$options$wght != "unweighted") {
                wght_arg <- paste0('wght = "', self$options$wght, '"')
                args_list <- c(args_list, wght_arg)
            }

            # Add exact kappa option if enabled
            if (self$options$exct) {
                args_list <- c(args_list, 'exct = TRUE')
            }

            # Add Krippendorff's alpha options if enabled
            if (self$options$kripp) {
                args_list <- c(args_list, 'kripp = TRUE')

                if (self$options$krippMethod != "nominal") {
                    args_list <- c(args_list, paste0('krippMethod = "', self$options$krippMethod, '"'))
                }

                if (self$options$bootstrap) {
                    args_list <- c(args_list, 'bootstrap = TRUE')
                }
            }

            # Add display options if enabled
            if (self$options$sft) {
                args_list <- c(args_list, 'sft = TRUE')
            }

            if (self$options$showSummary) {
                args_list <- c(args_list, 'showSummary = TRUE')
            }

            # Build final R code
            code <- paste0(
                '# Interrater Reliability Analysis\n',
                '# Generated by ClinicoPath jamovi module\n\n',
                'library(ClinicoPath)\n\n',
                '# Load your data\n',
                '# data <- read.csv("your_data.csv")\n\n',
                'agreement(\n',
                '    data = data,\n',
                '    ', paste(args_list, collapse = ',\n    '),
                '\n)'
            )

            return(code)
        }
    )  # End of public list
    )
