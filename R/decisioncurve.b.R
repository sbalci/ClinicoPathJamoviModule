#' @title Decision Curve Analysis
#' @importFrom R6 R6Class
#' @import jmvcore
#' @importFrom ggplot2 ggplot aes geom_line geom_ribbon geom_vline geom_hline
#' @importFrom ggplot2 labs theme_minimal scale_color_brewer annotate xlim ylim
#' @importFrom ggplot2 scale_x_continuous geom_text geom_bar facet_wrap scale_fill_manual
#' @importFrom ggplot2 scale_x_discrete element_text
#' @importFrom dplyr filter mutate group_by summarise arrange
#' @importFrom tidyr gather
#' @importFrom stats quantile complete.cases

decisioncurveClass <- if (requireNamespace("jmvcore")) R6::R6Class(
    "decisioncurveClass",
    inherit = decisioncurveBase,
    private = list(

        # Store analysis results
        .dcaResults = NULL,
        .plotData = NULL,
        .clinicalImpactData = NULL,
        
        # Constants for default values and thresholds
        DECISIONCURVE_DEFAULTS = list(
            selected_thresholds = c(0.05, 0.10, 0.15, 0.20, 0.25, 0.30),
            bootstrap_progress_threshold = 5000,
            performance_threshold_count = 1000,  # Threshold count for performance optimization
            bootstrap_chunk_size = 10000,       # Memory-efficient chunking threshold
            bootstrap_convergence_check = 500,  # Check convergence every N iterations
            convergence_tolerance = 0.001,      # CI stability tolerance
            max_models_full_plot = 10           # Plot optimization threshold
        ),

        # Calculate net benefit for a model at given threshold
        .calculateNetBenefit = function(predictions, outcomes, threshold, positive_outcome) {
            # Convert outcomes to binary (1 = positive, 0 = negative)
            binary_outcomes <- as.numeric(outcomes == positive_outcome)

            # Calculate predictions at threshold
            predicted_positive <- predictions >= threshold

            # Calculate confusion matrix elements
            tp <- sum(predicted_positive & binary_outcomes == 1)
            fp <- sum(predicted_positive & binary_outcomes == 0)
            tn <- sum(!predicted_positive & binary_outcomes == 0)
            fn <- sum(!predicted_positive & binary_outcomes == 1)

            n <- length(outcomes)
            prevalence <- sum(binary_outcomes) / n

            # Calculate net benefit
            if (tp + fn == 0) {
                sensitivity <- 0
            } else {
                sensitivity <- tp / (tp + fn)
            }

            if (fp + tn == 0) {
                specificity <- 1
            } else {
                specificity <- tn / (fp + tn)
            }

            # Net benefit formula
            nb <- (tp / n) - (fp / n) * (threshold / (1 - threshold))

            return(list(
                net_benefit = nb,
                sensitivity = sensitivity,
                specificity = specificity,
                tp = tp, fp = fp, tn = tn, fn = fn,
                prevalence = prevalence,
                interventions_per_100 = sum(predicted_positive) / n * 100,
                true_positives_per_100 = tp / n * 100,
                false_positives_per_100 = fp / n * 100
            ))
        },

        # Calculate net benefit for treat all strategy
        .calculateTreatAllNetBenefit = function(outcomes, threshold, positive_outcome) {
            binary_outcomes <- as.numeric(outcomes == positive_outcome)
            prevalence <- mean(binary_outcomes)

            # For treat all: sensitivity = 1, specificity = 0
            nb <- prevalence - (1 - prevalence) * (threshold / (1 - threshold))
            return(nb)
        },

        # Calculate net benefit for treat none strategy (always 0)
        .calculateTreatNoneNetBenefit = function() {
            return(0)
        },
        
        # Vectorized net benefit calculation for performance optimization
        .calculateNetBenefitsVectorized = function(predictions, outcomes, thresholds, positive_outcome) {
            # Convert outcomes to binary once
            binary_outcomes <- as.numeric(outcomes == positive_outcome)
            n <- length(outcomes)
            
            # Pre-allocate result vector
            net_benefits <- numeric(length(thresholds))
            
            # Calculate for each threshold (still a loop but optimized inner calculations)
            for (j in seq_along(thresholds)) {
                thresh <- thresholds[j]
                
                # Vectorized threshold comparison
                predicted_positive <- predictions >= thresh
                
                # Vectorized confusion matrix calculation
                tp <- sum(predicted_positive & binary_outcomes == 1)
                fp <- sum(predicted_positive & binary_outcomes == 0)
                
                # Net benefit formula
                net_benefits[j] <- (tp / n) - (fp / n) * (thresh / (1 - thresh))
            }
            
            return(net_benefits)
        },

        # Generate threshold sequence with enhanced validation
        .generateThresholds = function() {
            range_type <- self$options$thresholdRange
            step <- self$options$thresholdStep

            if (range_type == "auto") {
                thresholds <- seq(0.01, 0.99, by = step)
            } else if (range_type == "clinical") {
                thresholds <- seq(0.05, 0.50, by = step)
            } else { # custom
                min_thresh <- self$options$thresholdMin
                max_thresh <- self$options$thresholdMax
                
                # Enhanced threshold range validation with clinical guidance
                private$.validateThresholdRange(min_thresh, max_thresh)
                
                thresholds <- seq(min_thresh, max_thresh, by = step)
            }

            return(thresholds)
        },
        
        # Validate threshold ranges with clinical context and guidance
        .validateThresholdRange = function(min_thresh, max_thresh) {
            # Basic validation
            if (min_thresh >= max_thresh) {
                stop(sprintf("Minimum threshold (%.3f) must be less than maximum threshold (%.3f)", 
                           min_thresh, max_thresh))
            }
            
            if (min_thresh <= 0 || max_thresh >= 1) {
                stop("Threshold probabilities must be between 0 and 1 (exclusive)")
            }
            
            # Clinical guidance warnings for unusual ranges
            if (max_thresh > 0.8) {
                warning(sprintf("Very high maximum threshold (%.1f%%). Decision thresholds above 80%% are rarely clinically meaningful for most medical decisions.", 
                               max_thresh * 100))
            }
            
            if (min_thresh < 0.01) {
                warning(sprintf("Very low minimum threshold (%.1f%%). Thresholds below 1%% may not be clinically interpretable.", 
                               min_thresh * 100))
            }
            
            # Range size warnings
            range_size <- max_thresh - min_thresh
            if (range_size > 0.7) {
                warning(sprintf("Very wide threshold range (%.1f%% span). Consider focusing on clinically relevant range for your specific decision context.", 
                               range_size * 100))
            }
            
            if (range_size < 0.05) {
                warning(sprintf("Narrow threshold range (%.1f%% span). Decision curve analysis is most informative across wider probability ranges.", 
                               range_size * 100))
            }
            
            # Clinical context guidance
            private$.provideThresholdContextGuidance(min_thresh, max_thresh)
        },
        
        # Provide clinical context guidance for threshold selection
        .provideThresholdContextGuidance = function(min_thresh, max_thresh) {
            # Determine likely clinical contexts based on threshold range
            cancer_screening_range <- min_thresh <= 0.10 && max_thresh >= 0.15
            surgical_decision_range <- min_thresh <= 0.20 && max_thresh >= 0.40
            treatment_selection_range <- min_thresh <= 0.15 && max_thresh >= 0.35
            
            guidance_messages <- character(0)
            
            if (cancer_screening_range) {
                guidance_messages <- c(guidance_messages, 
                    "• Threshold range suitable for cancer screening decisions (typical range: 5-20%)")
            }
            
            if (surgical_decision_range) {
                guidance_messages <- c(guidance_messages, 
                    "• Threshold range suitable for surgical intervention decisions (typical range: 10-50%)")
            }
            
            if (treatment_selection_range) {
                guidance_messages <- c(guidance_messages, 
                    "• Threshold range suitable for treatment selection decisions (typical range: 15-40%)")
            }
            
            if (max_thresh <= 0.05) {
                guidance_messages <- c(guidance_messages, 
                    "• Very low threshold range - consider if this aligns with your clinical decision context")
            }
            
            if (min_thresh >= 0.60) {
                guidance_messages <- c(guidance_messages, 
                    "• Very high threshold range - ensure this reflects actual clinical decision thresholds")
            }
            
            if (length(guidance_messages) > 0) {
                message("Clinical threshold context guidance:")
                for (msg in guidance_messages) {
                    message(msg)
                }
            }
        },

        # Parse selected thresholds for table
        .parseSelectedThresholds = function() {
            threshold_str <- self$options$selectedThresholds
            if (threshold_str == "") {
                return(private$DECISIONCURVE_DEFAULTS$selected_thresholds)
            }

            # Parse comma-separated values
            thresholds <- as.numeric(unlist(strsplit(threshold_str, "[,;\\s]+")))
            thresholds <- thresholds[!is.na(thresholds)]
            thresholds <- thresholds[thresholds > 0 & thresholds < 1]

            if (length(thresholds) == 0) {
                return(private$DECISIONCURVE_DEFAULTS$selected_thresholds)
            }

            return(sort(thresholds))
        },

        # Parse model names
        .parseModelNames = function() {
            model_names_str <- self$options$modelNames
            model_vars <- self$options$models

            if (model_names_str == "" || is.null(model_names_str)) {
                return(model_vars)
            }

            # Parse comma-separated names
            names <- trimws(unlist(strsplit(model_names_str, ",")))

            # If number of names doesn't match variables, use variable names
            if (length(names) != length(model_vars)) {
                return(model_vars)
            }

            return(names)
        },

        # Check bootstrap convergence for early termination
        .checkBootstrapConvergence = function(ci_history_lower, ci_history_upper, tolerance = NULL) {
            if (is.null(tolerance)) tolerance <- private$DECISIONCURVE_DEFAULTS$convergence_tolerance
            
            # Need at least 100 iterations to assess convergence
            if (length(ci_history_lower) < 100) return(FALSE)
            
            # Check stability of recent CI estimates
            recent_lower <- tail(ci_history_lower, 20)
            recent_upper <- tail(ci_history_upper, 20)
            
            # Calculate moving range of recent estimates
            lower_changes <- abs(diff(recent_lower))
            upper_changes <- abs(diff(recent_upper))
            
            # Convergence achieved if recent changes are small
            lower_stable <- all(lower_changes < tolerance, na.rm = TRUE)
            upper_stable <- all(upper_changes < tolerance, na.rm = TRUE)
            
            return(lower_stable && upper_stable)
        },
        
        # Memory-efficient chunked bootstrap for very large n_boot
        .calculateBootstrapCIChunked = function(predictions, outcomes, thresholds, positive_outcome, n_boot = 1000) {
            chunk_size <- private$DECISIONCURVE_DEFAULTS$bootstrap_chunk_size
            
            if (n_boot <= chunk_size) {
                return(private$.calculateBootstrapCI(predictions, outcomes, thresholds, positive_outcome, n_boot))
            }
            
            message(sprintf("Large bootstrap (%d reps): Using memory-efficient chunked processing with %d reps per chunk", 
                           n_boot, chunk_size))
            
            n_chunks <- ceiling(n_boot / chunk_size)
            all_results <- list()
            
            for (chunk in 1:n_chunks) {
                chunk_start <- (chunk - 1) * chunk_size + 1
                chunk_end <- min(chunk * chunk_size, n_boot)
                chunk_n_boot <- chunk_end - chunk_start + 1
                
                message(sprintf("Processing chunk %d/%d (%d replications)...", chunk, n_chunks, chunk_n_boot))
                
                chunk_result <- private$.calculateBootstrapCI(
                    predictions, outcomes, thresholds, positive_outcome, chunk_n_boot
                )
                
                if (!is.null(chunk_result)) {
                    all_results[[chunk]] <- list(
                        lower = chunk_result$lower,
                        upper = chunk_result$upper,
                        n_boot = chunk_n_boot
                    )
                }
                
                # Memory cleanup
                gc(verbose = FALSE)
            }
            
            # Combine results from all chunks
            if (length(all_results) == 0) {
                return(list(lower = rep(NA, length(thresholds)), upper = rep(NA, length(thresholds))))
            }
            
            # Weight by chunk size and combine
            total_weight <- sum(sapply(all_results, function(x) x$n_boot))
            combined_lower <- numeric(length(thresholds))
            combined_upper <- numeric(length(thresholds))
            
            for (i in seq_along(all_results)) {
                weight <- all_results[[i]]$n_boot / total_weight
                combined_lower <- combined_lower + weight * all_results[[i]]$lower
                combined_upper <- combined_upper + weight * all_results[[i]]$upper
            }
            
            message("Chunked bootstrap processing completed successfully.")
            return(list(lower = combined_lower, upper = combined_upper))
        },

        # Bootstrap confidence intervals with enhanced error handling and progress reporting
        .calculateBootstrapCI = function(predictions, outcomes, thresholds, positive_outcome, n_boot = 1000) {
            
            # Validate inputs
            if (length(predictions) != length(outcomes)) {
                stop("Bootstrap CI: Predictions and outcomes must have same length")
            }
            
            if (n_boot < 100) {
                warning("Bootstrap CI: Using fewer than 100 replications may give unreliable confidence intervals")
            }
            
            # Use chunked bootstrap for very large n_boot to manage memory
            if (n_boot >= private$DECISIONCURVE_DEFAULTS$bootstrap_chunk_size) {
                return(private$.calculateBootstrapCIChunked(predictions, outcomes, thresholds, positive_outcome, n_boot))
            }
            
            # Progress reporting for large bootstrap runs
            if (n_boot >= private$DECISIONCURVE_DEFAULTS$bootstrap_progress_threshold) {
                message(sprintf("Bootstrap confidence intervals: Running %d replications (this may take several minutes)...", n_boot))
            }
            
            n <- length(outcomes)
            boot_results <- array(NA, dim = c(n_boot, length(thresholds)))
            
            # Convergence tracking for early termination
            convergence_check_interval <- private$DECISIONCURVE_DEFAULTS$bootstrap_convergence_check
            ci_history_lower <- list()
            ci_history_upper <- list()
            converged_early <- FALSE
            
            tryCatch({
                for (i in 1:n_boot) {
                    # Progress indicators for very large bootstrap runs
                    if (n_boot >= 10000 && i %% 2000 == 0) {
                        message(sprintf("Bootstrap progress: %d/%d replications completed (%.1f%%)", 
                                       i, n_boot, (i/n_boot)*100))
                    }
                    
                    # Check convergence periodically for early termination
                    if (i %% convergence_check_interval == 0 && i >= convergence_check_interval * 2) {
                        # Calculate interim CI estimates
                        interim_lower <- apply(boot_results[1:i, , drop = FALSE], 2, function(x) {
                            if (sum(!is.na(x)) < 10) return(NA)
                            quantile(x, probs = (1 - self$options$ciLevel) / 2, na.rm = TRUE)
                        })
                        interim_upper <- apply(boot_results[1:i, , drop = FALSE], 2, function(x) {
                            if (sum(!is.na(x)) < 10) return(NA)
                            quantile(x, probs = 1 - (1 - self$options$ciLevel) / 2, na.rm = TRUE)
                        })
                        
                        ci_history_lower[[length(ci_history_lower) + 1]] <- interim_lower
                        ci_history_upper[[length(ci_history_upper) + 1]] <- interim_upper
                        
                        # Check if converged
                        if (length(ci_history_lower) >= 3) {
                            last_lower <- sapply(ci_history_lower, function(x) mean(x, na.rm = TRUE))
                            last_upper <- sapply(ci_history_upper, function(x) mean(x, na.rm = TRUE))
                            
                            if (private$.checkBootstrapConvergence(last_lower, last_upper)) {
                                message(sprintf("Bootstrap confidence intervals converged early at iteration %d (%.1f%% of requested replications)", 
                                               i, (i/n_boot)*100))
                                converged_early <- TRUE
                                n_boot <- i  # Update effective n_boot
                                boot_results <- boot_results[1:i, , drop = FALSE]
                                break
                            }
                        }
                    }
                    
                    # Bootstrap sample with error checking
                    boot_idx <- sample(n, n, replace = TRUE)
                    boot_pred <- predictions[boot_idx]
                    boot_out <- outcomes[boot_idx]
                    
                    # Validate bootstrap sample has variation
                    if (length(unique(boot_out)) < 2) {
                        warning(sprintf("Bootstrap sample %d has no outcome variation, skipping", i))
                        next
                    }

                    # Calculate net benefits for this bootstrap sample
                    for (j in seq_along(thresholds)) {
                        thresh <- thresholds[j]
                        nb_result <- private$.calculateNetBenefit(
                            boot_pred, boot_out, thresh, positive_outcome
                        )
                        boot_results[i, j] <- nb_result$net_benefit
                    }
                }

                # Check for sufficient valid bootstrap samples
                valid_samples <- rowSums(!is.na(boot_results))
                if (any(valid_samples < n_boot * 0.5)) {
                    warning("Bootstrap CI: Less than 50% of bootstrap samples were valid. Results may be unreliable.")
                }

                # Calculate confidence intervals with error handling
                ci_lower <- apply(boot_results, 2, function(x) {
                    if (sum(!is.na(x)) < 10) return(NA)
                    quantile(x, probs = (1 - self$options$ciLevel) / 2, na.rm = TRUE)
                })
                
                ci_upper <- apply(boot_results, 2, function(x) {
                    if (sum(!is.na(x)) < 10) return(NA)
                    quantile(x, probs = 1 - (1 - self$options$ciLevel) / 2, na.rm = TRUE)
                })
                
                if (n_boot >= private$DECISIONCURVE_DEFAULTS$bootstrap_progress_threshold) {
                    message("Bootstrap confidence intervals completed successfully.")
                }

                return(list(lower = ci_lower, upper = ci_upper))
                
            }, error = function(e) {
                enhanced_msg <- sprintf("Bootstrap CI calculation failed: %s. Continuing without confidence intervals.", 
                                       conditionMessage(e))
                warning(enhanced_msg)
                return(list(
                    lower = rep(NA, length(thresholds)), 
                    upper = rep(NA, length(thresholds))
                ))
            })
        },

        # Find optimal threshold for a model
        .findOptimalThreshold = function(net_benefits, thresholds) {
            # Find threshold with maximum net benefit
            max_idx <- which.max(net_benefits)
            optimal_threshold <- thresholds[max_idx]
            max_net_benefit <- net_benefits[max_idx]

            # Find range where model is beneficial (net benefit > 0)
            beneficial <- net_benefits > 0
            if (any(beneficial)) {
                beneficial_thresholds <- thresholds[beneficial]
                range_start <- min(beneficial_thresholds)
                range_end <- max(beneficial_thresholds)
            } else {
                range_start <- NA
                range_end <- NA
            }

            return(list(
                optimal_threshold = optimal_threshold,
                max_net_benefit = max_net_benefit,
                range_start = range_start,
                range_end = range_end
            ))
        },

        # Calculate weighted AUC
        .calculateWeightedAUC = function(net_benefits, thresholds) {
            # Remove any missing values
            valid_idx <- !is.na(net_benefits) & !is.na(thresholds)
            nb_clean <- net_benefits[valid_idx]
            th_clean <- thresholds[valid_idx]

            if (length(nb_clean) < 2) {
                return(NA)
            }

            # Calculate AUC using trapezoidal rule
            # Sort by threshold
            ord <- order(th_clean)
            nb_sorted <- nb_clean[ord]
            th_sorted <- th_clean[ord]

            # Trapezoidal integration
            auc <- 0
            for (i in 2:length(th_sorted)) {
                width <- th_sorted[i] - th_sorted[i-1]
                height <- (nb_sorted[i] + nb_sorted[i-1]) / 2
                auc <- auc + width * height
            }

            # Normalize by range
            total_range <- max(th_sorted) - min(th_sorted)
            return(auc / total_range)
        },

        # Main analysis function
        .run = function() {

            # Show instructions if needed
            if (is.null(self$options$outcome) || is.null(self$options$models) ||
                length(self$options$models) == 0) {

                instructions <- "
                <html>
                <head></head>
                <body>
                <div class='instructions'>
                <p><b>Decision Curve Analysis</b></p>
                <p>Decision Curve Analysis evaluates the clinical utility of prediction models by calculating net benefit across different threshold probabilities.</p>
                <p>To get started:</p>
                <ol>
                <li>Select a binary <b>Outcome Variable</b> (the condition you want to predict)</li>
                <li>Specify which level represents the positive outcome</li>
                <li>Add one or more <b>Prediction Variables/Models</b> (predicted probabilities or risk scores)</li>
                <li>Configure the threshold range and other analysis options</li>
                </ol>
                <p>The analysis will show whether using your prediction model(s) provides more clinical benefit than treating all patients or treating no patients.</p>
                </div>
                </body>
                </html>
                "

                self$results$instructions$setContent(instructions)
                return()
            }

            # Hide instructions when analysis can proceed
            self$results$instructions$setVisible(FALSE)

            # Get data and variables
            data <- self$data
            outcome_var <- self$options$outcome
            outcome_positive <- self$options$outcomePositive
            model_vars <- self$options$models

            # Parse model names
            model_names <- private$.parseModelNames()

            # Get complete cases
            complete_vars <- c(outcome_var, model_vars)
            complete_cases <- complete.cases(data[complete_vars])

            if (sum(complete_cases) < 10) {
                stop("Insufficient complete cases for analysis (minimum 10 required)")
            }

            # Filter data to complete cases
            analysis_data <- data[complete_cases, ]
            outcomes <- analysis_data[[outcome_var]]

            # Check outcome is binary
            unique_outcomes <- unique(outcomes)
            if (length(unique_outcomes) != 2) {
                stop("Outcome variable must be binary (exactly 2 levels)")
            }

            # Validate positive outcome level
            if (!outcome_positive %in% unique_outcomes) {
                outcome_positive <- unique_outcomes[1]
                warning("Selected positive outcome level not found. Using first level.")
            }

            # Generate threshold sequence
            thresholds <- private$.generateThresholds()

            # Performance monitoring for large analyses
            n_calculations <- length(model_vars) * length(thresholds)
            if (n_calculations >= private$DECISIONCURVE_DEFAULTS$performance_threshold_count) {
                message(sprintf("Decision curve analysis: Processing %d models × %d thresholds (%d total calculations)...", 
                               length(model_vars), length(thresholds), n_calculations))
            }

            # Initialize results storage
            dca_results <- list()
            plot_data <- data.frame()

            # Calculate decision curves for each model
            for (i in seq_along(model_vars)) {
                model_var <- model_vars[i]
                model_name <- model_names[i]
                predictions <- analysis_data[[model_var]]

                # Progress reporting for multiple models
                if (length(model_vars) > 3) {
                    message(sprintf("Processing model %d/%d: %s", i, length(model_vars), model_name))
                }

                # Validate predictions are between 0 and 1 (or convert if needed)
                if (min(predictions, na.rm = TRUE) < 0 || max(predictions, na.rm = TRUE) > 1) {
                    # If not probabilities, assume they are risk scores and need conversion
                    # Simple normalization to 0-1 range
                    pred_range <- range(predictions, na.rm = TRUE)
                    if (pred_range[1] != pred_range[2]) {
                        predictions <- (predictions - pred_range[1]) / (pred_range[2] - pred_range[1])
                    }
                }

                # Optimized threshold calculations - vectorize when possible
                net_benefits <- private$.calculateNetBenefitsVectorized(
                    predictions, outcomes, thresholds, outcome_positive
                )
                
                # Detailed results for specific calculations (fallback to individual calculations)
                detailed_results <- list()
                for (j in seq_along(thresholds)) {
                    thresh <- thresholds[j]
                    detailed_results[[j]] <- private$.calculateNetBenefit(
                        predictions, outcomes, thresh, outcome_positive
                    )
                }

                # Store results
                dca_results[[model_name]] <- list(
                    net_benefits = net_benefits,
                    detailed_results = detailed_results,
                    thresholds = thresholds
                )

                # Add to plot data
                model_plot_data <- data.frame(
                    threshold = thresholds,
                    net_benefit = net_benefits,
                    model = model_name,
                    stringsAsFactors = FALSE
                )

                # Add confidence intervals if requested
                if (self$options$confidenceIntervals) {
                    ci_results <- private$.calculateBootstrapCI(
                        predictions, outcomes, thresholds, outcome_positive,
                        self$options$bootReps
                    )
                    model_plot_data$ci_lower <- ci_results$lower
                    model_plot_data$ci_upper <- ci_results$upper
                }

                plot_data <- rbind(plot_data, model_plot_data)
            }

            # Calculate net benefit for treat all strategy
            treat_all_nb <- numeric(length(thresholds))
            treat_none_nb <- numeric(length(thresholds))

            for (j in seq_along(thresholds)) {
                treat_all_nb[j] <- private$.calculateTreatAllNetBenefit(
                    outcomes, thresholds[j], outcome_positive
                )
                treat_none_nb[j] <- private$.calculateTreatNoneNetBenefit()
            }

            # Add reference strategies to plot data
            ref_data <- rbind(
                data.frame(
                    threshold = thresholds,
                    net_benefit = treat_all_nb,
                    model = "Treat All",
                    stringsAsFactors = FALSE
                ),
                data.frame(
                    threshold = thresholds,
                    net_benefit = treat_none_nb,
                    model = "Treat None",
                    stringsAsFactors = FALSE
                )
            )

            plot_data <- rbind(plot_data, ref_data)
            
            # Add clinical decision rule if requested
            if (self$options$clinicalDecisionRule) {
                decision_rule_data <- private$.calculateClinicalDecisionRule(
                    outcomes, thresholds, outcome_positive
                )
                plot_data <- rbind(plot_data, decision_rule_data)
            }

            # Store results for plotting
            private$.dcaResults <- dca_results
            private$.plotData <- plot_data

            # Create procedure notes
            procedure_notes <- paste0(
                "<html><body>",
                "<h4>Decision Curve Analysis Summary</h4>",
                "<p><strong>Outcome Variable:</strong> ", outcome_var, " (", outcome_positive, " = positive)</p>",
                "<p><strong>Models Analyzed:</strong> ", paste(model_names, collapse = ", "), "</p>",
                "<p><strong>Sample Size:</strong> ", sum(complete_cases), " complete cases</p>",
                "<p><strong>Prevalence:</strong> ", round(mean(outcomes == outcome_positive) * 100, 1), "%</p>",
                "<p><strong>Threshold Range:</strong> ", round(min(thresholds) * 100, 1), "% to ",
                round(max(thresholds) * 100, 1), "%</p>",
                "</body></html>"
            )

            self$results$procedureNotes$setContent(procedure_notes)

            # Populate results table
            if (self$options$showTable) {
                private$.populateResultsTable(treat_all_nb, treat_none_nb)
            }

            # Populate optimal thresholds table
            if (self$options$showOptimalThreshold) {
                private$.populateOptimalTable()
            }

            # Calculate clinical impact if requested
            if (self$options$calculateClinicalImpact) {
                private$.calculateClinicalImpactMetrics(outcomes, outcome_positive)
            }

            # Calculate weighted AUC if requested
            if (self$options$weightedAUC) {
                private$.populateWeightedAUCTable()
            }

            # Model comparison if requested
            if (self$options$compareModels && length(model_vars) > 1) {
                private$.performModelComparison()
            }

            # Generate clinical interpretation
            private$.generateClinicalInterpretation()
        },

        .populateResultsTable = function(treat_all_nb, treat_none_nb) {
            selected_thresholds <- private$.parseSelectedThresholds()
            results_table <- self$results$resultsTable

            # Clear existing rows
            results_table$deleteRows()

            # Add columns for each model dynamically
            model_names <- names(private$.dcaResults)

            for (model_name in model_names) {
                results_table$addColumn(
                    name = paste0("model_", gsub("[^A-Za-z0-9]", "_", model_name)),
                    title = model_name,
                    type = "number",
                    format = "zto"
                )
            }

            # Populate table
            for (i in seq_along(selected_thresholds)) {
                thresh <- selected_thresholds[i]

                # Find closest threshold in our analysis
                closest_idx <- which.min(abs(private$.dcaResults[[1]]$thresholds - thresh))
                actual_thresh <- private$.dcaResults[[1]]$thresholds[closest_idx]

                # Create row values
                row_values <- list(
                    threshold = thresh,
                    treat_all = treat_all_nb[closest_idx],
                    treat_none = treat_none_nb[closest_idx]
                )

                # Add model values
                for (model_name in model_names) {
                    col_name <- paste0("model_", gsub("[^A-Za-z0-9]", "_", model_name))
                    row_values[[col_name]] <- private$.dcaResults[[model_name]]$net_benefits[closest_idx]
                }

                results_table$addRow(rowKey = i, values = row_values)
            }
        },

        .populateOptimalTable = function() {
            optimal_table <- self$results$optimalTable
            optimal_table$deleteRows()

            model_names <- names(private$.dcaResults)

            for (i in seq_along(model_names)) {
                model_name <- model_names[i]
                model_results <- private$.dcaResults[[model_name]]

                optimal_info <- private$.findOptimalThreshold(
                    model_results$net_benefits,
                    model_results$thresholds
                )

                optimal_table$addRow(rowKey = i, values = list(
                    model = model_name,
                    optimal_threshold = optimal_info$optimal_threshold,
                    max_net_benefit = optimal_info$max_net_benefit,
                    threshold_range_start = optimal_info$range_start,
                    threshold_range_end = optimal_info$range_end
                ))
            }
        },

        .calculateClinicalImpactMetrics = function(outcomes, outcome_positive) {
            clinical_impact_table <- self$results$clinicalImpactTable
            clinical_impact_table$deleteRows()

            selected_thresholds <- private$.parseSelectedThresholds()
            model_names <- names(private$.dcaResults)
            pop_size <- self$options$populationSize

            # Calculate for each model at each selected threshold
            row_counter <- 1
            for (model_name in model_names) {
                model_results <- private$.dcaResults[[model_name]]

                for (thresh in selected_thresholds) {
                    # Find closest threshold
                    closest_idx <- which.min(abs(model_results$thresholds - thresh))
                    detailed_result <- model_results$detailed_results[[closest_idx]]

                    # Calculate interventions avoided compared to treat all
                    treat_all_interventions <- pop_size  # Treat all = 100% get intervention
                    model_interventions <- detailed_result$interventions_per_100 * pop_size / 100
                    interventions_avoided <- treat_all_interventions - model_interventions

                    # Number needed to screen (simplified calculation)
                    if (detailed_result$true_positives_per_100 > 0) {
                        nns <- 100 / detailed_result$true_positives_per_100
                    } else {
                        nns <- Inf
                    }

                    clinical_impact_table$addRow(rowKey = row_counter, values = list(
                        model = model_name,
                        threshold = thresh,
                        interventions_per_100 = detailed_result$interventions_per_100,
                        true_positives_per_100 = detailed_result$true_positives_per_100,
                        false_positives_per_100 = detailed_result$false_positives_per_100,
                        interventions_avoided = interventions_avoided,
                        number_needed_to_screen = if(is.finite(nns)) nns else NA
                    ))

                    row_counter <- row_counter + 1
                }
            }
        },

        .populateWeightedAUCTable = function() {
            weighted_auc_table <- self$results$weightedAUCTable
            weighted_auc_table$deleteRows()

            model_names <- names(private$.dcaResults)
            thresholds <- private$.dcaResults[[1]]$thresholds

            # Calculate treat all weighted AUC for comparison
            outcomes <- self$data[[self$options$outcome]]
            outcome_positive <- self$options$outcomePositive
            treat_all_nb <- numeric(length(thresholds))
            for (j in seq_along(thresholds)) {
                treat_all_nb[j] <- private$.calculateTreatAllNetBenefit(
                    outcomes, thresholds[j], outcome_positive
                )
            }
            treat_all_wauc <- private$.calculateWeightedAUC(treat_all_nb, thresholds)

            for (i in seq_along(model_names)) {
                model_name <- model_names[i]
                model_results <- private$.dcaResults[[model_name]]

                # Calculate weighted AUC
                wauc <- private$.calculateWeightedAUC(
                    model_results$net_benefits,
                    model_results$thresholds
                )

                # Calculate relative benefit vs treat all
                if (!is.na(wauc) && !is.na(treat_all_wauc) && treat_all_wauc != 0) {
                    relative_benefit <- (wauc - treat_all_wauc) / abs(treat_all_wauc)
                } else {
                    relative_benefit <- NA
                }

                weighted_auc_table$addRow(rowKey = i, values = list(
                    model = model_name,
                    weighted_auc = wauc,
                    auc_range = paste0(round(min(thresholds) * 100, 1), "% - ",
                                       round(max(thresholds) * 100, 1), "%"),
                    relative_benefit = relative_benefit
                ))
            }
        },

        .performModelComparison = function() {
            # This would implement statistical tests for comparing models
            # For now, implement a simple comparison based on weighted AUC
            comparison_table <- self$results$comparisonTable
            comparison_table$deleteRows()

            model_names <- names(private$.dcaResults)

            # Compare each pair of models
            row_counter <- 1
            for (i in 1:(length(model_names) - 1)) {
                for (j in (i + 1):length(model_names)) {
                    model1 <- model_names[i]
                    model2 <- model_names[j]

                    # Calculate weighted AUC difference
                    wauc1 <- private$.calculateWeightedAUC(
                        private$.dcaResults[[model1]]$net_benefits,
                        private$.dcaResults[[model1]]$thresholds
                    )
                    wauc2 <- private$.calculateWeightedAUC(
                        private$.dcaResults[[model2]]$net_benefits,
                        private$.dcaResults[[model2]]$thresholds
                    )

                    wauc_diff <- wauc1 - wauc2

                    # For now, set placeholder values for CI and p-value
                    # In a full implementation, these would come from bootstrap testing
                    comparison_table$addRow(rowKey = row_counter, values = list(
                        comparison = paste(model1, "vs", model2),
                        weighted_auc_diff = wauc_diff,
                        ci_lower = NA,  # Would implement bootstrap CI
                        ci_upper = NA,  # Would implement bootstrap CI
                        p_value = NA    # Would implement statistical test
                    ))

                    row_counter <- row_counter + 1
                }
            }
        },

        .generateClinicalInterpretation = function() {
            model_names <- names(private$.dcaResults)

            # Find the best performing model (highest weighted AUC)
            best_wauc <- -Inf
            best_model <- NULL

            for (model_name in model_names) {
                wauc <- private$.calculateWeightedAUC(
                    private$.dcaResults[[model_name]]$net_benefits,
                    private$.dcaResults[[model_name]]$thresholds
                )
                if (!is.na(wauc) && wauc > best_wauc) {
                    best_wauc <- wauc
                    best_model <- model_name
                }
            }

            # Generate interpretation text
            interpretation <- paste0(
                "<html><body>",
                "<h4>Clinical Interpretation</h4>"
            )

            if (!is.null(best_model)) {
                interpretation <- paste0(
                    interpretation,
                    "<p><strong>Best Performing Model:</strong> ", best_model, "</p>"
                )

                # Get optimal threshold for best model
                best_results <- private$.dcaResults[[best_model]]
                optimal_info <- private$.findOptimalThreshold(
                    best_results$net_benefits,
                    best_results$thresholds
                )

                if (!is.na(optimal_info$optimal_threshold)) {
                    interpretation <- paste0(
                        interpretation,
                        "<p><strong>Optimal Threshold:</strong> ",
                        round(optimal_info$optimal_threshold * 100, 1),
                        "% (Net Benefit = ", round(optimal_info$max_net_benefit, 3), ")</p>"
                    )
                }

                if (!is.na(optimal_info$range_start) && !is.na(optimal_info$range_end)) {
                    interpretation <- paste0(
                        interpretation,
                        "<p><strong>Beneficial Range:</strong> ",
                        round(optimal_info$range_start * 100, 1), "% to ",
                        round(optimal_info$range_end * 100, 1), "%</p>"
                    )
                }
            }

            interpretation <- paste0(
                interpretation,
                "<p><strong>Interpretation Guidelines:</strong></p>",
                "<ul>",
                "<li>Models above both reference lines provide clinical benefit</li>",
                "<li>Higher net benefit indicates greater clinical utility</li>",
                "<li>Consider the threshold range relevant to your clinical context</li>",
                "<li>Net benefit can be interpreted as additional true positives per 100 patients screened</li>",
                "</ul>",
                private$.generateMethodologicalFootnotes(),
                "</body></html>"
            )

            self$results$summaryText$setContent(interpretation)
        },
        
        # Generate methodological footnotes for enhanced clinical understanding
        .generateMethodologicalFootnotes = function() {
            footnotes <- "<div style='margin-top: 20px; font-size: 0.9em; color: #666;'>"
            footnotes <- paste0(footnotes, "<p><strong>Methodological Notes:</strong></p>")
            footnotes <- paste0(footnotes, "<ul style='font-size: 0.85em;'>")
            
            # Net benefit formula explanation
            footnotes <- paste0(footnotes, 
                "<li><strong>Net Benefit Formula:</strong> NB = (TP/n) - (FP/n) × [pt/(1-pt)], where pt is threshold probability</li>")
            
            # Reference strategies explanation
            footnotes <- paste0(footnotes,
                "<li><strong>Reference Strategies:</strong> 'Treat All' assumes all patients receive intervention; 'Treat None' assumes no intervention</li>")
            
            # Threshold interpretation
            footnotes <- paste0(footnotes,
                "<li><strong>Threshold Probability:</strong> The minimum probability at which a patient would choose intervention over no intervention</li>")
            
            # Bootstrap CI note if applicable
            if (self$options$confidenceIntervals) {
                footnotes <- paste0(footnotes,
                    "<li><strong>Confidence Intervals:</strong> Bootstrap ", self$options$bootReps, 
                    " replications with ", (self$options$ciLevel * 100), "% confidence level</li>")
            }
            
            # Clinical impact note if applicable
            if (self$options$calculateClinicalImpact) {
                footnotes <- paste0(footnotes,
                    "<li><strong>Clinical Impact:</strong> Calculated for population size of ", 
                    self$options$populationSize, " patients</li>")
            }
            
            # Clinical decision rule note if applicable
            if (self$options$clinicalDecisionRule) {
                footnotes <- paste0(footnotes,
                    "<li><strong>Clinical Decision Rule:</strong> Fixed threshold at ", 
                    round(self$options$decisionRuleThreshold * 100, 1), "% (", 
                    self$options$decisionRuleLabel, ")</li>")
            }
            
            footnotes <- paste0(footnotes, "</ul></div>")
            
            return(footnotes)
        },
        
        # Optimize plot data for many models to improve performance and readability
        .optimizePlotDataForManyModels = function(plot_data, n_models) {
            # Strategies for handling many models:
            # 1. Reduce line thickness
            # 2. Sample data points for smoother rendering
            # 3. Consider highlighting top-performing models
            
            # Sample data points if there are many thresholds
            n_thresholds_per_model <- nrow(plot_data) / n_models
            if (n_thresholds_per_model > 100) {
                # Sample every nth point to reduce rendering load
                sample_rate <- ceiling(n_thresholds_per_model / 50)  # Target ~50 points per model
                
                optimized_data <- data.frame()
                for (model in unique(plot_data$model)) {
                    model_data <- plot_data[plot_data$model == model, ]
                    model_data <- model_data[seq(1, nrow(model_data), by = sample_rate), ]
                    optimized_data <- rbind(optimized_data, model_data)
                }
                
                message(sprintf("Reduced data points from %d to %d for faster rendering", 
                               nrow(plot_data), nrow(optimized_data)))
                
                return(optimized_data)
            }
            
            return(plot_data)
        },
        
        # Calculate clinical decision rule net benefit
        .calculateClinicalDecisionRule = function(outcomes, thresholds, outcome_positive) {
            rule_threshold <- self$options$decisionRuleThreshold
            rule_label <- self$options$decisionRuleLabel
            
            if (rule_label == "") {
                rule_label <- paste0("Clinical Rule (", round(rule_threshold * 100, 1), "%)")
            }
            
            # Clinical decision rule: binary decision at fixed threshold
            # Net benefit = prevalence - (1-prevalence) * [rule_threshold/(1-rule_threshold)]
            binary_outcomes <- as.numeric(outcomes == outcome_positive)
            prevalence <- mean(binary_outcomes)
            
            # Calculate net benefit for the clinical decision rule at each threshold
            rule_net_benefits <- numeric(length(thresholds))
            
            for (j in seq_along(thresholds)) {
                threshold <- thresholds[j]
                
                # Clinical rule performance:
                # If current threshold <= rule threshold: apply rule (screen/treat all above rule threshold)
                # If current threshold > rule threshold: rule says "don't screen/treat"
                
                if (threshold <= rule_threshold) {
                    # Rule recommends intervention for patients above rule threshold
                    # This is equivalent to treating all patients with net benefit calculation
                    rule_net_benefits[j] <- prevalence - (1 - prevalence) * (rule_threshold / (1 - rule_threshold))
                } else {
                    # Rule recommends no intervention at this threshold
                    rule_net_benefits[j] <- 0
                }
            }
            
            # Create decision rule data
            decision_rule_data <- data.frame(
                threshold = thresholds,
                net_benefit = rule_net_benefits,
                model = rule_label,
                stringsAsFactors = FALSE
            )
            
            return(decision_rule_data)
        },

        # Optimized plotting functions with performance enhancements for many models
        .plotDCA = function(image, ggtheme, theme, ...) {
            if (is.null(private$.plotData) || nrow(private$.plotData) == 0) {
                return(FALSE)
            }

            plot_data <- private$.plotData
            
            # Performance optimization for many models
            n_models <- length(unique(plot_data$model))
            max_models_threshold <- private$DECISIONCURVE_DEFAULTS$max_models_full_plot
            
            if (n_models > max_models_threshold) {
                plot_data <- private$.optimizePlotDataForManyModels(plot_data, n_models)
                message(sprintf("Plot optimized for %d models: Using performance enhancements", n_models))
            }

            # Create base plot with optimized aesthetics
            p <- ggplot2::ggplot(plot_data, ggplot2::aes(x = threshold, y = net_benefit, color = model)) +
                ggplot2::geom_line(size = if(n_models > max_models_threshold) 0.8 else 1) +
                ggplot2::labs(
                    title = "Decision Curve Analysis",
                    x = "Threshold Probability",
                    y = "Net Benefit",
                    color = "Strategy"
                ) +
                ggplot2::scale_x_continuous(labels = function(x) paste0(round(x * 100), "%")) +
                ggtheme

            # Add confidence intervals if calculated
            if ("ci_lower" %in% names(plot_data)) {
                model_data <- plot_data[!plot_data$model %in% c("Treat All", "Treat None"), ]
                if (nrow(model_data) > 0) {
                    p <- p + ggplot2::geom_ribbon(
                        data = model_data,
                        ggplot2::aes(ymin = ci_lower, ymax = ci_upper, fill = model),
                        alpha = 0.2, color = NA
                    )
                }
            }

            # Highlight clinical range if requested
            if (self$options$highlightRange) {
                p <- p + ggplot2::annotate(
                    "rect",
                    xmin = self$options$highlightMin,
                    xmax = self$options$highlightMax,
                    ymin = -Inf, ymax = Inf,
                    alpha = 0.1, fill = "yellow"
                )
            }

            # Optimize legend and colors for many models
            if (n_models > max_models_threshold) {
                # Use more efficient legend positioning and reduce legend size
                p <- p + ggplot2::theme(
                    legend.position = "bottom",
                    legend.text = ggplot2::element_text(size = 8),
                    legend.title = ggplot2::element_text(size = 9),
                    legend.key.size = ggplot2::unit(0.4, "cm")
                )
                
                # Consider using fewer distinct colors and rely more on line patterns
                if (n_models > 15) {
                    p <- p + ggplot2::guides(color = ggplot2::guide_legend(ncol = 3))
                }
            }

            # Style reference lines differently
            if (self$options$plotStyle == "standard" || self$options$plotStyle == "detailed") {
                # Make treat all/none lines dashed
                treat_lines <- plot_data[plot_data$model %in% c("Treat All", "Treat None"), ]
                if (nrow(treat_lines) > 0) {
                    p <- p + ggplot2::geom_line(
                        data = treat_lines,
                        linetype = "dashed", 
                        size = if(n_models > max_models_threshold) 0.6 else 0.8
                    )
                }
            }

            # Add annotations for detailed style
            if (self$options$plotStyle == "detailed") {
                # Add horizontal line at 0
                p <- p + ggplot2::geom_hline(yintercept = 0, linetype = "dotted", alpha = 0.5)

                # Add labels if requested
                if (self$options$showReferenceLinesLabels) {
                    # This would add text annotations for reference lines
                }
            }

            print(p)
            return(TRUE)
        },

        .plotClinicalImpact = function(image, ggtheme, theme, ...) {
            if (is.null(private$.dcaResults) || !self$options$calculateClinicalImpact) {
                return(FALSE)
            }

            # Get selected thresholds and models
            selected_thresholds <- private$.parseSelectedThresholds()
            model_names <- names(private$.dcaResults)
            pop_size <- self$options$populationSize

            # Prepare data for clinical impact plot
            impact_data <- data.frame()

            for (model_name in model_names) {
                model_results <- private$.dcaResults[[model_name]]

                for (thresh in selected_thresholds) {
                    # Find closest threshold
                    closest_idx <- which.min(abs(model_results$thresholds - thresh))
                    detailed_result <- model_results$detailed_results[[closest_idx]]

                    # Add to plot data
                    impact_data <- rbind(impact_data, data.frame(
                        threshold = thresh,
                        model = model_name,
                        interventions_per_100 = detailed_result$interventions_per_100,
                        true_positives_per_100 = detailed_result$true_positives_per_100,
                        false_positives_per_100 = detailed_result$false_positives_per_100,
                        stringsAsFactors = FALSE
                    ))
                }
            }

            if (nrow(impact_data) == 0) return(FALSE)

            # Reshape data for stacked bar chart
            library(tidyr)
            plot_data <- impact_data %>%
                tidyr::gather(key = "outcome_type", value = "count",
                              true_positives_per_100, false_positives_per_100) %>%
                dplyr::mutate(
                    outcome_type = factor(outcome_type,
                                          levels = c("true_positives_per_100", "false_positives_per_100"),
                                          labels = c("True Positives", "False Positives"))
                )

            # Create stacked bar chart showing clinical impact
            p <- ggplot2::ggplot(plot_data, ggplot2::aes(x = factor(threshold), y = count, fill = outcome_type)) +
                ggplot2::geom_bar(stat = "identity", position = "stack") +
                ggplot2::facet_wrap(~ model, scales = "free_y") +
                ggplot2::labs(
                    title = "Clinical Impact: Interventions per 100 Patients",
                    x = "Threshold Probability",
                    y = "Patients per 100",
                    fill = "Outcome Type"
                ) +
                ggplot2::scale_x_discrete(labels = function(x) paste0(as.numeric(x) * 100, "%")) +
                ggplot2::scale_fill_manual(values = c("True Positives" = "darkgreen", "False Positives" = "darkred")) +
                ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1)) +
                ggtheme

            print(p)
            return(TRUE)
        },

        .plotInterventionsAvoided = function(image, ggtheme, theme, ...) {
            if (is.null(private$.dcaResults)) {
                return(FALSE)
            }

            # Calculate interventions avoided compared to "treat all" strategy
            thresholds <- private$.dcaResults[[1]]$thresholds
            model_names <- names(private$.dcaResults)

            # Prepare data
            avoided_data <- data.frame()

            for (model_name in model_names) {
                model_results <- private$.dcaResults[[model_name]]
                interventions_avoided <- numeric(length(thresholds))

                for (j in seq_along(thresholds)) {
                    detailed_result <- model_results$detailed_results[[j]]
                    # Treat all = 100% get intervention, model = actual intervention rate
                    interventions_avoided[j] <- 100 - detailed_result$interventions_per_100
                }

                avoided_data <- rbind(avoided_data, data.frame(
                    threshold = thresholds,
                    interventions_avoided = interventions_avoided,
                    model = model_name,
                    stringsAsFactors = FALSE
                ))
            }

            if (nrow(avoided_data) == 0) return(FALSE)

            # Create line plot showing interventions avoided
            p <- ggplot2::ggplot(avoided_data, ggplot2::aes(x = threshold, y = interventions_avoided, color = model)) +
                ggplot2::geom_line(size = 1) +
                ggplot2::geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
                ggplot2::labs(
                    title = "Interventions Avoided vs. Treat All Strategy",
                    subtitle = "Number of unnecessary interventions prevented per 100 patients",
                    x = "Threshold Probability",
                    y = "Interventions Avoided per 100 Patients",
                    color = "Model"
                ) +
                ggplot2::scale_x_continuous(labels = function(x) paste0(round(x * 100), "%")) +
                ggplot2::ylim(0, 100) +
                ggtheme

            # Add annotation explaining the interpretation
            p <- p + ggplot2::annotate(
                "text",
                x = max(thresholds) * 0.7,
                y = max(avoided_data$interventions_avoided, na.rm = TRUE) * 0.9,
                label = "Higher values = more\nunnecessary treatments avoided",
                hjust = 0.5,
                alpha = 0.7,
                size = 3
            )

            print(p)
            return(TRUE)
        }
    )
)
