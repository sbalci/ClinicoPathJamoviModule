
# This file is automatically generated, you probably don't want to edit this

praucOptions <- if (requireNamespace("jmvcore", quietly=TRUE)) R6::R6Class(
    "praucOptions",
    inherit = jmvcore::Options,
    public = list(
        initialize = function(
            outcome = NULL,
            predictor = NULL,
            positive_level = "",
            prevalence = -1,
            calculate_auc = TRUE,
            calculate_fscore = TRUE,
            beta_weights = "1, 2, 0.5",
            confidence_intervals = TRUE,
            ci_method = "bootstrap",
            bootstrap_samples = 1000,
            confidence_level = 0.95,
            compare_to_roc = TRUE,
            baseline_comparison = TRUE,
            interpolation_method = "step",
            plot_pr_curve = TRUE,
            plot_comparison = FALSE,
            plot_fscore = FALSE,
            min_threshold = 0,
            max_threshold = 1,
            random_seed = 12345, ...) {

            super$initialize(
                package="ClinicoPath",
                name="prauc",
                requiresData=TRUE,
                ...)

            private$..outcome <- jmvcore::OptionVariable$new(
                "outcome",
                outcome,
                suggested=list(
                    "nominal",
                    "ordinal"),
                permitted=list(
                    "factor"))
            private$..predictor <- jmvcore::OptionVariable$new(
                "predictor",
                predictor,
                suggested=list(
                    "continuous"),
                permitted=list(
                    "numeric"))
            private$..positive_level <- jmvcore::OptionString$new(
                "positive_level",
                positive_level,
                default="")
            private$..prevalence <- jmvcore::OptionNumber$new(
                "prevalence",
                prevalence,
                default=-1,
                min=0,
                max=1)
            private$..calculate_auc <- jmvcore::OptionBool$new(
                "calculate_auc",
                calculate_auc,
                default=TRUE)
            private$..calculate_fscore <- jmvcore::OptionBool$new(
                "calculate_fscore",
                calculate_fscore,
                default=TRUE)
            private$..beta_weights <- jmvcore::OptionString$new(
                "beta_weights",
                beta_weights,
                default="1, 2, 0.5")
            private$..confidence_intervals <- jmvcore::OptionBool$new(
                "confidence_intervals",
                confidence_intervals,
                default=TRUE)
            private$..ci_method <- jmvcore::OptionList$new(
                "ci_method",
                ci_method,
                options=list(
                    "bootstrap",
                    "bca"),
                default="bootstrap")
            private$..bootstrap_samples <- jmvcore::OptionInteger$new(
                "bootstrap_samples",
                bootstrap_samples,
                default=1000,
                min=100,
                max=10000)
            private$..confidence_level <- jmvcore::OptionNumber$new(
                "confidence_level",
                confidence_level,
                default=0.95,
                min=0.8,
                max=0.99)
            private$..compare_to_roc <- jmvcore::OptionBool$new(
                "compare_to_roc",
                compare_to_roc,
                default=TRUE)
            private$..baseline_comparison <- jmvcore::OptionBool$new(
                "baseline_comparison",
                baseline_comparison,
                default=TRUE)
            private$..interpolation_method <- jmvcore::OptionList$new(
                "interpolation_method",
                interpolation_method,
                options=list(
                    "step",
                    "linear"),
                default="step")
            private$..plot_pr_curve <- jmvcore::OptionBool$new(
                "plot_pr_curve",
                plot_pr_curve,
                default=TRUE)
            private$..plot_comparison <- jmvcore::OptionBool$new(
                "plot_comparison",
                plot_comparison,
                default=FALSE)
            private$..plot_fscore <- jmvcore::OptionBool$new(
                "plot_fscore",
                plot_fscore,
                default=FALSE)
            private$..min_threshold <- jmvcore::OptionNumber$new(
                "min_threshold",
                min_threshold,
                default=0,
                min=0,
                max=1)
            private$..max_threshold <- jmvcore::OptionNumber$new(
                "max_threshold",
                max_threshold,
                default=1,
                min=0,
                max=1)
            private$..random_seed <- jmvcore::OptionInteger$new(
                "random_seed",
                random_seed,
                default=12345,
                min=1,
                max=999999)

            self$.addOption(private$..outcome)
            self$.addOption(private$..predictor)
            self$.addOption(private$..positive_level)
            self$.addOption(private$..prevalence)
            self$.addOption(private$..calculate_auc)
            self$.addOption(private$..calculate_fscore)
            self$.addOption(private$..beta_weights)
            self$.addOption(private$..confidence_intervals)
            self$.addOption(private$..ci_method)
            self$.addOption(private$..bootstrap_samples)
            self$.addOption(private$..confidence_level)
            self$.addOption(private$..compare_to_roc)
            self$.addOption(private$..baseline_comparison)
            self$.addOption(private$..interpolation_method)
            self$.addOption(private$..plot_pr_curve)
            self$.addOption(private$..plot_comparison)
            self$.addOption(private$..plot_fscore)
            self$.addOption(private$..min_threshold)
            self$.addOption(private$..max_threshold)
            self$.addOption(private$..random_seed)
        }),
    active = list(
        outcome = function() private$..outcome$value,
        predictor = function() private$..predictor$value,
        positive_level = function() private$..positive_level$value,
        prevalence = function() private$..prevalence$value,
        calculate_auc = function() private$..calculate_auc$value,
        calculate_fscore = function() private$..calculate_fscore$value,
        beta_weights = function() private$..beta_weights$value,
        confidence_intervals = function() private$..confidence_intervals$value,
        ci_method = function() private$..ci_method$value,
        bootstrap_samples = function() private$..bootstrap_samples$value,
        confidence_level = function() private$..confidence_level$value,
        compare_to_roc = function() private$..compare_to_roc$value,
        baseline_comparison = function() private$..baseline_comparison$value,
        interpolation_method = function() private$..interpolation_method$value,
        plot_pr_curve = function() private$..plot_pr_curve$value,
        plot_comparison = function() private$..plot_comparison$value,
        plot_fscore = function() private$..plot_fscore$value,
        min_threshold = function() private$..min_threshold$value,
        max_threshold = function() private$..max_threshold$value,
        random_seed = function() private$..random_seed$value),
    private = list(
        ..outcome = NA,
        ..predictor = NA,
        ..positive_level = NA,
        ..prevalence = NA,
        ..calculate_auc = NA,
        ..calculate_fscore = NA,
        ..beta_weights = NA,
        ..confidence_intervals = NA,
        ..ci_method = NA,
        ..bootstrap_samples = NA,
        ..confidence_level = NA,
        ..compare_to_roc = NA,
        ..baseline_comparison = NA,
        ..interpolation_method = NA,
        ..plot_pr_curve = NA,
        ..plot_comparison = NA,
        ..plot_fscore = NA,
        ..min_threshold = NA,
        ..max_threshold = NA,
        ..random_seed = NA)
)

praucResults <- if (requireNamespace("jmvcore", quietly=TRUE)) R6::R6Class(
    "praucResults",
    inherit = jmvcore::Group,
    active = list(
        instructions = function() private$.items[["instructions"]],
        prSummary = function() private$.items[["prSummary"]],
        optimalThresholds = function() private$.items[["optimalThresholds"]],
        prCurveData = function() private$.items[["prCurveData"]],
        performanceAtKey = function() private$.items[["performanceAtKey"]],
        prCurvePlot = function() private$.items[["prCurvePlot"]],
        comparisonPlot = function() private$.items[["comparisonPlot"]],
        fscorePlot = function() private$.items[["fscorePlot"]],
        interpretation = function() private$.items[["interpretation"]]),
    private = list(),
    public=list(
        initialize=function(options) {
            super$initialize(
                options=options,
                name="",
                title="Precision-Recall Analysis")
            self$add(jmvcore::Html$new(
                options=options,
                name="instructions",
                title="Instructions",
                visible=TRUE))
            self$add(jmvcore::Table$new(
                options=options,
                name="prSummary",
                title="PR-AUC Summary",
                visible=TRUE,
                rows=1,
                columns=list(
                    list(
                        `name`="n", 
                        `title`="N", 
                        `type`="integer"),
                    list(
                        `name`="n_positive", 
                        `title`="N Positive", 
                        `type`="integer"),
                    list(
                        `name`="n_negative", 
                        `title`="N Negative", 
                        `type`="integer"),
                    list(
                        `name`="prevalence", 
                        `title`="Prevalence", 
                        `type`="number", 
                        `format`="pc"),
                    list(
                        `name`="prauc", 
                        `title`="PR-AUC", 
                        `type`="number", 
                        `format`="zto"),
                    list(
                        `name`="prauc_se", 
                        `title`="SE", 
                        `type`="number"),
                    list(
                        `name`="ci_lower", 
                        `title`="95% CI Lower", 
                        `type`="number", 
                        `format`="zto", 
                        `visible`="(confidence_intervals)"),
                    list(
                        `name`="ci_upper", 
                        `title`="95% CI Upper", 
                        `type`="number", 
                        `format`="zto", 
                        `visible`="(confidence_intervals)"),
                    list(
                        `name`="baseline_prauc", 
                        `title`="Baseline PR-AUC", 
                        `type`="number", 
                        `format`="zto", 
                        `visible`="(baseline_comparison)"),
                    list(
                        `name`="improvement", 
                        `title`="Improvement vs Baseline", 
                        `type`="number", 
                        `format`="zto", 
                        `visible`="(baseline_comparison)"),
                    list(
                        `name`="rocauc", 
                        `title`="ROC-AUC (for comparison)", 
                        `type`="number", 
                        `format`="zto", 
                        `visible`="(compare_to_roc)"))))
            self$add(jmvcore::Table$new(
                options=options,
                name="optimalThresholds",
                title="Optimal Thresholds",
                visible="(calculate_fscore)",
                columns=list(
                    list(
                        `name`="metric", 
                        `title`="Metric", 
                        `type`="text"),
                    list(
                        `name`="optimal_threshold", 
                        `title`="Optimal Threshold", 
                        `type`="number"),
                    list(
                        `name`="precision", 
                        `title`="Precision", 
                        `type`="number", 
                        `format`="zto"),
                    list(
                        `name`="recall", 
                        `title`="Recall (Sensitivity)", 
                        `type`="number", 
                        `format`="zto"),
                    list(
                        `name`="fscore", 
                        `title`="F-Score", 
                        `type`="number", 
                        `format`="zto"),
                    list(
                        `name`="tp", 
                        `title`="TP", 
                        `type`="integer"),
                    list(
                        `name`="fp", 
                        `title`="FP", 
                        `type`="integer"),
                    list(
                        `name`="fn", 
                        `title`="FN", 
                        `type`="integer"),
                    list(
                        `name`="tn", 
                        `title`="TN", 
                        `type`="integer"))))
            self$add(jmvcore::Table$new(
                options=options,
                name="prCurveData",
                title="Precision-Recall Curve Points",
                visible=FALSE,
                columns=list(
                    list(
                        `name`="threshold", 
                        `title`="Threshold", 
                        `type`="number"),
                    list(
                        `name`="precision", 
                        `title`="Precision", 
                        `type`="number", 
                        `format`="zto"),
                    list(
                        `name`="recall", 
                        `title`="Recall", 
                        `type`="number", 
                        `format`="zto"),
                    list(
                        `name`="f1_score", 
                        `title`="F1-Score", 
                        `type`="number", 
                        `format`="zto"))))
            self$add(jmvcore::Table$new(
                options=options,
                name="performanceAtKey",
                title="Performance at Key Thresholds",
                visible=TRUE,
                columns=list(
                    list(
                        `name`="threshold", 
                        `title`="Threshold", 
                        `type`="number"),
                    list(
                        `name`="precision", 
                        `title`="Precision", 
                        `type`="number", 
                        `format`="zto"),
                    list(
                        `name`="recall", 
                        `title`="Recall", 
                        `type`="number", 
                        `format`="zto"),
                    list(
                        `name`="f1_score", 
                        `title`="F1-Score", 
                        `type`="number", 
                        `format`="zto"),
                    list(
                        `name`="positive_predictions", 
                        `title`="Predicted Positive", 
                        `type`="integer"),
                    list(
                        `name`="negative_predictions", 
                        `title`="Predicted Negative", 
                        `type`="integer"),
                    list(
                        `name`="ppv_interpretation", 
                        `title`="PPV Interpretation", 
                        `type`="text"))))
            self$add(jmvcore::Image$new(
                options=options,
                name="prCurvePlot",
                title="Precision-Recall Curve",
                width=600,
                height=500,
                renderFun=".plotPRCurve",
                visible="(plot_pr_curve)"))
            self$add(jmvcore::Image$new(
                options=options,
                name="comparisonPlot",
                title="ROC vs PR Comparison",
                width=800,
                height=400,
                renderFun=".plotComparison",
                visible="(plot_comparison)"))
            self$add(jmvcore::Image$new(
                options=options,
                name="fscorePlot",
                title="F-Score vs Threshold",
                width=600,
                height=500,
                renderFun=".plotFScore",
                visible="(plot_fscore)"))
            self$add(jmvcore::Html$new(
                options=options,
                name="interpretation",
                title="Interpretation",
                visible=TRUE))}))

praucBase <- if (requireNamespace("jmvcore", quietly=TRUE)) R6::R6Class(
    "praucBase",
    inherit = jmvcore::Analysis,
    public = list(
        initialize = function(options, data=NULL, datasetId="", analysisId="", revision=0) {
            super$initialize(
                package = "ClinicoPath",
                name = "prauc",
                version = c(0,0,32),
                options = options,
                results = praucResults$new(options=options),
                data = data,
                datasetId = datasetId,
                analysisId = analysisId,
                revision = revision,
                pause = NULL,
                completeWhenFilled = FALSE,
                requiresMissings = FALSE,
                weightsSupport = 'auto')
        }))

#' Precision-Recall Analysis
#'
#' Precision-Recall (PR) analysis is superior to ROC analysis for highly 
#' imbalanced datasets, which are common in digital pathology and AI 
#' applications. While ROC curves can be misleading when the negative class 
#' vastly outnumbers the positive class, PR curves focus on the performance in 
#' the minority (positive) class. The area under the PR curve (PR-AUC or 
#' Average Precision) provides a single-number summary of model performance. 
#' PR analysis is essential for evaluating rare event detection (mitotic 
#' figures <1\%, rare tumor cells, micrometastases), cancer screening where 
#' positives are rare, quality control in digital pathology where defects are 
#' uncommon, and AI triage systems where abnormal cases are infrequent. This 
#' module provides comprehensive PR analysis with optimal threshold selection, 
#' comparison to baseline (random classifier), and F-score optimization for 
#' different precision-recall trade-offs.
#' 
#'
#' @examples
#' result <- prauc(
#'     data = pathology_data,
#'     outcome = "cancer",
#'     predictor = "ai_score",
#'     positive_level = "positive"
#' )
#'
#' @param data The data as a data frame.
#' @param outcome Binary outcome variable (e.g., cancer/non-cancer,
#'   positive/negative). Must have exactly two levels.
#' @param predictor Continuous predictor variable (e.g., AI score, biomarker
#'   level, probability). Higher values should indicate higher likelihood of
#'   positive outcome.
#' @param positive_level Level of outcome variable to treat as "positive"
#'   class. If not specified, the second level of the factor is used. Critical
#'   for correct PR analysis.
#' @param prevalence Observed prevalence of positive class in dataset.
#'   Automatically calculated if not specified (-1). Used for baseline
#'   comparison and interpretation.
#' @param calculate_auc Calculate area under precision-recall curve (also
#'   called Average Precision). Uses trapezoidal integration. Essential summary
#'   metric for imbalanced data.
#' @param calculate_fscore Find optimal threshold that maximizes F1-score
#'   (harmonic mean of precision and recall). Also calculates F2-score
#'   (emphasizes recall) and F0.5-score (emphasizes precision).
#' @param beta_weights Comma-separated list of beta values for F-score
#'   calculation. F1 (beta=1) balances precision and recall. F2 (beta=2) weights
#'   recall higher. F0.5 (beta=0.5) weights precision higher. Example: "1, 2,
#'   0.5, 3"
#' @param confidence_intervals Calculate confidence intervals for PR-AUC using
#'   bootstrap resampling. Provides uncertainty quantification for imbalanced
#'   datasets.
#' @param ci_method Method for confidence interval calculation. Bootstrap
#'   percentile is standard. BCa (bias-corrected and accelerated) adjusts for
#'   skewness.
#' @param bootstrap_samples Number of bootstrap resamples for confidence
#'   interval calculation.
#' @param confidence_level Confidence level for interval estimation.
#' @param compare_to_roc Calculate ROC-AUC for comparison. Demonstrates
#'   advantage of PR analysis for imbalanced data where ROC-AUC may be
#'   misleadingly high.
#' @param baseline_comparison Compare PR-AUC to baseline (random classifier).
#'   Baseline PR-AUC equals the prevalence. Shows improvement over chance
#'   performance.
#' @param interpolation_method Method for interpolating PR curve between
#'   points. Step function is standard for PR curves. Linear interpolation can
#'   smooth the curve.
#' @param plot_pr_curve Plot precision-recall curve with optimal F-score
#'   threshold marked. Shows trade-off between precision and recall across all
#'   thresholds.
#' @param plot_comparison Side-by-side comparison of ROC and PR curves to
#'   demonstrate differences in imbalanced dataset evaluation.
#' @param plot_fscore Plot F-scores (F1, F2, F0.5) across all thresholds to
#'   visualize optimal operating points for different precision-recall
#'   trade-offs.
#' @param min_threshold Minimum predictor threshold to evaluate. Use to focus
#'   on clinically relevant range.
#' @param max_threshold Maximum predictor threshold to evaluate.
#' @param random_seed Random seed for bootstrap procedures to ensure
#'   reproducibility.
#' @return A results object containing:
#' \tabular{llllll}{
#'   \code{results$instructions} \tab \tab \tab \tab \tab a html \cr
#'   \code{results$prSummary} \tab \tab \tab \tab \tab Precision-Recall area under curve with baseline comparison \cr
#'   \code{results$optimalThresholds} \tab \tab \tab \tab \tab Optimal operating points for different F-score metrics \cr
#'   \code{results$prCurveData} \tab \tab \tab \tab \tab Precision and recall values across all thresholds \cr
#'   \code{results$performanceAtKey} \tab \tab \tab \tab \tab Precision and recall at clinically relevant thresholds \cr
#'   \code{results$prCurvePlot} \tab \tab \tab \tab \tab Precision-recall curve with optimal F1 threshold marked \cr
#'   \code{results$comparisonPlot} \tab \tab \tab \tab \tab Side-by-side comparison of ROC and PR curves \cr
#'   \code{results$fscorePlot} \tab \tab \tab \tab \tab F-scores across thresholds for different beta values \cr
#'   \code{results$interpretation} \tab \tab \tab \tab \tab a html \cr
#' }
#'
#' Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:
#'
#' \code{results$prSummary$asDF}
#'
#' \code{as.data.frame(results$prSummary)}
#'
#' @export
prauc <- function(
    data,
    outcome,
    predictor,
    positive_level = "",
    prevalence = -1,
    calculate_auc = TRUE,
    calculate_fscore = TRUE,
    beta_weights = "1, 2, 0.5",
    confidence_intervals = TRUE,
    ci_method = "bootstrap",
    bootstrap_samples = 1000,
    confidence_level = 0.95,
    compare_to_roc = TRUE,
    baseline_comparison = TRUE,
    interpolation_method = "step",
    plot_pr_curve = TRUE,
    plot_comparison = FALSE,
    plot_fscore = FALSE,
    min_threshold = 0,
    max_threshold = 1,
    random_seed = 12345) {

    if ( ! requireNamespace("jmvcore", quietly=TRUE))
        stop("prauc requires jmvcore to be installed (restart may be required)")

    if ( ! missing(outcome)) outcome <- jmvcore::resolveQuo(jmvcore::enquo(outcome))
    if ( ! missing(predictor)) predictor <- jmvcore::resolveQuo(jmvcore::enquo(predictor))
    if (missing(data))
        data <- jmvcore::marshalData(
            parent.frame(),
            `if`( ! missing(outcome), outcome, NULL),
            `if`( ! missing(predictor), predictor, NULL))

    for (v in outcome) if (v %in% names(data)) data[[v]] <- as.factor(data[[v]])

    options <- praucOptions$new(
        outcome = outcome,
        predictor = predictor,
        positive_level = positive_level,
        prevalence = prevalence,
        calculate_auc = calculate_auc,
        calculate_fscore = calculate_fscore,
        beta_weights = beta_weights,
        confidence_intervals = confidence_intervals,
        ci_method = ci_method,
        bootstrap_samples = bootstrap_samples,
        confidence_level = confidence_level,
        compare_to_roc = compare_to_roc,
        baseline_comparison = baseline_comparison,
        interpolation_method = interpolation_method,
        plot_pr_curve = plot_pr_curve,
        plot_comparison = plot_comparison,
        plot_fscore = plot_fscore,
        min_threshold = min_threshold,
        max_threshold = max_threshold,
        random_seed = random_seed)

    analysis <- praucClass$new(
        options = options,
        data = data)

    analysis$run()

    analysis$results
}

