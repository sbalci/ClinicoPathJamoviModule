
# This file is automatically generated, you probably don't want to edit this

precisionrecallClass <- if (requireNamespace('jmvcore', quietly=TRUE)) R6::R6Class(
    "precisionrecallClass",
    inherit = precisionrecallBase,
    private = list(
        .init = function() {
            # Initialize plots and tables
            if (is.null(self$options$outcome) || length(self$options$scores) == 0) {
                self$results$instructions$setVisible(TRUE)
                self$results$prcPlot$setVisible(FALSE)
                self$results$aucTable$setVisible(FALSE)
            } else {
                self$results$instructions$setVisible(FALSE)
                self$results$prcPlot$setVisible(TRUE)
                self$results$aucTable$setVisible(TRUE)
            }
        },

        .run = function() {
            # Check prerequisites
            if (is.null(self$options$outcome) || length(self$options$scores) == 0)
                return()

            # Get data
            outcome <- self$data[[self$options$outcome]]
            positiveClass <- self$options$positiveClass

            # Ensure outcome is binary
            if (length(unique(na.omit(outcome))) != 2) {
                stop("Outcome variable must have exactly 2 levels")
            }

            # Calculate PRC for each score
            prcList <- list()
            aucValues <- numeric()

            for (scoreVar in self$options$scores) {
                score <- self$data[[scoreVar]]

                # Remove missing values
                valid <- complete.cases(outcome, score)
                outcomeClean <- outcome[valid]
                scoreClean <- score[valid]

                # Calculate precision-recall points
                prc <- private$.calculatePRC(outcomeClean, scoreClean, positiveClass)

                # Apply interpolation if requested
                if (self$options$interpolation == "nonlinear") {
                    # For now, use linear (TODO: implement proper non-linear)
                    # Full non-linear interpolation requires TP/FP tracking
                }

                # Calculate AUC
                auc <- private$.calculateAUC_PRC(prc, method = self$options$aucMethod)

                prcList[[scoreVar]] <- prc
                aucValues <- c(aucValues, auc)
            }

            # Calculate baseline (random classifier)
            nPos <- sum(outcome == positiveClass, na.rm = TRUE)
            nTotal <- sum(!is.na(outcome))
            baseline <- nPos / nTotal

            # Populate AUC table
            aucTable <- self$results$aucTable
            for (i in seq_along(self$options$scores)) {
                aucTable$setRow(rowKey = i, values = list(
                    model = self$options$scores[i],
                    auc = aucValues[i],
                    baseline = baseline,
                    improvement = aucValues[i] - baseline
                ))
            }

            # Bootstrap CIs if requested
            if (self$options$ci) {
                for (i in seq_along(self$options$scores)) {
                    score <- self$data[[self$options$scores[i]]]
                    valid <- complete.cases(outcome, score)

                    ci <- private$.bootstrapCI(
                        outcome[valid],
                        score[valid],
                        positiveClass,
                        samples = self$options$ciSamples,
                        method = self$options$ciMethod,
                        level = self$options$ciWidth
                    )

                    aucTable$setRow(rowNo = i, values = list(
                        lower = ci[1],
                        upper = ci[2]
                    ))
                }
            }

            # Model comparison if requested
            if (self$options$comparison && length(self$options$scores) > 1) {
                compTable <- self$results$comparisonTable
                compTable$deleteRows()

                # Pairwise comparisons
                for (i in 1:(length(self$options$scores)-1)) {
                    for (j in (i+1):length(self$options$scores)) {
                        score1 <- self$data[[self$options$scores[i]]]
                        score2 <- self$data[[self$options$scores[j]]]

                        valid <- complete.cases(outcome, score1, score2)

                        pValue <- private$.comparePRC(
                            outcome[valid],
                            score1[valid],
                            score2[valid],
                            positiveClass,
                            method = self$options$comparisonMethod
                        )

                        compTable$addRow(rowKey = paste0(i, "_", j), values = list(
                            model1 = self$options$scores[i],
                            model2 = self$options$scores[j],
                            aucDiff = aucValues[i] - aucValues[j],
                            pValue = pValue
                        ))
                    }
                }
            }

            # Create PRC plot
            image <- self$results$prcPlot
            image$setState(list(
                prcList = prcList,
                baseline = baseline,
                showBaseline = self$options$showBaseline,
                showFScore = self$options$showFScore,
                scoreNames = self$options$scores
            ))

            # Create companion ROC plot if requested
            if (self$options$showROC) {
                rocImage <- self$results$rocPlot
                rocList <- lapply(self$options$scores, function(scoreVar) {
                    score <- self$data[[scoreVar]]
                    valid <- complete.cases(outcome, score)
                    private$.calculateROC(outcome[valid], score[valid], positiveClass)
                })
                rocImage$setState(list(
                    rocList = rocList,
                    scoreNames = self$options$scores
                ))
            }
        },

        .calculatePRC = function(outcome, score, positiveClass) {
            # Calculate precision-recall points at all thresholds
            thresholds <- sort(unique(score), decreasing = TRUE)

            recall <- numeric(length(thresholds))
            precision <- numeric(length(thresholds))

            for (i in seq_along(thresholds)) {
                predicted <- score >= thresholds[i]
                tp <- sum(predicted & outcome == positiveClass, na.rm = TRUE)
                fp <- sum(predicted & outcome != positiveClass, na.rm = TRUE)
                fn <- sum(!predicted & outcome == positiveClass, na.rm = TRUE)

                recall[i] <- ifelse(tp + fn == 0, 0, tp / (tp + fn))
                precision[i] <- ifelse(tp + fp == 0, 1, tp / (tp + fp))
            }

            # Add (0,1) and (1, baseline) endpoints
            nPos <- sum(outcome == positiveClass, na.rm = TRUE)
            nTotal <- length(outcome)
            baseline <- nPos / nTotal

            data.frame(
                recall = c(0, recall, 1),
                precision = c(1, precision, baseline),
                threshold = c(Inf, thresholds, -Inf)
            )
        },

        .calculateAUC_PRC = function(prc, method = "trapezoid") {
            # Calculate area under PRC curve using trapezoidal rule
            auc <- 0
            for (i in 1:(nrow(prc)-1)) {
                width <- prc$recall[i+1] - prc$recall[i]
                height <- (prc$precision[i] + prc$precision[i+1]) / 2
                auc <- auc + width * height
            }

            return(abs(auc))
        },

        .calculateROC = function(outcome, score, positiveClass) {
            # Calculate ROC points at all thresholds
            thresholds <- sort(unique(score), decreasing = TRUE)

            tpr <- numeric(length(thresholds))
            fpr <- numeric(length(thresholds))

            for (i in seq_along(thresholds)) {
                predicted <- score >= thresholds[i]
                tp <- sum(predicted & outcome == positiveClass, na.rm = TRUE)
                fp <- sum(predicted & outcome != positiveClass, na.rm = TRUE)
                fn <- sum(!predicted & outcome == positiveClass, na.rm = TRUE)
                tn <- sum(!predicted & outcome != positiveClass, na.rm = TRUE)

                tpr[i] <- ifelse(tp + fn == 0, 0, tp / (tp + fn))
                fpr[i] <- ifelse(fp + tn == 0, 0, fp / (fp + tn))
            }

            # Add (0,0) and (1,1) endpoints
            data.frame(
                fpr = c(0, fpr, 1),
                tpr = c(0, tpr, 1),
                threshold = c(Inf, thresholds, -Inf)
            )
        },

        .bootstrapCI = function(outcome, score, positiveClass, samples = 1000,
                               method = "percentile", level = 95) {
            # Bootstrap confidence interval for AUC(PRC)
            aucBoot <- numeric(samples)

            n <- length(outcome)

            for (b in 1:samples) {
                # Resample with replacement
                idx <- sample(1:n, n, replace = TRUE)
                outcomeB <- outcome[idx]
                scoreB <- score[idx]

                # Calculate PRC and AUC
                prcB <- private$.calculatePRC(outcomeB, scoreB, positiveClass)
                aucBoot[b] <- private$.calculateAUC_PRC(prcB, method = self$options$aucMethod)
            }

            # Calculate CI
            alpha <- (100 - level) / 100
            if (method == "percentile" || method == "bootstrap") {
                ci <- quantile(aucBoot, probs = c(alpha/2, 1 - alpha/2), na.rm = TRUE)
            } else if (method == "bca") {
                # Simplified BCa (full implementation would require acceleration constant)
                ci <- quantile(aucBoot, probs = c(alpha/2, 1 - alpha/2), na.rm = TRUE)
            }

            return(ci)
        },

        .comparePRC = function(outcome, score1, score2, positiveClass, method = "bootstrap") {
            # Statistical comparison of two PRC curves

            if (method == "bootstrap") {
                # Bootstrap test for AUC difference
                nBoot <- 1000
                aucDiff <- numeric(nBoot)

                n <- length(outcome)

                for (b in 1:nBoot) {
                    idx <- sample(1:n, n, replace = TRUE)

                    prc1 <- private$.calculatePRC(outcome[idx], score1[idx], positiveClass)
                    prc2 <- private$.calculatePRC(outcome[idx], score2[idx], positiveClass)

                    auc1 <- private$.calculateAUC_PRC(prc1)
                    auc2 <- private$.calculateAUC_PRC(prc2)

                    aucDiff[b] <- auc1 - auc2
                }

                # Two-sided p-value
                pValue <- 2 * min(mean(aucDiff >= 0), mean(aucDiff <= 0))

            } else if (method == "permutation") {
                # Permutation test (simplified)
                pValue <- 0.5  # Placeholder
            }

            return(pValue)
        },

        .plotPRC = function(image, ...) {
            # Create PRC plot
            state <- image$state

            if (is.null(state))
                return(FALSE)

            library(ggplot2)

            # Combine all PRC curves
            prcData <- do.call(rbind, lapply(seq_along(state$prcList), function(i) {
                df <- state$prcList[[i]]
                df$model <- state$scoreNames[i]
                df
            }))

            # Create plot
            p <- ggplot(prcData, aes(x = recall, y = precision, color = model)) +
                geom_line(size = 1) +
                scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
                scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
                labs(
                    x = "Recall (Sensitivity)",
                    y = "Precision (PPV)",
                    title = "Precision-Recall Curve",
                    color = "Model"
                ) +
                theme_minimal() +
                theme(
                    legend.position = "right",
                    panel.grid.minor = element_blank()
                )

            # Add baseline if requested
            if (state$showBaseline) {
                p <- p + geom_hline(
                    yintercept = state$baseline,
                    linetype = "dashed",
                    color = "gray50",
                    size = 0.5
                ) +
                annotate("text", x = 0.5, y = state$baseline + 0.05,
                         label = paste0("Random baseline = ", round(state$baseline, 3)),
                         color = "gray50", size = 3)
            }

            # Add F-score iso-lines if requested
            if (state$showFScore) {
                fScores <- c(0.2, 0.4, 0.6, 0.8, 0.9)
                for (f in fScores) {
                    # F = 2*P*R / (P+R), solve for P: P = F*R / (2*R - F)
                    recallSeq <- seq(f/2, 1, length.out = 100)
                    precSeq <- (f * recallSeq) / (2 * recallSeq - f)
                    precSeq[precSeq > 1 | precSeq < 0] <- NA

                    p <- p + geom_line(
                        data = data.frame(recall = recallSeq, precision = precSeq),
                        aes(x = recall, y = precision),
                        linetype = "dotted",
                        color = "gray70",
                        size = 0.3,
                        inherit.aes = FALSE
                    ) +
                    annotate("text", x = 0.95, y = (f * 0.95) / (2 * 0.95 - f),
                             label = paste0("F=", f), color = "gray70", size = 2.5)
                }
            }

            print(p)

            return(TRUE)
        },

        .plotROC = function(image, ...) {
            # Companion ROC plot
            state <- image$state

            if (is.null(state))
                return(FALSE)

            library(ggplot2)

            # Combine all ROC curves
            rocData <- do.call(rbind, lapply(seq_along(state$rocList), function(i) {
                df <- state$rocList[[i]]
                df$model <- state$scoreNames[i]
                df
            }))

            # Create plot
            p <- ggplot(rocData, aes(x = fpr, y = tpr, color = model)) +
                geom_line(size = 1) +
                geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray50") +
                scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
                scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
                labs(
                    x = "False Positive Rate (1 - Specificity)",
                    y = "True Positive Rate (Sensitivity)",
                    title = "Companion ROC Curve",
                    color = "Model"
                ) +
                theme_minimal() +
                theme(
                    legend.position = "right",
                    panel.grid.minor = element_blank()
                )

            print(p)

            return(TRUE)
        }
    )
)
