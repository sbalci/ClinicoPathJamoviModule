# Multi-class Diagnostic Performance Evaluation
# This file is automatically generated, you should not edit it

multiclassdiagnosticsClass <- R6::R6Class(
    "multiclassdiagnosticsClass",
    inherit = multiclassdiagnosticsBase,
    private = list(
        .init = function() {
            if (is.null(self$options$predicted) || is.null(self$options$actual)) {
                return()
            }
        },
        
        .run = function() {
            # Check if required variables are specified
            if (is.null(self$options$predicted) || is.null(self$options$actual)) {
                return()
            }
            
            # Get the data
            data <- self$data
            predicted <- data[[self$options$predicted]]
            actual <- data[[self$options$actual]]
            
            # Remove missing values
            complete_cases <- complete.cases(predicted, actual)
            predicted <- predicted[complete_cases]
            actual <- actual[complete_cases]
            
            # Convert to factors if necessary
            if (!is.factor(predicted)) predicted <- as.factor(predicted)
            if (!is.factor(actual)) actual <- as.factor(actual)
            
            # Get unique classes
            classes <- unique(c(levels(actual), levels(predicted)))
            n_classes <- length(classes)
            
            # Calculate confusion matrix
            if (self$options$showConfusion) {
                self$.calculateConfusionMatrix(predicted, actual, classes)
            }
            
            # Calculate per-class metrics
            if (self$options$showPerClass) {
                self$.calculatePerClassMetrics(predicted, actual, classes)
            }
            
            # Calculate overall metrics
            if (self$options$showOverall) {
                self$.calculateOverallMetrics(predicted, actual)
            }
            
            # Model comparison if requested
            if (self$options$compareModels && !is.null(self$options$predicted2)) {
                predicted2 <- data[[self$options$predicted2]]
                predicted2 <- predicted2[complete_cases]
                if (!is.factor(predicted2)) predicted2 <- as.factor(predicted2)
                self$.compareModels(predicted, predicted2, actual)
            }
            
            # Generate plots
            if (self$options$showROC) {
                self$.rocPlot(predicted, actual, classes)
            }
            
            if (self$options$showConfusion) {
                self$.confusionPlot(predicted, actual, classes)
            }
            
            if (self$options$showPerClass) {
                self$.metricsPlot()
            }
        },
        
        .calculateConfusionMatrix = function(predicted, actual, classes) {
            # Create confusion matrix
            cm <- table(Actual = actual, Predicted = predicted)
            
            # Prepare table for output
            confTable <- self$results$confusionMatrix
            
            # Clear existing rows
            confTable$deleteRows()
            
            # Add rows for each actual class
            for (i in seq_along(classes)) {
                row <- list(actual_class = as.character(classes[i]))
                
                # Add predicted counts for each class
                for (j in seq_along(classes)) {
                    col_name <- paste0("predicted_", classes[j])
                    if (classes[i] %in% rownames(cm) && classes[j] %in% colnames(cm)) {
                        row[[col_name]] <- cm[classes[i], classes[j]]
                    } else {
                        row[[col_name]] <- 0
                    }
                }
                
                # Add total
                row[[".total[predicted]"]] <- sum(actual == classes[i])
                
                confTable$addRow(rowKey = i, values = row)
            }
            
            # Add columns dynamically for predicted classes
            for (j in seq_along(classes)) {
                col_name <- paste0("predicted_", classes[j])
                if (!col_name %in% names(confTable$columns)) {
                    confTable$addColumn(
                        name = col_name,
                        title = as.character(classes[j]),
                        type = "integer"
                    )
                }
            }
        },
        
        .calculatePerClassMetrics = function(predicted, actual, classes) {
            perClassTable <- self$results$perClassMetrics
            perClassTable$deleteRows()
            
            conf_level <- self$options$confidenceLevel
            
            for (i in seq_along(classes)) {
                class_name <- classes[i]
                
                # Binary classification for this class (one-vs-rest)
                binary_actual <- ifelse(actual == class_name, 1, 0)
                binary_predicted <- ifelse(predicted == class_name, 1, 0)
                
                # Calculate metrics
                tp <- sum(binary_actual == 1 & binary_predicted == 1)
                tn <- sum(binary_actual == 0 & binary_predicted == 0)
                fp <- sum(binary_actual == 0 & binary_predicted == 1)
                fn <- sum(binary_actual == 1 & binary_predicted == 0)
                
                n <- tp + fn  # Number in this class
                
                # Sensitivity (Recall)
                sensitivity <- if ((tp + fn) > 0) tp / (tp + fn) else NA
                
                # Specificity
                specificity <- if ((tn + fp) > 0) tn / (tn + fp) else NA
                
                # PPV (Precision)
                ppv <- if ((tp + fp) > 0) tp / (tp + fp) else NA
                
                # NPV
                npv <- if ((tn + fn) > 0) tn / (tn + fn) else NA
                
                # F1 Score
                f1 <- if (!is.na(sensitivity) && !is.na(ppv) && (sensitivity + ppv) > 0) {
                    2 * (sensitivity * ppv) / (sensitivity + ppv)
                } else {
                    NA
                }
                
                # Youden Index
                youden <- if (!is.na(sensitivity) && !is.na(specificity)) {
                    sensitivity + specificity - 1
                } else {
                    NA
                }
                
                # Calculate AUC if pROC is available
                auc_value <- NA
                if (requireNamespace("pROC", quietly = TRUE)) {
                    if (length(unique(binary_actual)) == 2) {
                        roc_obj <- pROC::roc(binary_actual, binary_predicted, quiet = TRUE)
                        auc_value <- as.numeric(pROC::auc(roc_obj))
                    }
                }
                
                perClassTable$addRow(
                    rowKey = i,
                    values = list(
                        class = as.character(class_name),
                        n = n,
                        sensitivity = sensitivity,
                        specificity = specificity,
                        ppv = ppv,
                        npv = npv,
                        f1 = f1,
                        youden = youden,
                        auc = auc_value
                    )
                )
            }
        },
        
        .calculateOverallMetrics = function(predicted, actual) {
            overallTable <- self$results$overallMetrics
            overallTable$deleteRows()
            
            conf_level <- self$options$confidenceLevel
            n <- length(predicted)
            
            # Overall accuracy
            accuracy <- sum(predicted == actual) / n
            
            # Calculate confidence interval for accuracy
            se_acc <- sqrt(accuracy * (1 - accuracy) / n)
            z <- qnorm((1 + conf_level) / 2)
            ci_lower_acc <- accuracy - z * se_acc
            ci_upper_acc <- accuracy + z * se_acc
            
            overallTable$addRow(
                rowKey = 1,
                values = list(
                    metric = "Overall Accuracy",
                    value = accuracy,
                    ci_lower = ci_lower_acc,
                    ci_upper = ci_upper_acc
                )
            )
            
            # Cohen's Kappa
            if (requireNamespace("psych", quietly = TRUE)) {
                kappa_result <- psych::cohen.kappa(table(predicted, actual))
                kappa_value <- kappa_result$kappa
                kappa_ci <- kappa_result$confid
                
                overallTable$addRow(
                    rowKey = 2,
                    values = list(
                        metric = "Cohen's Kappa",
                        value = kappa_value,
                        ci_lower = if (!is.null(kappa_ci)) kappa_ci[1, 1] else NA,
                        ci_upper = if (!is.null(kappa_ci)) kappa_ci[1, 2] else NA
                    )
                )
            }
            
            # Weighted average of per-class metrics
            classes <- unique(c(levels(actual), levels(predicted)))
            weighted_f1 <- 0
            weighted_sensitivity <- 0
            weighted_specificity <- 0
            
            for (class_name in classes) {
                class_weight <- sum(actual == class_name) / n
                
                binary_actual <- ifelse(actual == class_name, 1, 0)
                binary_predicted <- ifelse(predicted == class_name, 1, 0)
                
                tp <- sum(binary_actual == 1 & binary_predicted == 1)
                tn <- sum(binary_actual == 0 & binary_predicted == 0)
                fp <- sum(binary_actual == 0 & binary_predicted == 1)
                fn <- sum(binary_actual == 1 & binary_predicted == 0)
                
                sensitivity <- if ((tp + fn) > 0) tp / (tp + fn) else 0
                specificity <- if ((tn + fp) > 0) tn / (tn + fp) else 0
                precision <- if ((tp + fp) > 0) tp / (tp + fp) else 0
                
                f1 <- if ((sensitivity + precision) > 0) {
                    2 * (sensitivity * precision) / (sensitivity + precision)
                } else {
                    0
                }
                
                weighted_f1 <- weighted_f1 + f1 * class_weight
                weighted_sensitivity <- weighted_sensitivity + sensitivity * class_weight
                weighted_specificity <- weighted_specificity + specificity * class_weight
            }
            
            overallTable$addRow(
                rowKey = 3,
                values = list(
                    metric = "Weighted F1 Score",
                    value = weighted_f1,
                    ci_lower = NA,
                    ci_upper = NA
                )
            )
            
            overallTable$addRow(
                rowKey = 4,
                values = list(
                    metric = "Weighted Sensitivity",
                    value = weighted_sensitivity,
                    ci_lower = NA,
                    ci_upper = NA
                )
            )
            
            overallTable$addRow(
                rowKey = 5,
                values = list(
                    metric = "Weighted Specificity",
                    value = weighted_specificity,
                    ci_lower = NA,
                    ci_upper = NA
                )
            )
        },
        
        .compareModels = function(predicted1, predicted2, actual) {
            compTable <- self$results$modelComparison
            compTable$deleteRows()
            
            # Calculate metrics for both models
            acc1 <- sum(predicted1 == actual) / length(actual)
            acc2 <- sum(predicted2 == actual) / length(actual)
            
            # McNemar test if requested
            if (self$options$mcnemarTest) {
                mcTable <- self$results$mcnemarResults
                mcTable$deleteRows()
                
                # Create contingency table for McNemar test
                correct1 <- predicted1 == actual
                correct2 <- predicted2 == actual
                
                cont_table <- table(Model1 = correct1, Model2 = correct2)
                
                if (nrow(cont_table) == 2 && ncol(cont_table) == 2) {
                    mc_result <- mcnemar.test(cont_table)
                    
                    mcTable$addRow(
                        rowKey = 1,
                        values = list(
                            comparison = "Model 1 vs Model 2",
                            chi_squared = mc_result$statistic,
                            df = mc_result$parameter,
                            p_value = mc_result$p.value
                        )
                    )
                }
            }
            
            # Add accuracy comparison
            compTable$addRow(
                rowKey = 1,
                values = list(
                    metric = "Accuracy",
                    model1 = acc1,
                    model2 = acc2,
                    difference = acc2 - acc1,
                    p_value = NA
                )
            )
            
            # DeLong test for binary classification
            if (self$options$deLongTest && length(unique(actual)) == 2) {
                if (requireNamespace("pROC", quietly = TRUE)) {
                    deLongTable <- self$results$deLongResults
                    deLongTable$deleteRows()
                    
                    # Convert to numeric for ROC
                    actual_binary <- as.numeric(actual) - 1
                    pred1_binary <- as.numeric(predicted1) - 1
                    pred2_binary <- as.numeric(predicted2) - 1
                    
                    tryCatch({
                        roc1 <- pROC::roc(actual_binary, pred1_binary, quiet = TRUE)
                        roc2 <- pROC::roc(actual_binary, pred2_binary, quiet = TRUE)
                        
                        delong_test <- pROC::roc.test(roc1, roc2, method = "delong")
                        
                        deLongTable$addRow(
                            rowKey = 1,
                            values = list(
                                comparison = "ROC Curve Comparison",
                                z_statistic = delong_test$statistic,
                                p_value = delong_test$p.value
                            )
                        )
                        
                        # Add AUC comparison
                        compTable$addRow(
                            rowKey = 2,
                            values = list(
                                metric = "AUC",
                                model1 = as.numeric(pROC::auc(roc1)),
                                model2 = as.numeric(pROC::auc(roc2)),
                                difference = as.numeric(pROC::auc(roc2)) - as.numeric(pROC::auc(roc1)),
                                p_value = delong_test$p.value
                            )
                        )
                    }, error = function(e) {
                        # Handle error silently
                    })
                }
            }
        },
        
        .rocPlot = function(predicted, actual, classes) {
            if (!requireNamespace("pROC", quietly = TRUE)) {
                return()
            }
            
            image <- self$results$rocPlot
            
            image$setState(list(
                predicted = predicted,
                actual = actual,
                classes = classes,
                theme = self$options$plotTheme
            ))
        },
        
        .rocPlotSource = function(image, ...) {
            if (!requireNamespace("pROC", quietly = TRUE) || 
                !requireNamespace("ggplot2", quietly = TRUE)) {
                return(FALSE)
            }
            
            predicted <- image$state$predicted
            actual <- image$state$actual
            classes <- image$state$classes
            theme_choice <- image$state$theme
            
            # Create ROC curves for each class (one-vs-rest)
            roc_data <- data.frame()
            
            for (class_name in classes) {
                binary_actual <- ifelse(actual == class_name, 1, 0)
                binary_predicted <- ifelse(predicted == class_name, 1, 0)
                
                if (length(unique(binary_actual)) == 2) {
                    roc_obj <- pROC::roc(binary_actual, binary_predicted, quiet = TRUE)
                    
                    # Extract ROC curve points
                    roc_df <- data.frame(
                        sensitivity = rev(roc_obj$sensitivities),
                        specificity = rev(roc_obj$specificities),
                        class = as.character(class_name),
                        auc = as.numeric(pROC::auc(roc_obj))
                    )
                    
                    roc_data <- rbind(roc_data, roc_df)
                }
            }
            
            # Create plot
            p <- ggplot2::ggplot(roc_data, ggplot2::aes(x = 1 - specificity, y = sensitivity, color = class)) +
                ggplot2::geom_line(size = 1.2) +
                ggplot2::geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray50") +
                ggplot2::labs(
                    title = "ROC Curves (One-vs-Rest)",
                    x = "1 - Specificity (False Positive Rate)",
                    y = "Sensitivity (True Positive Rate)",
                    color = "Class"
                ) +
                ggplot2::coord_equal() +
                ggplot2::scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
                ggplot2::scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2))
            
            # Apply theme
            if (theme_choice == "minimal") {
                p <- p + ggplot2::theme_minimal()
            } else if (theme_choice == "classic") {
                p <- p + ggplot2::theme_classic()
            } else {
                p <- p + ggplot2::theme_bw()
            }
            
            # Add AUC values to legend
            unique_classes <- unique(roc_data$class)
            auc_values <- sapply(unique_classes, function(c) {
                unique(roc_data$auc[roc_data$class == c])[1]
            })
            
            labels <- paste0(unique_classes, " (AUC = ", round(auc_values, 3), ")")
            p <- p + ggplot2::scale_color_discrete(labels = labels)
            
            print(p)
            TRUE
        },
        
        .confusionPlot = function(predicted, actual, classes) {
            image <- self$results$confusionPlot
            
            image$setState(list(
                predicted = predicted,
                actual = actual,
                classes = classes,
                theme = self$options$plotTheme
            ))
        },
        
        .confusionPlotSource = function(image, ...) {
            if (!requireNamespace("ggplot2", quietly = TRUE)) {
                return(FALSE)
            }
            
            predicted <- image$state$predicted
            actual <- image$state$actual
            classes <- image$state$classes
            
            # Create confusion matrix
            cm <- table(Actual = actual, Predicted = predicted)
            
            # Convert to data frame for plotting
            cm_df <- as.data.frame(cm)
            
            # Calculate percentages
            cm_df$percentage <- cm_df$Freq / sum(cm_df$Freq) * 100
            
            # Create heatmap
            p <- ggplot2::ggplot(cm_df, ggplot2::aes(x = Predicted, y = Actual, fill = Freq)) +
                ggplot2::geom_tile() +
                ggplot2::geom_text(ggplot2::aes(label = paste0(Freq, "\n(", round(percentage, 1), "%)")), 
                                  color = "white", size = 4) +
                ggplot2::scale_fill_gradient(low = "lightblue", high = "darkblue") +
                ggplot2::labs(
                    title = "Confusion Matrix",
                    x = "Predicted Class",
                    y = "Actual Class",
                    fill = "Count"
                ) +
                ggplot2::theme_minimal() +
                ggplot2::theme(
                    axis.text.x = ggplot2::element_text(angle = 45, hjust = 1),
                    axis.text.y = ggplot2::element_text(angle = 0, hjust = 1)
                )
            
            print(p)
            TRUE
        },
        
        .metricsPlot = function() {
            image <- self$results$metricsPlot
            
            # Get data from perClassMetrics table
            metricsTable <- self$results$perClassMetrics
            
            if (metricsTable$rowCount == 0) {
                return()
            }
            
            image$setState(list(
                metricsTable = metricsTable$asDF(),
                theme = self$options$plotTheme
            ))
        },
        
        .metricsPlotSource = function(image, ...) {
            if (!requireNamespace("ggplot2", quietly = TRUE) || 
                !requireNamespace("tidyr", quietly = TRUE)) {
                return(FALSE)
            }
            
            metrics_df <- image$state$metricsTable
            
            # Reshape data for plotting
            metrics_long <- tidyr::pivot_longer(
                metrics_df,
                cols = c("sensitivity", "specificity", "ppv", "npv", "f1"),
                names_to = "metric",
                values_to = "value"
            )
            
            # Create grouped bar plot
            p <- ggplot2::ggplot(metrics_long, ggplot2::aes(x = class, y = value, fill = metric)) +
                ggplot2::geom_bar(stat = "identity", position = "dodge") +
                ggplot2::labs(
                    title = "Per-Class Performance Metrics",
                    x = "Class",
                    y = "Value",
                    fill = "Metric"
                ) +
                ggplot2::scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
                ggplot2::theme_minimal() +
                ggplot2::theme(
                    axis.text.x = ggplot2::element_text(angle = 45, hjust = 1),
                    legend.position = "bottom"
                )
            
            print(p)
            TRUE
        }
    )
)