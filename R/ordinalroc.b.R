
# This file is automatically generated, you probably don't want to edit this

ordinalrocClass <- if (requireNamespace('jmvcore', quietly=TRUE)) R6::R6Class(
    "ordinalrocClass",
    inherit = ordinalrocBase,
    private = list(
        .init = function() {
            private$.initInstructions()
        },

        .run = function() {
            # Check for required inputs
            if (is.null(self$options$predictor) || is.null(self$options$ordinal_outcome)) {
                return()
            }

            # Get data
            data <- self$data
            predictor <- as.numeric(data[[self$options$predictor]])
            outcome <- data[[self$options$ordinal_outcome]]

            # Remove missing values
            complete_cases <- complete.cases(predictor, outcome)
            predictor <- predictor[complete_cases]
            outcome <- outcome[complete_cases]

            if (length(predictor) < 10) {
                stop("Insufficient data for ordinal ROC analysis. Need at least 10 complete observations.")
            }

            # Validate ordinal outcome has 3+ levels
            outcome_levels <- levels(outcome)
            if (length(outcome_levels) < 3) {
                stop("Ordinal outcome must have at least 3 levels. For binary outcomes, use standard ROC analysis.")
            }

            # Check minimum observations per category
            category_counts <- table(outcome)
            if (any(category_counts < 3)) {
                stop(paste0("All outcome categories must have at least 3 observations. ",
                          "Current counts: ", paste(names(category_counts), "=", category_counts, collapse=", ")))
            }

            # Perform ordinal ROC analysis
            tryCatch({
                # Calculate overall summary
                private$.populateOverallSummary(predictor, outcome)

                # Calculate category-specific AUC if requested
                if (self$options$category_specific_auc) {
                    private$.populateCategorySpecificAUC(predictor, outcome)
                }

                # Test proportional odds assumption if requested
                if (self$options$test_proportional_odds) {
                    private$.testProportionalOdds(predictor, outcome)
                }

                # Calculate model coefficients if requested
                if (self$options$show_coefficients && self$options$roc_method == "proportional_odds") {
                    private$.populateModelCoefficients(predictor, outcome)
                }

                # Calculate optimal thresholds if requested
                if (self$options$optimal_thresholds) {
                    private$.populateOptimalThresholds(predictor, outcome)
                }

            }, error = function(e) {
                stop(paste0("Error in ordinal ROC analysis: ", e$message))
            })
        },

        .initInstructions = function() {
            instructions <- self$results$instructions
            html <- "<h3>Ordinal ROC Analysis</h3>
            <p>ROC curves for ordered categorical outcomes (tumor grading, fibrosis staging, cancer staging).</p>

            <h4>Required Inputs:</h4>
            <ul>
                <li><b>Predictor:</b> Continuous variable (biomarker, risk score, imaging feature)</li>
                <li><b>Ordinal Outcome:</b> Ordered categorical outcome (3+ levels)</li>
                <li><b>Outcome Order:</b> Specify ordering from low to high severity</li>
            </ul>

            <h4>Clinical Applications:</h4>
            <ul>
                <li><b>Tumor Differentiation:</b> Well/moderate/poor (e.g., Gleason score)</li>
                <li><b>Fibrosis Stage:</b> F0/F1/F2/F3/F4 (liver fibrosis)</li>
                <li><b>Cancer Stage:</b> I/II/III/IV or T1/T2/T3/T4</li>
                <li><b>Inflammation:</b> None/mild/moderate/severe</li>
                <li><b>Nottingham Grade:</b> Grade 1/2/3</li>
            </ul>

            <h4>Key Concepts:</h4>
            <ul>
                <li><b>Ordinal AUC:</b> Generalized concordance probability across all ordered pairs</li>
                <li><b>Cumulative ROC:</b> Separate curves for each dichotomization (e.g., 1 vs 2+3, 1+2 vs 3)</li>
                <li><b>Proportional Odds:</b> Assumes predictor effect is constant across thresholds</li>
                <li><b>Category-Specific AUC:</b> Discrimination at each severity level</li>
            </ul>

            <h4>ROC Methods:</h4>
            <ul>
                <li><b>Empirical:</b> Non-parametric, uses observed distributions (most flexible)</li>
                <li><b>Binormal:</b> Assumes normal distributions within categories (parametric)</li>
                <li><b>Proportional Odds:</b> Uses logistic regression framework (most efficient)</li>
            </ul>

            <h4>Interpretation Example (Tumor Differentiation):</h4>
            <ul>
                <li>Overall AUC 0.75 indicates good discrimination across all grades</li>
                <li>AUC(well vs moderate+poor) = 0.80 shows strong discrimination at low grades</li>
                <li>AUC(well+moderate vs poor) = 0.72 shows moderate discrimination at high grades</li>
                <li>If proportional odds violated, consider partial proportional odds model</li>
            </ul>

            <h4>Sample Size Considerations:</h4>
            <ul>
                <li>Minimum 10-20 observations per ordinal category</li>
                <li>For 4-5 categories, need at least 50-100 total observations</li>
                <li>More categories require larger samples for stable estimates</li>
            </ul>

            <p><b>Note:</b> Implementation in development. Full functionality requires ordinalROC package or custom cumulative probability calculations.</p>"

            instructions$setContent(html)
        },

        .populateOverallSummary = function(predictor, outcome) {
            table <- self$results$overallSummary

            # Calculate ordinal AUC (generalized concordance probability)
            # This is based on the proportional odds model or empirical calculation

            n_obs <- length(predictor)
            n_categories <- length(levels(outcome))
            outcome_numeric <- as.numeric(outcome)

            # Calculate concordance probability (ordinal AUC)
            # For each pair of observations, check if higher predictor corresponds to higher outcome
            concordant <- 0
            discordant <- 0
            ties <- 0

            for (i in 1:(n_obs-1)) {
                for (j in (i+1):n_obs) {
                    if (outcome_numeric[i] < outcome_numeric[j]) {
                        if (predictor[i] < predictor[j]) {
                            concordant <- concordant + 1
                        } else if (predictor[i] > predictor[j]) {
                            discordant <- discordant + 1
                        } else {
                            ties <- ties + 1
                        }
                    } else if (outcome_numeric[i] > outcome_numeric[j]) {
                        if (predictor[i] > predictor[j]) {
                            concordant <- concordant + 1
                        } else if (predictor[i] < predictor[j]) {
                            discordant <- discordant + 1
                        } else {
                            ties <- ties + 1
                        }
                    }
                }
            }

            # Calculate ordinal AUC (Somers' D based)
            total_pairs <- concordant + discordant + ties
            auc_ordinal <- if (total_pairs > 0) {
                (concordant + 0.5 * ties) / total_pairs
            } else {
                NA
            }

            # Calculate standard error using bootstrap if requested
            if (self$options$auc_ci_method == "bootstrap") {
                set.seed(self$options$random_seed)
                bootstrap_aucs <- replicate(self$options$bootstrap_samples, {
                    boot_idx <- sample(1:n_obs, n_obs, replace = TRUE)
                    boot_pred <- predictor[boot_idx]
                    boot_outcome <- outcome_numeric[boot_idx]

                    # Calculate AUC for bootstrap sample
                    conc <- 0
                    disc <- 0
                    t <- 0
                    for (i in 1:(n_obs-1)) {
                        for (j in (i+1):n_obs) {
                            if (boot_outcome[i] < boot_outcome[j]) {
                                if (boot_pred[i] < boot_pred[j]) conc <- conc + 1
                                else if (boot_pred[i] > boot_pred[j]) disc <- disc + 1
                                else t <- t + 1
                            } else if (boot_outcome[i] > boot_outcome[j]) {
                                if (boot_pred[i] > boot_pred[j]) conc <- conc + 1
                                else if (boot_pred[i] < boot_pred[j]) disc <- disc + 1
                                else t <- t + 1
                            }
                        }
                    }
                    (conc + 0.5 * t) / (conc + disc + t)
                })

                auc_se <- sd(bootstrap_aucs, na.rm = TRUE)
                alpha <- 1 - self$options$confidence_level
                auc_ci_lower <- quantile(bootstrap_aucs, alpha/2, na.rm = TRUE)
                auc_ci_upper <- quantile(bootstrap_aucs, 1 - alpha/2, na.rm = TRUE)
            } else {
                # DeLong method approximation
                auc_se <- sqrt(auc_ordinal * (1 - auc_ordinal) / n_obs)
                z_crit <- qnorm(1 - (1 - self$options$confidence_level) / 2)
                auc_ci_lower <- max(0, auc_ordinal - z_crit * auc_se)
                auc_ci_upper <- min(1, auc_ordinal + z_crit * auc_se)
            }

            # Test H0: AUC = 0.5 (no discrimination)
            z_stat <- (auc_ordinal - 0.5) / auc_se
            p_value <- 2 * (1 - pnorm(abs(z_stat)))

            table$setRow(rowNo=1, values=list(
                predictor = self$options$predictor,
                outcome = self$options$ordinal_outcome,
                n_categories = n_categories,
                n_obs = n_obs,
                auc = auc_ordinal,
                auc_se = auc_se,
                auc_ci_lower = auc_ci_lower,
                auc_ci_upper = auc_ci_upper,
                p_value = p_value
            ))
        },

        .populateCategorySpecificAUC = function(predictor, outcome) {
            # TODO: Calculate AUC for each cumulative dichotomization
            # For grades 1/2/3: AUC(1 vs 2+3), AUC(1+2 vs 3)
        },

        .testProportionalOdds = function(predictor, outcome) {
            # TODO: Test proportional odds assumption using likelihood ratio test
            # Compare proportional odds model vs. separate binary logistic regressions
        },

        .populateModelCoefficients = function(predictor, outcome) {
            # TODO: Extract and display proportional odds model coefficients
            # Use MASS::polr() for proportional odds logistic regression
        },

        .populateOptimalThresholds = function(predictor, outcome) {
            # TODO: Calculate optimal thresholds for each cumulative split
            # Use Youden index or other criteria
        },

        .plotOrdinalROC = function(image, ...) {
            # TODO: Plot ordinal ROC curves
            # Show cumulative ROC for each dichotomization
        },

        .plotCumulativeROC = function(image, ...) {
            # TODO: Plot all cumulative ROC curves on same plot
        },

        .plotCategoryDistributions = function(image, ...) {
            # TODO: Plot predictor distributions by outcome category
            # Use violin/box plots or density curves
        },

        .plotCumulativeProbabilities = function(image, ...) {
            # TODO: Plot cumulative probabilities vs. predictor value
        }
    )
)
