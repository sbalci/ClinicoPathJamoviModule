Pooling Diagnostic Accuracy Measures for an IHC Marker Across Studies

Generated by chatGPT: https://chatgpt.com/share/68a56f7b-b594-8002-bb9d-e7a7e8c39ed9 

Why Pool Diagnostic Test Performance?

Pooling diagnostic performance measures from multiple studies is essentially a meta-analysis of diagnostic test accuracy (DTA). A single study may provide an estimate of sensitivity, specificity, etc., but with limited precision. By combining studies that evaluate the same immunohistochemistry (IHC) marker, we increase statistical power and obtain more robust summary estimates of how the marker performs as a diagnostic test. This helps determine whether the test is sufficiently sensitive and specific for clinical use and how consistent its performance is across different settings ￼ ￼. Moreover, a meta-analysis can reveal heterogeneity – variations in results between studies – and prompts investigation of factors (e.g. differences in patient population or test protocols) that might explain discrepancies. In summary, pooling is done to achieve a comprehensive, generalizable picture of the marker’s diagnostic accuracy, rather than relying on any single study’s results.

How is pooling done? In practice, one gathers the 2×2 contingency data from each study (true positives, false positives, true negatives, false negatives). Traditional univariate meta-analysis techniques are not directly applicable because each study yields paired outcomes (sensitivity and specificity) rather than a single effect size. Naively pooling each measure (e.g. averaging sensitivities) without accounting for their interplay can be misleading. Instead, meta-analysis of DTA requires specialized models that synthesize sensitivity and specificity simultaneously, accounting for the trade-off between them ￼. This is typically achieved through hierarchical models (random-effects models) that operate on the study-level 2×2 data. These models consider both within-study sampling error (each study’s finite sample of patients) and between-study variability (true differences in study results). By doing so, we obtain pooled estimates (with confidence intervals) for sensitivity and specificity, often along with a summary receiver operating characteristic (ROC) curve that reflects the overall diagnostic ability of the test.

Statistical Approaches for Diagnostic Meta-Analysis

Bivariate Random-Effects Model (Reitsma Model)

The bivariate random-effects model is the recommended approach for pooling sensitivity and specificity ￼. In this model (first described by Reitsma et al. 2005), the logit-transformed sensitivity and logit-transformed specificity (or equivalently logit of false-positive rate) from each study are treated as bivariate outcomes. We assume these follow a bivariate normal distribution across studies, with some average (“fixed effect”) sensitivity and specificity and with between-study variance that captures heterogeneity. Crucially, this model allows a correlation between sensitivity and specificity across studies ￼. Such a correlation often arises due to different implicit thresholds used in each study – for example, some studies might interpret the IHC marker with a lenient criterion (yielding high sensitivity but lower specificity) while others use a stricter threshold (lower sensitivity, higher specificity). The bivariate model captures this inverse relationship. It effectively fits a hierarchical logistic regression: one level models the 2×2 outcomes within each study (assuming binomial variability), and the second level models how the study’s true sensitivity and specificity vary across studies (assuming a normal distribution in the logit scale).

By fitting this model, we obtain pooled sensitivity and specificity estimates on the original probability scale (by back-transformation from the logits) along with their confidence intervals. We also get an estimate of the between-study variance for sensitivity and specificity, and their covariance. From these, one can derive other metrics: for instance, the model yields a summary point in ROC space with a confidence region, and one can compute the summary diagnostic odds ratio (DOR) or the area under the SROC curve (AUC) as overall measures of test performance. (The DOR is a single number summarizing accuracy: DOR = (sensitivity/(1–sensitivity)) / (1–specificity)/specificity, and the AUC is the area under the SROC curve.) Typically, the output of a bivariate model will include an estimate of the AUC as a threshold-invariant summary; for example, in one analysis the pooled AUC was around 0.89 ￼, indicating excellent overall discrimination.

Technical note: The bivariate model of Reitsma et al. is mathematically equivalent to the HSROC model (below) when no covariates are included ￼. It’s called “bivariate” because it treats sensitivity and specificity as two correlated outcomes. Estimation can be done via either frequentist methods (e.g. using restricted maximum likelihood as in the mada package) or Bayesian methods. The model yields a summary ROC curve representing the trade-off between sensitivity and specificity, and a summary operating point (pooled sens, spec). Because it’s a random-effects model, the summary point is a kind of weighted average of study results, and the model accounts for heterogeneity by having random intercepts for sensitivity and specificity.

Hierarchical Summary ROC (HSROC) Model

The HSROC model (Rutter & Gatsonis 2001) is another popular approach to diagnostic meta-analysis, and as noted, it is closely related to the Reitsma bivariate model. The HSROC model is formulated in terms of a test’s accuracy and threshold parameters. In this model, each study has its own underlying threshold (cut-off criteria for the IHC marker to be considered positive) which affects its sensitivity and specificity. Studies are assumed to have a common underlying “accuracy” parameter (related to the test’s intrinsic discrimination ability, akin to a log-DOR) and to vary in their threshold. The model uses a random effect for the threshold, allowing each study’s operating point to shift along the ROC curve. Essentially, the HSROC is a 2-level mixed-effects model: at level 1, the within-study true positive and false positive rates follow a binomial model; at level 2, the study-specific “threshold” is a random effect and the accuracy (log-DOR) can be either fixed or include random variation.

In practice, the HSROC model will estimate parameters that can be used to draw a summary ROC curve. From the HSROC fit, one can derive a summary point as well, often by choosing a point on the SROC curve that corresponds to a typical threshold or by maximizing a combination of sensitivity and specificity. However, if no covariates are used, the HSROC and bivariate Reitsma models produce essentially the same fit and SROC ￼. The difference is mostly in parameterization: the bivariate model directly gives the pooled sensitivity and specificity, whereas the HSROC model provides an estimated ROC curve and emphasizes how accuracy might change with threshold. Both models are hierarchical and random-effects, accounting for study heterogeneity. Current guidelines and literature recommend using one of these hierarchical models for diagnostic meta-analyses ￼, as they account for the sensitivity-specificity trade-off and study-level differences.

Model Outputs and Interpretation

Both the bivariate and HSROC approaches yield a summary sensitivity and specificity (with confidence intervals) that represent the pooled performance of the IHC marker across studies. They also provide a 95% confidence region around this summary point in ROC space, and often a 95% prediction region as well – the latter indicates where one might expect the results of a new study to fall. For instance, after fitting a bivariate model, one might report: “The pooled sensitivity was 89% (95% CI: 81–94%) and pooled specificity was 78% (95% CI: 71–83%) for the marker” (these numbers are illustrative) ￼. It is also common to report the summary positive likelihood ratio (LR+) and negative likelihood ratio (LR–), which can be derived from pooled sens/spec (LR+ = sens/(1–spec), LR– = (1–sens)/spec). These can be useful for clinical interpretation. If needed, one can translate these into pooled predictive values, as discussed below.

Finally, it’s worth noting that simpler, non-hierarchical methods exist (for example, pooling the diagnostic odds ratio in a univariate meta-analysis, or using the older Moses-Littenberg summary ROC method). While such methods were used historically, they are less rigorous. The bivariate random-effects/HSROC framework is preferred because it uses the complete data (all 2×2 tables) and properly models within-study binomial variation and between-study heterogeneity ￼ ￼.

Handling Heterogeneity Between Studies

Heterogeneity is almost inevitable in a meta-analysis of diagnostic accuracy. Different studies of the same IHC marker might use varying patient inclusion criteria, different reference standards, different positivity thresholds for the marker, or different laboratory techniques – all of which can affect measured sensitivity and specificity. It’s important first to quantify heterogeneity and then, if possible, to explain it.

The hierarchical models inherently accommodate heterogeneity by including random effects. For example, the bivariate model estimates between-study variance in the logit sensitivity and logit specificity. If this variance is large, it implies substantial heterogeneity beyond chance. Some meta-analyses report an I² statistic for sensitivity and specificity (analogous to the I² in standard meta-analysis), but in diagnostic meta-analyses the computation of I² is not straightforward and methods are still evolving ￼. A more direct way to appreciate heterogeneity is to look at the spread of study points in the SROC plot or the prediction interval for the summary sensitivity/specificity – a wide prediction interval indicates that different settings yield quite different test performance.

To handle heterogeneity, one strategy is subgroup analysis or meta-regression. The hierarchical models can include covariates (moderators) at the study level ￼. For instance, if some studies used only early-stage patients and others included late-stage patients, one could include a covariate for “patient stage” to see if it shifts sensitivity or specificity. In the bivariate model, this is done by modeling the logit(sens) and logit(FPR) as functions of the covariate. An example in R (mada package) might be: reitsma(data, formula = cbind(tsens, tfpr) ~ covariate), which would allow the covariate to affect the pooled operating point. Similarly, the HSROC model can incorporate covariates that affect either the threshold or accuracy parameters (e.g., an indicator if studies used an automated vs manual assay might shift the threshold at which the test is called positive). Including covariates helps explain systematic differences: e.g., one might find the marker is more sensitive in immunohistochemistry when using tissue samples from site A versus site B.

It’s also possible that heterogeneity comes from studies using the marker in different ways. The question mentioned additional unrelated markers in some studies. If studies evaluated the target IHC marker alongside other markers, this in itself doesn’t directly affect the target marker’s sensitivity/specificity – but it could indicate differing study focus or patient spectrum. If the presence of other markers signals something like “multi-marker panel vs single-marker” usage, that could be a covariate to examine. Otherwise, one should extract the data for the target marker from each study and treat all studies equally in the meta-analysis. If a single study reported multiple results for the same marker (e.g., using different positivity cut-offs or in multiple sub-populations), one must be careful – including multiple data points from one study requires either choosing one or using advanced modeling (to avoid double-counting that study). In general, address known heterogeneity by either stratifying the meta-analysis (analyze subgroups separately) or by meta-regression, and use random-effects models to account for residual heterogeneity. Always report the extent of heterogeneity and consider its implications for how confident we are in the pooled results. High heterogeneity might mean the summary estimate should be used cautiously, and one might emphasize the prediction interval (the range in which future study results might lie).

Pooling Predictive Values and Accuracy

Positive predictive value (PPV) and negative predictive value (NPV) are important in practice because they tell us, given a test result, what is the probability of the disease being present or absent. However, unlike sensitivity and specificity, PPV and NPV are prevalence-dependent. This means that if one study has a higher prevalence of the disease, it will tend to report a higher PPV (and lower NPV) than another study with lower prevalence even if the test’s intrinsic performance (sens/spec) is identical. As a result, pooling PPV/NPV across studies is not as straightforward – differences in PPV/NPV may simply reflect different prevalence rather than true differences in test performance ￼.

The recommended approach is usually to perform the meta-analysis on sensitivity and specificity (which are prevalence-independent characteristics of the test in each study), and then derive pooled PPV and NPV for a given prevalence. In other words, once you have pooled sensitivity (Sens) and specificity (Spec), you can calculate what the PPV and NPV would be at a certain assumed prevalence using Bayes’ theorem:
 • PPV = Sensitivity × Prevalence / [Sensitivity × Prevalence + (1 – Specificity) × (1 – Prevalence)]
 • NPV = Specificity × (1 – Prevalence) / [(1 – Sensitivity) × Prevalence + Specificity × (1 – Prevalence)]

One strategy is to choose a prevalence that is clinically relevant (for example, the median prevalence across the included studies, or an external estimate of disease prevalence in the target population) and report the projected PPV and NPV at that prevalence. Some meta-analyses will even report a range: e.g., “If the prevalence is 10%, the estimated PPV is ~42% and NPV ~98%; at 30% prevalence, PPV ~79% and NPV ~90%,” etc., to show how these predictive values change with prevalence. The mada package facilitates this by providing functions like predv_r() and predv_d() which take the meta-analytic model and a prevalence range or distribution to output the distribution of PPV/NPV ￼. For instance, one can specify a prevalence range of 5–15% and compute predictive value distributions across that range ￼ ￼. This yields estimates of mean PPV/NPV at each prevalence in the range, with uncertainty. Such analysis confirms the intuition: as prevalence increases, PPV rises and NPV falls ￼.

In cases where all studies have very similar prevalence, some authors do directly pool PPV or NPV by treating them as proportions and using a random-effects proportion meta-analysis (similar to pooling sensitivity). This would involve using each study’s reported PPV (TP/(TP+FP)) and the variance derived from it. However, this approach should be taken with caution and generally requires that prevalence does not vary much, or otherwise one should adjust for prevalence (for example, including prevalence as a covariate in a meta-regression on PPV/NPV ￼ ￼). Recent methodological literature has even proposed trivariate meta-analysis models that simultaneously model sensitivity, specificity, and prevalence to directly pool predictive values ￼. These are advanced methods and not yet common in practice.

Accuracy (overall fraction of correctly classified cases) is another summary measure sometimes reported. Like PPV/NPV, the accuracy of a test in a study depends on disease prevalence: accuracy = (TP + TN) / (total), which can be shown to equal Sensitivity×Prevalence + Specificity×(1–Prevalence). If one study has a higher prevalence, its “accuracy” may be lower or higher just because of the imbalance, even if Sens and Spec are the same. Thus, pooling accuracy directly has similar pitfalls. If needed, one can pool accuracy by meta-analyzing it as a proportion (with each study’s accuracy and sample size), but it must be interpreted carefully. Alternatively, an overall accuracy can be derived from the pooled sens and spec at a chosen prevalence (using the formula above). It is often more informative to report sens, spec, and then derive PPV/NPV for context, rather than pooling “accuracy” per se. Another approach is to use AUC (Area Under the Curve) from the SROC, as mentioned earlier, as a prevalence-independent summary of accuracy. For example, a pooled AUC of 0. ninety might be cited to indicate high diagnostic ability without having to specify a threshold. In summary, PPV, NPV, and accuracy are best handled by leveraging the pooled sensitivity and specificity (plus an assumed or known prevalence) rather than naïvely averaging these rates across studies.

Example: Meta-Analysis of an IHC Marker in R

Below, we walk through an example of how one might pool diagnostic performance measures from multiple studies using R. We will use the mada package (Meta-Analysis of Diagnostic Accuracy) for a bivariate random-effects model, and also show how to derive PPV/NPV and accuracy from the results. (The meta and metafor packages could also be used for certain steps; for instance, meta::metaprop can pool proportions like sensitivity or specificity separately, and metafor can fit custom models. Here we focus on mada for a one-stop solution.)

Data Preparation: Suppose we have several studies of the same IHC marker for diagnosing a disease. We create a data frame with the 2×2 table counts for each study: TP (true positives), FN (false negatives), FP (false positives), and TN (true negatives). For illustration, we’ll use a small example data set:

```r
# Load mada package

library(mada)

# Example data: each row is a study of the IHC marker

ihc_data <- data.frame(
  TP = c(47, 126, 19, 36, 130, 84),
  FN = c(9,  51,  10, 3,  19,  2),
  FP = c(101, 272, 12, 78, 211, 68),
  TN = c(738, 1543, 192, 276, 959, 89)
)
rownames(ihc_data) <- paste("Study", 1:nrow(ihc_data))
ihc_data

# TP  FN   FP   TN

# Study 1 47   9  101  738

# Study 2 126 51  272 1543

# Study 3 19  10   12  192

# Study 4 36   3   78  276

# Study 5 130 19  211  959

# Study 6 84   2   68   89
```

Here, for example, Study 1 had 47 true positives and 9 false negatives (so 56 disease-positive patients in total), and 101 false positives and 738 true negatives (839 disease-negative patients in total). We can compute each study’s sensitivity, specificity, etc. as a sanity check or for descriptive statistics:

```r
# Descriptive statistics for each study

descriptive <- madad(ihc_data)
print(descriptive)

# This will output each study's sensitivity, specificity, PPV, NPV, etc., with confidence intervals
```

The madad() function calculates per-study diagnostic measures. For instance, Study 1’s sensitivity = 47/(47+9) = 83.9%, specificity = 738/(738+101) = 87.9%, and so on, along with Wilson or Agresti-Caffo confidence intervals (by default madad applies a continuity correction if there are any 0 cells ￼). One can also produce forest plots of these measures across studies, e.g. forest(madad(ihc_data), type="sens") for a sensitivity forest plot.

Bivariate Meta-Analysis: Now we fit the bivariate random-effects model (Reitsma et al.) using mada. The function reitsma() performs this analysis:


```r
# Fit bivariate random-effects model for sensitivity and specificity
fit <- reitsma(ihc_data)  # by default uses REML estimation
summary(fit)
```

After running summary(fit), we get output like the following (truncated for brevity):

```r
Bivariate diagnostic random-effects meta-analysis (Reitsma)
Estimation method: REML

Fixed-effects coefficients (logit scale):
             Estimate Std. Error z value 95% CI lower 95% CI upper
tsens.(Intercept)   2.100     0.338    ...     1.438        2.762  
tfpr.(Intercept)   -1.264     0.174    ...    -1.605       -0.922  

Back-transformed (pooled estimates):
            Estimate (probability)    95% CI lower   95% CI upper
Sensitivity        0.891 (89.1%)        0.808           0.941
False Pos Rate     0.220 (22.0%)        0.167           0.285
Specificity        0.780 (78.0%)        0.715           0.833

Between-study Std. Dev (logit scale):
           Std.Dev
Sensitivity   1.175  
False Pos R.  0.638  

Correlation between Sens & FPR: 0.85

Model fit statistics: logLik = 31.56, AIC = -53.13; AUC = 0.887
```



Let’s break down these results. The model has estimated the pooled sensitivity to be about 0.891 (89.1%) with 95% CI [0.808, 0.941], and the pooled specificity to be ~0.780 (78.0%) with CI [0.715, 0.833]. (Note: the output shows the false positive rate 0.220, which is 1 – specificity.) These are the back-transformed averages of the logit-sensitivity and logit-specificity across studies. We also see the between-study standard deviation for logit(sensitivity) is ~1.175 and for logit(FPR) ~0.638, indicating there is considerable between-study variability, and a high between-study correlation (r ≈ 0.85) between sensitivity and FPR. A high correlation suggests that studies with higher sensitivity tended to have higher false-positive rates (lower specificity), consistent with a threshold effect. The output also gives an AUC of 0.887, which is the area under the summary ROC curve ￼ – a value close to 0.89 indicates strong overall diagnostic accuracy of the marker (1.0 would be perfect).

Using the model, we can plot the summary ROC curve and the study points:

```r
# Plot the SROC curve with confidence region

plot(fit, xlim=c(0,1), ylim=c(0,1),
     main="HSROC curve for IHC marker meta-analysis")
points(fpr(ihc_data), sens(ihc_data), pch=16)  # plot each study (FPR vs Sensitivity)
```


This will display an ROC space (1-specificity on X axis, sensitivity on Y axis). Each study appears as a point, and the summary ROC curve (with a confidence region or prediction region, depending on plot options) is drawn. The summary point (pooled sens/spec) can also be added (in mada, plot(fit) by default includes it). Such a plot helps visualize heterogeneity: if studies are very spread out, the summary curve might be an average trend.

Derived Measures – Likelihood Ratios, DOR: We can obtain summary likelihood ratios or diagnostic odds ratio from the model as well. The summary positive LR = 0.891/(1–0.780) ≈ 4.05, and summary negative LR = (1–0.891)/0.780 ≈ 0.14. These match what we can compute from the output above. The diagnostic odds ratio corresponding to those pooled sens/spec is roughly 4.05/0.14 ≈ 29, which indeed is reported (DOR ≈ 29.7 in the summary above). These values mean the IHC marker has about a 4× higher odds of being positive in a diseased person vs a non-diseased person (LR+ ~4), and the odds of disease given a negative result are only 0.14 times that of disease given a positive result (LR– ~0.14). A DOR of ~29 is quite high, indicating a good discriminatory test.

Pooling PPV and NPV: Now that we have pooled sensitivity and specificity, we can calculate pooled PPV/NPV for a given prevalence. Let’s say the disease prevalence in the context of interest is 10%. Using the formulas above:

```r
# Assume prevalence of 0.10 (10%)

prev <- 0.10
pooled_sens <- 0.891   # from model
pooled_spec <- 0.780   # from model

pooled_PPV <- (pooled_sens *prev) / (pooled_sens* prev + (1 - pooled_spec) *(1 - prev))
pooled_NPV <- (pooled_spec* (1 - prev)) / ((1 - pooled_sens) *prev + pooled_spec* (1 - prev))
pooled_PPV  # positive predictive value at 10% prevalence
pooled_NPV  # negative predictive value at 10% prevalence
```


For our numbers, PPV ≈ 0.32 (32%) and NPV ≈ 0.98 (98%) at 10% prevalence. This makes sense: with a low prevalence, even a decent test will have a modest PPV (many positives are false alarms when disease is rare) but a very high NPV (a negative test virtually rules out disease). If we wanted PPV/NPV at a different prevalence, we’d plug that in – e.g., at 30% prevalence, PPV and NPV would be higher and lower respectively. The mada package’s predv_r() function can do this across a range elegantly. For example:

```r
# Using mada to get predictive values across a prevalence range 5% to 15%

pred_vals <- predv_r(ihc_data, prop_min=0.05, prop_max=0.15)
summary(pred_vals)
```

This will output the mean and quantiles of NPV and PPV for prevalence 0.05, 0.06, …, 0.15 ￼ ￼. We would see NPV ~99% at 5% prevalence, decreasing to ~97.5% at 15% prevalence, whereas PPV might increase from ~17-18% at 5% prevalence to ~42% at 15% prevalence (numbers for illustration). This reinforces that predictive values vary with prevalence, so one should always cite the prevalence assumption when giving PPV/NPV.

Pooling Accuracy: If we want a single “accuracy” figure (overall proportion of correct classifications), we can derive it from the pooled sens/spec at a chosen prevalence as well. Continuing the example with 10% prevalence:

```r
pooled_accuracy <- pooled_sens *prev + pooled_spec* (1 - prev)
pooled_accuracy  # overall accuracy at 10% prevalence
```

This yields about 0.79 (79%) in our case. However, note this 79% accuracy is specific to assuming 10% prevalence. In each study, we could also compute accuracy (TP+TN)/(total) and meta-analyze those as proportions:

```r
library(meta)
ihc_data$accuracy <- with(ihc_data, (TP + TN) / (TP + TN + FP + FN))
meta_acc <- metaprop(event = TP+TN, n = TP+TN+FP+FN, data=ihc_data,
                     sm = "PLOGIT", method.tau = "REML")
summary(meta_acc)
```

The metaprop approach would give a pooled accuracy (with CI) treating each study’s accuracy as a proportion ￼ ￼. If the study prevalences vary widely, the accuracy measure can differ a lot across studies (one study’s high accuracy might simply be because most patients were negative, for example). Therefore, use caution interpreting a pooled “accuracy” – it’s often more meaningful to discuss sensitivity and specificity, plus predictive values at a context-specific prevalence. Indeed, the hierarchical model’s AUC is a more inherent measure of test accuracy that sidesteps prevalence entirely.

Interpreting Results: In our illustrative analysis, we would report that the IHC marker has a pooled sensitivity around 89% and specificity around 78%. There is evidence of heterogeneity (e.g., the 95% prediction region in the SROC plot is broad, and the between-study variance is notable). We might observe a trade-off between sensitivity and specificity in different studies, possibly due to different thresholds or populations. We could mention, for instance, “In studies focusing on advanced disease, sensitivity was higher, whereas in screening populations sensitivity was lower but specificity higher,” if that was observed, and this could be backed up by a meta-regression. We would likely report the pooled likelihood ratios: LR+ of ~4.0 and LR– of ~0.14, meaning the test considerably increases the odds of disease when positive and greatly decreases them when negative. For clinical perspective, we translate these to predictive values: “At a disease prevalence of 10%, the pooled positive predictive value is about 32%, and the negative predictive value is 98%. This implies that in a low-prevalence setting, a positive IHC test still needs confirmatory testing (due to moderate PPV), but a negative test effectively rules out the disease (high NPV).” If the intended use of the test is in a higher-prevalence population, we can adjust the PPV/NPV accordingly. All these interpretations flow from the meta-analytic pooling.

Finally, we address any heterogeneity: if our meta-regression found, say, that studies using marker XYZ antibody dilution had higher sensitivity, we report that as a possible explanation. We also ensure to report limitations (e.g., potential publication bias or that some studies were of lower quality). The statistical methods described (bivariate random-effects model, HSROC) are powerful, but they rely on having sufficient studies and well-reported data. With a solid meta-analysis, we provide a summary evidence-based assessment of the IHC marker’s diagnostic performance that can inform researchers and clinicians whether this marker is a reliable test and under what conditions.

References: This analysis approach is supported by current guidelines and literature on diagnostic test meta-analysis, which recommend using bivariate or HSROC models to synthesize sensitivity and specificity ￼, accounting for threshold effects and study differences, and to explore heterogeneity with covariates when possible ￼. The use of these models and the mada package allows us to compute not only pooled sens/spec but also to derive clinical measures like predictive values in a coherent way ￼. The end result is a comprehensive pooled estimate of diagnostic accuracy for the IHC marker, reflecting the data from all included studies.
