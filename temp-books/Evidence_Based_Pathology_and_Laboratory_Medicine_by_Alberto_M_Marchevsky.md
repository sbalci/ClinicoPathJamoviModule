Evidence Based Pathology
and Laboratory Medicine

Alberto M. Marchevsky  •  Mark R. Wick
Editors

Evidence Based
Pathology and
Laboratory Medicine

Editors
Alberto M. Marchevsky, MD
Director, Pulmonary and Mediastinal
Pathology
Department of Pathology and
Laboratory Medicine
Cedars-Sinai Medical Center
and
Clinical Professor of Pathology
David Geffen School of Medicine
University of California
Los Angeles, CA, USA
marchevsky@cshs.org

Mark R. Wick, MD
Professor and Associate Director
of Surgical Pathology
Department of Pathology
University of Virginia Medical School
Charlottesville, VA, USA
MRW9C@hscmail.mcc.virginia.edu

ISBN 978-1-4419-1029-5
DOI 10.1007/978-1-4419-1030-1
Springer New York Dordrecht Heidelberg London

  e-ISBN 978-1-4419-1030-1

Library of Congress Control Number: 2011925569

© Springer Science+Business Media, LLC 2011
All rights reserved. This work may not be translated or copied in whole or in part without the
written  permission  of  the  publisher  (Springer  Science+Business  Media,  LLC,  233  Spring
Street,  New  York,  NY  10013,  USA),  except  for  brief  excerpts  in  connection  with  reviews
or scholarly analysis. Use in connection with any form of information storage and retrieval,
electronic  adaptation,  computer  software,  or  by  similar  or  dissimilar  methodology  now
known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even
if they are not identified as such, is not to be taken as an expression of opinion as to whether or
not they are subject to proprietary rights.
While the advice and information in this book are believed to be true and accurate at the date
of going to press, neither the authors nor the editors nor the publisher can accept any legal
responsibility for any errors or omissions that may be made. The publisher makes no warranty,
express or implied, with respect to the material contained herein.

Printed on acid-free paper

Springer is part of Springer Science+Business Media (www.springer.com)

Preface

Pathology  and  laboratory  medicine  are  currently  experiencing  paradigm
shifts that are likely to influence how our specialty is practiced in the not-too-
distant  future.  Technical  innovations  in  immunohistochemistry,  molecular
pathology,  and  pathology  informatics  are  driving  the  acquisition  of  many
new and exciting data. That phenomenon may well increase the quality and
scope  of  the  diagnostic  information  being  provided  by  laboratory  assays.
Simultaneously, however, as new technologies invariably increase the cost of
medical testing, considerable pressure has accrued concerning financial con-
tainment. Thus far, advocates of “the most, the newest, and the best, regard-
less of cost” have largely prevailed. Nonetheless, it is likely that in the near
future,  there  will  be  considerable  movement  toward  a  strict,  cost-effective
utilization of laboratory resources that is centered on clinical value and direct
applicability of test results in regard to individual patient care.

As practicing pathologists, it has been our impression that there is a great
interest in the generation of new data and the exploration of clinical applica-
tions for new technologies. At the same time, as a group, we do not often
pause to consider how well we are performing certain tasks, and how well we
fulfill  our  charges  as  members  of  clinical  teams  that  care  for  individual
patients. Residency education in pathology and laboratory medicine tends to
emphasize the acquisition of morphology-based diagnostic skills and infor-
mation on various laboratory tests. Nonetheless, interest has been limited in
teaching future pathologists to understand the pros and cons of various diag-
nostic  models;  critically  evaluate  the  contents  of  medical  publications;  sift
through  apparently  conflicting  information;  integrate  data  from  divergent
sources; effectively combine the medical literature with personal experience;
and practice pathology in a cost-effective manner that does not compromise
quality or waste resources.

Internal  medicine  and  other  medical  specialties  have  confronted  similar
issues. They have supported the development of an analytical approach to the
evaluation and use of medical information, under the rubric of evidence-based
medicine (EBM). That term is somewhat fustian, because it appears to imply
that other modes of medical practice are not “evidence-based” or objective.
Advocates of EBM have explored the advantages and disadvantages of dif-
fering study designs; emphasized the advantages of gathering data through
randomized clinical trials; classified medical data in terms of evidence-levels;
advocated the use of standardized guidelines for clinical care; and stressed the

v

vi

Preface

use of a patient-centered approach to diagnosis and treatment. Some of those
concepts have generated considerable resistance from the medical commu-
nity at large, in part because EBM tends to deride case reports or small case
series as anecdotal or inferior. Opponents of EBM have suggested that it leads
to “cookbook medicine” and de-emphasizes clinical experience and the art of
medicine. They have also pointed to the practical limitations of randomized
clinical trials as a gold standard for the collection of medical information.

A  debate  continues  between  advocates  of  EBM  and  other  physicians
who  favor  more  individualized  case-based  approaches  to  medical  practice.
However, regardless of that schism, the current trend toward EBM has pro-
vided a valuable service by emphasizing the importance of reliably produced
data and suggesting how to best apply it to individual patient care.

In this volume, we explore the application of selected EBM concepts to
anatomic pathology and laboratory medicine, embodied in a model that we
have dubbed as evidence-based pathology (EBP). This book is unusual in the
specialty of pathology, because it is not designed to provide readers with the
means to diagnose specific lesions in biopsies or interpret particular labora-
tory  tests.  Rather,  its  intent  is  to  discuss  a  variety  of  epistemological  and
practical issues, and to stimulate thoughts on how well we are doing in prac-
ticing truly scientific medicine as pathologists. Another focus is the contrast
between rapidly accruing new technologies and health system-related pres-
sures for cost containment.

This monograph addresses two general topics. One concerns a description
of problems that occur in applying EBM to laboratory medicine, and the other
considers available resources and possible modes of implementing EBP. The
first  section  of  the  book  includes  chapters  discussing  evidence  levels,  best
evidence, and other basic EBM concepts. This is followed by other material
that concerns statistics. It does not attempt to teach the intricacies of various
statistical tests, but instead is intended to familiarize readers with the basis of
the  probabilistic  thinking  that  underlies  the  specific  applications  of  such
 analyses.  The  use  and  misuse  of  pathological  data  for  prognostication  and
prediction in anatomic pathology is discussed in detail, and the technique of
meta-analysis is also summarized. The statistical discussion in this book is
followed  by  three  chapters  that  discuss  the  principles  of  classification  and
diagnosis in anatomic pathology, the general evaluation of oncopathological
studies, and medical decision-making.

The  second  section  of  the  book  includes  various  solutions  to  problems
in  anatomic  pathology  and  laboratory  medicine  that  are  offered  by  EBP.
It includes chapters concerning evaluation of the medical literature; a discus-
sion of how EBP might help advance histopathology in the future; an evalu-
ation  of  diagnostic  errors;  the  use  of  meta-analysis  to  investigate  unusual
diseases and select immunohistochemical tests; a consideration of the use of
molecular tests in hospital practice, the application of tools for decision anal-
ysis in laboratory medicine; cost-benefit analysis in the hospital laboratory;
and medicolegal aspects of EBP.

We sincerely thank all of our contributors for their willingness to partici-
pate in this project, and we hope that readers will be stimulated by the con-
cepts that are discussed in this book. It is our wish that greater awareness of

Preface

vii

the value of EBP will engender more comprehensive and explicit  guidelines
for publications in pathology. EBM also has the ability to improve education
in pathology; stimulate the future development of objective and reproduc-
ible  guidelines  for  the  practice  of  pathology;  and  further  the  longstanding
 identity of pathologists as physicians who provide intellectual leadership for
their colleagues.

Los Angeles, CA
Charlottesville, VA

Alberto M. Marchevsky, MD
Mark R. Wick, MD

wwwwwwwwwwwwwwwwwww

Contents

Part I  The Problem and Available Resources

  1  Introduction to Evidence-Based Pathology

and Laboratory Medicine ........................................................... 3
Alberto M. Marchevsky and Mark R. Wick

  2  Evidence-Based Pathology: A Stable Set of Principles

for a Rapidly Evolving Specialty ................................................
José Costa and Sarah Whitaker

19

  3  What Is Best Evidence in Pathology? .........................................

27

Peter J. Saunders and Christopher N. Otis

  4  Biostatistics 101 ............................................................................

41

Robin T. Vollmer

  5  Prognostication and Prediction in Anatomic Pathology:

Carcinoma of the Breast as an Illustrative Model ....................
Mark R. Wick, Paul E. Swanson, and Alberto M. Marchevsky

  6  Principles of Classification and Diagnosis in Anatomic

Pathology and the Inevitability of Problem Cases ....................
Michael Hendrickson

61

95

  7  Evaluating Oncopathological Studies:

The Need to Evaluate the Internal and External
Validity of Study Results .............................................................  121
Michael Hendrickson and Bonnie Balzer

  8  Power Analysis and Sample Sizes in Pathology Research ........  141

Robin T. Vollmer

  9  Meta-Analysis: A Statistical Method to Integrate

Information Provided by Different Studies ...............................  149
Eleftherios C. Vamvakas

ix

x

Contents

10  Decision Analysis and Decision Support Systems

in Anatomic Pathology ................................................................  173
Michael Hendrickson and Bonnie Balzer

Part II

 Solutions Offered by Evidence-Based Pathology
and Laboratory Medicine

11  Evidence-Based Approach to Evaluate Information
Published in the Pathology Literature and Integrate
It with Personal Experience ........................................................  189
Alberto M. Marchevsky and Mark R. Wick

12  Evidence-Based Cell Pathology Revisited:

A Personal View ...........................................................................  203
Kenneth A. Fleming

13  Development of Evidence-Based Diagnostic Criteria
and Prognostic/Predictive Models: Experience
at Cedars Sinai Medical Center ..................................................  213
Alberto M. Marchevsky and Ruta Gupta

14  Evaluation and Reduction of Diagnostic Errors

in Pathology Using an Evidence-Based Approach ....................  235
Raouf E. Nakhleh

15  Meta-Analysis 101 for Pathologists ............................................  245

Ruta Gupta and Alberto M. Marchevsky

16  Evidence-Based Practices in Applied

Immunohistochemistry: Dilemmas Caused
by Cross-Purposes ........................................................................  261
Mark R. Wick, Paul E. Swanson, and Alberto M. Marchevsky

17  Evidence-Based Pathology and Laboratory Medicine

in the Molecular Pathology Era: Transition of Tests
from the Research Bench into Practice ......................................  297
Jia-Perng Jennifer Wei and Wayne W. Grody

18  The Use of Decision Analysis Tools for the Selection
of Clinical Laboratory Tests: Developing Diagnostic
and Forecasting Models Using Laboratory Evidence ...............  305
Ji Yeon Kim, Elizabeth M. Van Cott,
and Kent B. Lewandrowski

Contents

xi

19  Implementation and Benefits of Computerized
Physician Order Entry and Evidence-Based
Clinical Decision Support Systems .............................................  323
Stacy E.F. Melanson, Aileen P. Morrison, David W. Bates,
and Milenko J. Tanasijevic

20  Evidence-Based Pathology and Tort Law:

How Do They Compare? .............................................................  337
Mark R. Wick and Elliott Foucar

Index ......................................................................................................  349

wwwwwwwwwwwwwwwwwww

Contributors

Bonnie Balzer, MD, PhD  Department of Pathology and Laboratory Medicine,
Cedars-Sinai Medical Center, Los Angeles, CA, USA

David W. Bates, MD, MSc  Department of Medicine, Division of General
Internal Medicine and Primary Care, Brigham and Women’s Hospital, Boston,
MA, USA; Clinical and Quality Analysis, Partners HealthCare System, Inc.,
Boston,  MA,  USA;  Department  of  Medicine,  Harvard  Medical  School,
Boston, MA, USA

José  Costa,  MD  Department  of  Pathology,  Yale  School  of  Medicine,
New Haven, CT, USA

Kenneth A. Fleming, MA (Oxon), DPhil, FRCPath, FRCP, MBChB  Director,
Oxford University Clinical Academic Graduate School, Associate Dean, Oxford
Post Graduate Medicine and Dental Deanery, Oxford, UK

Elliott  Foucar,  MD    Department  of  Pathology,  University  of  New  Mexico
School of Medicine, Albuquerque, NM, USA

Wayne W. Grody, MD, PhD    Divisions of Medical Genetics and Molecular
Pathology, Departments of Pathology and Laboratory Medicine, Pediatrics, and
Human Genetics, UCLA School of Medicine, Los Angeles, CA, USA

Ruta Gupta, MD  Department of Anatomic Pathology, The Canberra Hospital,
ACT Pathology, Garran, ACT, Australia

Michael  Hendrickson,  MD  Department  of  Pathology,  Stanford  University
Medical Center, Stanford, CA, USA

Ji  Yeon  Kim,  MD,  MPH  Department  of  Pathology,  Massachusetts  General
Hospital and Harvard Medical School, Boston, MA, USA

Kent B. Lewandrowski, MD  Department of Pathology, Massachusetts General
Hospital, Boston, MA, USA; Department of Pathology, Harvard Medical School,
Boston, MA, USA

Alberto  M.  Marchevsky,  MD  Department  of  Pathology  and  Laboratory
Medicine, Cedars-Sinai Medical Center, Los Angeles, CA, USA

Stacy  E.F.  Melanson,  MD,  PhD  Brigham  and  Women’s  Hospital/
Massachusetts  General  Healthcare  Center  Laboratory,  Harvard  Medical
School, Boston, MA, USA

xiii

xiv

Contributors

Aileen  P.  Morrison,  BS  Department  of  Pathology,  Clinical  Laboratories
Division, Brigham and Women’s Hospital, Boston, MA, USA

Raouf  E.  Nakhleh,  MD  Department  of  Pathology,  Mayo  Clinic  Florida,
Jacksonville, FL, USA

Christopher  N.  Otis,  MD  Department  of  Pathology,  Baystate  Medical
Center, Tufts University School of Medicine, Springfield, MA, USA

Peter J. Saunders, MD  Department of Pathology, Baystate Medical Center,
Tufts University School of Medicine, Springfield, MA, USA

Paul E. Swanson, MD  Department of Pathology, University of Washington
Medical Center, Seattle, WA, USA

Milenko J. Tanasijevic, MD, MBA  Department of Pathology, Brigham and
Women’s Hospital and Dana Faber Cancer Institute, Harvard Medical School,
Boston, MA, USA

Eleftherios C. Vamvakas, MD, PhD, MPH  Department of Pathology and
Laboratory Medicine, Cedars-Sinai Medical Center, Los Angeles, CA, USA

Elizabeth  M.  Van  Cott,  MD  Department  of  Pathology,  Massachusetts
General Hospital and Harvard Medical School, Boston, MA, USA

Robin T. Vollmer, MD, MS  Department of Laboratory Medicine, VA Medical
Center, Durham, NC, USA

Jia-Perng  Jennifer  Wei,  MD,  PhD  Ambry  Genetics,  Aliso  Viejo,  CA,
USA

Sarah Whitaker, BA  Department of Pathology, Yale School of Medicine,
New Haven, CT, USA

Mark  R.  Wick,  MD  Department  of  Pathology,  University  of  Virginia
Medical School, Charlottesville, VA, USA

The Problem and Available Resources

Part I

Introduction to Evidence-Based
Pathology and Laboratory Medicine

1

Alberto M. Marchevsky and Mark R. Wick

Keywords
Evidence-based  medicine,  definition  •  Evidence-based  pathology  and
 laboratory  medicine  •  Pathology  and  laboratory  medicine  •  Evidence
• Search engines for evidence-based medicine

Evidence-based medicine (EBM) has been defined
as “the conscientious, explicit, and  judicious use
of  current  best  evidence  in  making  decisions
about the care of individual patients” or as “the
integration of best research evidence with clini-
cal  expertise  and  patient  values”  [1–3].  It  is  an
evolving  discipline  that  applies  analytical  and
quantitative  methods  to  evaluate  the  validity  of
available   medical  information,  with  the  overall
goal  of  identifying  scientifically  sound  data  or
“best  evidence.”  This  evidence  is  integrated  to
improve medical practice through clinical guide-
lines and other tools that are used for education,
standardization  of  care,  quality  initiatives,  and
coverage  decisions  [4,  5].  The  ideas  of  EBM
have spread rapidly through medicine during the
past decade and are recently eliciting a growing
interest  in  Anatomic  Pathology  and  Laboratory
Medicine [6–8].

A.M. Marchevsky (*)
Department of Pathology and Laboratory Medicine,
Cedars-Sinai Medical Center, Los Angeles, CA, USA
e-mail: marchevsky@chs.org

Environment that Created the Need
for Evidence-Based Medicine

Traditional  medical  practice  has  been  based  on
the fundamental assumption that physicians edu-
cated  through  rigorous  medical  school  courses,
postgraduate training programs, continuing edu-
cation  activities,  journals,  personal  experiences,
and interaction with colleagues are well equipped
to  consistently  render  correct  diagnoses  and  do
the right things for their patients. Individual phy-
sicians  are  expected  to  integrate  complex  infor-
mation through “clinical judgment” or the “art of
medicine”  [6].  Decisions  about  the  need  for
insurance coverage, medical necessity, and “stan-
dards  of  practice”  are  generally  defined  by  the
loose  standard  of  “if  the  majority  of  physicians
are  doing  it,  it  must  be  necessary,  it  should  be
covered and it is clinically useful” [9]. The use of
more formal analytical methods and mathemati-
cal models to identify solutions to these questions
has been mostly limited to research projects.

Research in the 1970s and 1980s documented
several major flaws in these fundamental assump-
tions and stimulated an increasing focus on “tech-
nology assessment” [9]. For example, the United
States  Congressional  Office  of  Technology

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_1, © Springer Science+Business Media, LLC 2011

3

4

A.M. Marchevsky and M.R. Wick

Assessment of the Institute of Medicine empha-
sized as recently as the early 1980s the need to
develop  well-designed  studies  to  evaluate  tech-
nologies [10, 11]. Well-planned prospective ran-
domized clinical trials (RCT) demonstrated that
certain common practices, such as the use of anti-
arrhythmic drugs to prevent heart attacks, lacked
good  evidence  to  support  their  usefulness  and
could  be  harmful  to  patients  [12].  Few  clinical
guidelines were available at the time, but in the
mid-1980s,  an  increasing  interest  in  “outcomes
research” led to the development of a large num-
ber of clinical trials and “evidence-based” guide-
lines that have recently grown into the thousands
[13–16].

Medical data proliferate at an ever-increasing
rate and often include a variety of features that
are  far  too  complex,  uncertain,  or  even  contra-
dictory  for  analysis  using  simple  “If–Then”
logic. These problems have led to a deeper appre-
ciation  for  the  need  to  incorporate  computer-
based  analytical  methods  that  are  more  widely
used  in  other  disciplines  such  as  epidemiology,
engineering,  and  business  [6,  17–21].  They
include  various  analytical  tools  of  Decision
Analysis  theory  such  as  decision  trees,  utility
theory,  and  Bayes  theorem  that  can  be  used  to
estimate the validity of diagnostic tests, perform
cost-effectiveness  analysis,  analyze  with  meta-
analysis  the  effectiveness  of  various  interven-
tions,  render  more  consistent  and  effective
decisions  that  affect  the  welfare  of  individual
patients, and evaluate the effectiveness of the var-
ious paradigms used in medical care [6, 18–21].

Project  in  1981  to  promote  the  use  of  literature
reviews and guidelines for various topics [24]. The
American Cancer Society has sponsored the devel-
opment of Evidence-Based Guidelines (EBG) for
specific diseases using the following general con-
cepts: “First there must be good evidence that each
test or procedure recommended is medically effec-
tive  in  reducing  morbidity  or  mortality;  second,
the medical benefits must outweigh the risks; third,
the cost of each test or procedure must be reason-
able compared to its expected benefits; and finally,
the  recommended  actions  must  be  practical  and
feasible”  [6,  25–27].  Several  centers  have  been
dedicated to the development of medical practice
guidelines based on “best evidence,” such as the
Cochrane collaboration in Oxford, the Centre for
Evidence-Based  Medicine  at  Oxford  University,
Cancer  Care  Ontario  in  Canada,  the  National
Guideline Clearinghouse sponsored by the Agency
for  Healthcare  Research  and  Quality  (AHRQ),
and others [16, 28, 29]. AHRQ has also promoted
EBM-based  research  and  policies  and  the  devel-
opment  of  Evidence-based  Practice  Centers  to
produce reports and technology assessments [28].
A  wealth  of  books  and  other  publications  about
EBM and its applications to a variety of subjects is
available.  For  example,  the  British  Medical
Journal  publishing  group  launched  various  jour-
nals  available  online:  “Clinical  Evidence,”
“Evidence  Based  Medicine,”  “Evidence  Based
Mental  Health,”  “Evidence  Based  Nursing”  to
publish EBM type studies. The concepts of EBM
have  also  spread  beyond  EBG  into  “evidence-
based  coverage,”  “evidence-based  performance
measures,” and policies regarding quality improve-
ment, medical necessity, and regulations.

Evolution of Evidence-Based
Medicine into a Well-Established
Discipline

EBM evolved as a discipline in the United States,
Canada and the UK in the 1990s and is already a
well-established  discipline  that  is  now  taught  in
many medical schools, through graduate programs,
books,  and  other  educational  resources  [5,  22,
23]. The American College of Physicians (ACP)
the  Clinical  Efficacy  Assessment
developed

Evidence-Based Medicine as a New
Approach to Teaching the Practice
of Medicine

The  Evidence-based  Medicine  Working  Group
proposed  in  1992  the  use  of  EBM  as  a  new
approach  to  teaching  the  practice  of  Medicine
[1–4, 12]. The emphasis was on individual physi-
cians collecting computerized literature searches

1

Introduction to Evidence-Based Pathology and Laboratory Medicine

5

looking at the sensitivity and  specificity of tests,
selecting  a  test,  assigning  a  pretest  probability,
calculating a posttest probability, and developing
a  management  plan.  The  terminology  of  evi-
dence-based individual decision-making or EBID
has been proposed.

Basic Concepts of Evidence-Based
Medicine

How is the Use of Medical Information
Approached from the Standpoint
of Evidence-Based Medicine?

EBM  investigators  attempt  to  identify  the  best
current  and  relevant  research  information  avail-
able  for  a  particular  problem  and  to  integrate  it
into guidelines, rules, or other tools that will assist
medical  practitioners  in  their  daily  practice.
Sackett and associates have suggested the use of
five steps for the identification of “best evidence”
and its integration with personal clinical expertise
and values into guidelines, rules, or other proto-
cols  that  can  be  used  for  the  care  of  individual
patients (Table 1.1) [1–4, 12]. Richard Gross sum-
marizes the first four steps of Sacket et al., using
the  acronym  “FRAP”  –  framing  evidence-based
questions, retrieving relevant evidence, appraising
the  quality  and  appropriateness  of  the  evidence,
and patient-based decision-making [22].

Basic Process for the Identification of
Best Evidence and Its Integration into
Guidelines, Rules, or Other Protocols

 1.  Formulation  of  specific  questions  regarding
the  diagnosis,  prognosis,  causation,  and/or
treatment of a patient with a particular clini-
cal problem

   Evidence-based  questions  ideally  attempt  to
address those issues that are most relevant to
the  materials  being  studied  [1,  3–5].  These
questions  need  to  address  a  detailed  query
whose answer will provide useful and practi-
cal  information for patient care. For example,
if a pathologist is interested in comparing the
results  of  the  immunostains  of  a  particular
neoplasm, the summary of evidence from the
literature would need to include specific ques-
tions  such  as:  Which  tissues  were  studied?
What percentage of cells was used as a thresh-
old for positive immunoreactivity? How where
the  changes  quantitated  or  semiquantitated?
What  structures  exhibited  immunoreactivity?
What  antibodies  were  used?  Did  the  study
report the use of proper controls? What  dilutions
were used? What sensitivities and  specificities
were reported? Were the results compared with
appropriate statistical tests? Did the study have
sufficient power to detect significant differences
in immunoreactivity? Table 1.2 lists examples

Table  1.2  Queries  proposed  for  the  assessment  of
 “prognostic” information in the context of evidence-based
medicine

Table 1.1  Evidence-based medicine approach to the use
and evaluation of information in daily practice

Formulation of specific questions regarding diagnosis,
prognosis, causation, and/or treatment of any given
clinical problem
Search for specific information in the scientific
literature
Critical appraisal of the validity of the evidence, and its
impact, applicability, and usefulness in clinical practice
Incorporation of “best evidence” from several
“reliable” sources along with personal clinical
experience, for the development of “Evidence-based”
guidelines, rules, or other protocols
Evaluation of the effectiveness and efficiency of those
recommendations

Is the evidence valid?

Was the sample of patients assembled at the same
point of the disease?
Can it be applied to individual patients?
Was the follow-up period sufficiently long and
complete?
Were the results validated with a group of test
(holdout) cases?

Is it important?

How likely are the outcomes over time?
How precise are the prognostic estimates?

Are the patients in the study being referred to similar to
those of the physician using the evidence?
Will the evidence in hand have a significant impact in
managing the disease in question?

6

A.M. Marchevsky and M.R. Wick

of  questions  suggested  by  Sackett  and  col-
leagues to be considered in the assessment of
studies that report “prognostic” information.
 2.  Search  for  specific  information  in  the  scien-

tific literature

   Hundreds of electronic bibliographic databases
are  currently  available  online.  MEDLINE/
PubMed is probably the database most familiar
to pathologists, but it does not identify all known
published  RCT  [30–32].  Other  online  data-
bases include Cancerlit, Embase, “CENTRAL,”
developed by the Cochrane collaboration, MD
Consult,  UpToDate,  Micromedex,  STAT!Ref,
SKOLAR  MD,  Australasian  Medical  Index,
Chinese  Biomedical  Literature  Database,
Latin  American  Caribbean  Health  Sciences
Information
(LILACS),  Japan
Literatures

Centre  of  Scientific  and  Technology  File  on
Science, Technology and Medicine (JICST-E),
AIDSLINE,  SciSearch,  TrailsCentral,  and
many others. Subscription-based lists of EBM-
based  guidelines  such  as  EBMG  and  Web  of
Science  are  also  available  online  (Figs.  1.1
and 1.2) [33–35].

Such  a  bewildering  array  of  information
sources  has  stimulated  the  development  of
better search engines that apply more advanced
methods than simple Boolean searches based
on  the  analysis  of  previously  indexed  infor-
mation [36–39]. For example, the developers
of the widely used web search engine Google
have  recently  sponsored  the  development  of
Google Scholar to automatically analyze and
extract citations from a variety of “scholarly”

Fig.  1.1  Although  pathologists  are  most  familiar  with
the  search  engine  Pubmed  of  the  National  Library  of
Medicine, there are other online services to retrieve scien-

tific  references.  This  figure  shows  the  web  page  of
Essential Evidence Plus, sponsored by a publisher, Wiley-
Blackwell

1

Introduction to Evidence-Based Pathology and Laboratory Medicine

7

Fig. 1.2  Web page of another specialized search engine to retrieve scientific references, ISI Web of Knowledge. This
search engine and the one shown in Fig. 1.1 are available only through individual or institutional subscriptions

literature  and  present  them  as  separate  results
search  even  if  the  documents  they  refer  to
are not online [36–39]. The results of each query
are organized by how relevant the information
is to the query, using proprietary algorithms.

 (a) Best-evidence summaries
   Various formats have been proposed to sum-
marize  the  “best  evidence”  into  “evidence
summaries”  that  include,  in  addition  to  the
answers to the specific questions, information
about  the  sources  of  the  selected  evidence,
methods used for selection, estimates of preci-
sion and reliability, and other important details
[5,  22].  Multiple  Practice  Guidelines  and
Evidence summaries have been developed by
various organizations and are readily available
online.  For  example,  the  web  site  of  Cancer
Care  Ontario  makes  available  a  variety  of
Practice Guidelines and Evidence Summaries by
disease  site  (Fig.  1.3)  [40].  These  documents
generally  list  the  dates  of  the  original  guide-
lines  and  subsequent  updates,  the  guideline

questions, the target population, description of
methodology, recommendations, key evidence,
related guidelines, and key contacts for further
information. The Cochrane Collaboration also
makes available online an EBM manual sum-
marizing  a  variety  of  interesting  topics  and
numerous guidelines published using a com-
mon format (Fig. 1.4) [41, 42]. To our knowl-
edge, there are no such EBM-based Practice
Guidelines  and  Evidence  Summaries  in
Pathology.  The  Association  of  Directors  of
Anatomic and Surgical Pathology, the Cancer
Committee of the CAP, and other groups have
published “recommendations,” “cancer proto-
cols,” and other documents that provide guide-
lines  to  practicing  pathologists  (Fig.  1.5)
[43–46].  These  documents  have  been  devel-
oped by committees or other groups of experts,
based on their experience and understanding of
the  “current  state  of  the  art,”  rather  than  the
more analytical process followed by the propo-
nents of EBM.

8

A.M. Marchevsky and M.R. Wick

Fig. 1.3  Web site of Cancer Care Ontario showing the CCO toolbox with various practice guidelines. Other institutions
in multiple countries offer similar evidence-based practice guidelines

 (b) Text data mining for the automated analysis of

natural language texts

   A vast amount of information is available on
the  Web,  textbooks,  and  other  formats  in
unstructured  text  written  in  the  natural  lan-
guage form [33–35, 39]. There is an increas-
ing interest in computer science at developing
tools to “mine” textual information with tools
that can navigate text bases, creating summa-
ries of documents, cluster them, and carry out
semantic retrieval of information using neural
network  tools  and  other  “intelligent”  agents.
Novel  software  tools  such  as  TextAnalyst
(Megaputer,  Inc.),  SAS  TextMiners  (SAS,
Cary, NC), and others provide interesting tools
for the future automated analysis of data avail-
able in pathology reports and other  repositories

of  documents.  Multiple  online  resources  are
available  listing  software,  books,  and  other
resources for text mining.

 3.  Critical appraisal of the validity of the avail-
able  evidence,  and  its  impact,  applicability,
and usefulness in clinical practice

 (a) Statistical significance: Type I and II statisti-

cal errors

   The  quality  and  appropriateness  of  medical
evidence is generally assessed with quantita-
tive  tools  that  are  well  known  by  clinical
pathologists  and  some  anatomic  pathologists
[47–51].  The  purpose  of  most  research  proj-
ects is to search for “statistically significant”
evidence  that  the  value  of  a  parameter  in  a
population  of  interest  is  different  from  the
value of this feature in a control group [6, 26].

Fig. 1.4  Web site of the Cochrane collaboration, an international institution that has been a pioneer in the development
of evidence-based guidelines

Fig. 1.5  The web site of the College of American Pathologists makes available numerous cancer protocols and check-
lists that are now being used in daily practice by most American pathologists

10

A.M. Marchevsky and M.R. Wick

these

The basic assumption that there would be no
significant  difference  between
two
 values  is  termed  the  “null  hypothesis”  [51].
The results collected in a study from the group
of cases of interest are compared with those of
the  reference  control  group  using  the  t-test,
ANOVA, chi-square, and/or other appropriate
descriptive statistical tests. If the p-value of a
parameter measured from a study group is sig-
nificantly different from the value in the con-
trol  group  by  a  p  value  smaller  by  some
arbitrary cutoff value, such as p < 0.05, the null
hypothesis is rejected in favor of the alterna-
tive  [6,  51].  A  p < 0.05  value  indicates  that
there is a 5% probability that the null hypoth-
esis  was  rejected  by  spurious  factors  other
than those being tested in the study. This type
of error is classified as the type I error in sta-
tistical textbooks.

An additional important potential source of
error  that  is  seldom  given  consideration  in
observational studies in pathology is whether
the research study was designed with enough
“power” to reject the null hypothesis when it
is appropriate to do so [51]. Type II statistical
error is the probability that the test in question
will erroneously fail to reject the null hypoth-
esis when the latter is true. For example, the
fact that a particular study fails to establish a
statistically  significant  difference  between
immunoreactivity  for  a  particular  epitope  in
two  different  groups  of  cases  may  be  biased
by  the  characteristics  of  the  staining  proce-
dure, staining selection, sample size, variabil-
ity  of  the  data,  and  other  variables.  Several
“power  analysis”  statistical  tests  have  been
designed to estimate for the probability of type
II errors in scientific studies and are used rou-
tinely in RCT and in other scientific studies,
but  have  seldom  been  used  in  observational
studies  by  anatomic  pathologists  [52,  53].
A value of power = 0.80 or higher is generally
recommended.

Statistical calculations that have been used
in  laboratory  medicine  studies  include  mea-
sures  of  sensitivity,  specificity,  negative  and
positive  predictive  values,  likelihood  ratios,
receiver-operator  curves,  misclassification

rates, and others [54, 55]. These tests provide
good  information  about  potential  type  I  sta-
tistical  errors,  but  do  not  include  “power”
analysis to analyze for possible type II errors.
Sensitivity is the proportion of patients with a
disease  who  have  a  positive  test,  whereas
specificity is the proportion of true negatives
of all the negative samples tested. The posi-
tive predictive value of a test is the proportion
of patients with a positive result who actually
have  the  disease,  while  negative  predictive
value  is  represented  by  the  proportion  of
patients with a negative test who are actually
free of disease. Likelihood ratio (LR) associ-
ated with a positive test calculates the proba-
bility  that  the  finding  is  seen  in  diseased
patients,  divided  by  the  probability  that  is
present in healthy people; the posttest odds of
disease  are  equal  to  the  pretest  odds  of  dis-
ease multiplied by the LR.

 (a) Bayesian  approach  to  the  analysis  of  data:
influence of prior probability of a finding and
need  to  study  “holdout”  data  to  verify  the
results of a study

   The statistical tests listed above offer limited
information  about  other  features  that  can
influence the outcome of observational stud-
ies, such as the prevalence of a disease within
the population study and in the control group
and the prior probability of a finding [56–60].
For example, the sensitivity of the AFB test in
cases with caseating granulomas is probably
better  than  in  cases  without  granulomatous
disease, as the pathologist is likely to examine
more  carefully  the  slides  from  cases  that
exhibit  pathological  findings  that  are  known
to be caused by mycobacteria. The prior prob-
ability  of  a  finding  can  be  simplistically
defined as the probability that it is present in
the control group. The prior probability for a
particular finding can change dramatically the
significance of the results of a particular study.
For example, it is well known that lymph node
status has a statistically significant prognostic
significance  in  most  patients  with  cancer.
However, in patients with Stage IV neoplasms
who have a high “prior probability” of dying
from their disease, the prognostic value of the

1

Introduction to Evidence-Based Pathology and Laboratory Medicine

11

feature  lymph  node  status  is  probably  rather
limited. Likewise, the value of certain immu-
nophenotype for the determination of the site
of origin of a neoplasm is also probably quite
variable,  dependent  on  what  other  clinico-
pathologic  information  is  available  [61].  For
example,  an  adenocarcinoma  within  a  medi-
astinal  lymph  node  that  exhibits  negative
immunoreactivity for TTF-1 has, in our expe-
rience, a higher prior probability of represent-
ing a metastasis from a lung cancer than from
an extrathoracic neoplasm with similar histo-
logical  features.  The  “prior  probability”  of
this assessment is probably a lot higher if the
patient  also  has  a  single  lung  mass  on  chest
imaging  studies  and  a  positron  emission
tomogram  (PET)  that  shows  only  a  positive
lung  mass.  These  considerations  are  intui-
tively used in daily practice by most patholo-
gists, but there are few, if any, available EBG
or other protocols that take into consideration
the  prevalence  and  prior  probability  of  vari-
ous findings into the selection and/or interpre-
tation of immunostains or other ancillary tests
in Anatomic Pathology.

Another  consideration  that  has  not  been
addressed  in  most  observational  studies  in
pathology  is  the  need  to  divide  the  data  into
“training” or “testing” sets (“study” and “hold-
out”  cases)  in  observational  studies  attempt-
ing  to  derive  classification  or  prognostic
models [57, 59, 62]. Most clinico-pathological
categorization has been based on data derived
from  analyzing  the  data  from  study  groups
and control groups with descriptive univariate,
and less often multivariate, statistical  methods.
However,  multiple  studies  using  Bayesian
methods  have  shown  that  models  derived  by
the use of 100% of a dataset are not necessar-
ily  robust  when  applied  to  other  datasets,  as
there is a certain element of “circular reason-
ing”  in  the  modeling  methodology.  EBM
emphasizes  the  value  of  RCT,  of  using  pro-
spective and retrospective data, and the need
to  compare  the  results  from  a  study  set  with
those of an “unknown” set that has not been
used  for  the  derivation  of  the  classificatory
model [1, 2, 15, 63, 64]. As discussed later on

in this article, scientific papers using the latter
methodology are given a higher value of cred-
ibility. To our knowledge, there have been few
attempts  to  apply  this  methodology  to  most
classification  schema  being  used  in  Surgical
Pathology  and  Cytopathology,  perhaps  pro-
viding  an  explanation  for  the  high  interob-
server variability of certain diagnoses.

 (b) Interobserver  variability:  assessment  with
kappa  statistics  and  effect  on  the  interpreta-
tion of observational studies
It is well known that the diagnosis of various
neoplasms and nonneoplastic conditions using
histopathology is subject to a certain degree of
interobserver  variability  [65].  For  example,
lung pathologists can disagree in the classifi-
cation of about 30% of certain neuroendocrine
pulmonary neoplasms and 50% of poorly dif-
ferentiated  nonsmall  cell  lung  carcinomas
[65–67]. This variability can be measured with
the so-called kappa statistics that estimate the
proportion  of  chance  versus  expected  agree-
ments  taking  into  consideration  the  fact  that
the raters and the samples are not independent
from  each  other  [68–72].  Kappa  coefficients
of 0.8 or higher are considered as good agree-
ment  rates.  This  methodology  has  been  used
mostly in cytopathology and in some surgical
pathology and other studies.

However, little consideration has been gen-
erally given to the influence of interobserver
variability in the assessment of the reproduc-
ibility  of  certain  classification  schema  in
Anatomic  Pathology  and  the  prognostic  and
predictive value of selected observations. For
example, in a situation when pathologists have
difficulties  distinguishing  small  cell  carci-
noma, atypical carcinoid tumor, and nonsmall
cell carcinomas of the lung in about a third of
the cases, and the 5-year survival proportions
for patients with these neoplasms vary from 0
to 50%, what would be the statistical “power”
of  a  study  needed  to  determine  whether  all
these diagnostic categories have independent
prognostic  or  predictive  value?  Could
other stratification of the cases into categories
such as “high-grade neuroendocrine carcino-
mas” and “atypical carcinoid” provide better

12

A.M. Marchevsky and M.R. Wick

 discriminatory data? To our knowledge, there
is no “best evidence” in the pathology litera-
ture to answer these questions. Another exam-
ple  could  be  a  hypothetical  situation  where
pathologists agree less than 100% of the time
whether a resection margin is involved or not
by a neoplasm and subsequent resection spec-
imens  detect  residual  tumor  in  a  slightly
smaller  proportion  of  the  patients  who  had
negative margins than those with initially pos-
itive margins. How can we determine whether
reexcision  is  a  valuable  procedure  based  on
this  “evidence”?  How  many  cases  would  be
needed  to  study  this  question  with  sufficient
power?  Could  there  be  some  definition  of
“positive” margin that would decrease the rate
of  interobserver  variability,  changing  the
parameters  used  for  the  evaluation  of  this
problem? Future studies that address this type
of  practical  problem  with  methodology  that
takes into account some of the analytical con-
cepts being promoted by practitioners of EBM
may  improve  the  precision  of  specimen-
derived data.

 4.  Incorporation of “best evidence” from several
reliable  sources  along  with  personal  clinical
experience into “evidence-based” guidelines,
rules, or other protocols

 (a) Evaluating the quality of published studies in

the medical literature

   The medical literature includes many descrip-
tive  studies  that  include  single  case  reports,
large  observational  analyses  involving  many
patients,  and  scientific  studies  in  which  a
hypothesis is tested prospectively with appro-
priate controls [6, 26]. Observational studies
are  definitely  valuable,  but  they  suffer  from
biases  owing  to  case  selection,  reporting
methods,  characteristics  of  control  groups
(“healthy  cohort  effect”),  and  other  factors
listed in Table 1.3 [1, 2, 4, 16, 73]. EBM stud-
ies also are influenced by “publication bias.”
Although pathologists frequently have a lim-
ited ability to control all these possible sources
of  bias  in  their  observational  studies,  EBM
does  raise  interesting  questions  about  study
design and interpretation of the data that could
lead to better future approaches to the use of

Table  1.3  Sources  of  bias  in  observational  and  other
comparative studies

Selection bias (samples of convenience and others)
Sample size
Ratio between the number of observations and the
number of variables
Characteristics of the control group (healthy cohort effect)
Performance bias
Attrition bias
Detection bias
Distribution of the data (normal vs. others)
Interpretation of the results
Lack of independent validation group
Publication bias

“specimen-based” data in improved  diagnostic
and prognostic models.

Ebell has proposed a system for classifying
published  medical  evidence  into  four  levels,
with “grade I” being the best (most reliable)
[23].  Grade  I  studies  are  those  that  include
data validated with a “test” group that is from
a  different  and  distinct  population  from  the
“training”  cohort.  For  example,  a  classifica-
tion or a prognostic rule might be developed
in  one  group  of  patients  and  validated  in
another. Grade II studies report data that are
obtained from the same population, the mem-
bers  of  which  are  divided  into  independent
“training” and “validation” subsets and evalu-
ated  prospectively.  Grade  III  analysis  also
include  “training”  and  “validation”  subsets
from  the  same  population,  but  data  are  col-
lected  contemporaneously  rather  than  pro-
spectively. Grade IV studies are those in which
the “training” group is also used as the “vali-
dation group.” According to this scheme, most
studies in the pathology literature would prob-
ably be classified as Grade IV and are particu-
larly  vulnerable  to  the  problems  listed  in
Table 1.3.

 (b) Integration of “best evidence” from the litera-
ture  with  personal  clinical  experience  into
“evidence-based”  guidelines,  rules,  or  other
protocols

   As mentioned earlier, advocates of EBM have
attempted to organize “best evidence” from the
scientific  literature  and  their  own  experience

1

Introduction to Evidence-Based Pathology and Laboratory Medicine

13

into  algorithms,  protocols,  guidelines,  or
“rules”  that  guide  individual  patient  care  by
practitioners.  Pathologists  may  benefit  from
emulating  this  approach,  in  future  efforts  at
constructing  “patient-based”  prognostic  and
predictive models. For example, immunostains
are  most  often  used  to  distinguish  between
various  neoplasms  in  a  descriptive  manner.
Studies  using  immunostains  in  the  pathology
literature usually list the percentage of lesions
that label for particular epitopes, as well as the
sensitivity, specificity, and predictive values of
such  markers  in  narrow  morphological  con-
texts. However, few studies have assessed LR
or  other  probabilistic  measures  as  applied  to
panels of markers in selected differential diag-
noses [60, 74] . At an even more basic level, the
relative  statistical  values  attending  particular
morphological findings have seldom been ana-
lyzed in the same fashion, to our knowledge.

In  contrast,  several  prognostic  scoring
models  or  “rules”  that  integrate  multivariate
pathological,  clinical,  imaging,  and  other
information are being developed by other spe-
cialists [75]. For example, Kattan and associ-
ates have developed pretreatment nomograms
[76]  that  combine  clinical  and  pathological
data from prostate cancer patients and predict
5-year probability of metastasis .

 5.  Evaluation of the effectiveness and efficiency
of those “evidence-based” recommendations
   The fact that a scientific study has been pub-
lished  in  a  peer-reviewed  journal  probably
does not guarantee that the study design was
methodologically  sound,  that  the  research
was  well  conducted,  the  data  analyzed  cor-
rectly, and/or the results interpreted properly.
Therefore, “evidence-based” information has
become almost a “de rigueur” label in health
care  to  convey  a  measure  of  credibility.
However, as discussed recently by Steinberg
and Luce, there is considerable variability in
how  information  is  been  assembled,  evalu-
ated, and synthesized in different EBM type
studies  [77].  Different  systems  have  been
proposed for rating the stability and strength
of  medical  evidence  and  are  discussed  in
Chapter 13. [78] .

What Has Been the Impact of EBM
in Improving the Quality of Medical
Practices in the United States?

EBG have had a limited success at improving the
overall  quality  of  Medicine  in  the  U.S  and  has
elicited somewhat of a backlash from practitioners
revolting  against  “cookbook  medicine”  [79–83].
Organizations  sponsoring  EBG  have  at  times
struggled  to  maintain  these  guidelines  current.
Research  into  the  daily  practices  of  physicians
has demonstrated that the wide availability of new
scientific  data  and/or  clinical  guidelines  using
“best evidence” has had a rather limited effect in
changing  the  behavior  of  medical  practitioners.
Somewhat surprisingly, it can take years for phy-
sicians to incorporate new information into their
practices and change their approach to the diagno-
sis and treatment of individual patients.

Pathology and Evidence-Based
Medicine

Interestingly,  pathology  has  not  been  an  active
participant in the EBM “movement” in spite of
being considered as one of the more “scientific”
branches of Medicine and a long and proud his-
tory of providing strong leadership among med-
ical specialties in quality assurance and quality
improvement  issues  [62].  Pathologists  have
faced to date limited scrutiny about the specific-
ity  and  the  cost-effectiveness  of  multiple  prac-
tices. For example, although it is well documented
that  there  can  be  considerable  interobserver
variability  in  the  diagnosis  of  various  disease
entities  with  histopathology,  there  have  been
limited  attempts  at  developing  formal  EBG  or
diagnostic algorithms to standardize these prac-
tices and proficiency testing programs to assess
the  effectiveness  of  these  efforts.  There  is  cur-
rently little consensus about “standard of prac-
tices”
interpretation  of
immunostains and other ancillary studies for the
diagnosis of various diseases and for the devel-
opment of prognostic and predictive models for
patients  with  various   neoplasms  and  nonneo-
plastic conditions. Pathologists have had limited

the  use  and

for

14

A.M. Marchevsky and M.R. Wick

opportunities  in  the  past  to   provide  input  into
schema such as the TNM  system  developed by
the  American  Joint  Commission  on  Cancer
(AJCC) [43]. Indeed, some of the current stag-
ing  guidelines  lack  specific  definitional  detail
that could help to decrease some of the variabil-
ity in pathology practice. Most attempts at pro-
viding  tools  to  improve  the  standardization  of
reporting information to practicing pathologists
have  been  via  published  protocols  developed
by  professional  societies,  such  as  the  Cancer
Protocols developed by the College of American
Pathologists (CAP) or the Reporting Recommen-
dations  by  the  Association  of  Directors  of
Surgical  Pathology  and  Anatomic  Pathology
(Fig.  1.6)  [43].  Those  documents  have  been
written by groups of pathologists appointed by
these  organizations  for  their  subspecialty  or
other experience and are based on the semisub-
jective  “authoritative”  interpretation  of  current
practices  and  available  information  by  these
individuals.  This  approach  may  be  effective,
but  it  is  based  on  opinion  rather  than  on  “best

evidence”  taken  from  a  systematic  analysis  of
data collected from controlled studies. Moreover,
there have been to our knowledge few attempts
at  evaluating  whether  practicing  pathologists
are  using  the  elements  suggested  in  these
guidelines in their daily practice and in estimat-
ing the effectiveness of these recommendations
and  protocols  for  the  improvement  of  patient
outcomes.

The  College  of  American  Pathologists  (CAP)
has also sponsored several multidisciplinary “con-
sensus conferences,” in which groups of special-
ists in different medical fields convened to perform
systematic  reviews  of  the  literature,  discussed
salient  problems,  selected  “best  evidence,”  and
proposed guidelines for their clinical management.
These  sessions  have  closely  approximated  the
general  idiom  of  EBM.  More  recently,  the  CAP
has offered an EBM course at its annual meeting
and the US and Canadian Academy of Pathology
(USCAP)  has  sponsored  a  course  on  Evidence
Based  Pathology  and  Decision  Analysis  that  is
now available on line at http://www.uscap.org.

Fig. 1.6  The web site of the Association of Directors of Anatomic and Surgical Pathology (ADASP) also has multiple
practice guidelines labeled as recommendations

1

Introduction to Evidence-Based Pathology and Laboratory Medicine

15

Evidence-Based Medicine
in the Future of Pathology

The  increased  interest  in  EBM  through  the
healthcare environment poses risks for Pathology
and  Laboratory  Medicine.  Physicians  and
healthcare  administrators  familiar  with
the
methodology  being  used  for  RCT  and  other
studies  that  evaluate  the  efficacy  and  cost-
effectiveness of selected procedures may decide
that the utility provided by certain lab tests gen-
erated by either anatomic pathology or the clini-
cal laboratory is not supported by “best evidence”
and should not be reimbursed. EBM also offers
an opportunity to use some of the concepts and
methods  described  in  this  book  to  reassess  the
clinical  effectiveness  of  classification  schema
being used by pathologists and to develop better
diagnostic and prognostic models, more rational
approaches for test  selection, and better tools to
evaluate  the  cost-effectiveness  of  various  tests
[25, 26, 84]. Examples of topics that could ben-
efit  from  an  EBM  approach  include  evaluation
of  whether  certain  “pathologic  entities”  are
based  on  “best  evidence”  and/or  provide  clini-
cally valuable information, the development of
EBG for the use and the interpretation of immu-
nostains and other ancillary tests for specific dif-
ferential  diagnosis  situations  and  for
the
selection and interpretation of laboratory tests in
the context of specific clinical problems, evalua-
tion  of  the  effectiveness  of  selected  practices
such  as  the  use  of  synoptic  reports  and  check-
lists versus narrative reports, assessment of the
effectiveness of various teaching activities, and
others.

References

  5.  Straus SE, Richardson WS, Glasziou P, et al. Evidence-
based  medicine.  How  to  practice  and  teach  EBM.
New York, NY: Elsevier; 2005.

  6.  Marchevsky  AM,  Wick  MR.  Evidence-based  medi-
cine, medical decision analysis, and pathology. Hum
Pathol. 2004;35:1179–88.

  7.  Fleming  KA.  Evidence-based  pathology.  J  Pathol.

1996;179:127–8.

  8.  Costa J. Reflections about evidence-based pathology.

Int J Surg Pathol. 2007;15:230–2.

  9.  U.S.  Department  of  Health  and  Human  Services
Agency  for  Healthcare  Research  and  Quality.
Technology assessment. 2010.

 10. Steinberg  EP,  Graziano  S.  Integrating  technology
assessment  and  medical  practice  evaluation  into
 hospital  operations.  QRB  Qual  Rev  Bull.  1990;16:
218–22.

 11. Steinberg  EP.  Health  care  technology  assessment.

Med Sect Proc. 1986;53–63.

 12. Sackett DL, Rosenberg WM. The need for evidence-

based medicine. J R Soc Med. 1995;88:620–4.

 13. Carson SS. Outcomes research: methods and implica-
tions. Semin Respir Crit Care Med. 2010;31:3–12.
 14. Carter  BS.  A  new  era  of  outcomes  research.

Neurosurgery. 2009;64:N15.

 15. Krumholz HM. Outcomes research: myths and reali-
ties. Circ Cardiovasc Qual Outcomes. 2009;2:1–3.
 16. Tanjong-Ghogomu E, Tugwell P, Welch V. Evidence-
based medicine and the Cochrane Collaboration. Bull
NYU Hosp Jt Dis. 2009;67:198–205.

 17. Peirolo R, Scalerandi M. Markovian model of growth
and  histologic  progression  in  prostate  cancer.  Phys
Rev  E  Stat  Nonlin  Soft  Matter  Phys.  2004;70:
011902.

 18. Brown AW, Malec JF, McClelland RL, et al. Clinical
elements  that  predict  outcome  after  traumatic  brain
injury:  a  prospective  multicenter  recursive  partition-
ing (decision-tree) analysis. J Neurotrauma. 2005;22:
1040–51.

 19. Galligan DT, Ramberg C, Curtis C, et al. Application
of portfolio theory in decision tree analysis. J Dairy
Sci. 1991;74:2138–44.

 20. Hui  L,  Liping  G.  Statistical  estimation  of  diagnosis
with  genetic  markers  based  on  decision
tree
 analysis  of  complex  disease.  Comput  Biol  Med.
2009;39:989–92.

 21. Link RE, Allaf ME, Pili R, et al. Modeling the cost of
management  options  for  stage  I  nonseminomatous
germ  cell  tumors:  a  decision  tree  analysis.  J  Clin
Oncol. 2005;23:5762–73.

 22. Gross R. Decisions and evidence in medical practice.

  1.  Sackett  D.  Evidence-based  medicine.  Lancet.

St. Louis, MO: Mosby; 2001.

1995;346:1171.

 23. Ebell MH. Evidence-based diagnosis. New York, NY:

  2.  Sackett DL, Rosenberg WM, Gray JA, et al. Evidence
based  medicine:  what  it  is  and  what  it  isn’t.  BMJ.
1996;312:71–2.

  3.  Sackett  DL.  Evidence-based  medicine.  Semin

Perinatol. 1997;21:3–5.

  4.  Straus  SE,  Sackett  DL.  Bringing  evidence  to  the

clinic. Arch Dermatol. 1998;134:1519–20.

Springer; 2001.

 24. American  College  of  Physicians.  Clinical  efficacy
assessment project. Internet Communication. 2010.
 25. Marchevsky AM. The application of special technolo-
gies in diagnostic anatomic pathology: is it consistent
with  the  principles  of  evidence-based  medicine?
Semin Diagn Pathol. 2005;22:156–66.

16

A.M. Marchevsky and M.R. Wick

 26.  Marchevsky  AM.  Evidence-based  medicine

in
 pathology:  an  introduction.  Semin  Diagn  Pathol.
2005;22:105–15.

 27. American  Cancer  Society.  Treatment  decision  tools.

Internet Communication. 2010.

 28. Agency  for  Healthcare  Research  and  Quality
(AHRQ). National guideline  clearinghouse.  Internet
Communication. 2010.

 29. Clarke  M.  The  Cochrane  Collaboration  and  the
Cochrane  Library.  Otolaryngol  Head  Neck  Surg.
2007;137:S52–4.

 30. Chen  TH,  Li  L,  Kochen  MM.  A  systematic  review:
how  to  choose  appropriate  health-related  quality  of
life (HRQOL) measures in routine general practice?
J Zhejiang Univ Sci B. 2005;6:936–40.

 31. Deenadayalan Y, Grimmer-Somers K, Prior M, et al.
How  to  run  an  effective  journal  club:  a  systematic
review. J Eval Clin Pract. 2008;14:898–911.

 32. Hunt  DL,  Haynes  RB.  How  to  read  a  systematic

review. Indian J Pediatr. 2000;67:63–6.

 33. Vanhecke  TE,  Barnes  MA,  Zimmerman  J,  et  al.
PubMed vs. HighWire Press: a head-to-head compari-
son of two medical literature search engines. Comput
Biol Med. 2007;37:1252–8.

 34. Booth  A.  Mapping  the  evidence  base  of  pathology.

J Pathol. 1999;188:344–50.

 35. Rapport RL, Lancaster FW, Penry JK. Critical evalua-
tion of a computer-based medical literature search and
retrieval system. Postgrad Med. 1972;51:47–50.
 36. Bakkalbasi N, Bauer K, Glover J, et al. Three options
for  citation  tracking:  Google  Scholar,  Scopus  and
Web of Science. Biomed Digit Libr. 2006;3:7.

 37. Freeman  MK,  Lauderdale  SA,  Kendrach  MG,  et  al.
Google  Scholar  versus  PubMed  in  locating  primary
literature  to  answer  drug-related  questions.  Ann
Pharmacother. 2009;43:478–84.

 38. Kulkarni AV, Aziz B, Shams I, et al. Comparisons of
citations  in  Web  of  Science,  Scopus,  and  Google
Scholar for articles published in general medical jour-
nals. JAMA. 2009;302:1092–6.

 39. Shultz  M.  Comparing  test  searches  in  PubMed  and
Google Scholar. J Med Libr Assoc. 2007;95:442–5.
 40. Cancer  Care  Ontario.  Cancer  Care  Ontario.  Internet

Communication. 2010.

 41. Anonymous. What does the Cochrane Collaboration
say  about  adherence  to  evidence-based  practice  rec-
ommendations? Physiother Can. 2009;61:116.

 42. Winkelstein Jr W. The remarkable Archie: origins of the
Cochrane Collaboration. Epidemiology. 2009;20:779.
 43. Amin MB. The 2009 version of the cancer protocols
of the college of American pathologists. Arch Pathol
Lab Med. 2010;134:326–30.

 44. Amin  MB.  Key  issues  in  reporting  common  cancer
specimen  findings  using  the  College  of  American
Pathologists cancer protocols. Arch Pathol Lab Med.
2006;130:284–6.

 45. Fechner RE. Selected topics from ADASP. Am J Clin

Pathol. 1996;106:S1–2.

 46. Simpson PR, Tschang TP. ADASP recommendations:
consultations  in  surgical  pathology.  Association  of

Directors of Anatomic and Surgical Pathology. Hum
Pathol. 1993;24:1382.

 47. Vollmer RT. Primary lung cancer vs metastatic breast
cancer:  a  probabilistic  approach.  Am  J  Clin  Pathol.
2009;132:391–5.

 48. Multivariate  statistical  analysis  for  anatomic  pathol-
ogy. Part II: failure time analysis. Am J Clin Pathol.
1996;106:522–34.

 49. Multivariate statistical analysis for pathologist. Part I,
The  logistic  model.  Am  J  Clin  Pathol.  1996;105:
115–26.

 50. Vollmer  RT.  Twin  concordance:  a  set  theoretic  and
J  Theor  Biol.

approach.

theory

probability
1972;36:367–78.

 51. Snedecor  GW,  Cochran  WG.  Statistical  methods.
Ames, IA: The Iowa State University Press; 1980.
 52. Connelly LM. Research considerations: power analy-
sis and effect size. Medsurg Nurs. 2008;17:41–2.
 53. Zodpey SP. Sample size and power analysis in medi-
cal  research.  Indian  J  Dermatol  Venereol  Leprol.
2004;70:123–8.

 54. Giard RW, Hermans J. The diagnostic information of
tests for the detection of cancer: the usefulness of the
likelihood  ratio  concept.  Eur  J  Cancer.  1996;32A:
2042–8.

 55. Hara M, Kanemitsu Y, Hirai T, et al. Negative serum
carcinoembryonic  antigen  has  insufficient  accuracy
for excluding recurrence from patients with Dukes C
colorectal  cancer:  analysis  with  likelihood  ratio  and
posttest  probability  in  a  follow-up  study.  Dis  Colon
Rectum. 2008;51:1675–80.

 56. Gupta R, Dastane AM, McKenna Jr R, et al. The pre-
dictive value of epidermal growth factor receptor tests
in  patients  with  pulmonary  adenocarcinoma:  review
of  current  “best  evidence”  with  meta-analysis.  Hum
Pathol. 2009;40:356–65.

 57. Gupta R, Dastane A, McKenna Jr RJ, et al. What can
we learn from the errors in the frozen section diagno-
sis  of  pulmonary  carcinoid  tumors?  An  evidence-
based approach. Hum Pathol. 2009;40:1–9.

 58. Gupta  R,  McKenna  Jr  R,  Marchevsky  AM.  Lessons
learned from mistakes and deferrals in the frozen sec-
tion  diagnosis  of  bronchioloalveolar  carcinoma  and
well-differentiated  pulmonary  adenocarcinoma:  an
evidence-based  pathology  approach.  Am  J  Clin
Pathol. 2008;130:11–20.

 59. Herbst  J,  Jenders  R,  McKenna  R,  et  al.  Evidence-
based  criteria  to  help  distinguish  metastatic  breast
cancer  from  primary  lung  adenocarcinoma  on  tho-
racic  frozen  section.  Am  J  Clin  Pathol.  2009;
131:122–8.

 60. Westfall  DE,  Fan  X,  Marchevsky  AM.  Evidence-
based guidelines to optimize the selection of antibody
panels in cytopathology: pleural effusions with malig-
nant  epithelioid  cells.  Diagn  Cytopathol.  2010;
38:9–14.

 61. Marchevsky  AM,  Gupta  R,  Balzer  B.  Diagnosis  of
metastatic  neoplasms:  a  clinicopathologic  and  mor-
phologic
approach.  Arch  Pathol  Lab  Med.
2010;134:194–206.

1

Introduction to Evidence-Based Pathology and Laboratory Medicine

17

 62.  Marchevsky AM, Wick MR. Evidence levels for pub-
lications in pathology and laboratory medicine. Am J
Clin Pathol. 2010;133:366–7.

 63. Cundiff  DK.  Evidence-based  medicine  and  the
Cochrane Collaboration on trial. MedGenMed. 2007;
9:56.

 64. Overman VP. The Cochrane collaboration. Int J Dent

Hyg. 2007;5:62.

 65. Travis WD, Gal AA, Colby TV, et al. Reproducibility
of  neuroendocrine  lung  tumor  classification.  Hum
Pathol. 1998;29:272–9.

 66. Hirsch FR, Matthews MJ, Yesner R. Histopathologic
classification  of  small  cell  carcinoma  of  the  lung:
comments  based  on  an  interobserver  examination.
Cancer. 1982;50:1360–6.

 67. Roggli VL, Vollmer RT, Greenberg SD, et al. Lung can-
cer  heterogeneity:  a  blinded  and  randomized  study  of
100 consecutive cases. Hum Pathol. 1985;16:569–79.
 68. Cross  SS.  Kappa  statistics  as  indicators  of  quality
assurance in histopathology and cytopathology. J Clin
Pathol. 1996;49:597–9.

 69. Jensen  P,  Krogsgaard  MR,  Christiansen  J,  et  al.
Observer  variability  in  the  assessment  of  type  and
dysplasia  of  colorectal  adenomas,  analyzed  using
kappa  statistics.  Dis  Colon  Rectum.  1995;38:
195–8.

 70. Malpica  A,  Matisic  JP,  Niekirk  DV,  et  al.  Kappa
 statistics  to  measure  interrater  and  intrarater  agree-
ment  for  1790  cervical  biopsy  specimens  among
twelve pathologists: qualitative histopathologic anal-
ysis and methodologic issues. Gynecol Oncol. 2005;
99:S38–52.

 71. Tezuka  F,  Namiki  T,  Higashiiwai  H.  Observer  vari-
ability in endometrial cytology using kappa statistics.
J Clin Pathol. 1992;45:292–4.

 72. Venkataraman  G,  Ananthanarayanan  V,  Paner  GP.
Accessible  calculation  of  multirater  kappa  statistics
for pathologists. Virchows Arch. 2006;449:272.
 73. Summerskill W. Cochrane Collaboration and the evo-

lution of evidence. Lancet. 2005;366:1760.

 74. Marchevsky AM, Wick MR. Evidence-based guidelines
for the utilization of immunostains in diagnostic pathol-
ogy: pulmonary adenocarcinoma versus  mesothelioma.
Appl Immunohistochem Mol Morphol. 2007;15:140–4.
 75. Moussa AS, Kattan MW, Berglund R, et al. A nomo-
gram for predicting upgrading in patients with low- and
intermediate-grade  prostate  cancer  in  the  era  of
extended prostate sampling. BJU Int. 2010;105:352–8.
 76. Kattan  MW.  Do  we  need  more  nomograms  for  pre-
dicting outcomes in patients with prostate cancer? Nat
Clin Pract Urol. 2008;5:366–7.

 77. Steinberg EP, Luce BR. Evidence based? Caveat emp-

tor! Health Aff (Millwood). 2005;24:80–92.

 78. Treadwell JR, Tregear SJ, Reston JT, et al. A system
for  rating  the  stability  and  strength  of  medical  evi-
dence. BMC Med Res Methodol. 2006;6:52.

 79. Guerette PH. Managed care: cookbook medicine, or
quality, cost-effective care? Can Nurse. 1995;91:16.
 80. Holm  RP.  Cookbook  medicine.  S  D  Med.  2009;

62:371.

 81. Leape L. Are practice guidelines cookbook medicine?

J Ark Med Soc. 1989;86:73–5.

 82. Parmley  WW.  Practice  guidelines  and  cookbook
medicine–who  are  the  cooks?  J  Am  Coll  Cardiol.
1994;24:567–8.

 83. Steinberg KE. Cookbook medicine: recipe for disas-

ter? J Am Med Dir Assoc. 2006;7:470–2.

 84. Wick MR, Bourne TD, Patterson JW, et al. Evidence-
based principles and practices in pathology: selected
problem areas. Semin Diagn Pathol. 2005;22:116–25.

Evidence-Based Pathology: A Stable
Set of Principles for a Rapidly
Evolving Specialty

2

José Costa and Sarah Whitaker

Keywords
Evidence-based pathology • Diagnostic pathology • Immunohistochemistry
and  evidence-based  medicine  •  Molecular  medicine  and  evidence-based
medicine • Patient–physician relationship and evidence-based medicine

Of  all  the  specialties  in  medicine,  pathology,
 particularly diagnostic anatomical pathology, has
been relatively slow in embracing the practice and
principles  of  evidence-based  medicine  (EBM).
Two reasons for this are as follows. First, pathol-
ogy  has  been  regarded  for  a  long  time  as  “the
evidence” with respect to clinical inference. The
classic clinico-pathological-correlation would fin-
ish with the pathologist lifting the veil from the
hidden  truth  and  providing  the  last  word,  often
followed by a scholarly discussion of the science
behind the disease. Second, pathologists involved
in  clinical  care  –  particularly  surgical  patholo-
gists – are expected to  render a clear-cut diagno-
sis  that  will  provide  the  basis  for  a  therapeutic
decision. Thus, there is a decisive moment in the
clinic when there is little room for doubt, and it is
easy to see why the  processes of EBM – which, to
a great extent, consist in managing uncertainty by
using  evidence  of  high  quality  –  have  not  been
readily embraced by the surgical pathologist. This
initial reluctance is, however, slowly transforming

J. Costa ()
Department of Pathology, Yale School of Medicine,
New Haven, CT, USA
e-mail: Jose.costa@yale.edu

into acceptance: it is hard to claim that pathology
is  an  essential  part  of  the  medical  practice,  but
that it is off-limits to the critical analysis driven
by the EBM proponents. Practice guidelines have
progressively  been  introduced  in  the  diagnostic
work-up  of  tissue  samples,  and  technological
innovation  has  significantly  altered  diagnostic
methods.  New  technologies  being  applied  to
cytological  and  tissue  specimens  demand  EBM
not  only  at  many  points  in  the  course  of  their
development but also in their final application to
the analysis of clinical samples.

EBM,  a  discipline  that  in  part  had  its  begin-
nings  in  technology  assessment,  evolved  by
adopting methodologies common in other domains
of  medicine  such  as  epidemiology,  but  also  by
learning from the more remote fields of econom-
ics, business, and engineering. As it has matured,
EBM has been incorporated into medical school
curricula,  and  its  principles,  constantly  refined,
are used in the elaboration of widely used prac-
tice guidelines and consensus statements.

In  this  chapter,  we  consider  how  the  recent
advances  in  science  and  technology,  as  well  as
changes in cultural and social trends, act as power-
ful forces that argue in favor of the incorporation
of  the  tenets  of  EBM  into  the  rapidly  changing

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_2, © Springer Science+Business Media, LLC 2011

19

20

J. Costa and S. Whitaker

 discipline  of  diagnostic  pathology.  We  also
 consider some of the arguments of those who are
critical of integrating EBM in the mainstream of
pathology.

The Socio-Economical Context
of the Changing Technological
Landscape

Since  the  middle  of  the  twentieth  century,  the
pace  of  technical  evolution  in  the  medical  sci-
ences has accelerated. The consequences of this
have been wide reaching. The practice of almost
every single specialty of medicine today has been
drastically affected by the technological innova-
tion  resulting  from  the  unprecedented  conver-
gence  of  the  progress  made  in  each  of  several
unrelated disciplines. The complexity of practic-
ing  medicine  increased  and  required  constant
adaptation  of  the  healthcare  delivery  models.
Studies  undertaken  in  the  1970s  began  to  show
that there was room for improvement in the way
medicine  was  being  practiced.  Both  academics
and public interest groups began to question the
efficiency of the medical system [1, 2].

Coming hand in hand with the rapid therapeu-
tic and technological advances of the 1960s was a
significant increase in the intrinsic cost of treat-
ing illness. An increase in diagnostic procedures
and means to establish the cause of disease mul-
tiplied the cost of health care. Thus from a purely
practical  standpoint,  the  need  emerged  to  criti-
cally  evaluate  all  new  technologies  before  they
would  be  widely  adopted.  In  1973,  as  a  conse-
quence of the first oil crisis, the economic burden
imposed by the cost of medical care was under-
scored further as the crisis revealed how vulner-
able national economies were to perturbation and
how the subsequent destabilization of the econ-
omy  and  inflation  affected  medicine.  Both  the
cost  of  health  care  and  the  cost  of  medical
research  increased.  Those  bearing  the  cost  of
health  care,  whether  governments,  nonprofit  or
private enterprise, began to seek ways to actively
manage  the  resources  needed  to  provide  health
care.  Thus  by  the  last  quarter  of  the  twentieth
century,  it  became  clear  there  was  a  need  for  a

framework through which to look at the objective
evidence that was the basis of medical practice.

Finally,  through  globalization,  the  industrial-
ized nations realized how much the improvement
of  the  health  and  life  chances  of  the  neediest
impacted on the wealthiest. Effective therapies and
diagnostic technologies available to the  developed
nations have not been and are still not yet available
to the poor. As a consequence, many of the com-
ponents  of  the  medico-industrial   complex  have
intensified their engagement in generating robust
and  cheap  diagnostic  technologies  and  therapies
suitably adapted to be deployed in the developing
world  and  among  underserved  populations.  As
these new tools are created and used in the clinic,
each  requires  a  rigorous  evidence-based  analysis
of its precision and efficacy.

Recent Forces Reshaping
the Practice of Pathology

At  the  core  of  EBM  is  the  question  of  how  we
handle information that serves to support medical
intervention.  What  value  we  decide  to  place  on
the information, how we go about obtaining new
information, and how we compile existing knowl-
edge  are  all  crucial  processes  of  EBM.  And  of
paramount  importance  is  how  we  obtain  the
information pertinent to the diagnosis and man-
agement of a single patient.

technologies

In recent years, pathology, and more specifi-
cally  diagnostic  pathology,  has  undergone  pro-
found  change  due  to  the  rapid  accumulation  of
basic knowledge and due to the rapid, almost ver-
that
tiginous,  development  of
expand the possibilities of tissue and cell analy-
sis.  New  information,  which  is  not  necessarily
clinically  worthwhile,  is  accumulating  so  fast
that it is difficult to distinguish the truly impor-
tant content from the noise. This proliferation of
available information is another reason why the
principles of evaluating the value of the evidence
are becoming ever more crucial for both the gen-
eral practitioner and the academician.

In laboratory medicine, two types of informa-
tion  are  used  in  medical  decision-making:  (1)
laboratory  values  and  (2)  anatomical  pathology

2  Evidence-Based Pathology

21

these

diagnoses  and  values.  Each  of
two
 subdisciplines  has  its  specific  challenges  and  is
moving toward EBM at a different speed. Because
of  its  inter pretative  nature,  however,  anatomical
patho logy  tends to remain anchored in “ eminence-
based  medicine”  mode  rather  than  relying  on
strong grades of evidence. It is precisely here, in
the  realm  of  tissue  analysis,  that  modern  tech-
nologies are opening inroads and calling for the
rigorous use of evidence-based tools. The tissue
samples  interrogated  under  the  microscope  are
now amenable to a workup that provides resolu-
tive answers to the questions raised by the diag-
nostic pathologist. The question is not only what
kind  of  disease,  process,  or  lesion  are  we  con-
fronting but also what is the best and most efficient
therapy and what response is to be anticipated.

The first tissue analysis technology to make an
impact in diagnostic surgical pathology was immu-
nohistochemistry  (IHC),  and  it  has  served  as  an
effective  vehicle  for  the  adoption  of  EBM.  For
example,  IHC  not  only  provided  evidence  for  a
diagnosis but it also began to introduce quantita-
tive histopathology by enumerating cells express-
ing  a  given  antigenic  determinant.  Where  the
quantitative approaches of morphometry had failed
to impact daily diagnostic practice, IHC changed it
by storm and brought the rigor of the laboratorian
to histopathology, creating best practices, practice
algorithms, and practice standards [3].

Yet  one  of  the  most  profound  developments
to affect the practice of medicine in the last 20
years has unquestionably been the emergence of
the field of molecular medicine. Molecular med-
icine  has  brought  unprecedented  knowledge
about  the  pathogenesis  of  many  diseases  and
served  as  a  rational  basis  for  therapy  design.
Molecular  technologies  have  brought  and  con-
tinue to bring  constant innovation to all branches
of laboratory medicine, and with that, a quantum
leap  in  the  volume  of  information  to  be  man-
aged.  The  ability  to  extract  tissue  components
such  as  proteins  or  nucleic  acids  from  tissues
and  subject  them  to  a  comprehensive  analysis
has  provided  us  with  high-density  data  sets
(“omics”) that can be mined by artificial intelli-
gence [4]. The general strategy is to reduce these
large  assemblies  of  data  to  a  few  features  that

can  then  be  turned  into  a  clinically  applicable
test  in  the  laboratory.  In  other  instances,  PCR-
based  approaches  applied  to  a  micro-dissected
sample  enable  the  patho logist  to  detect  with
specificity an infectious agent or a genetic lesion
and thus diagnose with  precision the etiology of
a lesion.

The  modern  tools  of  molecular  diagnostics
allow  us  to  obtain  information  from  a  patient
with unprecedented precision and breadth. Two
tumors arising in the same organ and histologi-
cally similar can now be sorted out by analyzing
which  signal  transduction  pathway  is  preferen-
tially and differentially activated in each one of
them  or  what  specific  mutational  spectrum  is
present  in  each  one  of  the  tumors  [5–7].  The
molecular  alterations  found  in  each  tumor  may
dictate  specific  targeted  therapies.  This  type  of
characterization of a lesion is the basis for per-
sonalized medicine, “the right treatment for the
right  person  at  the  right  time,”  and  the  corner-
stone for predictive medicine: the ability to pre-
dict  the  response  of  an  individual  patient  to  a
specific  therapy.  The  crucial  characteristics  of
this type of evidence are (1) its objective preci-
sion  inherent  in  modern  molecular  analytical
techniques and (2) the fact that in most instances
the molecular alteration is causally linked to the
pathophysiology  of  the  disease.  When  present,
the  causal  nature  of  the  link  established  by
experimental  studies  and  refined  by  observa-
tional and therapeutic studies in the human con-
stitutes  the  highest  quality  of  evidence  upon
which to base a targeted therapy for an individ-
ual patient.

With the availability of reliable, fast, and eco-
nomic  sequencing  technologies,  the  individual
genome  is  becoming  a  reality,  and  it  has  been
argued that the requirements for the recovery of
clinically  useful  insights  from  an  individual’s
genome  are  different  from  those  of  traditional
cohort-based medical knowledge.

Since  evidence  rules  must  be  applied  to  the
singularity  of  the  individual  (her  or  his  unique
sequence),  we  ought  to  consider  how  the  tradi-
tional tenets of EBM will be applied to specific
information  only  valid  for  a  single  patient.  The
case  is  being  made  for  an  alternative  approach

22

J. Costa and S. Whitaker

based on translational engineering and  intelligence
(biointelligence)  for  interpreting  the  genomic
information  from  an  individual  patient  [8].  The
ability  to  sequence  the  1–2%  of  a  patient’s
genome that encodes for structural proteins of the
cell can enable the detection of disease causing
mutations  in  a  single  patient.  For  example,  the
detailed  examination  of  the  DNA  of  a  single
patient suffering from Bartter syndrome revealed
a novel mutation in the gene coding for a protein
responsible for the absorption of water and salt in
the intestine. Not only was the case of the index
patient  resolved,  but  when  other  infants  with  a
presumptive diagnosis of Bartter syndrome were
examined, five more mutations were identified in
the transporter protein [9]. These results illustrate
how the new technologies, in this case exon cap-
ture  and  sequencing,  generate  clinically  useful
results.

In  parallel  to  the  advances  in  biomedical
technologies, there have been advances in infor-
mation processing, acquisition, and display that
have allowed the pathologist to continue as the
physician-integrator of information. The capac-
ity of an individual to apprehend and integrate
different streams of general evidence and infor-
mation about a given patient has been progres-
sively taxed. Fortunately, information technology
and  computational  science  have  come  along  at
the right time, expanding our capacities to dis-
play,  analyze,  and  integrate  complex  and  rich
streams of data. It is now possible to enlist com-
putational power to carry out the integration of
thousands of features and select a small subset
of parameters that solve the question (diagnos-
tic,  prognostic,  predictive).  Statistical  methods
can then be used to test thousands of features for
predictive  power  and  select  the  most  powerful
ones  (feature  reduction)  to  generate  a  test  that
can be validated. Modern machine vision tech-
nologies that use segmentation, object identifi-
cation,  and  topology  can  derive  thousands  of
objective  reproducible  features  from  a  tissue
section  and  then  proceed  to  overlay  specific
molecular  markers  on  the  segmented  image  to
produce  a  “quantitative  functional  histopathol-
ogy,” thus creating a powerful and precise diag-
nostic tool [10, 11].

A task once done by a master diagnostician,
who,  however,  was  informed  by  many  fewer
elementary features, can now reach every single
patient and be performed in a reproducible man-
ner.  When  done  by  artificial  intelligence  as
opposed  to  an  unaided  human  mind,  the  pro-
cessing will be repeated without error 100% of
the time.

From Precision Medicine to Efficient
Medicine

With  the  advent  of  precision  technologies  that
identify and measure one or several components
in  a  clinical  specimen  with  high  specificity  and
sensitivity  or  reveal  a  submolecular  alteration,
the  science  of  diagnostics  enters  the  realm  of
“precision  medicine.”  The  evidence  obtained  is
objective and precise, and the principles of EBM
can then be turned to the task of refining preci-
sion medicine into efficient medicine. Efficiency
is to be considered with the patient in mind: Are
we subjecting the person to the minimal number
of  tests  necessary  to  best  identify  and  treat  the
problem? Are we using the best combination of
drugs for that particular patient? EBM offers the
optimal path to define the most economical way
to  deliver  the  personalized  precision  medicine
that we can provide today. It is important to keep
in mind that “economical” is used in the sense of
the  most  benefit  for  the  resources  used  and  not
necessarily the cheapest.

In  our  current  climate,  the  cost  of  medical
resources is a major concern. At a time when the
cost  of  health  care  is  becoming  prohibitive  for
industrialized  nations  (U.S.  health  expenditures
are projected to reach 20% of the GNP by 2020),
the tenets of EBM are being used to base policy
and  resolve  debate.  Right-thinking  people  may
come to different conclusions based on the avail-
able evidence, but to oppose someone’s evidence-
based  stance  does  not  require  invective,  rather
facts  and  logical  argument.  Many  government
funding research in healthcare quality are banking
on the power of EBM to decrease the rising share
of  the  national  economies  taken  by  healthcare
expenditures.  Costs  can  be  brought  down  by

2  Evidence-Based Pathology

23

encouraging efficient medicine and by  discouraging
ineffective  medical  practices,  but  only  with  the
acceptance of the EBM process can we arrive at a
consensus concerning what is medically efficient
and what is ineffective. In the U.S., Comparative
Effectiveness Research (CER), a broad initiative
sponsored by the Agency for Health Care Research
and  Quality,  funds  a  wide  spectrum  of  research
ranging from meta-analyses of trials, to methods
of behavior modification, to methods for formu-
lating health policy. Whereas traditionally the evi-
dence  has  been  produced  by  studies  designed
specifically to generate the data to support a state-
ment  or  recommendation,  the  widespread  appli-
cation  of  information  technology  to  medical
practice is enabling the collection and aggregation
of  data  from  the  routine  medical  “day  to  day”
practice [12].

Anatomical  Pathology  has  been  a  low-cost
discipline, a highly efficient one considering the
value it contributes, but with the increase in the
use  of  sophisticated  technologies  and  methods
the question of efficiency will surface more often.
Let us not ignore that pathology tests will become
the gatekeepers of expensive therapies as person-
alized medicine gains momentum.

Evidence-Based Medicine Must
Take the Patient into Account:
Participatory Medicine

One  of  the  interesting  aspects  of  the  real-world
approach in gathering data is taking into account
the patient–physician relationship as one crucial
component of the system to be analyzed. In fact,
we  have  little  detailed  evidence  of  how  natural
phenomena such as disease interact with a social
construct such as a health system [13].

The  present  emphasis  on  patient’s  choices
de facto introduces the patient into the process of
generating data. With the information revolution
in  full  gear,  much  of  the  knowledge  that  was
exclusive  to  physicians  and  other  trained  health
personnel  is  now  accessible  to  the  lay  public.
Information is read and absorbed with avidity by
those facing the distressing but motivating condi-
tion of being a patient. Through the aggregation

of  many  patients’  personal  experiences,  new
communities are organized around the common-
ality  of  shared  medical  circumstance,  such  as
physical illness or genetic condition. The forma-
tion of virtual communities or support networks,
a phenomenon for which Rabinow has proposed
the  concept  of  “biosociality”  [14],  has  the
 potential  of  becoming  an  active  contributing
 factor to data sets that can be further mined using
computational  tools.  It  does  not  seem  risky  to
predict  that  the  communication  revolution  will
enable observations made and rigorously recorded
by lay individuals to be admitted as “evidence”
and form the basis for future observational stud-
ies. In the near future, patients will be contribut-
ing  to  shape,  in  many  ways,  the  evidence  with
which the EBM methods will generate the “best
practice standards.”

Is There Evidence to Support
the Need for Evidence-Based
Medicine in Pathology?

The  overarching  argument  we  have  put  forth  is
that  the  best  way  to  handle  the  vertiginous
changes  affecting  pathology,  particularly  diag-
nostic  pathology,  is  to  adhere  to  the  tenets  of
EBM.  Critics  of  this  argument  will  present  a
number of objections. They will hasten to point
out  that  there  is  no  robust  body  of  evidence  to
support our position; that time and resources are
limited  and  are  less  and  less  available  to  busy
practitioners;  that  EBM  will  require  training  in
additional skills to search for the available infor-
mation and evaluate the strength of the available
evidence; that EBM is “cookbook medicine” and
“takes  the  art  out  of  diagnostic  clinical  medi-
cine”;  that  it  will  threaten  current  standards  of
therapeutic  excellence  as  initiatives  of  the  CER
type use EBM to cut costs without regard for the
quality of care [15].

It is certainly true that stricto sensu there is no
formal evidence to support EBM. A randomiza-
tion study of traditional style versus EBM prac-
tice  style  in  diagnostic  pathology  is  practically
impossible  and  would  very  likely  be  unethical.
The fact is, however, that pathologists, because of

24

J. Costa and S. Whitaker

the nature of their practice, have operated close to
EBM  standards  for  a  long  time  and  have  more
often than not recorded their diagnostic outcomes
in observational studies involving case series or,
more recently, in studies coupled to clinical trials.
The leap to formalizing the principles of EBM in
the practice of pathology is not great. As a disci-
pline,  pathology  has  traditionally  been  seen  as
providing  “the  evidence,”  and  yet  pathologists
and clinicians have come to realize that appear-
ances can be deceiving and that very similar if not
identical  morphologies  can  have  very  different
clinical behaviors that demand different therapeu-
tic strategies. Not knowing how to distinguish the
mimics from the authentic lesion constitutes indi-
vidual ignorance that can be repaired by acquiring
the  knowledge  to  make  the  distinction.  By  con-
trast, being confronted by lesions that are identi-
cal  and  thus  indistinguishable  but  with  a  very
different  behavior  constitutes  collective  igno-
rance.  Two  prominent  examples  presenting  a
dilemma rooted in this type of ignorance are intra-
ductal low-grade breast cancers and prostate can-
cers with a Gleason grade of 6 or less. Both are
early cancers often found in asymptomatic patients
at screening, and their therapy ranges from watch-
and-wait  surveillance  to  aggressive  intervention
designed to eradicate the tumor. We are just begin-
ning to learn how to make such distinctions, mak-
ing appeal to objective tools such as the ones used
in systems pathology. Conclusive evidence upon
which  to  base  a  distinction  and  rational  therapy
will hopefully be validated in the near future.

The  paradox  is  that  the  same  diagnosticians
who have acquired new powerful tools must now
seek additional evidence to support their reasons
for  saying  what  they  say,  for  diagnosing  what
they diagnose, and for recommending what they
recommend.  In  other  words,  pathologists  have
transitioned  from  embodying  the  evidence  to
having the tools to uncover it and having to jus-
tify the use of these tools. The principles of EBM
may not be perfect, but they are probably the best
for the evaluation of technologies, codifying their
use in practice, and assessing their cost and effec-
tiveness.  The  accuracy,  value,  and  efficacy  of
these  new  ways  must  be  methodically  docu-
mented, ideally by randomized trials that  compare
a  new  diagnostic  or  predictive  modality  to  the

conventional  approach  used  to  solve  a  specific
clinical  problem.  It  behooves  the  practitioner
working  on  a  specific  case  to  follow  the  well-
defined  steps  involved  in  the  practice  of  EBM:
(1)  convert  information  needs  into  answerable
questions, (2) track down the best evidence with
which  to  answer  these  questions,  (3)  critically
appraise the evidence for its validity and impor-
tance,  (4)  integrate  this  appraisal  with   clinical
expertise and patient values to apply the result in
clinical  practice,  (5)  evaluate  performance.
Adherence to these tenets will go a long way to
manage uncertainty in clinical practice.

Objections to EBM, on the basis of the increas-
ingly limited time and resources available to busy
practitioners and on the perceived additional bur-
den  of  developing  the  skills  necessary  to  search
for  the  available  information  and  evaluate  the
strength of the available evidence, raise legitimate
concerns. Fortunately, however, the IT revolution
has gone a long way to mitigate these factors. The
skills  necessary  to  access  information  can  be
learned  at  any  stage  of  clinical  training  and  are
now taught to medical students in most medical
schools. More articles of the “ systematic review”
type are appearing in general, not just in subspe-
cialty journals, and brief summaries of evidence
relevant  to  common  clinical  questions  can  be
accessed at the point of care.

Many  of  the  objections  articulated  by  oppo-
nents of EBM are based more on misperception
than on substance. Two of the major arguments
of  opponents  to  EBM  are  that  “it  is  cookbook
medicine” and that “it takes the art out of clinical
medicine.” Following the principles of EBM does
by  no  means  exclude  creativity.  The  best  clini-
cians  are  the  ones  capable  of  making  cognitive
connections between facts and rules. That is the
product of a creative process – a process that, if
grounded on the rules of evidence, will be able to
be taught, learned, and constantly perfected.

It is also a misperception that EBM is used by
initiatives of the CER type simply to cut costs with-
out regard for therapeutic standards or the quality of
care.  Those  who  feel  uncomfortable  with  EBM
argue that the use of the findings will not be geared
to the benefit of the patient, but to the rationing of
health care [12]. As noted earlier, many aspects of
EBM  lead  directly  to  more  effective  patient  care.

2  Evidence-Based Pathology

25

EBM  is  not  designed  to  answer  philosophical
 questions about the values and priorities of a society
and therefore cannot pretend to. But one can strive
for a democratically based transparent process that,
after  informed  dialog  and  debate,  will  generate  a
consensus that accommodates the values and priori-
ties of the vast majority of peoples and interests.

Conclusion

Modern  technologies  and  ever  more  incisive
methods of tissue analysis are providing increas-
ing  accuracy,  resolution,  and  effectiveness  to
modern diagnostic sciences. We are immersed in
a  rapidly  evolving  world  where  disruptive
 technologies come at such speed and information
is generated in such abundance that EBM becomes
an essential philosophical and practical factor of
stability.  It  behooves  all  of  us  in  patho logy  to
establish  EBM  as  the  linkage  of  technological
innovation and research to the resolution of patient
illness and problems in the delivery of care.

References

  1.  Hardy  A,  Tansy  EM.  Medical  enterprise  and  global
response, 1945–2000. In: The Western medical tradi-
tion  1800–2000.  Cambridge:  Cambridge  University
Press; 2006.

  2.  Office of Technology Assessment. Assessing the effi-
cacy and safety of medical technologies (OTA-H-75).
Washington, DC: OTHA; 1978.

  3.  Wolff  AC  et  al.  American  Society  of  Clinical
Oncology/College  of  American  Pathologists  guide-
line  recommendations  for  human  epidermal  growth
factor receptor 2 testing in breast cancer. Arch Pathol
Lab Med. 2007;131:18–43.

  4.  Costa J. Systems approach to the practice of pathology:
a new role for the pathologist. Arch Pathol Lab Med.
2009;133:524–6.

  5.  Hayden EC. Personalized cancer therapy gets closer.

Nature. 2009;458:131–2.

  6.  Brown RE. Morphoproteomics: exposing protein cir-
cuitries in tumors to identify potential therapeutic tar-
gets  in  cancer  patients.  Expert  Rev  Proteomics.
2005;2:337–48.

  7.  Lievre  A,  Blons  H,  Laurent-Puig  P.  Oncogenic
 mutations  as  predictive  factors  in  colorectal  cancer.
Oncogene. 2010;29:3033–43.

  8.  Mousses S et al. Using biointelligence to search the
cancer  genome:  an  epistemological  perspective  on
knowledge  recovery  strategies  to  enable  precision
medical genomics. Oncogene. 2008;Suppl 2:S-58–66.
  9.  Choi M et al. Genetic diagnosis by whole exome cap-
ture  and  massively  parallel  DNA  sequencing.  Proc
Natl Acad Sci USA. 2009;106:19096–101.

 10. Donovan MJ et al. Personalized prediction of tumor
response  and  cancer  progression  on  prostate  needle
biopsy. J Urol. 2009;182:125–32.

 11. Donovan  MJ  et  al.  A  systems  pathology  model  for
predicting overall survival in patients with refractory,
advanced  non-small-cell  lung  cancer  treated  with
gefitinib. Eur J Cancer. 2009;45:1518–26.

 12. Topol EJ. Transforming medicine via digital innova-

tion. Sci Transl Med. 2010;2:16.

 13. Liu J et al. Complexity of coupled human and natural

systems. Science. 2007;317:1513–6.

 14. Rabinow P. Artificiality and enlightenment. From socio-
biology to biosociality. In: Essays on the anthropology
of reason. Princeton: Princeton University Press; 1996.

 15. Straus SE, McAlister FA. Evidence-based medicine: a
commentary on common criticisms. CMAJ. 2000;163:
837–41.

What Is Best Evidence in Pathology?

Peter J. Saunders and Christopher N. Otis

3

Keywords
Evidence-based medicine in pathology • Best evidence, defined • Evidence
evaluation • Internal validity of evidence

What is Evidence?

With  the  advent  of  evidence-based  medicine
(EBM), the word and concept “evidence” needs to
be explored. The concept of evidence is the crux of
what is attractive (or controversial) about the prac-
tice  of  evidence-based  medicine.  Webster’s  dic-
tionary  defines  evidence  as  “something  that
furnishes truth” or “an outward sign” [1]. The for-
mer definition embraces the idea of objectivity and
has a great deal of purpose to it. The latter seems to
equate  evidence  with  an  observation  that  gives
insight into an occult process and makes no judg-
ment of how likely said process is to occur. In this
context,  evidence  is  an  indication,  not  proof.  By
including these two definitions, Merriam-Webster’s
English dictionary comments on the heterogeneity
of the worth of evidence. In other words, evidence
is  of  variable  quality.  Merriam-Webster’s  Legal
dictionary also embraces this ambiguity by includ-
ing an element of doubt when it comes to the merit
of  evidence,  defining  it  as  “something  that  fur-

C.N. Otis (*)
Department of Pathology, Baystate
Medical Center, Tufts University
School of Medicine, Springfield, MA, USA
e-mail: Christopher.otis@bhs.org

nishes  or  tends  to  furnish  proof”  [2].  Something
that tends to achieve a certain goal is obviously not
completely efficacious. If we consider these defi-
nitions to be equally valid, we arrive at the conclu-
sion  that  in  a  linguistic  or  legal  environment,
evidence  is  recognized  to  be  neither  completely
specific  nor  completely  sensitive.  In  the  spirit  of
incomplete  sensitivity  and  specificity,  we  might
define evidence as something that tends to furnish
proof. Of interest are the lack of entries for “evi-
dence” and “evidence based medicine” in the 28th
and  most  recent  edition  of  Stedman’s  Medical
Dictionary, published in 2006.

Best Evidence:  Where Did the Term
Come from?

The  term  “best  evidence”  first  appeared  in
accounts  of  English  legal  proceedings  dating  to
the mid-eighteenth century. It was born of the idea
that some types of legal evidence are better than
 others  which  are  nevertheless  useful  and
admissible  in court if no other evidence is avail-
able. It was codified into law in the UK soon after
the  case  in  which  the  principle  made  its  first
appearance was decided [3] and subsequently in
the  United  States  in  1975  [4].  According  to

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_3, © Springer Science+Business Media, LLC 2011

27

28

P.J. Saunders and C.N. Otis

Webster’s  New  World  Law  Dictionary,  the  best
evidence rule is defined as: “The rule that, to prove
the   contents  of  a  writing,  recording,  or  photo-
graph,  the  original  is  required  unless  it  is  not
available  for  some  reason  other  than  the  serious
fault  of  the  party  trying  to  prove  the  contents
thereof.  If  the  original  is  unavailable,  the  testi-
mony of the person who created the original or the
person who read it (if a writing), listened to it (if a
recording), or saw it (if a photograph) may testify
to its content. However,  modern evidentiary rules
usually permit the use of mechanical, electronic,
or other similar copy instead of the original” [2].

Thus, in the legal arena, “best evidence” may
be interpreted as “best available evidence,” rather
than “the best type of evidence.” Physicians desire
to provide the best for patients, but are intimately
aware  of  the  real-life  constraints  that  make  for
suboptimal  solutions.  Perhaps  practitioners  of
medicine might take a cue from the legal profes-
sion  and  regard  “best  evidence”  as  something
that is the best that can be done at a given point in
time (that is, the best that is available), and subse-
quently  something  that  may  be  improved  upon
(something that may be bested).

How Is the Quality of Evidence
Evaluated in Evidence-Based
Medicine?

Study quality is synonymous with study internal
validity.  Internal  validity  is  generally  defined  as
how well a study measures what it is designed to
measure. A study without design flaws that is exe-
cuted  properly  and  is  not  subject  to  unexpected
environmental  influences  will  not  yield  poor-
quality data, and vice versa: a poorly conceived,
executed,  and  storm-tossed  study  will  not  yield
good  quality  data.  Thus,  quality  of  evidence  is
evaluated by evaluating the study from which it is
derived. Study quality is currently evaluated using
different  criteria  for  different  kinds  of  studies.
This is to say that a meta-analysis and a  case–control
study must meet different criteria to be deemed of
high  quality.  There  are  multiple  different  bodies
across the globe that create criteria in order to per-
form this function, and as one might expect, the

criteria  differ  from  one  organization  to  the  next.
Overall, the decision of which criteria to include
and which to exclude appears fairly subjective.

What Is Internal Validity?

The Center for Evidence Based Medicine (CEBM)
defines validity and internal validity as follows:

The extent to which a variable or intervention
measures  what  it  is  supposed  to  measure  or
accomplishes what it is supposed to accomplish.
The internal validity of a study refers to the integ-
rity of the experimental design [5].

The  U.S.  Preventive  Services  Task  Force
(USPTF), an offshoot of the Agency for Health-
care Research and Quality and a part of the U.S.
Department of Health and Human Services, is the
CEBM’s  American  counterpart.  The  USPTF
refers  to  internal  validity  as  “strength  of  study
design”  [6].  The  USPTF  uses  a  ranking  system
for assessing the internal validity of evidence at
an  individual  study  level.  The  hierarchy  of  this
ranking system is as follows:
     I.  Properly  powered  and  conducted  random-
ized controlled trial (RCT); well-conducted
systematic  review  or  meta-analysis  of
homogeneous RCTs
II-1.  Well-designed  controlled

trial  without

randomization

II-2.  Well-designed  cohort  or  case–control  ana-

lytic study

II-3.  Multiple  time  series  with  or  without  the
intervention;  dramatic  results  from  uncon-
trolled experiments

  III.  Opinions  of  respected  authorities,  based  on
clinical experience; descriptive studies or case
reports; reports of expert committees [7]
The USPFT uses sets of criteria that are spe-
cific to study type to determine whether a study
possesses “good,” “fair,” or “poor” internal valid-
ity. For example, systematic reviews are checked
to determine whether or not they meet the following
criteria that have been deemed critical for  minimal
internal validity:
•

Comprehensiveness  of  sources  considered/
search strategy used.
Standard appraisal of included studies.

•

3  What Is Best Evidence in Pathology?

29

•
•

Validity of conclusions.
Recency  and  relevance  are  especially  impor-
tant for systematic reviews [7].
A good study is described as a “recent, relevant
review  with  comprehensive  sources  and  search
strategies; explicit and relevant selection criteria;
standard appraisal of included studies; and valid
conclusions”  [7].  A  fair  study  is  described  as  a
“recent, relevant review that is not clearly biased
but lacks comprehensive sources and search strat-
egies” [7]. A poor study is described as “outdated,
irrelevant,  or  biased  review  without  systematic
search  for  studies,  explicit  selection  criteria,  or
standard appraisal of studies” [7].

Good  internal  validity  is  the  result  of  good
study design. An appraisal of the design of a study
under consideration might include searching for
evidence of the above types, as well as answering
the following questions:
•
•
•
•

Is the question properly defined?
Are inclusion and exclusion criteria stated?
Is the sample size large enough?
Are  the  units  of  analysis  well  defined?  Are
they independent of each other?
Are  measurements  made  in  the  same  way
(same time, same conditions, etc.) for all?
8]
Is the scale of measurement objective? [

•

•

What Statistics Are Generally Used
to Analyze Data in Studies? How
Should Their Results be Interpreted
to Determine the Internal Validity
of the Studies?

Before proceeding, it might be prudent to review
the basic concepts of sensitivity, specificity, posi-
tive likelihood ratio, and positive predictive value
(PPV).  Sensitivity  is  perhaps  best  viewed  as  the
true positive rate. It is the number of times that a
test is deemed positive divided by the number of
times that the test should be positive if the assay
were  to  detect  all  cases  of  the  condition  under
investigation:  TP/(TP + FN).  Otto  von  Bismarck,
Germany’s first Chancellor, is reputed to have said
“it is better that ten innocent men suffer than one
guilty man escape” [9]. This is an example of high

sensitivity  for  guilt  at  the  cost  of  specificity.
Specificity can analogously be viewed as the true
negative  rate.  It  is  the  number  of  true  negatives
divided by the number of times the test is initially
thought to be negative: TN/(TN + FP). Blackstone’s
formulation,  named  for  English  jurist  William
Blackstone and later echoed by Benjamin Franklin,
states that it is “better that ten guilty persons escape
than that one innocent man suffer” [10]. This is an
example of high specificity (for guilt) at the cost of
sensitivity. The positive likelihood ratio (+LR) is a
very powerful tool that determines how much pre-
test probability increases due to the procurement of
a positive test result. The positive likelihood ratio
changes as the threshold for positivity changes, as
do  the  sensitivity  and  specificity  of  a  given  test
type. This can be demonstrated graphically as the
derivative of the slope of a plot of sensitivity versus
1-specificity  (the  receiver  operating  characteristic
(ROC) curve, to be discussed in more depth later in
this chapter). Positive predictive value (PPV) is the
proportion of patients with positive test results who
are correctly diagnosed, that is to say the number of
true positives divided by the number of all results
initially  considered  positive  (including  those  that
are  later  determined  to  be  false  positives):  TP/
(TP + FP).  An  interesting  attribute  of  PPV  is  that
with this concept we are beginning to venture into
the realm of creating a denominator that is not one
group (like all the patients who have a disease), but
rather comprises parts of two groups (some of the
patients who have a disease and some of the patients
who do not have the disease). This results in a situ-
ation in which the population from which the sam-
ple size is derived can drastically affect the outcome
of a test. Specifically, two populations with differ-
ent prevalences of a certain disease will yield two
different PPVs so that clinical use of PPV should
be restricted to populations in which the prevalence
is the same as test population from which the results
were derived [11].

Relative risk and absolute risk are frequently
employed  statistical
that,  when  used
tools
together,  unveil  the  degree  of  additional  risk
above a baseline. Odds ratio, the ratio of the odds
of a given phenomenon occurring in two sepa-
rate groups, is used to determine the differential
risk  for  the  two  groups.  The  ROC  curve  is  an

30

P.J. Saunders and C.N. Otis

 underused statistical method of determining like-
lihood ratios, the approximate accuracy of a test,
and  for  evaluating  sensitivity  and  specificity  at
given  diagnostic  thresholds.  Positive  and  nega-
tive predictive values are statistical tools that help
 determine how likely a positive or negative test is
to represent the presence or absence of a condi-
tion within a given population.

There  are  other  statistical  functions  within
most studies that are included to showcase inter-
nal  validity  (a  form  of  internal  quality  assess-
ment),  rather  than  to  prove  causation  (the  usual
goal of studies investigating both therapeutic and
diagnostic interventions). These include the con-
fidence interval, the confidence level, the p-value,
the funnel plot, and Cohen’s kappa.

Confidence intervals are usually included after a
measurement of probability (e.g., odds ratio), as an
assessment of the reliability of the measurement. If
a distribution is nonparametric (i.e., non-Gaussian),
statistics like the odds ratio and relative risk do not
accurately  portray  reality.  For  example,  if  there
exists a bimodal distribution of events, one single
odds  ratio  may  spear  the  nadir  between  the  two
peaks  which  is  misleading.  Assuming  something
like this is not overlooked, a large confidence inter-
val will result from a distribution with a flattened
appearance graphically – the result of minimal cor-
relation between supposed cause and effect. Large
confidence intervals for multiple parameters indi-
cate that a study may possess poor internal validity
that  is  contributing  to  difficulty  in  determining
accurate statistical estimates.

P-values are ubiquitous throughout medical tri-
als. A p-value greater than 0.0X indicates the prob-
ability that the results of a study were achieved with
less than X% likelihood that this is due to chance.
Simply put, the p-value is the percent chance that
the  null  hypothesis  explains  the  results  obtained.
The preferred p-value for the majority of studies is
0.05 or 5%. The concept that the results of a given
test resulted from serendipity (a 1 in 20 chance) is
somewhat arbitrary. Nevertheless, it functions such
that there is a statistical standard. The p-value can
be  looked  upon  as  a  statistical  gauge  of  internal
validity: studies with large p-values are more likely
to falsely connect cause and effects, and thus more
likely to be designed in a way that does not isolate
and exclude the mechanism by which chance may

lead to seemingly significant results. The lower the
p-value, the greater is the likelihood that the study
has good internal validity.

Cohen’s kappa is a powerful tool for internal
quality assessment of a study and relates specifi-
cally to how often multiple detectors arrive at the
same  conclusion  for  the  same  reasons  as  each
other. It is a way of determining inter-rater agree-
ment  that  does  not  occur  by  chance  (e.g.,  how
often two pathologists agree on the same diagno-
sis other than the times they agree for divergent
reasons). Cohen’s kappa is calculated by dividing
the rate of nonrandom agreement (total agreement
minus chance agreement) by the percent of occur-
rences  that  something  other  than  chance  agree-
ment  occurs  (1-rate  of  chance  agreement).  The
rate  of  nonrandom  agreement  is  calculated  by
summing  the  product  of  the  positive  result  rate
from two detectors and the product of the negative
result rate from the two detectors. For example, if
pathologists A and B are given 50 cases and they
agree on a positive diagnosis 20 times (or 40% of
the time) and they agree on a negative diagnosis 15
times (or 30% of the time), then the total agree-
ment is 70% of the time, or 0.70. Now assume that
pathologist A makes a positive diagnosis 50% of
the time and negative diagnosis 50% of the time,
while  pathologist  B  makes  a  positive  diagnosis
60% of the time and a negative diagnosis 40% of
the  time.  This  last  piece  of  data  can  be  used  to
determine  how  frequently  the  two  pathologists
agree  by  chance:  the  hypothetical  probability  of
chance agreement is calculated by multiplying the
rates  of  both  pathologists’  positive  diagnoses
(0.5 × 0.6 = 0.3),  multiplying  the  rates  of  both
pathologists’  negative  diagnoses  (0.5 × 0.4 = 0.2),
and summing them (0.3 + 0.2 = 0.5). In this exam-
ple, a kappa of 0.4 is generated by applying these
elements  to  the  kappa  function  defined  above
([0.7 − 0.5]/[1 − 0.5]).  Generally,  kappa  values  of
less  than  0  represent  no  agreement,  0–0.2  slight
agreement,  0.21–0.40  fair  agreement,  0.41–0.60
moderate agreement, 0.61–0.80 substantial agree-
ment, and 0.81–1.00 almost perfect agreement.

When  attempting  to  determine  the  internal
 validity  of  a  meta-analysis,  it  is  imperative  to
determine  whether  or  not  as  many  individual
 studies  as  possible  have  been  incorporated.
Unfortunately,  not  all  studies  that  are  conducted

3  What Is Best Evidence in Pathology?

31

are published. Frequently, this is due to effect size
lying outside (too high or too low) a desired target,
which is often the result of studying a population
larger or smaller than that which might yield a sig-
nificant result. A funnel plot is a plot of study size
against  a  marker  of  treatment  effect  (e.g.,  mean
standard  error),  which  can  uncover  silencing  of
results due to unwanted results derived from large
or small studies. If publication bias is absent, the
points plotted on the funnel plot will take the form
of  an  inverted  funnel.  Inclusion  of  a  funnel  plot
that  has  this  inverted-funnel  shape  is  reassuring
that  publication  bias  is  not  impeding  the  flow  of
such studies into the information pool.

Besides  the  above  statistical  tests  that  one
should  expect  to  see  in  a  given  study  (and  the
absence of which might raise questions about the
rigor of the study), there are other more compli-
cated manipulations, the inclusion of which should
prompt careful critique of the study. Per Dr. Trisha
Greenhalgh, professor of primary care at University
College, London “The number of possible statisti-
cal  tests  sometimes  seems  infinite.  In  fact,  most
statisticians  could  survive  with  a  formulary  of
about a dozen” [12]. These critical statistical tests
include: the t-test; the Mann–Whitney U-test; the
Wilcoxon matched pairs test; the one-way analysis
of variance (F) test; the Kruskal–Wallis one-way
analysis of variance; the two-way  analysis of vari-
ance;  the  c2  test;  Fisher’s  exact  test;  Pearson’s  r;
Spearman’s  rank  correlation  coefficient;  regres-
sion  by  least  squares  method;  nonparametric
regression  [12];  and  the  ROC  curve.  Of  course,
just because a test not mentioned above is employed
does not mean that it is part of an attempt to make
the data fit at all costs, but the more complicated
the  function  the  less  useful  the  resulting  trends
are  to  everyday  diagnosis  or  intervention.  Per
Dr. Greenhalgh’s comments on the statistical tests
other  than  those  listed  above,  “The  rest  should
generally be reserved for special indications. If the
paper you are reading seems to describe a standard
set of data which have been collected in a standard
way,  but  the  test  used  has  an  unpronounceable
name and is not listed in a basic statistics textbook,
you should smell a rat. The authors should, in such
circumstances, state why they have used this test,
and  give  a  reference  (with  page  numbers)  for  a
definitive description of it” [12].

The Receiver Operating
Characteristic Curve: A Special Tool

This is a statistical tool that first saw use during
World War II by the Royal Air Force. It was used
to  optimize  radar  operators’  level  of  suspicion
regarding the identification of objects in the air-
space off the English coast. It was crucial to the
survival of the United Kingdom that the appear-
ance  of  phosphorescence  on  an  oscilloscope
accurately  alerted  operators  to  incoming  enemy
aircraft. An operator’s threshold was adjusted by
plotting a graph of the percent of the time he or
she identified enemy aircraft correctly (true posi-
tive rate) against the percentage of the time he or
she  identified  other  entities  (clouds,  birds,  etc.)
incorrectly (the false positive rate). As true posi-
tive rate is equal to sensitivity and false positive
rate is equal to 1-specificity, this is also a plot of
sensitivity versus 1-specificity. The curve that is
generated  is  known  as  the  Receiver  Operating
Characteristic  (ROC)  curve.  As  with  all  curves,
the derivative and the integral provide interesting
information.  The  derivative,  that  is  to  say  the
slope of the curve, equates to the positive likeli-
hood ratio (+LR). The integral, i.e., the area under
the curve (AUC), is approximately equivalent to
the accuracy of the test (it is not absolute accu-
racy because it does not take into account values
that  have  fallen  under  the  curve  due  to  chance,
but this is accepted to be a small proportion of the
total measurements and can be addressed with a
correcting  factor  if  need  be).  It  would  be  fairly
easy to generate ROC curves (using the true posi-
tive rate and the false positive rate) for a number
of studies and to determine which ones generated
the maximal +LRs and AUCs. These values could
be  averaged  over  multiple  studies  of  the  same
type and then compared with other study types in
order  to  determine  which  study  type  was
optimal.

Although  the  ROC  curve  is  a  powerful  tool,
statistical  methods  like  the  ROC  curve  do  not
eliminate  the  biases  that  exist  in  a  poorly  desi-
gned  study,  and  in  fact,  may  be  misleading  by
 generating  inaccurate  assessments  of  the  inter-
vention as they may not reveal the internal biases
in studies.

32

P.J. Saunders and C.N. Otis

What is External Validity?

The CEBM defines external validity in reference
to a study as “The appropriateness by which its
results  can  be  applied  to  non-study  patients  or
populations” [5].. The USPTF refers to external
validity  as  “applicability”  [13]  and  “generaliz-
ability” [14].

What Role Does External Validity
Play in the Evaluation of Evidence
Quality?

External  validity  is  important  in  determining
whether or not evidence should be incorporated
into a specific recommendation and the strength
(or  the  grade)  associated  with  the  recom-
mendation.  Applicability,  or  lack  thereof,  has
nothing  to  do  with  how  good  the  study  is  at
answering a particular question. It is a mistake to
confuse the trueness of an answer with how use-
ful that answer will be in a given situation. The
CEBM’s  levels  of  evidence  are  grouped  into
“grades  of  recommendation”  denoted  A,  B,  C,
and  D.  The  groupings  are  not  based  upon  the
quality (internal validity) of the evidence, but the
clinical applicability of the study (external valid-
ity) which is influenced by such factors as “cost,
ease of implementation (of treatment, diagnostic
test ,etc.), and importance of the  disease” [15].

Study Designs: How Are They Used
to Rank the Quality of Studies?

by the U.K.’s National Health Service, includes
only  systematic reviews in its highest stratum of
evidence-generating studies even if the individ-
ual studies are not RCTs. It is difficult to imag-
ine  that  there  exists  evidence  to  prove  that  a
single  large,  well-conducted  RCT  generates
lower  quality  data  than  a  systematic  review  of
multiple small RCTs, each with its own micro-
cosm of variables.

Internal validity alone cannot be used to rank
studies.  For  example,  consider  a  common  diag-
nostic  dilemma  encountered  in  pathology:  The
distinction of mesothelioma from mimics such as
reactive  mesothelium  and  adenocarcinoma.  It
would  be  very  helpful  to  the  pathologist  if  an
immunohistochemical (IHC) stain was available
to  identify  neoplastic  mesothelial  cells.  A  well-
developed study would be prospective, random-
ized,  blinded,  and  controlled.  Such  a  study,  if
conducted in concordance with these principles,
would  be  said  to  be  internally  valid.  However,
such a study would be extremely difficult to con-
struct in the everyday environment of pathology
practice. The closest study likely to be conducted
would be a retrospective review of known cases
of mesothelioma and compared to cases of known
mesothelioma  mimics,  employing  a  novel  anti-
body  hypothesized  to  discriminate  between  the
two  groups.  This  typical  study  conducted  in
pathology  is  at  perhaps  level  III  according  to
USPFT criteria. Obviously, such common pathol-
ogy studies do not reach the upper levels of best
evidence.  Even  though  both  studies  might  have
excellent internal validity, the RCT is deemed to
have generated better evidence because the design
is intrinsically better.

This  is  a  critical  concept  and  one  that  the
 individual  pathologist  (or  any  other  physician)
must consider. Ranking one study or one set of
diagnostic  criteria  over  another  is  the  patholo-
gist’s  bread  and  butter.  It  might  seem  that  this
should  have  been  well  fleshed  out  by  now.
However,  the  ranking  systems  seem  to  contain
important
the
University  of  Oxford  CEBM,  a  body  consulted

 contradictions.  For  example,

What Is the “Evidence Pyramid”?

This leads to the concept of evidence stratification
based upon the quality of various study designs.
The evidence pyramid is a simple graphical way
of representing relative quality of evidence gener-
ated  by  different  study  types.  Many  versions  of
the evidence pyramid exist. For our purposes, we
refer to the following evidence pyramid:

3  What Is Best Evidence in Pathology?

33

Meta
Reviews

Systematic
reviews

Randomized Controlled Trials

Cohort studies

Case control studies

Case report/Case Series/Expert Opinion

Animal studies

The  differences  in  the  relative  sizes  of  the
 pyramid levels represent the approximate number
of each kind of study in existence. The location
with regard to superior and inferior is relative to
the  quality  of  evidence  that  each  type  of  study
generates: the type of study generating the high-
est quality data (meta-analysis) is at the top of the
pyramid,  and  the  type  of  study  generating  the
lowest quality data (animal study) is at the base.

Many  charts  similar  to  this  one  have  been
generated  for  the  new  discipline  of  evidence-
based  medicine,  which  has  frequently  been
concerned with treatment, and less so with diag-
nosis. It can be readily appreciated from a chart
of  evidence  stratification  like  the  CEBM’s  [16]
(essentially  a  deconstructed  evidence  pyramid)
that the relative value of a particular study type
changes with respect to whether diagnosis, inter-
vention, or some other parameter is being evalu-
ated.  Although  the  stratification  of  evidence
quality seen in the EBM pyramid is fairly subjec-
tive,  pathologists  must  not  simply  adopt  this
pyramid  without  some  investigation  into  the
possibility  that  diagnostic  tests  may  require  a
differently organized algorithm. For example, the
pyramid  above  is  slightly  different  from  those
seen in EBM in that it incorporates a level of the
pyramid for case report/case series/expert opinion.
We  choose  to  rate  this  as  better  evidence  than

animal studies, but inferior to controlled studies.
It seems logical that these human-based endeavors
should  yield  evidence  of  better  quality  than
nonhuman-based  studies  if  we  are  dealing  with
diagnosing and treating humans (the astute reader
might  point  out  that  this  is  an  external  validity
problem, not an internal validity problem, which
would  be  a  valid  argument).  Expert  opinion  is
included in this tier by default: it is not controlled
in  any  formal  way,  but  again,  it  is,  after  all,
presumably  based  on  human  data.  If  an  expert
has personally scrutinized many cases of a patho-
logic  entity  and  taken  a  strong  interest  in  many
cases representing the entity with knowledge of
clinical  outcome,  the  expert’s  opinion  may  be
significantly  better  than  second-to-bottom  tier
level evidence. The time-honored expert opinion
in  pathology  may  yet  represent  good  evidence,
although proving this point remains difficult.

Study Designs Providing “Best
Evidence” in Clinical Medicine:
Systematic Reviews and Meta-
Analysis

A  systematic  review  is  “an  article  in  which  the
authors  have
for,
systematically
appraised,  and  summarized  all  of  the  medical

searched

34

P.J. Saunders and C.N. Otis

 literature  for  a  specific  topic”  [17].  Systematic
reviews  are   compilations  of  multiple  individual
studies  performed  in  attempt  to  overcome  the
problem of low power. Low power is principally
due  to  small  sample  sizes  of  individual  studies.
A systematic review can be viewed as a study in
which the “subjects” are a number of other stud-
ies.  Systematic  reviews  can  be  qualitative  where
the results of the primary studies are summarized
but  not  statistically  combined,  or  quantitative
where the results of the primary studies are statis-
tically combined. Quantitative systematic reviews
are  also  known  as  meta-analyses  [17].  The  term
“overview” is sometimes used to denote a system-
atic  review,  whether  qualitative  or  quantitative
[17].  Summaries  of  research  that  lack  explicit
descriptions  of  systematic  methods  are  often
called  narrative  reviews  [18].  Both  the  USPTF
and the CEBM rank systematic reviews and meta-
analyses as the highest level of evidence. Numerous
other textbooks on evidence-based medicine laud
systemic analysis, and specifically meta-analysis,
as  generating  the  best  quality  of  evidence.
Of  course,  the  validity  of  a  systematic  study  is
only as strong as the individual studies that it com-
prises.  Additionally,  as  for  any  study  type,  the
validity  may  suffer  if  the  design  of  the  review
itself is poor. Two important statistical tools used
in meta-analysis are subgroup analysis and meta-
regression (a type of regression analysis).

What Is Subgroup Analysis?

Subgroup  analysis  compares  smaller  groups
within  the  test  group  and  the  control  group  in
order to determine if heterogeneity within these
groups skews the data derived from the trial. This
is a subjective, nonmathematical endeavor.

What Is Meta-Regression Analysis?

While  the  mathematical  details  of  regression
analysis are beyond the scope of this chapter, suf-
fice it to say that regression analysis is an extremely
powerful  tool  for  modeling  systems  with  many
variables, some of which may not be quantifiable.

These unquantifiable variables do, however, have
effects on the dependent variables (that which we
are measuring and are most interested in) creating
“wobble”  in  the  dependent  variable.  The  idea
behind regression analysis is to create equations
in terms of quantifiable variables to represent the
unquantifiable  variables,  thus  accounting  for
the  “wobble”.  This  is  most  important  when  the
results of different studies are aggregated in meta-
analyses:  the  “wobble”  in  the  final  values  being
compared (e.g., relative risk, PPVs, or other val-
ues  used  to  compare  studies  in  meta-analyses)
needs to be explained mathematically so that an
equation can be developed to describe the collec-
tive  environment  of  the  studies.  The  commonly
employed types of regression analysis are linear
regression (dependent variable solutions fall along
a straight line, seen in system without “wobble”),
fixed  meta-regression  (which  uses  effect  size  as
a constant, and therefore cannot be used to com-
pare  across  studies,  only  within  a  single  study),
and random meta-regression (which does not set
the  effect  size,  and  so  can  be  used  to  compare
 different studies that have different effect sizes).

What Is More Commonly Employed
to Evaluate the Significance
of Results Generated
by Meta-Analysis, Subgroup
Analysis or Meta-Regression?

Unfortunately,  meta-regression,  the  most  objec-
tive tool that we have to help generate the highest
level of evidence, is difficult to employ due to its
complexity.  Per  the  Cochrane  Collaboration,  an
international initiative that produces and dissemi-
nates  systematic  reviews  of  healthcare  interven-
tions,  “Meta-regression  is  rarely  performed  in
Cochrane reviews and not an available option in
Cochrane  software,  so  should  you  have  strong
reason to include a meta-regression in your review,
you  will  need  the  help  of  a  statistician”  [18].
As physicians, we need to educate ourselves about
how statistical tools such as meta-regression func-
tion and then to advocate for their use if we expect
to receive the highest quality data.

3  What Is Best Evidence in Pathology?

35

So Is the Final Meta-Analysis Really
Used to Make Clinical Decisions?

The Cochrane Collaboration has received recent
attention regarding breast cancer screening guide-
lines. The USPTF has recommended that women
receive mammograms starting at age 50, and at 2
years intervals (as opposed to annually screening
starting  at  age  40,  as  previously  recommended)
[19].  A  1993  review  of  annual  screening  mam-
mography estimated that it reduced breast cancer-
related  mortality  by  20–30%  [20].  However,  a
2005 Cochrane review estimated that the relative
risk  reduction  was  15%,  the  absolute  reduction
of risk was 0.05%, and that mammography may
do  more  harm  than  good  [21].  This  Cochrane
Collaboration  systematic  review  from  2005  is
being cited as evidence in favor of the recent sug-
gested  changes  in  mammographic  screening.
In  order  to  determine  which  review  generated
the best quality evidence, we have to explore how
to critically assess the designs of the studies.

How to Tell a Good Systematic
Review from a Bad One

For systematic reviews, Oxford’s CEBM provides
a checklist of questions to help with this process
[22]. In the first step, the question “What question
did the systematic review address?” is asked. This
is  aimed  at  determining  the  basic  parameters  of
study design: the nature of the test population; the
intervention;  what  was  compared;  and  what  out-
come resulted. The second question, “Is it unlikely
that important, relevant studies were missed?,” is
posed  to  help  determine  whether  or  not  the  cre-
ators of the meta-analysis found most of the stud-
ies on the topic. This should include a search of
major  bibliographic  databases,  a  search  of  refer-
ence lists from relevant studies, and contact with
experts to inquire about unpublished studies. The
search “should not be limited to the English lan-
guage and should include medical subject heading
(MeSH) terms and text words.” The third question,
“Were the criteria used to select articles for inclu-
sion appropriate?,” involves determining whether

the  studies  selected  by  the  creators  are  of  good
quality. In a good systematic review, the inclusion
and  exclusion  criteria  of  the  individual  studies
“should be clearly defined a priori, and the eligibility
criteria used should specify the patients, interven-
tions,  or  exposures  and  outcomes  of  interest.”
In many cases, the type of study design will also
be a key component of the eligibility criteria. The
fourth question “Were the included studies suffi-
ciently  [internally]  valid  for  the  type  of  question
asked?” attempts to uncover systemic reviews that
are based on poor-quality data. The fifth question
“Were  the  results  similar  from  study  to  study?”
deals  with  determining  if  the  results  of  the  indi-
vidual studies are similar enough to combine.

As noted previously, the USPTF uses the fol-
lowing  criteria  when  determining  the  internal
validity of an individual systematic review:
•

Comprehensiveness  of  sources  considered/
search strategy used.
Standard appraisal of included studies.
Validity of conclusions.
Recency  and  relevance  are  especially  impor-
tant for systematic reviews [7].

•
•
•

Typically, What Types of Studies
Generate the Best Evidence?

The  sine  qua  non  of  scientific  study  is  the  pro-
spective,  randomized,  controlled,  double-blind
(PRCDB) study, a type of RCT. A PubMed search
for “randomized controlled” yields 358,053 arti-
cles.  The  oldest  of  these  is  an  article  titled
Interactions  between  pharmacodynamic  and
placebo  effect  in  drug  evaluations  in  man  by
Modell  and  Garrett,  published  in  the  February
1960  edition  of  Nature.  The  idea  behind  the
PRCDB design is simply that a group of subjects
is intervened upon and that the group is compared
to a group not receiving intervention in an envi-
ronment  free  from  tampering  (intentional  or
otherwise).  The  prospective,  double-blind,  and
randomization design of such studies attempts to
minimize  subjectivity.  Retrospective  studies  are
plagued  by  problems  related  to  data  retrieval,
incomplete records, and internal biases. Studies that

36

P.J. Saunders and C.N. Otis

are not blinded may be led astray by unintended
(and  often  unconscious)  interpretation  biases.
Nonrandomized  studies  may  generate  evidence
that  is  reflective  of  the  patient  characteristics
rather  than  by  the  intervention  imposed  on  the
test and nontest groups. A great deal of time and
effort may be dedicated to minimizing these con-
founding  variables  that  disturb  and  obscure  the
primary purpose of the study, to determine the effect
of the imposed intervention.

Are There Any More Comprehensive
Evidence-Level Tables in Existence?

The  University  of  Oxford  Centre  for  Evidence-
based  Medicine  (CEBM)  is  a  body  that  was
assembled to promote evidence-based health care
in  the  United  Kingdom.  One  of  its  roles  is  to
stratify the quality of evidence coming from vari-
ous  study  types.  The  CEBM  has  published  a
comprehensive  table  which  stratifies  evidence
into ten levels (the rows of the table): 1a–c, 2a–c,
3a–b, 4, and 5 [16]. There are five columns, each
representing a different parameter of patient care
for which a physician may seek guiding evidence.
These  correspond  to:  therapy/prevention,  etiol-
ogy/harm; prognosis; diagnosis; differential diag-
nosis/symptom prevalence study; and economic/
decision analysis. The study type that is assigned
to each level varies by column. For example, for
the highest level of evidence for guiding therapy/
prevention, etiology/harm is listed as the system-
atic review of randomized controlled trials. The
highest level of evidence for differential diagno-
sis/symptom  prevalence  study  is  the  systematic
review  of  prospective  cohort  studies.  There  are
50 individual fields in the chart and 41 different
study types assigned to the fields.

The part of the chart most useful to patholo-
gists is the column corresponding to diagnostic
studies. Within this category, level 1a is system-
atic  review  of  studies  (with  homogeneity)  as
well  as  the  clinical  decision  rule  involving  1b
studies from different clinical courses. Level 1b
is  the  validating  cohort  study  with  good  refer-
ence standards or the clinical decision rule tested

within one clinical center. Level 1c encompasses
tests that exhibit specificity so great that a posi-
tive result rules a diagnosis in, or sensitivity so
great  that  a  negative  test  rules  a  diagnosis  out.
Level 2a is a homogenous systematic review of
diagnostic  studies  that  are  level  2  or  higher.
Level  2b  is  an  exploratory  cohort  study  with
good  reference  standards  or  a  clinical  decision
rule.  Level  3a  is  the  homogenous  systematic
review  of  studies  determined  to  be  level  3b  or
better. Level 3b is the nonconsecutive study or a
study  without  consistently  applied  reference
standards. Level 4 is the case–control study with
a  poor  or  nonindependent  reference  standard.
Level  5  is  the  expert  opinion  without  explicit
critical appraisal, or based on physiology, bench
research or “first principles” [16]. This is illus-
trated  more  simply  in  the  evidence  pyramid
described earlier.

Best Evidence in the Pathology
Literature.  How Good Is It?

Pathologists are at a distinct disadvantage regard-
ing the quality of the evidence we rely upon. As a
group,  pathologists  generally  use  data  that  are
generated  by  studies  that  fall  within  the  bottom
reaches of the evidence pyramid. Pathologists are
both  aided  and  disadvantaged  by  large  volumes
of  archived  material:  while  this  provides  ample
fodder  for  research,  it  means  pathologists  rely
heavily  on  retrospective  studies.  Retrospective
studies are universally accepted to generate evi-
dence  of  lower  quality  than  prospective  studies
due to the loss of data that inevitably occurs rela-
tive  to  prospective  studies.  Sometimes  this  is
unavoidable. Some diseases are rare or progress
very slowly to hinder accrual of significant num-
ber of cases or arrive at the final outcome. When
especially  uncommon  diseases  are  encountered,
it may be that there are not enough cases in exis-
tence  to  derive  statistically  significant  results.
This often leads to the generation of case studies
and case series, which are considered by most to
be of even lower quality evidence than retrospec-
tive studies.

3  What Is Best Evidence in Pathology?

37

Should Pathologists Concern
Themselves with Understanding
only the Studies that Exclusively
Aim to Answer Questions
Pertaining to Diagnosis?

In  addition  to  deriving  the  best  diagnosis,  the
pathologist is also responsible in part for direct-
ing  the  course  of  patient  care.  As  the  clinical
colleagues of pathologists feel increasingly over-
whelmed by mounting volumes of data, the pathol-
ogist is relied upon more heavily in this capacity.
As  per  the  concept  described  by  Sinard  and
Morrow, the pathologist is at the center of receiv-
ing evidence from a multitude of sources and is
responsible  for  integrating  these  elements  into
the best information in an effort to properly guide
patient care [23]. In order to do this, the patholo-
gist must be familiar with the literature of other
branches of medicine as well as pathology. This
is especially critical in the case of the expert. The
best  anatomic  pathology  data  come  from  meta-
analyses of cohort studies (randomized controlled
trials  are  rare),  and  for  this  reason,  the  expert
must be familiar with clinical literature (in which
randomized  controlled  trials  and  meta-analyses
of such trials are common). The expert should be
familiar  with  the  available  literature  and  under-
stand which studies have generated higher quality
data and why this is the case.

Are Different Strata of Evidence
Preferable for Different Applications
in Pathology Research?

The standard of the PRCDB has historically been
held above the rest. The PRCDB is not necessar-
ily the best study type for many types of research
questions. According to the guidelines published
by  the  USPTF  (which  includes  proper  design
with prospective and blinded characteristics), the
highest  quality  data  come  from  PRCDBs  only
when  considering  studies  that  examine  the
“ benefits or harms of various interventions” [14].

The USPSTF notes that “RCTs cannot answer all
questions” [14], which implies that studies other
than RCTs are better employed to answer those
questions. The CEBM’s stratification strategy is
another  example  of  an  algorithm  that  does  not
rank the RCT above all other studies. In fact, sys-
tematic  reviews  that  do  not  include  RCTs  are
ranked higher.

The  PRCDB  study  is  not  the  highest  rated
study type for determining diagnosis or progno-
sis (many systems rank it as the highest level of
evidence only when the questions asked pertain
to therapy or harm). It is difficult to understand
why any study would benefit from not being ran-
domized or controlled. However, real-world limi-
tations  may  cause  some  researchers  to  abandon
the PRCDB when investigating a diagnostic test
when the potential harm to patients as a result of
an incorrect diagnosis is considered. The aim of
such  studies  is  to  determine  the  accuracy  of  a
diagnostic test and a control group is essential to
developing  tests  that  accurately  diagnose  dis-
eases. However, such studies may not be appro-
priate  or  justifiable  if  the  potential  harm  of  the
diagnostic  test  in  the  test  and  control  groups  is
considered.

A close approximation of a randomized, con-
trolled,  and  blinded  study  in  pathology  may  be
constructed  in  the  case  of  a  hypothetical  IHC
assay being investigated to determine its useful-
ness in the diagnosis of mesothelioma. Pathologist
“X” applies a current gold standard in identifying
cases of mesothelioma as well as mimics such as
reactive mesothelial hyperplasia. Pathologist “X”
randomly  separates  them  into  two  groups  and
applies  the  IHC  stain  to  one  group.  Pathologist
“X”  gives  pathologist  “Y”  slides  of  the  two
groups. Pathologist “Y” determines if mesothe-
lioma or reactive mesothelial hyperplasia is pres-
ent  or  absent.  After  pathologist  “Y”  records  his
answers, the diagnoses of pathologist “X” based
upon the gold standard are compared to the inter-
pretations of pathologist Y who applied the IHC
assay. The result of this comparison measures, at
least in part, the usefulness of the IHC assay in
discriminating mesothelioma from reactive meso-
thelial hyperplasia.

38

P.J. Saunders and C.N. Otis

How Can a Pathologist Distinguish
a Good Case–Control Study
from a Bad One

The  USPFT  uses  the  following  criteria  when
evaluating  the  internal  validity  of  case–control
studies. These criteria apply to treatment studies
that employ a diagnostic test and therefore assume
a certain level of accuracy in the diagnostic test.
•
•

Accurate ascertainment of cases.
Nonbiased  selection  of  cases/controls  with
exclusion criteria applied equally to both.
Response rate.
Diagnostic testing procedures applied equally
to each group.
Measurement of exposure accurate and applied
equally to each group.
Appropriate  attention  to  potential  confound-
ing variables [7].

•
•

•

•

How Can a Pathologist Distinguish
a Good Case Series from a Bad One

Criteria for determining a good case series from a
bad case series might include: number of cases in
the series; the similarity of the cases with regard
to the disease process, case selection and manner
of  accrual  (e.g.,  consultation  file  bias);  and
whether or not the patients are treated similarly. It
would  also  be  advantageous  to  know  follow-up
or outcome endpoints.

How Can a Pathologist Distinguish
a Good Expert Opinion
from a Great One

Determining a good expert opinion from a great
one is highly subjective. Miriam-Webster defines
“expert” as “having, involving, or displaying spe-
cial skill or knowledge derived from training or
experience”  [1]  and  opinion  as  “belief  stronger
than  impression  and  less  strong  than  positive
knowledge”  [1].  Given  this,  one  might  want  to
know  if  the  expert’s  opinions  are  skewed  more
towards  knowledge  or  impression.  The  expert’s

reputation in the field may give a glimpse of how
weak or strong others perceive to be the expert’s
reasoning  and  knowledge.  It  might  be  advanta-
geous to ascertain how often the expert has been
proven  to  be  correct.  This  may  be  difficult  or
impossible, but in some instances may be based
on prior experience relative to outcome in cases
previously referred for expert opinion. An addi-
tional  indicator  may  be  how  extensively  the
expert has published on the topic, the number of
relevant cases involved in these publications, and
if these publications are recent.

Summary

Most of the experience in evidence-based medi-
cine has been derived from clinical medicine that
seems  more  easily  suited  to  the  rigors  of  the
higher tiers of quality. In pathology, it is difficult
to apply many of the stringent requirements nec-
essary  to  generate  high  quality.  For  example,
there  is  some  debate  over  what  tests  should  be
used  to  distinguish  squamous  cell  carcinoma  of
the lung from other types of non-small cell carci-
noma,  as  the  former  may  be  associated  with
severe pulmonary hemorrhage when treated with
recently  developed  agents  which  inhibit  angio-
genesis [24]. Should the identification rely only
on routine histologic criteria (keratinization and
intercellular bridges), or should it include a group
of  IHC  studies  such  as  p63,  TTF-1,  and  high
molecular  weight  keratin  antibodies  without
regard to the presence of morphologic features of
squamous differentiation? Obviously, the patient
selection for this new therapy differs depending
on the diagnostic test. One manner to obtain best
evidence  regarding  the  diagnostic  test  would
include  two  randomized  groups  of  patients  of
similar characteristics selected but distinguished
by how the carcinoma is characterized (by mor-
phology alone or by the use of the IHC tests). The
pathologist could be assigned to one study group
only  and  blinded  to  the  other  group  as  well  as
outcome. The outcome would be the frequency of
pulmonary hemorrhage in the two groups. Such a
study is difficult to envision as a viable manner of
determining which diagnostic criteria are best in

3  What Is Best Evidence in Pathology?

39

predicting the outcome for both obvious, serious
ethical as well as logistical reasons. On the other
hand, it remains unknown if patients are unneces-
sarily  excluded  from  receiving  antiangiogenesis
factors in the treatment of lung carcinoma based
on the IHC studies without consideration of mor-
phology. With the introduction of new therapies
and  diagnostic  tests  including  molecular  and
cytogenetic  assays,  this  scenario  may  become
common place.

Nevertheless,  there  are  effective  strategies  to
improve  the  quality  of  evidence  in  pathology
which have been illustrated in this chapter, build-
ing  upon  the  cornerstones  of  observation  and
clinical correlation which have heretofore defined
much of what we know as pathologists about dis-
ease and diagnosis.

References

  1.  Merriam-Webster’s  Online  Dictionary,  F.C.  Mish,
Editor.  2010.  http://www.merriam-webster.com/
dictionary/evidence.

  2.  Wild  SE.  Webster’s  new  world  law  dictionary.

Hoboken, NJ: Wiley; 2006. p. 320.

  3.  Lieberman D. The province of legislation determined:
legal theory in eighteenth-century Britain. Cambridge:
The Press Syndicate of the University of Cambridge;
1989. p. 312.

  4.  Rule 609. Impeachment by evidence of conviction of

crime. 2010.

  5.  Center  for  Evidence  Based  Medicine.  2010.  http://

www.cebm.net/?o=1116.

  6.  McCrory  DC,  Samsa  GP,  Hamilton  BB,  et  al.
Treatment  of  pulmonary  disease  following  cervical
spinal cord injury: evidence report/technology assess-
ment no. 27. Rockville, MD: Agency for Healthcare
Research and Quality; 2001.

  7.  Appendix VII. Criteria for assessing internal validity
of  individual  studies.  2010.  http://www.ahrq.gov/
clinic/uspstf08/methods/procmanualap7.htm.

  8. Röhrig B, du Prel JB, Blettner M. Study design in medical
research: part 2 of a series on the evaluation of scientific
publications. Dtsch Arztebl Int. 2009;106(11):184–9.

  9.  Volokh  A.  n  Guilty  Men.  Univ  PA  Law  Rev.  1997;

146(1):173–216.

 10. Trosset M. An introduction to statistical inference and
its applications with R. Boca Raton, FL: Taylor and
Francis Group LLC; 2009. p. 208.

 11. Gunnarsson  RK,  Lanke  J.  The  predictive  value  of
microbiologic diagnostic tests if asymptomatic carri-
ers are present. Stat Med. 2002;21(12):1773–85.
 12. Greenhalgh T. How to read a paper: Statistics for the
non-statistician. I: Different types of data need differ-
ent statistical tests. Br Med J. 1997;315: 364–6.
 13. Meenan RT, Saha S, Chou R, et al. Effectiveness and
cost-effectiveness  of  echocardiography  and  carotid
imaging in the management of stroke: evidence report/
technology  assessment  no.  49.  Rockville,  MD:
Agency for Healthcare Research and Quality; 2002.
 14. Procedure Manual. Section 4: Evidence report devel-
opment.  2010.  http://www.ahrq.gov/clinic/uspstf08/
methods/procmanual4.htm.

 15. Levels  of  evidence.  2002.  http://www.eboncall.org/

content/levels.html.

 16. Oxford  Center  for  Evidence-based  Medicine-Levels
of  Evidence  (March  2009).  http://www.cebm.net/
index.aspx?o=1025. 2009.

 17. Cook  DJ,  Mulrow  CD,  Haynes  RB.  Systematic
reviews: synthesis of best evidence for clinical deci-
sions. Ann Intern Med. 1997;126(5):376–80.

 18. Garg AX, Hackam D, Tonelli M. Systematic review
and meta-analysis: when one study is just not enough.
Clin J Am Soc Nephrol. 2008;3:253–60.

 19. The Cochrane Collaboration’s open learning  material.
diversity and heterogeneity. http://www.cochrane-net.
org/openlearning/HTML/mod13-5.htm. 2002.

 20. From  the  U.S.  Preventive  Services  Task  Force,
AfHRaQ, Rockville, Maryland. Screening for breast
cancer:  U.S.  Preventive  Services  Task  Force
Recommendation Statement. Ann Intern Med. 2009;
10:716–26.

 21. Elwood JM, Cox B, Richardson AK. The effectiveness
of breast cancer screening by mammography in younger
women. Online J Curr Clin Trials. 1993;Vol. 2.

 22. Systematic Review Critical Appraisal Sheet. Critical
appraisal sheets. http://www.cebm.net/index.aspx?o=
1913. 2010.

 23. Sinard  JH,  Morrow  JS.  Informatics  and  anatomic
pathology: meeting challenges and charting the future.
Hum Pathol. 2001;32:143–8.

 24. Ricciardi S, Tomao S, de Marinis F. Toxicity of tar-
geted therapy in non-small-cell lung cancer manage-
ment. Clin Lung Cancer. 2009;10:28–35.

Biostatistics 101

Robin T.  Vollmer

4

Keywords
Probability  •  Biostatistics  in  evidence-based  medicine  •  Cox  model
• Hazard function • Log rank • Conditional probabilities • Receiver  operator
curve

Probability

The  notion  of  probability  began  with  questions
regarding games of chance during the seventeenth
century, so that the probability of an event can be
defined as the chance of observing that event. In
a more experimental mode, probability relates to
the  relative  frequency  of  observing  an  outcome
after many repeated trials. For example, suppose
we   perform  an  experiment  and  observe  an  out-
come E. Now let us repeat the experiment n times
and tally the number of times E occurs to be m.
The probability of E, P(E), would be estimated as
m/n.  In  this  manner,  the  probability  of  the  out-
come could be defined as the limit of m/n when n
becomes infinite.

In the twentieth century, however, probability
was redefined in set theoretic and mathematical
terms. What follows is an abbreviated version of
this mathematics. If we have a discrete set, S, of

all possible observed events, then the  probability
of  observing  an  event  E  is  symbolized  as  P(E).
P(E)  must  be  a  real  number  between  0  and  1.
The probability of any event in S must be 1, that
is, P(S) = 1. The probability of no event in S is 0.
If  the  event  E  does  not  happen,  then  this  is
also  an  event  that  is  termed  “not  E”  or  some-
times  symbolized as ~E. The probability of ~E
is given as:

P(~ E) 1 P(E).
= −

The odds of an event E is defined as the ratio
of the probability of E divided by the probability
of ~E, or as:

Odds (E) P(E) / (1 P(E)).

−

=

Finally, if there are two events E1 and E2, the

probability of observing either E1 or E2 is:

P(E or E ) P(E ) P(E ) P(E  and E ).

−

=

+

1

2

2

1

1

2

R.T. Vollmer (*)
Department of Laboratory Medicine,
VA Medical Center, Durham, NC, USA
e-mail: Robin.Vollmer@va.gov

Here, P(E1 and E2) is the probability of observ-
ing both events. Two events, E1 and E2, are mutually
exclusive if the probability of observing both is
zero, i.e. P(E1 and E2) = 0.

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_4, © Springer Science+Business Media, LLC 2011

41

42

R.T. Vollmer

Statistical Independence

Two  events  E1  and  E2  are  statistically  indepen-
dent  if  and  only  if  the  probability  of  observing
both is the product of each of their separate prob-
abilities, that is, if and only if:

P(E  and E ) P(E ) P(E ).
=

×

1

2

1

2

This is the only true notion of statistical inde-
pendence. When we read that variables like tumor
stage and grade provided “independent”  prognostic
information,  such  conclusions  and  wordings  are
most often wrong, because variables important to
survival  are  often  codependent,  not  statistically
independent. What they may provide is additive,
not independent, information.

Conditional Probabilities

Conditional probabilities are of great importance
in  medicine,  including  pathology.  For  example,
sensitivities, specificities, and positive predictive
values are examples of conditional probabilities.
Using a general approach, consider the two events
E1 and E2. The probability of observing event E2
given  that  E1  has  already  been  observed  is  the
conditional probability P(E2 | E1). If the two events
are the presence of a positive laboratory test  T+
and  the  presence  of  a  particular  diagnosis  D+,
then 100 × P(T+ | D+) is the sensitivity of the test
for the diagnosis (expressed here as a percent). In
other words, the sensitivity of the test T is the con-
ditional probability that T is +, given the presence
of the diagnosis D. (In what follows, the 100× will
be omitted, and the conditional probabilities will
be  expressed  as  fractions  rather  than  percents.)
Table 4.1 lists and defines several commonly used
conditional probabilities for tests and diseases.

One can also form ratios of these conditional
probabilities.  For  example,  the  relative  risk  is
defined as the positive predictive value of the test
divided  by  the  probability  of  a  false  negative
test or:

Relative risk

=

P(D | T ) / P(D | T ).

+

+

+

−

Table 4.1  Conditional probabilities in pathology

Sensitivity of test T for diagnosis D:
Specificity of test T for diagnosis D:
Probability of false positive test T:

Probability of false negative test T:

Positive predictive value of test T for
diagnosis D:
Negative predictive value of test T for
diagnosis D:

P(T+ | D+)
P(T− | D−)
P(T+ | D−)
= 1 − P(T− | D−)
P(T− | D+)
= 1 − P(T+ | D+)
P(D+ | T+)

P(D− | T−)

And  the  likelihood  ratio  is  defined  as  the
sensitivity  divided  by  the  probability  of  a  false
positive test or as:

Likelihood ratio P(T | D ) / P(T | D ).

=

+

+

+

−

ROC Curves

A graphical tool that has been helpful for evaluating
new tests in clinical medicine is the ROC curve.
ROC  stands  for  receiver  operator  curve,  and  it
was used in World War II to evaluate the abilities
of  plane  spotters  to  classify  aircraft  as  either
friendly  or  enemy.  In  medicine,  the  ROC  is
formed by plotting sensitivity against 1-specificity.
In other words, the vertical axis is P(T+ | D+) and
the horizontal axis is 1 − P(T− | D−). Furthermore,
1-specificity  is  the  same  as  the  probability  of  a
false positive test. Consequently, a perfect diag-
nostic test – one with sensitivity of 1 and a false
positive  probability  of  0  –  would  appear  as
single point in the upper left corner of the ROC.
An  example  of  an  ROC  is  the  following  plot,
which demonstrates the value of serum PSA for
the diagnosis of prostate cancer (Fig. 4.1).

The  points  on  the  curve  (Brawer  et  al.’s  data)
and angles in the line (Catalona et al.’s data) are due
to the use of different cutpoints in serum PSA for
the diagnosis of prostate cancer [1, 2]. For example,
locations on the ROC in the lower left side of the
plot are for cutpoints at high values of serum PSA,
where the sensitivity is low and the specificity high.
Locations on the ROC in the upper right side of the
plot are for cutpoints at low values of serum PSA,
where the sensitivity is high and the specificity low.
The straight line on the plot indicates ROC  locations

4  Biostatistics 101

43

1.0

0.8

0.6

0.4

0.2

y
t
i

v
i
t
i
s
n
e
S

0

0

0.2

0.4
0.6
1-Specificity

0.8

1.0

Fig.  4.1  ROC  plot  of  sensitivity  versus  1-specificity  for
serum PSA and the diagnosis of prostate cancer. The data
come from studies by Brawer et al. (points) [1] and Catalona
et al. (curved line) [2]. For each point and each angle in the
curve,  a  different  cutpoint  in  serum  PSA  was  used.  The
straight  line  indicates  the  locations  where  tests  are  not
 helpful, that is, where sensitivity equals 1-specificity

for a test that is not diagnostically helpful, that is,
where  sensitivity  equals  1-specificity.  In  fact,  the
further the curve lies above this line, the better will
be the test. Higher ROC curves also imply larger
areas  under  the  ROC  curve,  and  this  area,  which
commonly ranges from 0.5 to 1.0, is often used as
a measure of a good test.

Bayes Theorem or Rule

Thomas Bayes was a Presbyterian minister who
lived in England in the eighteenth century, and he
also was a mathematician with interests in calcu-
lus and numerical series. But he is best known for
his  formula.  Although  he  developed  the  rule  to
solve a problem dealing with billiard tables, it is
seen most clearly in set theoretic notation. What
Bayes’ rule allows us to do is to estimate the pos-
itive  predictive  value  of  a  positive  test  T+  for  a
diagnosis D+ by using the sensitivity and under-
lying probabilities of T+ and D+ as follows:

P(D | T )
+

+ =

+

P(T | D ) P(D )
+
+ ×
P(T )
+

.

Here, P(D+) is the a priori probability of the
disease without consideration of the test T. The
denominator P(T+) is the a priori probability of a

positive  test,  and  if  there  are  just  two  possibili-
ties, D+ and D−, it can be calculated as:

P(T )

+ =

=

+

P(T | D ) P(D )
+
+ ×
P(T | D ) P(D )
−
− ×
+
sensitivity prevalence
×

+

+ −

(1 specificity)

× −

(1 prevalence)

Thus, if one knows the sensitivity and the spec-
ificity of a test and the prevalence or incidence of
the  disease,  then  one  can  estimate  the  positive
 predictive value of the test for the disease.

Random Variables

In the foregoing, we have talked of probability of
events such as E1, E2, or of T+ and Dx+, but in fact
many events of interest are numerical. We call such
numerical  events  random  variables,  and  in  what
follows,  we  will  use  the  symbol  x  to  represent  a
generic  random  variable.  Examples  of  random
variables  include  the  Gleason  score  for  prostate
cancer, the Breslow thickness for malignant mela-
noma,  and  the  values  of  many  clinical  chemistry
results such as serum PSA. Random variables can
be  classified  as  discrete  or  continuous.  Discrete
random variables include binary ones, which take
just two values like 0 or 1, or yes or no. Discrete
random  variables  can  also  be  categorical  and
ordered, and an example is the Gleason score which
takes the integer values of 2–10. Continuous ran-
dom  variables,  by  contrast,  can  have  an  infinite
number of values, and examples include serum Na,
serum creatinine, and serum PSA.

Probability Distributions for Discrete
Random Variables

For a discrete random variable like the Gleason
score, the probability that the Gleason score takes
a particular numerical value is called its probabil-
ity distribution, which we symbolize here as f(x).
In  other  words,  the  probability  that  x  takes  the
value of a  is written as:

P(x

)
= α = α

f(

).

44

y
c
n
e
u
q
e
r
f
e
v
i
t
a
l
e
R

0.6

0.5

0.4

0.3

0.2

0.1

0

4

5

8
6
7
Gleason score

9

10

Fig. 4.2  Frequency distribution of cases of prostate  cancer
according to their Gleason grading scores (the author’s data)

For  a  population  of  891  tissue  samples  of
prostate  with  cancer  (data  collected  by  the
author),  the  probability  distribution  for  the
Gleason score f(Gleason score) took the follow
values: f(2) ~ 0, f(3) ~ 0, f(4) = 0.002, f(5) = 0.016,
f(6) = 0.32,  f(7) = 0.54,  f(8) = 0.063,  f(9) = 0.053,
and  f(10) = 0.0067.  Notice  that  the  values  of
f(Gleason  score)  sum  to  approximately  1.  This
distribution  function  is  illustrated  in  the  histo-
gram in Fig. 4.2.

The Binomial and Poisson Probability
Distributions for Counted Random
Variables

Two mathematical forms appropriate for discrete
random variables that are counted phenomena are
the binomial and Poisson probability distribution
functions. These are of special interest to pathol-
ogists, because both can deal with counts of cells.
For example, if one counts n cells and observes
that x number of these cells stain positive for an
immunohistochemical  marker,  then  the  fraction
of cells with staining would be estimated as x/n.
If the underlying probability of observing a cell
with staining is symbolized as q, then the bino-
mial distribution for the probability of observing
x cells with staining is given as:

f(x; n, )

θ =

c (n, x)

(x)

× θ × − θ

(1

(n x)
−

)

,

R.T. Vollmer

where C(n,x) stands for the number of combinations
of n things taken x at a time. C(n,x) is calculated as:

C(n, x)

=

n! /{x!

×

(n x)!}
−

and ! is the symbol for factorial function.

The  Poisson  probability  distribution  is  given

as:

f(x; n, )

θ =

(
θ ×

n)

x

×

−θ ×

n)

.

exp(
x!

Here, exp stands for the exponential function. In
practice, the binomial and Poisson probability dis-
tribution functions agree closely with one another,
especially if n exceeds 20 and q is less than 0.05.
The Poisson function, however, can be applied to
situations  when  the  counts  of  x  are  expressed  as
number per area. An example comes from primary
cutaneous melanoma for which the mitotic count is
expressed as number per square millimeter. All one
needs to do is to substitute area for n in the above
equation.

Probability Distributions for Continuous
Random Variables

If the random variable x is continuous, then it can
take an infinite number of values, and its proba-
bility distribution must rely on calculus. Instead
of  writing  the  probability  that  x  takes  a  certain
value a as P(x = a), we are restricted to consider,
for example, the probability that x £ a which we
write as an integral as follows:

P(x

)
≤ α = ∫

f(x) dx.

Here, the limits of the integration are from – ¥
to a, and once again f(x) is the distribution function
for x. Distribution functions commonly used for
continuous random variables include the normal,
the log-normal, the chi-square, and the exponen-
tial. The log-normal and exponential distribution
functions are particularly suitable for continuous
random variables used in pathology because they
deal with random variables that always take posi-
tive values, and this is the case for many continu-
ous  variables  in  clinical  medicine.  For  example,
the  following  bar  graph  shows  the  observed

4  Biostatistics 101

45

1600

1400

1200

1000

800

600

400

200

0

0

5

psa

10

15

Fig. 4.3  Frequency distribution for values of serum PSA
taken from data reported by Morgan et al. [3]. The smooth
line is a superimposed fit obtained by using the exponential
distribution function

 frequency  distribution  for  values  of  serum  PSA
taken  from  a  study  by  Morgan  et  al.  [3].  The
smooth  line  is  a  superimposed  fit  obtained  by
using
function
Fig.  4.3.  This  distribution  demonstrates  that  for
most men with negative biopsies, PSA is less than
5 ng/mL.

the  exponential  distribution

Distribution Functions of Two Random
Variables and Statistical
Independence

If there are two random variables, x and y, then
the  distribution  function  for  observing  both  is
symbolized  as  f(x,y).  Following  the  same  logic
used  for  independent  events,  two  random  vari-
ables, x and y, are statistically independent of one
another if and only if:

f(x, y)

=

f1(x)

×

f2(y),

where f1(x) is the distribution function for x and
f2(y) is for y.

Independent Samples

If we observe a random variable, x, on each patient
in a study of n patients, we can symbolize the entire
sample as x1, x2, x3, … xn. For such a sample to be

a random sample, the joint probability  distribution
of observing x1, x2, x3, …, xn should be given as:

(
f x , x , x ,
3

1

2

)… = ∏

x

n

fi(xi).

(Here, Õ is a symbol for product, and the index i
goes  from  1  to  n.)  In  other  words,  to  comprise  a
random  sample,  the  joint  distribution  function
should equal the product of all the individual distri-
bution functions for the individual patients. In short,
the  patients  and  their  random  variables  should  be
statistically independent from one another.

Statistics

Statistics  are  numerical  summarizing  measures
of  random  variables  taken  from  a  (usually  ran-
dom)  sample  as  described  above.  Examples  of
statistics  include  the  mean,  median,  variance,
standard deviation, the t statistic, the F statistic,
and the chi-square statistic.

Statistical Hypothesis Testing

Statistical testing most commonly involves making
what is called the null hypothesis. For example, if
we have observed two random variables, x and y,
and if we suspect that these two are related to one
another,  then  we  begin  the  process  with  the  null
hypothesis that there is no such relationship. Having
made the null hypothesis, we then apply a statisti-
cal test or model to the data and calculate a statistic,
s. Under conditions of the test or model, we obtain
the probability of observing s if the null hypothesis
is true. This probability is called the p value. If the
p value is low (typically less than 0.05 or 0.01), we
conclude  that  the  null  hypothesis  is  unlikely  and
reject it. In other words, we accept the alternative
hypothesis,  which  in  the  above  example  is  that  x
and y are related to one another. If there are multi-
ple random variables involved in the study, then a
multivariable model and analysis may yield multi-
ple statistics and multiple p values.

The p value is also known as the probability
of  making  a  Type  I  error,  which  is  defined  as
the error of rejecting a null hypothesis that is in
fact  true.  Thus,  many  researchers  choose  low

46

R.T. Vollmer

 thresholds for the p value, such as 0.01, in order
to make their Type I errors unlikely.

Type II Errors, Statistical Power,
and Sample Sizes

If there is a Type I error, then there must be a Type
II error, and it is defined as the error of rejecting
the  null  hypothesis  when  in  fact  it  is  true.  The
probability of making a Type II error is often sym-
bolized as b. Naturally, researchers desire to make
b small. Statistical power equals 1 − b, but is often
difficult to calculate. Researchers minimize b by
choosing statistical tests that are naturally power-
ful  and  by  increasing  the  number  of  cases  or
patients that they are studying, because the larger
the sample size, the smaller will be the b. In gen-
eral, numbers of cases or patients less than 100 are
sufficient  for  exploratory  analyses,  but  usually
numbers in the 100s will be required for definitive

Table 4.2  Features of commonly used statistical tests

Test
Chi-square

Fisher exact

t test

Wilcoxon

One-way AOV

Kruskal–Wallis

Two-way AOV

Linear regression

Logistic regression

Log-rank

Cox model

Application
Test for effects of two categorical
variables on one another
Test for effects of two categorical
variables on one another
Comparison of means of a
continuous variable between two
groups
Nonparametric comparison of
means of a continuous variable
between two groups
Test for effects of one categorical
variable on the mean of a
continuous variable
Nonparametric test for effects of
one categorical variable on the
mean of a continuous variable
Test for effects of two categorical
variables on the mean of a
continuous variable
Test for effects of one or more
explanatory variables on a
continuous-dependent variable
Test for effects of one or more
explanatory variables on a
binary-dependent variable
Test for effects of a categorical
variable on survival time
Test for effects of one or more
explanatory variables on survival
time

results. Numbers in the 1,000s will allow statisti-
cal models to include multiple important random
variables, and such models, if validated with new
data, may then provide prognostic algorithms that
can be applied to new patients.

Overview of Common Statistical Tests

To a large extent, the choice of statistical test we
use for analyzing data and testing the null hypoth-
esis depends upon the nature of the random vari-
ables  in  the  data.  For  example,  if  there  are  two
random variables and both are binary (i.e. they have
just two values), then the chi-square or Fisher tests
could be used. If there are two  random  variables, x
and y, and if y is a dependent continuous  variable
and x is a categorical one, then the t test or one-way
analysis of variance (AOV) would be appropriate
so long as y was approximately normally distrib-
uted. If y were not normal, then nonparametric tests
like the Wilcoxon or Kruskal–Wallis tests could be
used. If y is a dependent continuous random vari-
able and there are several continuous or categorical
explanatory variables x1, x2, x3, etc., then regression
analysis is appropriate so long as the residual error
measurements are approximately normally distrib-
uted. If y is a binary-dependent variable and there
are  several  continuous  or  categorical  explanatory
variables x1, x2, x3, etc., then logistic regression is
appropriate. If y is a failure time and there are sev-
eral continuous or categorical explanatory variables
x1,  x2,  x3,  etc.,  then  the  Cox  proportional  hazard
model  is  appropriate.  Table  4.2  summarizes  fea-
tures for commonly used statistical tests.

Chi-Square Tests

The chi-square test has been the workhorse of med-
ical statistics for decades. It most often deals with
two binary random variables, x and y. The data are
typically presented in a 2 × 2 table as follows:

x
Negative
a
c

Positive
b
d

y

Negative
Positive

Here, a is the count of patients negative for
both  x  and  y,  b  the  count  of  those  positive  for

4  Biostatistics 101

47

x and negative for y, c the count of those  negative
for x and positive for y, and d the count of those
positive  for  both  x  and  y.  The  null  hypothesis
for this test assumes that y and x are statistically
independent, that is, that P(y | x) = P(y) and P(x
| y) = P(x). Then the software estimates the prob-
abilities P(y) and P(x) from the data and  without
regard to each other. Next, the test compares the

number of observed results for each category of
y  and  x  with  that  expected  from  the  pooled
 estimates of P(y) and P(x). Specifically, it forms
the  squared  differences
ratios  comprising
between observed and expected numbers divided
by  the  expected  number  and  sums  these  over
the  cells  in  the  table  to  yield  a  statistic  s  as
follows:

s

= ∑

(no. observed

-

2
 no. expected) / no. expected.

Because under the null hypothesis, this s follows
the chi-square distribution, the test is called the chi-
square test, and it can also be used for categorical
random  variables  with  more  than  two  results.  If
there are r categories or possible values of y and c or
possible categories for x, then the total number of
possible  categories  using  both  variables  is  r × c;
however, the numbers of observations in each com-
bined category or cell should exceed 5. The product
(r − 1) × (c − 1)  is  called  the  degrees  of  freedom.
When the estimated chi-square is sufficiently large,
then the deviations of observed from expected num-
bers are high, and the null hypothesis is rejected.

When  the  counts  of  cases  in  the  cells  of  the
table are smaller than 5, then the statistic does not
follow  a  chi-square  distribution,  and  one  must
use  an  alternative  test  such  as  the  Fisher  exact
test, which relies on the geometric distribution.

Sometimes  the  categorical  observations  of  y
and  x  variables  are  paired.  This  could  happen
when  one  evaluates  two  immunohistochemical
stains  on  a  set  of  tumors,  one  tumor  from  each
patient.  In  such  a  study,  the  routine  chi-square
test would be inappropriate and one must use the
McNemar variant of the chi-square test. Its chi-
square statistic relies on just the discordant results
for each pair of staining results.

Another variant of the chi-square test applies
when  one  questions  whether  the  proportions  of
cases with a key result are the same across  several
studies.  This  issue  commonly  arises  in  meta-
analyses. Before studies can be combined to pro-
duce an overall result, one must usually test if the
studies  are  homogeneous  in  their  design  and  in
the  way  they  recruited  patients.  For  example,
Table 4.3 lists observed probabilities that patients
had  cancer  of  the  prostate  given  a  PSA  value
³4  ng/mL.  The  data  come  from  four  different

Table 4.3  Observed probabilities that patients had cancer
of the prostate given a PSA value ³4 ng/mL

Study
Babaian
Catalona
Brawer
Morgan

n

404
750
227
5258

ppv
0.45
0.34
0.34
0.76

The data come from four different studies (Babaian et al.
[4] Catalona et al. [2]; Brawer et al. [1]; and Morgan et al.
[3]). The probability is listed as ppv, and the total number
of patients is listed as n

studies  [1–4].  The  probability  is  listed  as  ppv,
and the total number of patients is listed as n.

Whereas  the  values  of  ppv  in  the  first  three
studies  appear  reasonably  close  to  one  another,
the  value  of  0.76  in  the  last  study  is  approxi-
mately  twice  as  high.  Are  these  results  signifi-
cantly  different  from  one  another?  The  test  of
equality  of  proportions  can  provide  an  answer.
First, a weighted estimate of the overall propor-
tion  positive  is  calculated.  Then  this  estimate
with  its  derived  variance  is  used  to  once  again
calculate  the  difference  between  observed  and
expected values as above. The result gives another
statistic with a chi-square distribution. In S-PLUS,
this test is done with the call to prop.test, and for
these data, it yielded a chi-square statistic of 358
(p ~ 0)  for  the  null  hypothesis  that  the  observed
proportions  were  the  same.  Thus,  we  can  con-
clude  that  there  were  significant  differences
between these four studies, and in fact, the design
for the first three differed from that of the fourth.
Whereas  the  first  three  assayed  serum  PSA  in
men all of whom underwent biopsy of the pros-
tate, the fourth collected PSA data from two pop-
ulations, one selected because they had a positive
biopsy  for  prostate  cancer  and  a  control  group
that  included  many  who  did  not  have  biopsies

48

R.T. Vollmer

done. Because the control groups in the first three
studies  included  many  men  with  BPH  or  other
conditions  which  required  biopsy,  they  also
included  many  with  elevated  PSA  but  without
cancer, and this had the effect of lowering the ppv
in comparison with the fourth study.

Finally, the Mantel–Haenszel chi-square test is
done to test for statistical independence between y
and  x  when  there  is  a  third  confounding  variable
present. The third variable could be the presence of
another  disease,  a  drug,  or  that  the  observations
came from different institutions or overall catego-
ries. For example, Morgan et al. published the fre-
quencies of patients with prostate cancer (y variable)
versus PSA levels >= 4 ng/mL (x variable), strati-
fied  by  eight  categories  of  age  and  race  [3].
A Mantel–Haenszel test for the relationship between
presence of cancer and PSA, while controlling for
these eight categories, yielded a chi-square value of
2,519 and a p value of approximately 0, thus allow-
ing one to reject the null hypothesis of no associa-
tion between PSA and presence of cancer.

t Test

The t test is another long-used workhorse in statis-
tical analysis of medical data. Although its popu-
larity is now less than that for tests that can deal
with multiple random variables, it continues to be
used and has proven useful as a screening device
for proteomic data. The t test is most commonly
used to see if the means of a continuous random
variable, y, are the same in two separate popula-
tions, and it requires that y be normally distributed
and that its variance is the same in the two popula-
tions. Consider the following example.

In  2002,  Petricoin  et  al.  published  a  SELDI-
TOF  analysis  of  the  serum  proteome  on  50
women with ovarian cancer and 50 women with-
out ovarian cancer [5]. The data comprised mass
spectral patterns of intensities versus mass/charge
ratio (M/Z). Figure 4.4 plot shows a portion of the
spectrum  with  the  mean  intensities  for  the  two
groups of women (two lines on the plot).

Where  the  lines  separate,  the  higher  line
shows the means for women with ovarian cancer.
The question is whether these sites of separation
are significantly so. A series of t tests was applied

y
t
i
s
n
e
t
n

i

n
a
e
M

50

40

30

20

10

0

3000

3200

3400

3600

3800

4000

M / Z

Fig.  4.4  A  portion  of  mass  spectral  patterns  of  intensities
 versus mass/charge ratio (M/Z) for two groups of women (two
lines on the plot), one with ovarian cancer (upper line) and one
without cancer (lower line) (data from Petricoin et al. [5])

to  the  data,  one  for  each  value  of  M/Z,  and  the
values for these t statistics appear in Fig. 4.5.

The graph now shows calculated values of the
t statistic versus values of M/Z, and the upper and
lower  horizontal  lines  show  thresholds  for  a  p
value  of  0.01  in  the  t  test.  Consequently,  in  the
region near M/Z values of 3,400, there were nega-
tive  t  values  of  such  a  magnitude  that  they  fell
beyond  the  p = 0.01  threshold.  This  result  then
suggested that there were serum proteins in this
M/Z  range  which  were  likely  to  differ  between
women with and without ovarian cancer.

The t test is also commonly applied to paired
observations  of  a  continuous  random  variable,
which may arise when a continuous random vari-
able is observed before and after some interven-
tion. Furthermore, even when the original random
variable  is  not  normally  distributed,  the  differ-
ence  between  the  paired  values  may  be  at  least
approximately  normal,  and  in  this  circumstance
the prerequisites for the t test are satisfied.

Parametric Tests and Normally
Distributed Random Variables

Tests designed to be used on normally distributed
random variables are often described as “ parametric”
as  opposed  to  “non-parametric”.  In  general,  these
parametric  tests  will  be  more  powerful  than  their
nonparametric  counterparts.  In  other  words,  the
parametric  tests  will  yield  lower  p  values  for  the

4  Biostatistics 101

49

5

0

–5

c
i
t
s
i
t
a
t
S
1

–10

3000

3200

3400

3600

3800

4000

M/Z

Fig. 4.5  Plot of t statistics versus M/ Z for the portion of
the M/ Z spectrum in Fig. 4.4. The t statistic evaluates the
difference  in  mean  intensities  between  women  with
ovarian cancer and those without cancer, and this is done

for each value of M/ Z in the spectrum. The upper and lower
lines on the plot show where values of t indicate significant
differences  in  means  for  the  two  groups  of  women  at  a
p value of 0.01

800

600

400

200

0

0

400

300

200

100

5

10
Thickness, mm

15

0

–2

20

–1

0

1

Log (thickness)

2

3

Fig.  4.6  Frequency  distribution  of  tumor  thickness  in
malignant melanoma (the author’s data)

Fig. 4.7  Frequency distribution of natural logarithm (Log)
of tumor thickness in malignant melanoma (the author’s data)

null hypotheses and will require fewer data to do so.
Nevertheless,  before  using  parametric  tests,  one
should at least attempt to see if the random variables
or their residual errors are normally distributed. For
a given continuous random variable, the easiest way
to do this is to plot its frequency distribution, see if
it is symmetric (versus skewed) with the peak in the
middle of the range, and to see that it is neither too
flat  nor  too  narrow.  If  the  frequency  distribution
does not appear normal, then some transformation
of the variable, such as the  logarithm or square root,
may be normally distributed. In that case, the para-
metric  tests  can  still  be  applied.  For  example,  the
Breslow thickness in over 1,000 cases of cutaneous
melanoma has the following approximately skewed
and exponential frequency distribution (Fig. 4.6).

Yet,  using  the  natural  logarithm  converts
tumor  thickness  to  an  approximately  normally
distributed as seen in Fig. 4.7.

In some tests like AOV and regression analy-
ses,  it  is  more  important  that  the  residual  error
values are normally distributed than to have the
original dependent random variables be normal.
Finally,  in  general,  regression  analyses  do  not
require that the explanatory variables be normal.

One-Way Analysis of Variance

AOV provides a way to see if the means of a nor-
mally distributed random variable y are the same
across several levels of a categorical variable x.

50

400

300

200

100

0

–2

–1

0
Residual error

1

2

3

Fig. 4.8  Frequency  distribution  of  residual  error  values
from the AOV of logarithm of tumor thickness according
to Clark levels in malignant melanoma (the author’s data)

For example, common familiarity with  cutaneous
melanoma suggests that the thickness should be
positively  related  to  Clark  levels  2–5.  Exami-
nation of the cases used in the above frequency
distributions showed that the mean thickness for
levels  2–5  were  respectively  0.46,  1.65,  1.94,
2.50,  and  6.29  mm.  AOV  of  the  logarithm  of
thickness demonstrated that this association was
significant  (F  statistic = 784,  p ~ 0).  Justifying
use  of  the  AOV  model,  the  frequency  distribu-
tion of the residual errors from the AOV showed
a close approximation to normality as shown in
Fig. 4.8:

Wilcoxon and Kruskal–Wallis Tests

The Wilcoxon test is the nonparametric counter-
part to the t test. In other words, it is appropriate
for the null hypothesis that a continuous variable,
y, is the same for two groups of patients, and it
can deal with paired or nonpaired data. Its analy-
sis and results are based on ranks of y rather than
the values of y directly, and it does not require y

R.T. Vollmer

to be normally distributed. This test is equivalent
to  the  Mann–Whitley  test  based  on  the  calcula-
tion of a Uy2 statistic, which provides the num-
ber of times y is larger in one group than in the
second.

The Kruskal–Wallis test is the nonparametric
counterpart  to  one-way  AOV  and  also  does  not
require that the random variable or the residuals
be normally distributed. This test is for the null
hypothesis that the values of a continuous y vari-
able are the same for categories of an x variable.
Like  the  Wilcoxon  test,  the  Kruskal–Wallis  test
orders the values of y along a single virtual row
and then sums the ranks for each category of x. It
then computes an H statistic based on the sum of
squared  values  of  these  ranks  divided  by  the
number of patients in each x category. If k is the
number of categories of x, then H has an approxi-
mate chi-square distribution with k-1 degrees of
freedom,  so  that  the  final  test  statistic  is  a  chi-
square. In the melanoma data used above for the
AOV, the Kruskal–Wallis test yielded a chi-square
value of 434 and a p value of ~0.

Regression Analyses

Many  statistical  studies  in  pathology  and  medi-
cine  deal  with  a  response  random  variable,  y,
which is to be related to explanatory random vari-
ables, x1, x2, …, xn. This is the domain of regres-
sion analyses. The y variable is the dependent one,
and the x variables are usually called the indepen-
dent  variables,  explanatory  variables,  or  covari-
ates.  Examples  of  regression  analyses  include
linear  regression,  general  linear  model  analysis,
logistic regression analysis, and the Cox propor-
tional  hazard  model  for  survival  time.  In  these
examples, some function of y, f(y), is related to a
linear combination of the x variables as follows:

f(y)

=

b

0

+

b
1

×

x

1

+

b

2

×

x

2

+

b

3

×

x

3

+ …+

b

n

×

x

n

+

error.

Here, the b0 is an intercept, and the b1, b2, … ,
bn  are  coefficients  to  be  multiplied  times  their
respective  x  variables.  The  x  variables  can  be

binary  (e.g.,  0  vs.  1),  categorical  like  Gleason
grade,  or  continuous;  and  if  continuous,  they
need not be normally distributed.

4  Biostatistics 101

51

In  linear  regression,  even  some  degree  of
 nonlinearity can be accommodated. For example,
one  can  use  interaction  terms  that  combine  the
effects of two or more explanatory variables. For
example, adding a variable x4 equal to the prod-
uct x2 × x3 would make it an interaction variable.
Such  an  interaction  might  apply  when  f(y)
increases with positive x2, increases with x3, but
does  not increase as much when both  x2 and  x3
are positive. In this example, coefficients b2 and
b3 would be positive, but the coefficient b4 for the
interaction variable x4 would be negative. Second
and  third  powers  of  explanatory  variables  can
also be used to accommodate nonlinearity in the
relationship  between  y  and  the  explanatory
variables.

Linear Regression Analysis

In  linear  regression,  the  dependent  y  variable  is
continuous  and  may  be  used  as  it  is  or  trans-
formed but it does not need to be normally dis-
tributed.  By  contrast,  the  error  term  must  be
approximately  normally  distributed,  and  in  this
circumstance, linear regression results in t statis-
tics for the null hypotheses that the b coefficients
in the regression equation are 0. Large values of
the t statistics imply low p values, and then allow

e
t
a
r

c
i
t
o
t
i

m

f
o
t
o
o
r

e
r
a
u
q
S

10

8

6

4

2

0

–1

0

1

2

3

Log of thickness

Fig. 4.9  Plot of square root of mitotic rate (number per
square millimeter) versus logarithm of tumor thickness in
malignant melanoma (the author’s data)

us to reject the null hypothesis of no association
between  y  and  the  respective  x  variable.  For
example,  consider
the  relationship  between
mitotic  rate  in  cutaneous  melanoma  and  tumor
thickness.  Mitotic  rate  in  melanoma  is  usually
expressed as number per square millimeter, and
thickness as millimeters. A plot of square root of
mitotic rate versus log(thickness) for over 1,000
patients with melanoma appears in Fig. 4.9, and
in spite of scatter in the data, there is a hint of a
positive relationship.

Linear regression analysis of this data uses the

following equation:

Sqrt(mitoses)

=

b

0

+

b
1

×

log(thickness) b

+

2

×

ulcer

+

error,

where Sqrt symbolizes the square root transfor-
mation,  log  is  the  natural  logarithm,  and  ulcer
takes the values 0 or 1. Table 4.4 shows the results
of the linear regression.

Table 4.4  Linear regression of square root of mitotic rate
in melanoma

Variable
Intercept
log(thickness)
Ulceration

Coefficient
0.989
0.568
0.718

t
24.3
12.0
  8.9

p Value
~0
~0
~0

These  results,  including  the  high  values  of  t
and low p values, demonstrate that mitotic rate is
not independent of either thickness or ulceration.
Examination  of  the  residuals  from  the  analysis
shows  an  approximately  normal  distribution  as
shown in Fig. 4.10.

(A histogram of the error residuals should be
routinely  examined  to  see  if  the  assumption  of
normally  distributed  residuals  is  justified.)  The
breadth  of  the  residual  histogram  hints  that  the
linear model explained a fraction of the scatter
in  the  former  plot,  and  the  R2  value  from  the

52

500

400

300

200

100

0

–2

0

2
Error

4

6

Fig. 4.10  Frequency distribution of residual error values
from  the  linear  regression  analysis  of  how  mitotic  rate
depends upon tumor thickness and ulceration in malignant
melanoma (the author’s data)

R.T. Vollmer

 regression analysis tells us more specifically how
much of the variance in the data was explained by
the regression model. For this data, R2 was 0.23
indicating that the model explained just 23% of
the variance in the data.

General Linear Model (GLM)

The  GLM  is  analogous  to  the  linear  regression
model except that GLM does not assume that the
residuals are normally distributed. Nevertheless,
GLM does assume that the variance of the depen-
dent  random  variable,  y,  is  constant.  The  GLM
has the same linear form as in linear regression:

f(y)

=

b

0

+

b
1

×

x

1

+

b

2

×

x

2

+

b

3

×

x

3

+…+

b

n

×

x

n

+

error.

The  y  variables  may  be  either  continuous  or
categorical. Instead of obtaining a least squares fit
to the collected data of {y, x1, x2, … } as done in
linear  regression,  the  GLM  obtains  estimates  of
the b coefficients to maximize a likelihood func-
tion, L, or its natural logarithm ln(L). The exact
form  of  L  depends  on  the  nature  of  f(y).  GLM
obtains its solutions for the b coefficients through
an iterative fitting procedure. As in linear regres-
sion the null hypothesis is that the values for the b
coefficients are 0, and when this is the case, a like-
lihood ratio statistic has a chi-square distribution
with the number of degrees of freedom equal to
the number of x variables used in the model. The
result is termed the “likelihood ratio test” for test-
ing the significance of one or more of the x vari-

ables. The next model to be discussed, the logistic
model, provides an example. Others can be found
in the McCullach and Nelder text [6].

The Logistic Regression Model

The logistic regression model deals with a depen-
dent random variable, y, which is binary, that is,
either 0 or 1. In other words, logistic regression is
appropriate when we want to know which x vari-
ables increase, or decrease, the chance of a diag-
nosis or an important clinical outcome. In logistic
regression, the transformation of y, which is con-
sidered linear, is the natural logarithm of the odds
as follows:

f(y)

=

log{odds (y

=

1)}

=

b

0

+

b
1

×

x

1

+

b

2

×

x

2

+

b

3

×

x

3

+ …+

b

n

×

x .
n

Because the odds (y = 1) is defined in terms of

probability P(y = 1) as:

Odds (y

=

1) P / (1 P),

=

−

the probability P can also be written as:

P(y 1) 1 / {1 exp( E)}

=

+

−

=

with E given as:

E b
=

0

+

b
1

×

x

1

+

b

2

×

x

2

+

b

3

×

x

3

+…+

b

n

×

x .
n

Once again the null hypothesis is that the coef-
ficients  b  are  equal  to  0.  Peduzzi  et  al.  suggest
that  for  the  logistic  analysis  to  produce  reliable
results, the data should include at least ten events
for  each  x  variable  with  event  being  defined  as
the smaller number of those with either y = 1 or
y = 0  [7].  Because  of  the  importance  of  this

4  Biostatistics 101

53

multivariable  logistic  model,  three  examples  of
its use on real data follow.

Logistic Regression Analysis of HPV
DNA Testing in Women with ASCUS

Recently,  Siddiqi  et  al.  examined  the  results  of
hybrid capture two human papillomavirus DNA
testing  (HC2)  in  8,195  women  with  atypical
squamous  cells  of  undetermined  significance
(ASCUS)  in  their  liquid-based  cervical  sam-
ples.[8] The authors used the SurePath technique
for 4,235 specimens and the ThinPrep technique
for 3,960 specimens, and one of the goals of their
study was to see if the technique affected a posi-
tive HC2 test. They stratified the women accord-
ing to six age groups and demonstrated that age
affected  the  probability  of  a  positive  HC2  test.
Then they ran six chi-square tests – one for each
age group – to see if the technique affected the
probability of a positive HC2 test within the age
groups.  They  found  that  only  in  the  group  of
women under 19 years of age was the HC2 test
dependent on the technique (the ThinPrep tech-
nique yielded more positive HC2 results).

The  data  from  this  study  comprise  a  single
binary-dependent variable – a positive HC2 test –
and 2 explanatory variables: age of the patient and
the  technique  for  the  liquid-based  PAP  process-
ing.  Consequently,  the  logistic  regression  model
is  ideal  for  this  three  variable  data  and  has  the
advantage of analyzing all the data without break-
ing  it  into  subsets  or  relying  on  multiple  chi-
square tests and multiple p values. Furthermore,
when there is one variable that strongly affects the
outcome – in this case age – it is important to con-
trol for its effect while analyzing the effect of the
variable of interest – in this case, the liquid-based
technique.  Consider,  for  example,  the  following
plot  of  the  probability  of  a  positive  HC2  versus
median patient age in authors’ data (Fig. 4.11).

The  smooth  line  shows  the  relationship  and
demonstrates  that  the  probability  of  a  positive
HC2  decreases  smoothly  with  increase  in  age.
This plot also suggests that age should be used as
a  continuous  variable,  rather  than  categorized
into six groups. Logistic regression analysis can

2
C
H
e
v

i
t
i

s
o
p
f
o
y
t
i
l
i

b
a
b
o
r
P

1.0

0.8

0.6

0.4

0.2

0

20

30

40

50

Patient age

Fig.  4.11  A  plot  of  the  probability  of  a  positive  hybrid
 capture 2 human papillomavirus DNA testing (HC2) versus
median patient age in the data reported by Siddiqi et al. [8]

deal with continuous variables like age, and for
this data, the logistic regression analysis yielded
the following results.

Variable
Age
ThinPrep technique

Coefficient
−0.0707
    0.0548

p Value
~0
0.03

The  negative  coefficient  of  −0.0707  for  age
demonstrates  that  increased  age  was  associated
with decreased probability of a positive HC2, as
the above plot shows, and the p value of approxi-
mately zero indicates that age was strongly asso-
ciated with the HC2 result. The positive coefficient
of  0.0548  indicates  that  after  controlling  for  the
age  effect,  the  ThinPrep  technique  resulted  in
more  positive  HC2  tests  than  did  the  SurePath
technique, and that this difference was significant
at a p value of 0.03. Thus, unlike the multiple chi-
square tests done by the authors, the multivariable
logistic model was able to demonstrate a signifi-
cant overall effect of technique on the probability
of a positive HC2 test.

Logistic Regression Analysis
of Antiphospholipid Antibodies
in Acute Coronary Artery Syndrome

To further illustrate the logistic regression model,
consider the data published by Greco et al. regard-
ing the importance of antiphospholipid antibodies

54

R.T. Vollmer

(aPL’s) in patients with acute coronary artery syn-
drome  [9].  They  studied  334  patients  who  pre-
sented to their acute care facility with chest pain
and  suspected  coronary  artery  syndromes.  They
categorized  coronary  artery  disease  (CAD)  into
six grades of increasing severity based on cathe-
terization  data,  and  they  recorded  subsequent
adverse  outcomes,  including  adverse  vascular
events and deaths. In their results, they used pair-
wise  statistical  tests  to  demonstrate  that  aPL’s
were  associated  with  severity  of  CAD  and  that
adverse  outcomes  were  associated  with  aPL’s,
with severity of CAD, and with aPL’s within some
categories of CAD. But logistic regression analy-
sis offers the advantage of one statistical analysis
of  all  the  data  to  see  how  the  binary  event  of
adverse outcome depends on both aPL’s as well as
CAD grade. For the analysis, severity of CAD was
collapsed into 4 levels of a single variable coded
(0–III,  IV,  V  and  VI),  because  just  one  adverse
event  occurred  in  the  0–III  group.  Presence  of
aPL was coded as absent (0) versus positive (1).
The results appear in the following table.

Variable
CAD
aPL

Coefficient
1.03
1.3

p Value
5.2 × 10 − 8
6.9 × 10 − 4

The  very  low  p  values  demonstrate  first  that
these  two,  related  variables  can  provide  additive
information  about  the  probability  of  an  adverse
outcome. After controlling for the information that
CAD provides, the logistic model results demon-
strate that aPL’s provide additional helpful infor-
mation. The positive coefficients demonstrate that
both  CAD  as  well  as  presence  of  aPL’s  imply
increased probability of an adverse outcome.

Finally, the coefficients of the logistic regres-
sion can be used to form a predictive model to be
used for new patients as follows. Using the mod-
el’s intercept value, which was found to be −4.04,
E can be calculated as

E

= −

4.04 1.03 CAD 1.30 aPL

×

+

×

+

and the probability P of an adverse outcome for a
new patient’s values of CAD and aPL can then be
estimated as:

P(adverse event)

=

1
1 exp( E)

+

−

(The intercept value may need to be adjusted
to reflect the local prevalence of adverse events.)

Logistic Regression Analysis of Atypical
Epithelium in the Prostate

A  third  example  of  logistic  regression  analysis
comes from studies of atypical small glands (ASAP)
and  high-grade  prostatic  intraepithelial  neoplasia
(HGPIN) in needle biopsies of the prostate. In 2005,
Schlesinger  et  al.  published  their  experience  with
336  men  who  had  either  HGPIN  or  ASAP  in  an
initial set of biopsies of the prostate and who subse-
quently  had  follow-up  biopsies  [10].  Importantly,
there was not a  control group of men with follow-
up biopsies, but who had neither HGPIN nor ASAP.
The question to consider is whether HGPIN adds
information to the presence of ASAP regarding the
outcome  of  cancer  in  the  follow-up  biopsies.
A  logistic  regression  analysis  on  their  published
data yielded the following results:

Variable
ASAP
HGPIN

Coefficient
    0.512
−0.16

p Value
0.012
0.65

The results suggest that in this restricted situ-
ation where all men had either HGPIN or ASAP,
the presence of ASAP was associated with cancer
in the follow-up biopsy, but HGPIN was not.

In their publication, Schlesinger et al. summa-
rized prior studies of HGPIN and ASAP, and in
their summary, they demonstrated that the prob-
ability of cancer in the follow-up biopsy decreased
with  the  time  of  the  study  [10].  The  following
plot  shows  the  fraction  of  positive  follow-up
biopsies  on  the  vertical  axis  versus  the  median
study year on the horizontal axis, and the line for
the trend in the data demonstrates that the proba-
bility  of  a  positive  follow-up  biopsy  decreased
with time of the study (Fig. 4.12).

Thus,  in  the  overall  analysis,  three  variables
seemed to be important: ASAP, HGPIN, and the
time  of  the  study  cases.  Because  the  logistic
 regression  model  can  easily  accommodate
 continuous  variables  such  as  time  of  study  and
easily accommodate three explanatory variables,
I applied it to this summarizing data. (To do this,
one must form a composite response variable that

4  Biostatistics 101

55

i

y
s
p
o
b
d
n
2
n

i

r
e
c
n
a
c
h
t
i

w
n
o
i
t
c
a
r
F

0.6

0.5

0.4

0.3

0.2

0.1

0

1990

1992

1994
1996
1998
Median year of cases

2000

2002

Fig. 4.12  Plot of the probability of prostate cancer in a
second, follow-up biopsy of the prostate versus the median
year  of  cases  in  studies  summarized  in  the  study  by
Schlesinger et al. [10]

combines the number of cases with cancer in the
follow-up biopsy and the number of cases without
cancer.) This analysis yielded the following results:

Variable
Median year
ASAP
HGPIN

Coefficient
−0.0639
    0.726
    0.0577

p Value
~0
~0
0.77

Once  again,  logistic  regression  demonstrated
that after controlling for the important variables of
year of study and presence of ASAP, HGPIN was
not  related  to  a  positive  follow-up  biopsy.  This
example also demonstrates how helpful the logistic
model can be in meta-analyses of prior studies.

Introduction to Survival Analysis

Whereas  logistic  regression  deals  with  a  binary
outcome,  survival  analysis  deals  with  two  out-
comes: a binary failure event like death and the
time to the occurrence of that event. Survival data
thus comprise the following categories:
Failure event: 1 if it occurred at the last observed

time, 0 if it had not
Time of last observation: T
Explanatory regression variables: x1, x2, …, xn

The  most  commonly  studied  failure  event  in
medicine is death, but other binary failure events
can be analyzed, such as tumor recurrence, metas-
tasis, and diagnosis of malignancy. Furthermore,
the failure events need not be what we normally

perceive as failures. For example, the event could
be the achieving of a cure, the ending of symp-
toms, or the return to normal levels of some labo-
ratory test. Similarly, the time variable need not
be time. Other positive, continuous variables can
be used such as the value of serum PSA.

If  the  patient  has  failed  by  the  last  observed
time,  then  the  value  of  the  event  is  1,  and  the
patient is said to be uncensored. If the patient has
not failed at the last time, then the value of event
is 0, and the patient is said to be censored at the
last  time.  One  of  the  great  strengths  of  survival
analysis  is  its  ability  to  deal  with  censored
patients,  but  there  is  a  cost.  In  general,  most  of
the  results  come  from  the  uncensored  patients.
Data rich in censored patients provide few  helpful
results,  and  Concato  and  Peduzzi  et  al.  suggest
that  there  should  be  at  least  ten  uncensored
patients for every explanatory variable [7].

The Survival Plot

Survival probability S(t) is defined as the probabil-
ity that survival time exceeds t. The most common
way  to  illustrate  S(t)  is  the  Kaplan–Meier  plot,
which plots S(t) on the vertical axis versus time on
the  horizontal  axis.  For  each  time,  the  Kaplan–
Meier method considers the number of persons at
risk and the number of persons who fail. Times of
observed failures cause vertical drops in the plot,
and  times  when  patients  are  censored  are  often
illustrated with short vertical lines. As an example,
consider two studies of pleomorphic liposarcoma
published by Gebhard et al. and by Hornick et al.
[11, 12]. Altogether, these two studies comprised
98 patients with follow-up. Forty were observed to
die (uncensored), and 58 were living at last follow-
up  (censored).  The  Kaplan–Meier  plot  of  all  98
appears as seen in Fig. 4.13.

The short vertical bars mark the times of last
observation for the 58 censored patients, and the
stair-step  drops  in  the  curve  mark  the  times  of
death  for  the  40  uncensored  patients.  The  faint
lines above and below the curve indicate the esti-
mates of 95% confidence limits.

As  time  t  increases  in  Kaplan–Meier  plots,
there are fewer patients available for the analysis,
because most have been either censored or died.

56

a

)
t
(

S

1.0

0.8

0.6

0.4

0.2

0

0

50

100

150
Time, mo

200

250

b

)
t
(

S

1.0

0.8

0.6

0.4

0.2

0

0

R.T. Vollmer

Study 1
Study 2

50

100

150
Time, mo

200

250

Fig. 4.13 ( a,  b)  Plot  of  probability  of  survival  versus  time  of  follow-up  in  patients  with  pleomorphic  liposarcoma
reported by Gebhard et al. (study 1) and by Hornick et al. (study 2) [13, 14]

For example, in these two studies, less than 25%
of  the  patients  were  observed  past  64  months.
What limited follow-up times does is to decrease
the denominator of patients at risk for later times.
Consequently,  any  deaths  at  these  times  cause
steep  drops  in  S(t).  This  is  also  why  the  95%
 confidence lines widen.

The Log-Rank Test for Equality
of Survival Plots

In  the  above  example  of  pleomorphic  liposar-
coma,  48  patients  were  studied  in  France  (the
Gebhard et al. study) [11], and the remaining 50
were  studied in either England or the USA (the
Hornick et al. study) [12]. Before combining data
from both studies, one needs to test to see if the
study affected survival. For example, study biases
of potential importance could include how differ-
ent pathologists in different countries defined and
graded liposarcomas. The Kaplan–Meier plot can
help by displaying survival curves for each study
on the same graph as Fig. 4.13b: This plot shows
that  the  survival  curves  for  the  two  studies  are
quite close.

To  statistically  test  the  null  hypothesis  that
there  is  no  difference  between  these  survival
curves, we use the log-rank test, which is based on
comparisons of observed versus expected deaths
at  the  various  times  for  the  two  studies.  The

1.0

0.8

0.6

0.4

0.2

)
t
(

S

0

0

Superficial
Muscle
Viscera

50

100

150
Time, mo

200

250

Fig. 4.14  Plot of probability of survival versus time of fol-
low-up in patients with pleomorphic liposarcoma reported
by Gebhard et al. and by Hornick et al. with survival broken
into  groups  according  to  the  level  of  tissue  involved  by
tumor [13, 14]

expected deaths are formed by assuming there is
no  difference  between  the  studies,  so  that  their
results can be combined into a multinomial table.
Then,  comparisons  of  observed  versus  expected
numbers of deaths yield a chi-square statistic. For
these two studies, the log-rank test yielded a chi-
square value of 0.7 and a p value of 0.4 suggesting
that the null hypothesis of no difference is true.

Both studies of pleomorphic liposarcoma also
classified the tumors into three levels: superficial
(skin or subcutaneous), deep skeletal muscle, or
internal viscera. The Fig. 4.14 survival plot dem-
onstrates how these levels affected survival.

ER positive
ER negative

4  Biostatistics 101

y
t
i
l
i

b
a
b
o
r
p

l

a
v

i

v
r
u
S

1.0

0.8

0.6

0.4

0.2

0

0

5

10

15

20

25

Years

Fig.  4.15  Plot  of  probability  of  survival  versus  time  of
follow-up in patients with invasive ductal carcinoma of the
breast and broken into estrogen receptor (ER) positive and
negative status. The plots were obtained from digitization
of the data reported by Pestalozzi et al. [14]

The plots suggest that pleomorphic liposarco-
mas  located  in  skin  and  subcutaneous  tissues
have the best prognosis, that those located in deep
viscera have the worst prognosis, and that those
located in deep skeletal muscle have an interme-
diate  prognosis.  The  way  to  test  if  these  differ-
ences in survival are significant is to once again
use the log-rank test, which yielded a chi-square
value of 8.5 (p = 0.01). Thus, the combined data
from the two studies validate the notion that the
tissue level of origin for these sarcomas affected
overall survival.

The Hazard Function

Next,  consider  the  following  survival  plots  of
women  with  invasive  ductal  carcinoma  of  the
breast  sorted  into  two  groups  according  to
estrogen  receptor (ER) status (Fig. 4.15).

The  data  come  from  Pestalozzi  et  al.’s
 collection  of  over  9,000  patients  with  invasive
ductal carcinoma of the breast [13]. Although the
two curves are close to one another and have sim-
ilar shapes, the ER-positive patients have higher
survival probabilities in the first 10 years of fol-
low-up time. The slopes of these survival curves
relate  closely  to  something  called  the  hazard
function,  h(t),  which  sometimes  is  called  the

0.10

0.08

0.06

0.04

0.02

n
o
i
t
c
n
u
f
d
r
a
z
a
H

0

0

57

ER positive
ER negative

5

10

15

20

25

Years

Fig. 4.16  Plot of hazard function versus time of follow-up
in patients with invasive ductal carcinoma of the breast and
broken into estrogen receptor (ER) positive and negative
status. The hazard functions were obtained from curve fit-
ting analysis of the survival curves in Fig. 4.15 and using
the  foregoing  equation  as  well  as  gamma  functions  to
model  the  hazard  functions.  The  accuracy  of  the  hazard
functions was then checked by showing that they regener-
ated the survival curves of Fig. 4.15 accurately

force of mortality. We define the hazard function
as follows:

h(t)

= −

d ln[s(t)] / dt,

where ln stands for the natural logarithm (ln) and
the right side of the equation is the derivative of
ln[S(t)]  with  respect  to  time.  The  minus  sign
implies that when the survival probability drops,
the  hazard  function  h(t)  is  positive.  In  other
words,  the  higher  and  more  positive  the  hazard
function  is,  the  faster  the  survival  plot  should
drop. One can see this effect if one plots the haz-
ard  functions  for  the  women  with  ER-positive
and -negative tumors as follows (Fig. 4.16):

The hazard function for ER-negative patients
is  much  higher  than  that  for  the  ER-positive
patients in the first 5 years after diagnosis. After
that  period,  the  hazard  for  ER-negative  patients
drops suggesting that if a woman with ER-negative
tumor  survives  5  years,  then  her  survival  will
improve.  For  ER-positive  tumors,  the  hazard
steadily  increases  in  the  first  5  years  and  then
becomes  nearly  stable.  In  this  way,  the  hazard
function tells us much about the dynamics of sur-
vival after the diagnosis of breast cancer.

58

The Cox Model

The most popular statistical model for analysis of
survival  was  introduced  by  Cox  in  1972  [14].
Since  then  it  has  increased  understanding  of
prognostic  factors  and  treatments  for  all  forms
of  cancer.  The  model  relates  survival  time  to

R.T. Vollmer

 multiple explanatory variables, symbolized once
again as x1, x2, …, xn, and its analysis deals with
ratios of hazard functions. If b1, b2, b3, …, bn are
fixed coefficients for the explanatory variables, h
the hazard function and h0 an unspecified base-
line hazard function, then the Cox model solves
the following regression equation:

log(h / h )
0

=

b
1

×

x

1

+

b

2

×

x

2

+

b

3

×

x

3

+ +

... b

n

×

x .
n

Because  the  baseline  hazard  function,  h0,  is
left unspecified, the Cox model is  semiparametric.
The  Cox  model  requires  that  hazard  function
ratios do not change with time – an assumption
that can be checked, and it obtains solutions for
b1, b2, b3, …, bn through an iterative process that
maximizes a partial likelihood function. As with
other regression analyses, the null hypothesis is
that the b coefficients are 0. In practice, the Cox
model has been found to be robust, and its itera-
tive  solution  is  completed  within  seconds  on
desk-top computers.

As  an  example,  consider  men  with  advanced
prostate cancer. Whereas most men with prostate
cancer die of other causes, a fraction have tumors
that  progress  to  eventually  become  refractory  to
hormonal therapy. At this stage, these men have
rising  values  of  serum  PSA  while  on  hormonal
treatment, and most have boney metastases. One
of the most important prognostic variables for this
group of men is their clinical performance status
(PS), which can be classified as 0 for normal, 1 for
fatigue  but  without  decrease  in  daily  activities,
and 2 for fatigue with impairment of daily activity
but with less than 50% time in bed. In a group of
575 men with hormone refractory prostate cancer
studied by the author, the performance status was
significantly associated with subsequent survival
(p ~ 0  by  log-rank  test).  However,  other  factors
such as serum PSA are important, and before test-
ing new therapies for advanced stage of prostate
cancer, it is important to control for all prognostic
variables. The Cox model is ideal for this multi-
variable analyses. In three Cancer and Leukemia
Group B (CALGB) studies, Cox model analysis
yielded the following results [15–17].

Variable
PS
Log PSA
Log
hemoglobin
Study No. 2

Coefficient(b)
   0.497
   0.0942
−1.27

Exp(b)
1.64
1.10
0.281

p Value
5.5 × 10 − 12
0.0011
0.00067

−0.191

0.826

0.036

Log indicates that serum PSA and  hemoglobin
were both transformed into natural logarithms, and
exp(b)  symbolizes  the  function  of  natural  expo-
nentiation, i.e., 2.718 raised to the exponent b. The
low p values for these four variables indicated that
each provided additive information about survival.
Furthermore,  the  lower  the  p  value,  the  more
important the variable. Thus, clinical performance
status  was  most  important,  followed  by  serum
hemoglobin, serum PSA, and finally study number
2. The positive values of the coefficients for per-
formance and serum PSA indicate that the hazard
increased  with  increased  performance  status  and
PSA, that is, survival time shortened. The negative
coefficient for hemoglobin indicates that the haz-
ard was lower with higher values of serum hemo-
globin, and survival time lengthened. The negative
coefficient for study number 2 indicates that after
controlling for the effects of performance, serum
PSA  and  serum  hemoglobin,  those  on  this  study
had a lower hazard and survived longer.

The effect of each variable on the hazard ratio
is given in the column labeled exp(b), which pro-
vides  the  hazard  ratios.  For  example,  each
increase in performance category raised the haz-
ard  by  a  multiplicative  factor  of  1.64,  and  each
unit increase in log(PSA) raised the hazard by a
multiplicative  factor  of  1.1.  By  contrast,  each
unit  increase  in  log(hemoglobin)  decreased  the

4  Biostatistics 101

59

hazard  to  approximately  0.281  of  what  it  was,
and  presence  of  a  patient  on  study  number  2
decreased the hazard to approximately 0.826 of
what it was for the other two CALGB studies.

Using the Cox Model to Form
a Hazard Score

Graphical  nomograms  and  other  prognostic
 models have become popular for several cancers

including prostate cancer. These models combine
information from several prognostic variables to
attain  a  hazard  score  (HS),  which  then  can  be
related to survival time. When these models have
been derived from a Cox model analysis, the haz-
ard score can be calculated from the coefficients
of  the  Cox  model.  For  example,  a  hazard  score
for men with hormone refractory prostate cancer
and using just PS, PSA, and hemoglobin (Hgb)
and the above Cox model results would be formed
as follows:

HS 0.497 PS 0.0942 log(PSA) 1.27 log(Hgb).
×

=

×

+

−

×

In  a  graphical  nomogram,  the  value  of  HS
 corresponds to the sum of the individual variable
scores. The final survival probability then comes
from whatever survival model and corresponding
software  is  used  to  estimate  both  the  baseline
hazard as well as the hazard ratio.

For  example,  for  men  with  hormone  refrac-
tory prostate cancer, the following plot (Fig. 4.17)
demonstrates  how  expected  survival  probability
at 2 years and 5 years depends upon the HS. (The
values of HS are less than zero, because the range
of  HS  in  the  CALGB  patients  was  from  −3  to
approximately 0.)

1.0

0.8

0.6

0.4

0.2

y
t
i
l
i

b
a
b
o
r
p

l
a
v
i
v
r
u
S

0

–4

2 years
5 years

–3

–2
H8

–1

0

Fig. 4.17  Plots of the probability of survival at 2 and 5 years
versus  hazard  score  (HS)  in  men  with  hormone  refractory
prostate cancer. The plots were obtained with use of a Weibull
parametric survival model

References

  1.  Brawer  MK,  Aramburu  EAG,  Chen  GL,  et  al.  The
inability  of  prostate  specific  antigen  to  enhance  the
predictive  value  of  prostate  specific  antigen  in
the  diagnosis  of  prostatic  carcinoma.  J  Urol.  1993;
150:369–73.

  2.  Catalona  WJ,  Hudson  MA,  Scardino  PT,  et  al.
Selection of optimal prostate specific antigen cutoffs
for early detection of prostate cancer: receiver operat-
ing characteristic curves. J Urol. 1994;152:2037–42.
  3.  Morgan TO, Jacobsen SJ, McCarthy WF, et al. Age-
specific  reference  ranges  for  serum  prostate-specific
antigen  in  Black  men.  N  Engl  J  Med.  1996;335:
304–10.

  4. Babaian RJ, Mettlin C, Kane R, et al. The relationship of
prostate-specific antigen to digital rectal examination and
transrectal ultrasonography. Cancer. 1992;69:1195–200.

  5.  Petricoin  EF,  Ardekani  AM,  Hitt  BA,  et  al.  Use  of
proteomic patterns in serum to identify ovarian can-
cer. Lancet. 2002;158:1491–502.

  6.  McCullagh P, Nelder JA. Generalized linear models.

2nd ed. London: Chapman & Hall; 1989.

  7.  Peduzzi  P,  Concato  J,  Kemper  E,  Holford  TR,
Feinstein  AR.  A  simulation  study  of  the  number  of
events  per  variable  in  logistic  regression  analysis.
J Clin Epidemiol. 1996;49:1373–9.

  8. Siddiqi A, Spataro M, McIntire H, et al. Hybrid Capture
2 human papillomavirus DNA testing for women with
atypical squamous cells of undetermined significance.
Papanicolaou results in SurePath and ThinPrep speci-
mens. Cancer Cytopathol. 2009;117:318–25.

  9.  Greco TP, Conti-Kelly AM, Creco Jr T, et al. Newer
antiphospholipid antibodies predict adverse outcomes
in patients with acute coronary syndrome. Am J Clin
Pathol. 2009;132:613–20.

 10. Schlesinger  C,  Bostwick  DG,  Iczkowski  KA.  High-
grade  prostatic  intraepithelial  neoplasia  and  atypical

60

R.T. Vollmer

small acinar proliferation. Predictive value for cancer
in  current  practice.  Am  J  Surg  Pathol.  2005;
29:1201–7.

 11. Gebhard  S,  Coindre  CJ-M,  Michels  J-J,  et  al.
Pleomorphic
clinicopathologic,
liposarcarcoma:
immunohistochemical,  and  follow-up  analysis  of  63
cases. Am J Surg Pathol. 2002;26:601–16.

 12. Hornick  JL,  Bosenbert  MW,  Mentzel  T,  et  al.
Pleomorphic liposarcoma. Clinicopathologic analysis
of 57 cases. Am J Surg Pathol. 2004;28:1257–67.
 13. Pestalozzi  BC,  Zahrieh  D,  Mallon  E,  et  al.  Distinct
clinical and prognostic features of infiltrating lobular
carcinoma  of  the  breast:  combined  results  of  15
International  Breast  Cancer  Study  Group  Clinical
Trails. J Clin Oncol. 2008;26:3006–14.

 14. Cox DR. Regression models and life tables (with dis-

cussion). J R Stat Soc B. 1972;34:187–220.

 15. D’Amico  AV,  Halabi  S,  Vollmer  R,  Loffredo  M,
McMahon E, Sanford B, et al. Cancer and Leukemia
Group  B.  p53  protein  expression  status  and  recur-
rence  in  men  treated  with  radiation  and  androgen
suppression therapy for higher-risk prostate cancer:
a prospective phase II Cancer and Leukemia Group B
Study
(CALGB  9682).  Urology.  2008;71(5):
933–7.

 16. D’Amico  AV,  Halabi  S,  Tempany  C,  Titelbaum  D,
Philips GK, Loffredo M, et al. Cancer and Leukemia
Group B. Tumor volume changes on 1.5 tesla endorec-
tal  MRI  during  neoadjuvant  androgen  suppression
therapy for higher-risk prostate cancer and recurrence
in  men  treated  using  radiation  therapy  results  of  the
phase II CALGB 9682 study. Int J Radiat Oncol Biol
Phys. 2008;71(1):9–15.

 17.  Humphrey PA, Halabi S, Picus J, Sanford B, Vogelzang
NJ, Small EJ, et al. Prognostic significance of plasma
scatter factor/hepatocyte growth factor levels in patients
with metastatic hormone – refractory prostate cancer:
results from cancer and leukemia group B 150005/9480.
Clin Genitourin Cancer. 2006;4(4):269–74.

Suggested Readings

Casella G, Berger RL. Statistical inference. 2nd ed. Pacific

Grove, CA: Duxbury; 2002.

Venables WN, Ripley BD. Modern applied statistics with
S-PLUS. 3rd ed. New York, NY: Springer; 1999.
Cox  DR,  Oakes  D.  Analysis  of  survival  data.  London:

Chapman & Hall; 1984.

Therneau  TM,  Grambsch  PM.  Modeling  survival  data.
Extending the Cox model. New York, NY: Springer; 2000.
Harrell Jr FE. Regression modeling strategies. With appli-
cations to linear models, logistic regression, and sur-
vival analysis. New York, NY: Springer; 2001.

Hosmer DW, Lemeshow S. Applied logistic regression. 2nd

ed. New York, NY: Wiley; 2000.

Hosmer  DW,  Lemeshow  S.  Applied  survival  analysis.

New York, NY: Wiley; 1999.

Miller I, Miller M, John E. Freund’s mathematical statistics
with  applications.  7th  ed.  Upper  Saddle  River,  NJ:
Pearson Prentice Hall; 2004.

Vollmer  RT,  Kantoff  PW,  Dawson  NA,  Vogelzang  NJ.
Importance of serum hemoglobin in hormone  refractory
prostate cancer. Clin Cancer Res. 2002;8:1049–53.

Prognostication and Prediction
in Anatomic Pathology: Carcinoma
of the Breast as an Illustrative Model

5

Mark R. Wick, Paul E. Swanson,
and Alberto M. Marchevsky

Keywords
Prediction in anatomic pathology • Breast cancer as model of prognostication
• Prognostication in anatomic pathology • Anatomic pathology

Along  with  other  physicians,  pathologists  have
long been interested in how to forecast the future
for patients with a variety of diseases. Common
questions  for  anyone  having  an  illness  are  the
following:
 1.  How will this problem affect my life and how

long will I live after today?

 2.  Which  treatments  could  I  receive  for  this

problem?

 3.  What  is  the  likelihood  that  therapy  will  be

effective, and at what cost?
Such queries are particularly pointed for peo-
ple with malignant neoplasms, or for clearly life-
threatening  non-neoplastic  illnesses  such  as
Wegener granulomatosis, usual interstitial pneu-
monitis, scleroderma, and others.

Intuitively,  physicians  learned  long  ago  that
marked  anatomic  or  physiological  deviations
from  the  norm  likely  indicated  a  problem  of
unusual severity, and, therefore, a more guarded
outlook for the patient. During the early part of
the twentieth century, this awareness led doctors

M.R. Wick (*)
Professor and Associate Director of Surgical Pathology,
Department of Pathology, University of Virginia Medical
School, Charlottesville, VA, USA
e-mail: MRW9C@hscmail.mcc.virginia.edu

to  develop  schemes  for  the  semiquantitation  of
adverse  risk.  Histological  grading  of  malignant
tumors  was  devised,  as  a  reflection  of  progres-
sively  increasing  visual  differences  from  the
images  of  corresponding  normal  tissues  [3–12].
The  higher  the  grade  of  a  neoplasm,  the  less  it
was  felt  to  resemble  its  non-neoplastic  counter-
part under the microscope.

The scope of tumor growth was also codified
in tumor staging systems. Even before the current
“primary  tumor-lymph  node-distant  metastasis”
(TNM) system for tumor staging was proposed in
the  mid-1940s  [13],  other  effective  paradigms
were  devised  in  reference  to  specific  malignan-
cies. For example, Dr. Cuthbert Dukes published
an effective surgical staging system for colorectal
adenocarcinoma  in  1932  [14].  Once  again,  the
underlying principle attached to increasing tumor
stages is a progressive departure from the normal
state.  In  other  words,  the  farther  a  neoplasm
grows  from  its  anatomical  origin,  the  more
aggressive its behavior is felt to be.

As  the  natural  history  of  malignant  tumors
was  better  understood  using  such  tools,  efforts
at biological interdiction became more focused.
For  example,  because  axillary  lymph  nodes
were  often  involved  by  metastatic  carcinomas
of  the  breast,  pro  forma  removal  of  the  nodes

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_5, © Springer Science+Business Media, LLC 2011

61

62

M.R. Wick et al.

was  incorporated  into  surgical  treatment  for
 mammary cancer [15]. After intraosseous “skip”
lesions  of  bone  sarcomas  were  characterized,
limb  amputation  was  employed  more  freely  in
the  days  before  effective  drug  treatments  were
available  [16].  The  recognition  that  leukemia
could  use  the  central  nervous  system  as  a
“haven”  to  escape  the  effects  of  chemotherapy
prompted systematic irradiation of the neuraxis
and  the  use  of  “Ommaya  reservoirs”  for  drug
delivery  as  prophylaxes  against  that  phenome-
non [17, 18]. The use of such preemptive mea-
sures in treating human malignancies continues
to this day.

One can rightly conclude that two major goals
exist for medical prognostication and prediction.
One  is  forecasting  the  future  for  individual
patients, and the other is choosing the most effec-
tive treatments for the types, grades, and stages of
the illnesses they have. Pathologists have become
important providers of measurable and seemingly
objective  “prognostic”  information  on  diseases
of all kinds, but with a particular focus on malig-
nant neoplasms. This role is quite different than
the one played by most laboratory-based physi-
cians  until  the  1980s.  Although  pathological
observations  did  play  a  definite  role  in  medical
forecasting  in  the  past,  as  discussed  above,  the
principal task of pathologists was the attainment
of diagnostic certitude. Once they had recognized
and properly classified an illness, the subsequent
role of prognosticator was largely situated in the
bailiwick of clinical physicians.

Roughly 30 years ago, the advent of diagnostic
immunohistology altered that scenario drastically
[19]. The latter technique allowed pathologists to
“map” the protein chemistry of tissues and tumors
in a theretofore-unparalleled fashion, quickly and
reproducibly. For the first time, biological mole-
cules with possibly determinative functions could
be detected in situ in clinical specimens without
the need for laborious and special tissue process-
ing.  A  tidal  wave  of  medical  publications  on
“pathological prognostic factors” began in the late
1980s [20, 21] and has yet to abate.

To those who are naïve regarding the practice
of laboratory medicine, it would seem that pathol-
ogists and oncologists have now reached the state
of Hindu Moksha. Surely, neoplastic cells no longer

can  hold  secrets  unto  themselves  in  the  face  of
immunohistochemistry, in situ nucleic-acid hybri-
dization,  proteomics,  and  gene-sequencing.
Nonetheless,  in  a  real  sense,  that  assumption  is
incorrect. Several obstacles continue to encumber
the  task  of  pathobiological  prognostication,  and
this chapter aims to discuss them. We will review
the definitions and basic concepts of risk, prognosis
and prediction, and consider the important role of
pathologists  as  assessors  of  “new”  tests  using
current  information  about  mammary  carcinoma
as an example.

Risk, Prognosis and Prediction

The  terms  risk,  prognosis  and  prediction  have
been inconsistently and ambiguously used in the
medical  literature  as  indicators  of  the  likely
course of a disease and/or response to a particular
treatment. The term risk is derived from a Greek
word rizikon, literally meaning root but later on
used  in  Latin  for  “cliff ”  [22].  It  describes  the
deviation of one or more future events from their
expected course, and usually focuses on the harm
that  may  arise  from  such  events.  The  term  risk
has  been  used  in  various  disciplines,  as  health
risk, economic risk, psychological risk, and oth-
ers. It has been used variably as the probability of
certain negative events or hazards or to describe
future issues that should be avoided or mitigated.
In  Medicine  and  Epidemiology,  risk  is  usually
estimated simply as the probability of an event,
based on past experience. In business and engi-
neering, more complex mathematical risk models
have  been  proposed,  using  functions  that  inte-
grate the probability of a threat, the probability of
various  other  vulnerabilities,  and  their  potential
impact  to  a  business  or  product  [23].  Risk  has
been distinguished semantically from uncertainty,
the lack of complete certainty, resulting from the
possibility  of  various  possible  outcomes  for  an
event  or  situation  [24].  Uncertainty  is  usually
measured  as  sets  of  probabilities  of  the  various
possible  events  or  outcomes.  The  term  risk  has
been generally used in pathology to describe the
probability that patients with certain findings will
develop  a  future  malignancy  in  an  attempt  to
develop strategies that will prevent the development

5  Prognostication and Prediction in Anatomic Pathology

63

of cancer or lead to its early detection [25]. For
example,  patients  with  atypical  adenomatous
hyperplasia  (ADH)  and  other  conditions  of  the
breast  have  a  higher  risk  of  developing  breast
cancer  and  are  followed  more  carefully  with
mammography than patients without these find-
ings,  in  efforts  at  detecting  early  breast  cancer
[26]. The term risk has also been used in a differ-
ent context to describe the probability of detect-
ing a malignancy in a subsequent specimen [27].
For example, various “risks” of finding a malig-
nancy  in  a  thyroidectomy  specimen  have  been
described for various findings detectable on fine
needle aspirate specimens of the thyroid [28].

The  term  prognosis  is  also  derived  from  a
Greek  work  describing  foreknowing  or  foresee-
ing and is used in Medicine to describe the likely
outcome  of  a  patient  with  a  particular  disease
[29]. Prognostic estimates are usually calculated
as percentages or other proportions and are gen-
erally  variably  accurate  when  applied  to  large
populations of patients with a disease. Physicians
since  the  time  of  Hippocrates  have  been  inter-
ested  in  understanding  the  prognosis  of  various
illnesses  and  have  devised  various  prognostic
models based on astrology or other theories [30].
For  example,  medieval  physicians  would  use
numerology  to  calculate  a  prognosis,  using  the
Sphere of Petorisis, a circular chart designed by
one  of  the  founders  of  astrology,  while  modern
medical informaticians currently propose the use
of prognostic models based on data mining, mul-
tivariate  numerical  data,  and  various  classifica-
tion  models  based  on  decision  trees,  decision
rules,  logistic  regression,  artificial  neural  net-
works,  and  other  computational  models  derived
from probability theory [31, 32]. It is beyond the
scope  of  this  chapter  to  discuss  the  concept  of
prognosis and various methodologies used for its
estimation in further detail, but it is important to
consider  that  prognostic  estimates  are  not  static
for a particular disease. For example, the progno-
sis  of  a  patient  with  mammary  carcinoma  is
dependent on the age of the individual, presence
or  absence  of  other  medical  conditions,  time  of
diagnosis  during  the  natural  history  of  the  dis-
ease,  treatment  effectiveness,  and  many  other
known  and  unknown  variables  [33].  It  is  also
important  to  consider  that  prognostic  estimates

have  been  usually  calculated  for  populations  of
patients with a particular disease. An individual
patient may have a prognosis that varies consid-
erably from the mean or median estimates for a
population of individuals with the same disease.

The term prediction is based on Latin pre or
before  and  dicere  or  say  [34].  A  prediction  or
forecast is used to estimate future events, usually
but  not  always  based  on  experience  or  knowl-
edge. Predictions can be rendered as statements
regarding the outcome that is expected, a proba-
bility of the occurrence of the expected event or
as forecasts describing a range of possible events
[35]. In Medicine, prediction has been used in the
context of estimating the efficacy of specific ther-
apeutic  interventions  [36].  However,  the  influ-
ence of various other variables that can affect the
prognosis  of  a  disease  is  frequently  not  consid-
ered  as  covariates  in  the  forecasting  models.
Moreover, most predictive information in pathol-
ogy  is  currently  available  for  populations  of
patients with particular disease and treated with
specific  therapeutic  agents.  No  generally  used
predictive models have been devised for estimat-
ing the future course of diseases after treatment
in individual patients, a limitation that is impor-
tant to consider in the era of “personalized medi-
cine”  [37].  As  famously  stated  by  Niels  Bohr,
“prediction is difficult, especially if it is about the
future” [38].

Personalized Medicine: Current
Environment for the Development
of Prognostic and Predictive
Laboratory Tests

Advances in molecular medicine and our under-
standing  of  the  human  genome  have  opened  a
new paradigm in Medicine, where new therapies
will be developed based on the understanding of
the molecular basis of neoplasms and other dis-
eases and the treatment of patients will be indi-
vidualized  [39].  There  is  great  hope  and  hype
about the great potential of Personalized Medicine
[40]. This paradigm is based on the availability of
sensitive,  specific  and  accurate  prognostic  and
predictive laboratory tests, and of new effective drugs.

64

M.R. Wick et al.

In a perfect world, one would be able to evaluate
each  prospective  prognostic  or  predictive  medi-
cal test (PPMT) on a large-scale, in a measured
way, and with the use of proper statistical guide-
lines. Unfortunately, that ideal may never be real-
ized. Pragmatic influences that hinder the process
of  medical  research  and  development  are  basi-
cally threefold – financial factors, political imper-
atives,  and  test  reproducibility  and  applicability
to “routine” clinical specimens.

In order to understand the role of financial and
political factors on PPMT development, one must
look outside the realm of medicine and science to
the fields of business administration and sociology.
Projections  for  the  cost  of  health  care  in  the
United  States  in  the  next  15  years  are  sobering
[41]. Healthcare spending (HCS) already approx-
imates 20% of the gross domestic product (GDP),
and, if the system is unchanged, it will steadily
climb  ever-higher  (Fig.  5.1).  Because  a  sizable
fraction of U.S. citizens comprises “baby boomers”
in the 50-and-older age range, who are increas-
ingly  becoming  eligible  for  Medicare  health
coverage, federal HCS could soon exceed 10% of
the  GDP.  Most  private  health  insurers  have
adopted practices that parallel those of Medicare.
Thus,  patients  in  their  HCS  plans   confront  the
same patterns of medical practice and billing as

the U.S. government does, albeit with a different,
more capitalistic, business model. James Traficant,
a  business  executive  who  underwent  two  liver
transplants,  has  written  the  following  about
the cost of modern U.S. health care [42], using a
currently popular television medical drama as a
reference:

Each  week,  a  team  of  five  doctors  works  around
the  clock  and  orders  countless  long-shot  tests  to
diagnose a single patient suffering from an incred-
ibly rare condition. That the American healthcare
system  doesn’t  [really]  work  this  way  isn’t  the
issue.  It’s  that  everyone  believes  it  should,  when
they’re the ones in the hospital gowns.

Returning to the issue of PPMTs, patients with
malignancies are the ones in the hospital gowns,
and, being human, they want ever-more and bet-
ter information about their personal medical out-
looks.  They  also  have  a  natural  tendency  to
believe that any new treatment which is tied to a
“cutting-edge” PPMT is the one they should get.
Technological  medical  entrepreneurs  are
 all-too-ready  to  respond  to  this  philosophical
atmosphere. Some companies may exert not-so-
subtle  pressure  on  physicians  who  are  testing
their new products – both PPMTs and associated
therapies  –  to  give  them  “positive”  information
that can be used in successful marketing and sales

Fig. 5.1  Bar graph
demonstrating relative
international healthcare
expenditures in 2006, as
fractions of the gross
domestic product. (From
The World Health Report
2006 – Working together
for health, http://www.
who.int/whr/2006/en/, with
permission from the World
Health Organization)

5  Prognostication and Prediction in Anatomic Pathology

65

campaigns.  Such  partnerships  are  scientifically
unsound and also may be unethical. One can eas-
ily  envision  a  situation  in  which  a  large,  well-
funded,  well-connected  medical  development
firm could effectively sell a marginally functional
test or treatment, whereas a much smaller corpo-
ration could not so succeed, even if it had a clearly
superior product.

Federal politicians are caught in a three-way
vise  between  their  constituents’  demands  for
comprehensive health care, including few if any
limits;  the  interests  of  local  businesses  in  the
regions  they  represent;  and  the  exigencies  of
maintaining  a  balanced  national  budget  for  the
general welfare of the country. Interestingly, the
effects of lobby-pressure on this tri-cornered tee-
ter-totter are not often mentioned. Over time, for
example, the sub rosa financial influence of U.S.
tobacco companies undeniably impeded medical
advances in the control of smoking-related can-
cers,  particularly  lung  carcinomas  [43].  Now,
many years later, American politicians must find
the  monetary  support  to  treat  the  malignancies
(and other disorders) that are related directly to
their prior decisions. This situation includes the
development of associated PPMTs.

What  are  the  conclusions  that  one  can  draw
from this information? First, the burden of overall
health care will almost certainly curb the extrava-
gant use of medical testing that is not cost-effective.
Second, patients will have to undergo a “religious
conversion” regarding their presumed entitlement
to unlimited medical services. Third, politicians –
and medical care-providers – will need to look past
their fiduciary and personal interests to establish a
truly evidence-based and effective system of health
care for their patients and constituents. They will
need the concerted help of laboratory professionals
and other scientists to do that task properly.

As the U.S. Congressional Budget Office has

stated:

Two  potentially  complementary  approaches  to
reducing  spending  on  Medicare,  Medicaid,  and
health care generally – rather than simply reallocat-
ing  spending  among  different  sectors  of  the
 economy  –  involve  generating  more  information
about
relative  effectiveness  of  medical
 treatments  [and  testing,  including  PPMTs]  and
changing the incentives for providers and consumers
in the supply and demand of health care … Medicare

the

could tie its payment to providers to the cost of the
most  effective  or  most  efficient  treatment.  If  that
payment was less than the cost of providing a more
expensive service, then doctors and hospitals would
probably  elect  not  to  provide  it  …  Alternatively,
enrollees could be required to pay for the additional
costs of less effective procedures [41].

Out with the Old, in with the New?

In their excellent treatise on the vicissitudes of mod-
ern health care, entitled Hope or Hype: the Obse-
ssion with Medical Advances and the High Cost of
False Promises, Deyo and Patrick address a com-
mon trait of both doctors and patients [44]. That is,
both  groups  are  extremely  eager  to  dismiss  “the
old” in Medicine in favor of “the new.” The latter
statement applies to any number of contextual top-
ics,  such  as  the  value  of  good  physical  diagnosis
and history-taking, contrasted with data from reflex-
ive barrages of laboratory testing and radiological
studies; the relative diagnostic benefits of plain-film
radiographs as compared with magnetic resonance-
imaging or positron-emission tomography; support
for  a  rational  and  humane  use  of  hospice-care
instead of heroic but pointless end-of-life medical
intervention; and reliance on time-tested and proven
PPMTs  in  pathology  and  laboratory  medicine
[45–47] as compared with wholesale dependence on
genomics and proteomics [48, 49]. Medical advances
have almost always been heralded initially as “break-
throughs,”  despite  accrual  of  subsequent  infor-
mation – usually not shared with the public – that
has debunked the efficacy of many of them [50].

Mammary Carcinoma as a Model
to Discuss the Challenges of
“Prognostication” and “Prediction”

In order to provide a tangible focus for discussion,
we will use “usual” ductal adenocarcinoma (UDA)
of the breast as a model to discuss the challenges
associated with the development of various prog-
nostic and predictive laboratory tests and integrat-
ing the information developed by these tests into
daily clinical practice. The information on breast
carcinoma  that  will  be  presented  is  certainly  not
identical  to  that  associated  with  colon  cancer,

66

M.R. Wick et al.

 prostate  cancer,  lung  cancer,  or  other  human
 malignancies. Nevertheless, general  principles  are
the same concerning the forecasting of outcomes
for neoplastic diseases.

 Breast cancer is the most common malignancy
in  American  women,  and  the  second-leading
cause of death in that group. It has been predicted
that  in  the  year  2012,  the  annual  prevalence  of
mammary  carcinoma  will  be  >950,000  cases  in
the  U.S.,  and  greater  than  four  million  cases
worldwide.  More  than  210,000  new  cases  will
accrue  each  year  in  this  country,  and  >43,000
women will die of the disease  [  51  ] . In the face of
those daunting fi gures, efforts have been redou-
bled to improve “forecasting” of individual breast
cancer  cases,  and  to  match  therapies  with  indi-
vidual tumors in an optimal fashion.

 In sorting through the statistics just listed, one
must delve further to identify the most formida-
ble  challenge  to  the  process  of  prognostication
for UDA. Among the 194,300 new instances of
breast  carcinoma  in  the  U.S.  in  2009,  70%
(136,000)  were  classifi ed  as  UDAs  pathologi-
cally,  and  60%  (81,600)  of  those  patients  had
stage I tumors (localized to the breast) at diagno-
sis  [  52  ] . The latter subgroup is the crux of very
pressing  problems,  the  pertinent  questions  for
which are –  how many new stage I breast cancers
will resist therapy and threaten life, and how can
they be identifi ed prospectively ? Based on histori-
cal data, the answer to the fi rst question would be
approximately 24,500  [  51  ] . An accurate response
to the second query is much more diffi cult to for-
mulate, as discussed subsequently.

 For other UDAs that are stage  ³ II at presenta-
tion, the biological attributes of the tumor (a rela-
tively large size and/or metastatic involvement of
regional lymph nodes) have already made it appar-
ent that such lesions have aggressive potential (i.e.,
a relatively poorer prognosis) and must be treated
accordingly. In reference to that cohort of patients,
the likely clinical outlook is not quite as uncertain –
especially with no further treatment – but the pos-
sible  individual  benefi t  of  various   therapeutic
interventions is still problematic. In that context, it
must  be  understood  that  forecasting  a  biological
response of a tumor to any given treatment type is
properly  termed   prediction ,  whereas  foretelling

the overall outcome of a case (life vs. death; short
vs. long survival; low vs. high morbidity) is appro-
priately labeled as  prognostication . The two terms
must not, and cannot, be interchanged, for breast
cancer or any other malignant neoplasm.

   Forecasting the Prognosis of Mammary
Carcinoma Patients

 There are several “old” evaluations of mammary
carcinoma,  which  not  only  still  have  value  but
also  match  or  even  out-perform  newer  methods
as  forecasting  tools   [  53–  57  ] .  Moreover,  these
“old”  procedures  can  be  done  by  pathologists
anywhere,  with  standard  hematoxylin–eosin
(H&E) stains and a microscope.

   Effects of Tissue Sampling
on Prognostication and Prediction
 Before considering the specifi cs of prognostic and
predictive  factors  for  mammary  carcinomas,  one
must attend to the issue of tissue sampling. In mod-
ern practice, a common modality for the surgical
diagnosis of mass lesions uses cutting biopsy nee-
dles of variable diameters. These instruments com-
monly allow for a generic morphological diagnosis
of malignancy to be made pathologically (Fig.  5.2 ),

  Fig. 5.2     Needle-core biopsy specimen of invasive breast
carcinoma. Samples such as this may contain signifi cant
artifacts that impede prognostic and predictive studies

5  Prognostication and Prediction in Anatomic Pathology

67

but there are real limitations to the use of needle
biopsies for prognostic and predictive studies.

The  latter  statement  is  true  because  of  two
important factors [58–64]. The first is the distor-
tion of tissue that may be seen in small, “closed”
biopsy specimens, such that important histologi-
cal  details  may  be  artifactually  obscured.  The
second relates to the inherent spatial heterogene-
ity, which is a part of many human malignancies.
In other words, if one samples several aspects of
a tumor, several and conflicting data on prognosis
may be obtained. Conversely, very limited sam-
pling can produce artificial results that in fact do
not represent the biological lesion as a whole.

As a result of those realities, our opinion is that
cutting-needle or fine-needle aspiration biopsies of
breast masses are best used for diagnostic purposes
only. If additional information is requested of the
pathologist  –  concerning  tumor  type,  grade,  or
expression  of  various  biochemical  markers  –  a
caveat should be included in the surgical pathol-
ogy report on the possibly confounding  effects of
limited sampling methods.

“special” variants [65–73]. These differ  structurally
and biologically from the most common form of
mammary carcinoma, UDA, and these can be seg-
regated into three groups, which relate to the rela-
tive  behavioral  characteristics  of  the  tumors  in
question. They are group I – more favorable behav-
ior than that of comparably sized UDA; group II –
similar behavior to that of comparably sized UDA;
and group III – more aggressive behavior than that
of comparably sized UDA. These are segregated as
follows, with the percentage fraction of all breast
cancers they represent in parentheses:

Group  I  (Figs.  5.3–5.8)  –  “Pure”  lobular  carci-
noma  (10–15%);  “pure”  mucinous  carcinoma
(2%);  “pure”  tubular  carcinoma  and  low-grade
invasive  cribriform  carcinoma  (1–2%);  salivary
gland-type  carcinomas  of  the  breast  (adenoid
cystic carcinoma, acinic cell carcinoma, mucoepi-
dermoid  carcinoma,  low-grade  adenosquamous
carcinoma;  malignant  [adeno-]myoepithelioma)
(1%); intracystic papillary carcinoma (1%); pri-
mary  mammary  “carcinoid”  tumor  (<1%);  and
“pure” medullary carcinoma (5%).

Recognition of “Special” Histologic Breast
Cancer Variants
The first of the established methods for prognosti-
cation of breast carcinoma concerns the accurate
morphological and conceptual identification of its

Group II (Figs. 5.9–5.11) – “Pure squamous cell
 carcinoma”  (<1%);  secretory  adenocarcinoma
(<1%); “pleomorphic” lobular carcinoma (<1%);
and  “ atypical”  or  mixed  medullary  carcinoma
(1–2%).

Fig. 5.3 ( a) Fine-needle aspiration biopsy and (b) excisional biopsy specimen of “classical” invasive lobular carcinoma

68

M.R. Wick et al.

Fig.  5.4 ( a,  b)  “Pure”  mucinous  adenocarcinoma  of  the  breast,  showing  aggregates  of  rather  bland  tumor  cells
 suspended in extracellular mucin

Fig. 5.5 ( a, b) “Pure” tubular carcinoma of the breast, showing open tubular profiles that contain secretory “snouts”

Group  III  (Figs.  5.12–5.15)  –  Metaplastic
 carcinoma (sarcomatoid carcinoma; spindle-cell
carcinoma;  “carcinosarcoma”)  (1%);  neuro-
endocrine carcinoma (<1%); invasive micropap-
illary  carcinoma  (<1%);  and  undifferentiated
carcinoma, not otherwise specified (<1%).

In current practice, the lamentable tendency of
surgeons,  radiotherapists,  and  medical  oncolo-
gists is to adopt a “one size fits all” mentality in
reference  to  breast  carcinoma.  In  that  approach,
the histologically defined entities listed above are
not distinguished conceptually from UDA. Studies
for  estrogen  and  progesterone  receptor  proteins,
HER-2 gene amplification, and other biochemical
and  genetic  analytes  are  demanded  pro  forma
that  morphological
fact
[74],  despite

the

Fig. 5.6  Adenoid cystic carcinoma of the breast, compris-
ing tubules and nests of monotonous basaloid cells

5  Prognostication and Prediction in Anatomic Pathology

69

Fig. 5.7 ( a) Gross and (b) microscopic photographs of intracystic papillary carcinoma of the breast. Numerous micro-
papillary structures, most of which lack fibrovascular cores, are present

Fig. 5.8 ( a) Gross and (b) microscopic images of medullary breast carcinoma. The “cerebroid” nature of the macro-
scopic tumor is apparent, and its histologic image features a syncytium of neoplastic cells with admixed lymphocytes

 assignment alone clearly overrides the importance
of those evaluations [65, 66].

In  other  words,  histopathologic  descriptors
preceding  the  word  “carcinoma”  are  regarded
only with feeble interest and as having no practi-
cal significance. That attitude sets the stage for an
uninformed  and  unscientific  approach  to  man-
agement of the tumor types in question. In group I,
all of the lesions (except, possibly, for medullary
carcinoma)  do  not  require  anything  more  than
local excision if they measure <3 cm. in maximal
dimension  [55].  The  presence  of  larger  masses
should  prompt  a  sentinel  axillary  lymph  node

biopsy,  but,  if  it  shows  no  metastasis,   nothing
further needs be done. Lesions in group II can be
managed  surgically  in  the  same  fashion  as  that
used  for  UDAs,  stage-for-stage.  However,  in
the former of those cohorts, it should be under-
stood that squamous cell carcinomas and med-
ullary  carcinomas  almost  always  fail  to  show
immunoreactivity  for  estrogen  or  progesterone
receptor  proteins  (ERP/PRP),  or  to  manifest
amplification  of  the  HER-2  gene  [65,  66].
Furthermore,  they  respond  differently  to  con-
ventional  chemotherapy,  as  compared  with
UDAs. Finally, metaplastic carcinomas, small-cell

70

M.R. Wick et al.

Fig.  5.9  Poorly  differentiated  “pure”  squamous  carci-
noma of the breast, with notable cytoplasmic  eosinophilia
and individually dyskeratotic cells

Fig. 5.10  “Secretory” adenocarcinoma of the breast, as
seen in a 12-year-old girl. A lacework of epithelial tumor
cells encloses eosinophilic secretory material

 neuroendocrine  carcinomas,  invasive  micropa-
pillary  carcinomas, and undifferentiated carcino-
mas  of  the  breast  in  group  III  comprise  a
behaviorally  aggressive  subgroup  and  must  be
treated accordingly with a multimodal approach
[71, 72, 75–82].

In summary, then, if one were to make no dis-
tinction between any of these pathologic-variant
carcinomas and UDA, managing all breast can-
cers in the same fashion, the end result would be
overtreatment (or, sometimes, undertreatment) of
at least 10% of cases. That statistic may not seem
critical,  but  it  represents  a  sufficient  number  to

Fig. 5.11  Metastatic “pleomorphic” lobular mammary car-
cinoma in an axillary lymph node. There is much more cel-
lular heterogeneity than that seen in the tumor in Fig. 5.3

falsely skew the results of a new treatment or a
novel  variation  on  an  old  one.  With  additional
regard to the use of medical resources, automatic
studies for ERP/PRP and HER-2 are unnecessary
in reference to many special breast tumor types.
Lobular,  tubular,  invasive  cribriform,  mucinous,
and papillary carcinoma are essentially all capa-
ble of expressing hormone receptors, and they all
lack  HER-2  amplification  [65,  66].  Medullary
and  metaplastic  carcinomas  consistently  lack
ERP/PRP, and, usually, HER-2 abnormalities as
well [71, 83, 84].

Accurate Measurement of Tumor Size
Tumor size is, by itself, a meaningful prognostic
factor [85–88], and there are two probable expla-
nations for that fact. The most likely one is that
robust  local  growth  of  a  primary  malignancy
indicates an overall dominance of tumor cell pro-
liferation  over  the  host’s  mechanisms  for  con-
taining  it  [53].  That  same  supremacy  applies  in
metastatic  sites  as  well.  A  second  explanation
regarding the significance of primary tumor size
is that large masses of replicating, clonal, malig-
nant cells are prone to undergo additional muta-
tional  events  that  may  increase  their  growth
potential,  viability,  and  capacity  for  metastasis
[88, 89].

The greatest dimension of any given invasive
breast cancer has usually been measured from the
gross specimen in the pathology laboratory. That
is  still  the  best  method,  although  it  must  be

5  Prognostication and Prediction in Anatomic Pathology

71

Fig. 5.12 ( a) Gross and (b) microscopic images of “metaplastic” (sarcomatoid) breast carcinoma. The tumor is bulky
macroscopically, and comprises fusiform and pleomorphic cells with no ductal structures

Fig. 5.13 ( a, b) High-grade primary neuroendocrine carcinoma of the breast, showing dispersed nuclear chromatin,
numerous mitotic figures, nuclear “molding,” and abundant apoptosis

acknowledged that peritumoral desmoplasia may
falsely increase the result somewhat. On the other
hand, very small tumors (<0.5 cm) can be diffi-
cult to see well macroscopically, and their dimen-
sions must then be taken from microscopic slides
or radiological images [87, 89].

Histologic Grading of Invasive Breast
Carcinoma
As  stated  earlier,
iterations  of  histological
schemes  for  the  grading  of  malignant  tumors
have been extant for almost 100 years, but some
have been more useful than others. With regard to

breast cancer – and, in particular, UDA – Bloom
and  Richardson  introduced  an  effective  grading
system in 1957 [90]. It was subsequently modi-
fied slightly by Scarff and Torloni in 1968 [91],
and again by Le Doussal et al. in 1989 [92], and
continues to be used today on a worldwide scale.
Details  of  the  Bloom–Scarff–Richardson  (BSR)
grading method are shown in Table 5.1.

Many publications have attested to the inter-
observer reproducibility and prognostic value of
the BSR grade, when used by itself and in combi-
nation with other clinicopathologic observations
[93–99]. For example, the Nottingham Prognostic

72

M.R. Wick et al.

Fig. 5.14 ( a) Extensive intramammary lymphatic involvement is present in this invasive micropapillary breast carci-
noma. The tumor cells are variably pleomorphic (b); they form small tubules and micropapillae

regional lymph nodes are the principal contextual
determinants of final case outcome.

The BSR and MBSR grading methods are best
applied  to  UDAs,  because  the  morphologic
details  of  other  nosological  tumor  types  are  so
reproducible  that  they  are  also  dispositive  of
grade.  For  example,  tubular  and  invasive  cribri-
form carcinomas are all grade I tumors, whereas
medullary,  neuroendocrine,  and  metaplastic
 carcinomas are all grade III lesions [53].

Lymph Node Status as a Prognosticator
for Breast Cancer
Dr. William S. Halsted, the first chief of surgery
at Johns Hopkins University Hospital [100, 101],
had  a  lasting  influence  on  the  way  in  which
 physicians  thought  about  breast  cancer  and  its
natural evolution. Up until 1882, the diagnosis of
mammary  carcinoma  was  usually  a  lethal  one
[102], and, owing to the lack of an effective treat-
ment  for  that  tumor,  many  cases  had  been
observed from their initial manifestations through
advanced  stages  of  growth.  Halsted  recognized
that  “scirrhous”  (invasive)  breast  cancers  had  a
reproducible  tendency  for  involvement  of  the
skin and chest wall, and for metastasis to regional
(axillary,  intramammary,  and  supraclavicular)
lymph nodes. He postulated that if those tissues
were  removed  early,  tumors  could  be  blocked
from exercising their ability for “in-line” growth
from  the  primary  intramammary  lesion  [101].

Fig.  5.15  Large-cell  undifferentiated  carcinoma  of  the
breast, with “rhabdoid” features

Index (NPI) melds the BSR grade with the tumor
stage, as defined by the system put forth by the
American  Joint  Committee  on  Cancer  (AJCC)
[95].  The  latter  includes  factors  reflecting  pri-
mary tumor size, lymph nodal involvement, and
distant  metastasis.  Relative  weights  have  been
assessed  for  individual  components  of  the  BSR
grade and the AJCC stage, as in the modified BSR
(MBSR)  system  of  Le  Doussal  and  coworkers
[92].  From  those  analyses,  it  would  appear  that
the  degree  of  nuclear  atypia-pleomorphism,
mitotic  rate  per  10  high-power  (×400)  micro-
scopic  fields,  and  metastatic  involvement  of

5  Prognostication and Prediction in Anatomic Pathology

73

Table 5.1  Scarff–Bloom–Richardson grading scheme for invasive ductal breast carcinomaa

Tumor tubule formation
>75% of tumor cells arranged in tubules
>10% and <75%
<10%
Number of mitoses
Low power scanning (×100), find most mitotically tumor area, proceed to
high power (×400)
<10 mitoses in 10 high-power fields
>10 and <20 mitoses
>20 mitoses per 10 high power fields
Nuclear pleomorphism
Cell nuclei are uniform in size and shape, relatively small, have dispersed
chromatin patterns, and are without prominent nucleoli
Cell nuclei are somewhat pleomorphic, have nucleoli, and are intermediate size
Cell nuclei are relatively large, have prominent nucleoli or multiple nucleoli,
coarse chromatin patterns, and vary in size and shape
Combined scoresb
3, 4, 5
6, 7
8, 9

Differentiation/grade
Well-differentiated (grade I)
Moderately differentiated (grade II)
Poorly differentiated (grade III)

Score
1
2
3

1
2
3

1

2
3

a The SBR grading scheme is based on three morphologic features: (1) degree of tumor tubule
formation; (2) mitotic activity; and (3) degree of nuclear pleomorphism. Seven possible scores
are condensed into three grades
b To obtain the final SBR score, one adds subscores from tubule formation, mitotic activity, and
nuclear pleomorphism. The combined score yields the final grade

A  logical  extension  of  Halsted’s  concepts  held
that embolic tumor implants in lymph nodes were
“way-station” sources of additional, distant metas-
tases  to  deep  structures  such  as  the  lungs,  liver,
brain, and bones. Hence, radical surgical excision
of the breast, pectoralis muscles, and all acces-
sible  regional  nodes  was  undertaken  from  the
early 1880s onward – the so-called “radical” mas-
tectomy or Halsted procedure. It was the therapy
of  choice  for  breast  carcinoma  through  the  late
1970s [102].

Even  today,  many  surgeons  believe  that
aggressive axillary lymphadenectomy has a cura-
tive  purpose,  hypothetically  preventing  the  vis-
ceralization  of  mammary  cancers.  Nonetheless,
that  line  of  reasoning  is  fallacious.  Malignant
neoplasms  with  the  ability  to  metastasize  at  all
will  exercise  that  capacity  globally  as  soon  as
they acquire it. Metastases may first be detected
in regional lymph nodes, but distant implants are
also  concurrently  present  in  viscera  with  the
capability of growing to attain clinical visibility
at some time in the future [103]. Strong evidence

the  second  underwent

in  favor  of  that  mechanistic  construct  was
 published  in  1981  by  Fisher  et  al.  [104].  Those
investigators randomized patients with clinically
lymph-node-negative  breast  cancers  to  three
groups. The first was treated with radical mastec-
tomy,  whereas
total
 mastectomy  and  thoraco-axillary  irradiation,  to
“sterilize”  any  occult  tumor  deposits  in  axillary
and  internal  mammary  lymph  nodes.  The  third
group was managed with total mastectomy alone.
Long-term  surveillance  showed  no  difference
whatsoever in survival or rates of distant metasta-
sis among the three groups [104].

Using those data, Fisher and coworkers rightly
concluded that tumor implants in regional lymph
nodes were not the source of visceral metastases.
Instead, they were … indicators of a host-tumor
relationship  which  permits  the  development  of
metastases and…not important instigators of dis-
tant  disease  [104].  Put  another  way,  regional
lymph node metastases are merely tangible proof
that  any  given  breast  cancer  can  successfully
spread  from  the  mammary  gland  and  grow  in  a

74

M.R. Wick et al.

secondary site. As such, in specific reference to
UDAs, they are also markers for the presence of
systemic disease [105].

Thus, it comes as no surprise that neoplastic
implants  in  regional  lymph  nodes  are,  in  fact,
associated with a significant worsening of prog-
nosis.  How  much  it  is  lessened  depends  on  the
number and locations of nodes that are involved
[106–108]  –  indirectly  indicating  the  vigor  of
tumor  proliferation  in  anatomically  “foreign”
sites  –  as  well  as  markers  of  growth  potential
(especially mitotic rate) in the neoplastic popula-
tion itself.

The latter comments have a direct bearing on a
related topic – that is, the biological “meaning” of
very few (Fig. 5.16) vs. very many tumor cells in
a lymph node. Conflicting data have been recorded
in  reference  to  that  subject  [109].  However,  our
synthesis of them suggests that many other factors
have a bearing on ultimate case outcomes besides
tumor-cell-counting.  Not  all  metastasizing  neo-
plasms are the same behaviorally; some may have
the  capacity  for  angiolymphatic  invasion  and
embolic spread to other sites, but they may lack
the  necessary  metabolic  machinery  to  thrive  in
those locations. A concrete example of that situa-
tion  is  illustrated  by  lymph  node  positivity  in
cases  of  “pure”  tubular  or  invasive  cribriform

Fig.  5.16  Isolated  tumor  cells  are  seen  in  an  axillary
lymph node in a case of invasive ductal adenocarcinoma
of the usual type, with this pankeratin immunostain. The
prognostic  significance  of  this  finding  is  nil,  and  such
nodes should be classified as “negative for tumor”

breast  carcinoma,  which  empirically  has  been
shown to have no association with a decrement in
prognosis.  Host  immunity  is  also  variable  from
case to case, but it represents another crucial part
of  the  biological  mix  that  determines  whether
metastatic cells can gain a foothold and flourish.
In sum, we believe that “micrometastatic” tumor
implants in lymph nodes (or viscera) reflect a lack
of robustness in the neoplastic cell population in
general, and we agree with recommendations that
they be grouped with true “N0” lymph nodes for
purposes of staging [110].

In  light  of  these  considerations,  we  cannot
support  the  practice  of  reflexive  immunohis-
tochemical  staining  or  “molecular  assessment”
[111, 112] of regional nodes for epithelial mark-
ers, with the aim of finding histologically occult,
dispersed tumor cells. Moreover, we see no thera-
peutic purpose – beyond, perhaps, a small step in
benefitting the local control of tumor growth – in
doing  extensive  “completion  lymphadenecto-
mies”  for  UDAs  with  clearly  positive  axillary
“sentinel”  lymph  node  biopsies  [113].  Those
individuals  have  systemic  disease,  cannot  be
cured  by  the  surgeon,  and  will  all  require  adju-
vant  treatments  of  some  type.  Therefore,  the
“sentinel” node technique is a generic prognostic
tool, and, if the node is involved by a UDA of the
breast, it should drive the decision to employ an
appropriate nonsurgical therapy [114].

Is There a Surrogate for Formal Lymph
Node Substaging of Breast Cancer?
Interest  has  grown  in  recent  years  over  the
 possibility that histological nuances of a primary
breast  carcinoma  could  obviate  the  need  for
 formal  lymph  node  substaging.  In  particular,  a
logical focus has been drawn on the presence of
intramammary angiolymphatic invasion by tumor
cells,  as  seen  in  H&E  sections  [115–119]
or  in  immunostained  preparations  with  D2-40
(Fig.  5.17),  an  antibody  recognizing  lymphatic
endothelium [120–122]. In particular, de Mascarel
et  al.  have  shown  that  if  lymphovascular  tumor
emboli (LTE) are seen in the breast, with an unin-
volved zone of normal tissue between the primary
carcinoma  and  the  emboli,  the  likelihood  of
metastasis-free survival is significantly lessened

5  Prognostication and Prediction in Anatomic Pathology

75

Fig. 5.17  Angiolymphatic invasion by breast carcinoma, as seen with in a hematoxylin and eosin-stained slide (a) and
an immunostain for podoplanin (b) done with antibody “D2-40”

[120].  Moreover,  Gurleyik  and  coworkers  and
Klar  et  al.  demonstrated  a  strong  correlation
between  the  presence  of  LTE  and  metastasis  to
regional  lymph  nodes  [115].  These  findings  are
not unexpected, because acquisition of the ability
for tumor cells to cross vascular basement mem-
branes goes hand-in-hand with development of a
metastasis-capable genotype and phenotype [123].

“New” Putatively Prognostic Analytes
in Breast Carcinomas

In the wake of the worldwide “genome project”
and  the  common  use  of  comparative  genomic
hybridization,  many  candidate  genes  have  been
identified  with  possible  behavioral  importance
for breast carcinomas. These include nm23, p53,
c-myc, H-, K-, and N-ras, PS2, c-erbB-2(HER-2/
neu)  and  c-erbB-3  (HER-3),  epidermal  growth
factor  receptor-1,  “heat  shock”  genes,  int-2/hst/
bcl-1, RB1, and many others [48, 49, 124–136].
Mutations  or  amplifications  of  such  moieties
have  been  correlated  with  allegedly  worsened
behavior  of  mammary  cancers.  In  addition,
“molecular” markers of cell replication, such as
Ki-67/MIB-1,  proliferating  cell  nuclear  antigen
(PCNA), and the cyclin family of proteins, have
been  studied  as  substitutes  for,  or  adjuncts  to,
morphologic  quantitation  of  mitotic  activity  in
breast carcinomas [137, 138].

Assertions have been made that such analytes
should  replace  morphology-based  observations
in the prognostication of malignant tumors of the
breast  and  other  organs  [135–156].  The  follow-
ing sections address several problems, which are
attached to those recommendations.

Heterogeneous Data Types Affecting
Prognostic Factors
In any discipline, data exist in one of three basic
forms. They are categorical (nominal), binary, or
semiquantitative-quantitative [157]. An example
of  categorical  data  is  represented  by  discrete,
mutually  exclusive,  morphologically  defined
diseases or disease subsets, which must be inher-
ently uniform internally. That type of information
has  formed  the  backbone  of  investigations  in
anatomic  pathology.  A  common  form  of  binary
data  is  the  positive/negative  reporting  format
that  pertains  to  many  medical  tests,  defined
by  the  presence  or  absence  of  a  predefined
 analyte. Binary information has a tendency to be
artificially delineated, because very few (if any)
constituents of biological systems are either ubiq-
uitous  or   undetectable  in  a  mutually  exclusive
way.  The  semiquantitative-quantitative  category
is  self-explanatory  and  best  suited  to  measure-
ments in biology and Medicine. It is also the most
 dependent  of  all  data  sets  on  methodological
 precision, reproducibility, and accuracy.

76

M.R. Wick et al.

Case Example: Effects of Incorrect
Categorical and Binary Data Generation
A  53-year-old  woman  detected  a  mass  in  her
left  axilla  while  bathing.  It  was  confirmed  on
physical examination by her physician, and by
computed tomography of the thorax (Fig. 5.18).
Surgical excision and histological examination
demonstrated  a  malignant,  large-cell  undiffer-
entiated  neoplasm  in  an  axillary  lymph  node
(Fig.  5.19).  The  morphological  differential

Fig. 5.18  Thoracic computed tomogram showing a large
left axillary lymph node in a middle-aged woman

Fig.  5.19  Excision  of  the  mass  shown  in  the  previous
 figure  demonstrated  a  large-cell  undifferentiated  malig-
nancy.  The  principal  differential  diagnosis  was  between
carcinoma and melanoma

diagnosis centered on metastatic carcinoma vs.
metastatic  melanoma.  Accordingly,  immunos-
tains  were  obtained  for  pankeratin  and  S100
protein; these were interpreted as negative and
positive, respectively (binary data generation),
and  a  final  diagnosis  of  metastatic  melanoma
(categorical  data  generation).
was  made
Reexamination of the skin showed no evidence
of  a  pigmented  lesion,  but  it  was  thought  to
have  regressed.  The  patient  was  referred  to
another  institution  for  entry  into  a  melanoma-
vaccine trial.

Pathologists at the second institution wished to
perform additional immunohistologic studies, and
they  asked  for  the  original  paraffin  blocks  of
tumor  tissue.  Immunostains  for  pankeratin  and
S100  protein  were  also  repeated.  This  time,  the
tumor  was  found  to  be  reactive  for  both  keratin
and  S100  protein  [158];  in  addition,  it  lacked
melan-A, tyrosinase, and PNL2, all of which are
melanocytic  markers  [159].  Another  stain  for
gross  cystic  disease  fluid  protein-15  (a  breast
epithelium-related  analyte)  [160]  was  positive
(Fig.  5.20),  establishing  the  diagnosis  of  meta-
static  breast  carcinoma.  Mammography  then
disclosed a mass in the left breast, a fine needle
aspiration  biopsy  of  which  showed  adenocarci-
noma (Fig. 5.21). Subsequent discussion with the
referring pathologists revealed that recommended
epitope-retrieval  methods  were  not  used  in
doing  pankeratin  immunostains  [161]  at  their
institution, accounting for the false negativity  of
that analyte.

Had the original pathological data been used
in  prognostication  and  treatment  planning,  sev-
eral derivative mistakes would have been made.
Incorrect  categorical  information  –  which,  in
turn,  was  produced  by  incorrect  binary  data  –
would have put the patient in the wrong treatment
“bin,”  “contaminating”  accrued  results  of  mela-
noma  vaccine  therapy  at  institution  no.  2  and
excluding the possibility of effective breast can-
cer-directed  intervention.  Parenthically,  predic-
tive  markers  for  mammary  cancers,  including
ERP/PRP and HER-2, also would not have been
assessed.  It  is  likewise  probable  that  identifica-
tion of the primary mammary tumor would have
been delayed or not made at all.

5  Prognostication and Prediction in Anatomic Pathology

77

Fig. 5.20 ( a) S100 protein-staining of the lesion shown
in  Fig.  5.19  produced  positive  results,  and  a  pankeratin
stain  (b)  was  interpreted  as  negative.  The  tumor  was
therefore  classified  as  a  melanoma.  However,  repeated

keratin immunostaining with proper epitope retrieval showed
obvious positivity (c). An additional study showed gross
cystic disease fluid protein-15 in the tumor cells (d). The final
diagnosis was that of metastatic mammary adenocarcinoma

The particular danger that is tied to binary data
is that they often are used in a contingent fashion
for  treatment  planning.  In  other  words,  a  “posi-
tive” result leads in one direction, a “negative” in
another. Hence, mistakes in generating binary data
can  be  crucial  ones.  Returning  to  the  illustrative
case, the only practical way for pathology labora-
tories to quality-control immunohistologic results
is  to  utilize  a  combination  of  internal  duplicate-
testing and extramural validation by another refer-
ence laboratory [162–164] (see Chap. 16).

The situation is even more complicated if one
attempts to substitute one binary test as a  surro-
gate for another one, or to use a binary assay for

 multivariate targets. For example, several studies
have shown that immunohistological assessment
of HER-2 gene amplification is an imperfect sub-
stitute  for  in-situ  hybridization  or  polymerase
chain reaction-based assays [165–177]. In other
words, a “positive” HER-2 immunostain may be
unassociated with actual gene amplification in a
sizable proportion of breast cancer cases. Similar
comments  apply  to  the  relationship  between
“positive”  immunostains  for  epidermal  growth
factor  receptor  (EGFR)  and  actual  mutations  in
the EGFR gene, in reference to lung or colon car-
cinomas. Yet another example of the same hiatus
is “positive” immunostaining for CD117 (c-kit),

78

M.R. Wick et al.

Fig. 5.21  Subsequent mammography of the left breast, in the case discussed in Figs. 5.19 and 5.20, showed a mass
with the phenotype of carcinoma (a); that impression was confirmed by fine-needle aspiration biopsy (b)

but  with  no  actual  activating  mutation  in  the
CD117 gene [178, 179].

The outcome of all of those scenarios is again
a  likely  misdirection  of  treatment.  Biological
agents  that  are  inhibitors  of  HER-2,  EGFR,  or
CD117 may be administered on the basis of “pos-
itive” respective immunostains, but there will be
no  clinical  response  because  the  surrogate
“binary” tests are poor ones.

As  considered  elsewhere  in  this  monograph,
statistical methods also differ significantly for the
evaluation  of  binary  and  semiquantitative  or
quantitative data of prognostic or predictive use.
Binary  information  is  often  assessed  using
Bayesian  techniques;  nonbinary  data  require
evaluations using receiver-operator-characteristic
(ROC)  curves;
likelihood  ratios,  Wilcoxon
 analysis, Kruskal–Wallis testing, and other simi-
lar procedures [180].

Interpretative  and  decision-making  applica-
tions of binary or categorical data can be facili-
tated  by  constructing  partially
redundant
algorithms that are based on constellations of test
results.  An  example  is  shown  in  Fig.  5.22,  in
reference to immunohistochemical identification
of  metastatic  carcinomas  of  unknown  origin.
Nevertheless, such constructions cannot compen-
sate for poor methodology.

Methodological Reproducibility
and Cross-Validation
Methodological  reproducibility  is,  sadly,  rarely
discussed in the practice of anatomic pathology
[162, 164]. As an example, one could obtain bio-
logically “proven,” analyte-positive cases to use
as “in-run” controls. In the context of immunos-
tains in breast cancer cases, such specimens are
exemplified by ERP-positive invasive carcinomas
that  are  known  to  have  responded  clinically  to
hormonal  therapy.  Unfortunately,  the  latter  por-
tion of that requirement is typically ignored.

Cross-validation of methods (CVM: also known
as interanalytical agreement) is also a cornerstone
of proper testing for prognostic and predictive fac-
tors.  In  the  realm  of  breast  cancer  evaluation,
examples of CVM are represented by parallel eval-
uations of ERP content by dextran-coated charcoal
assays and immunostaining, done on the same tis-
sue  specimens  [181–183];  immunostaining  for
nuclear p53-reactivity (Fig. 5.23), compared with
formal  gene-sequence  analysis  to  identify  p53
mutations,  again  on  the  same  tissue  samples
[184–189] (Fig. 5.24); and HER-2 immunostaining
compared  with  results  of  in-situ  hybridization
(ISH)  using  the  same  tissue  substrate  [168–177].
Again,  the  routine  application  of  CVM  is  a  sad
 rarity in the practice of surgical pathology.

5  Prognostication and Prediction in Anatomic Pathology

79

Fig. 5.22  Algorithm for the immunohistochemical identification of metastatic epithelioid malignancies. The inherent
redundancy in this approach compensates, at least in part, for biological variation in this group of tumors

Fig. 5.23  Nuclear immunolabeling of ductal breast carci-
noma for putatively mutant p53 protein

Fig. 5.24  Southern blot preparation for mutant p53, dem-
onstrating a nonmutated patient sample as compared with
“wild type” control tissues

Case Example: Effects of Omitting
Cross-Validation of Methods
A  39-year-old  woman  found  a  mass  in  her  right
breast  by  monthly  self-examination.  The  lesion
was confirmed mammographically, and its image
suggested a malignancy. Excision and pathological

examination of the mass showed an invasive 1 cm,
UDA of BSR grade II (Fig. 5.25). It exhibited no
angiolymphatic  invasion  and  had  a  low  mitotic
rate; all surgical margins were uninvolved by tumor.
It  was  immunoreactive  for  ERP  and  PRP,  and
lacked HER-2 amplification in ISH studies.

80

M.R. Wick et al.

followed  by  adjuvant  chemotherapy.  The  patient
had severe vomiting during her course of treatment
and developed a local “seroma” at the surgical site,
which gradually resolved.

By coincidence, it happened that frozen tissue
from  the  original  excision  specimen  had  been
saved in the institutional tumor bank. It was later
analyzed,  as  part  of  a  research  study,  for  p53
mutations  by  polymerase  chain  reaction-medi-
ated analysis of single-strand conformation poly-
morphisms  and  by  direct  sequencing.  No  p53
mutations were found.

It is well known that “positive” nuclear label-
ing  for  putatively  mutant  p53  protein  can  be
caused by several other mechanisms that do not
involve  gene  mutation  [184,  185,  188,  189].  In
the absence of a real gene aberration, such results
have no biological importance.

Thus, in this illustrative case, several mistakes
derived  from  the  failure  to  validate  immunos-
taining  results  by  more  sophisticated  methods.
First, a conceptual failing by the surgeon – that is,
using an isolated test result to determine therapy –
undeniably occurred in the face of morphological
and  supplementary  laboratory  information  that
was prognostically favorable. That misstep produced
unnecessarily aggressive treatment with unwanted
morbidity. Second, a failure of the pathologist to
explain the limitations of p53 immunostaining in
the  surgical  pathology  report  indirectly  fostered
incorrect decisions by the surgeon. If it is done at
all  in  UDA  cases,  p53  immunostaining  should
be  viewed  as  a  screening  procedure;  “positive”
results must be validated by additional studies.

Fig. 5.25  Bloom–Scarff–Richardson grade II ductal ade-
nocarcinoma, as seen in a 39-year-old woman

Fig. 5.26  The tumor in Fig. 5.25 unexpectedly immuno-
labeled for p53 protein, but no actual p53 mutations were
found on subsequent blotting studies

The  surgeon  handling  the  case  specifically
requested that an immunostain for p53 be done,
predicated on his recent perusal of literature on
that  analyte  in  breast  cancer.  Without  asking
about the use the surgeon intended for that result,
the test was done by the pathologist. Unexpectedly,
it  showed  significant  nuclear  reactivity  for  p53
(Fig. 5.26), which was reported in an addendum
with no additional comments.

Based on that finding, the surgeon informed the
patient  that  she  had  a  poor-prognosis   neoplasm.
He recommended, and performed, a modified rad-
ical completion mastectomy (which demonstrated
no  residual  tumor  or  lymph  node  involvement) ,

Sources of Clinical Bias in Reference
to New “Prognostic” Markers
Some pathologists pay little attention to medical
publications  in  other  specialty  areas,  including
those that discuss new “prognostic” markers for
breast  carcinoma  and  other  malignant  tumors.
That  is  an  unfortunate  oversight.  Pathologists
must be able to discern whether or not such clini-
cal  studies  have  been  properly  constructed  and
performed,  in  order  to  help  their  colleagues
decide which new “prognostic” laboratory assays
are worthy of implementation and which are not.
Reproducible  mistakes  exist  in  a  substantial
number of “forecast”-oriented clinical publications

5  Prognostication and Prediction in Anatomic Pathology

81

in oncology. The first is the inclusion of tumors of
different histologic types, grades, and stages in the
same cohort of cases. The second is the indiscrimi-
nate  mixing  of  patients  who  have  never  before
been  treated  for  their  malignancies,  with  others
who have failed prior therapies, in the same study
group.  The  third  is  represented  by  attempts  to
compare  the  outcomes  of  patients  who  have
received  a  heterogeneous  hodgepodge  of  treat-
ments, but with focus on a single “prognostic” fac-
tor. Finally, there may be inattention to important
and variable comorbid conditions in the study pop-
ulation. Any one of these flaws casts serious doubt
on the validity of conclusions regarding “progno-
sis.”  Another  problem  concerns  a  failure  to  use
“power  analysis;”  that  is,  predefined  statistical
construction of studies with sufficient case num-
bers and controls to yield valid information [190].
Those  measures  are  necessary  because  “large
groups” of study cases may, in fact, be inadequate
to  allow  for  definite  conclusions  about  them.
A study set of 50 prostatic leiomyosarcomas would
seem  huge  to  any  given  surgical  pathologist
because  of  the  rarity  of  that  tumor  type,  but,  in
fact, it would not allow for any truly meaningful
studies on the prognosis of the lesion.

An additional pertinent issue is the definition of
“outcomes.”  They  may  be  binary  (e.g.,  dead  or
alive; tumor-free or not), or qualified (overall sur-
vival  vs.  disease-free  survival).  These  definitions
have  distinct  implications  for  the  estimation  of
prognosis.  Some  analytes  may  be  prognostic  in
regard to one outcome measure, but not another.
For example, factor “X” may correlate well with
disease-free  survival  but  not  overall  survival.
Others  may  be  prognostic  for  one  patient  sub-
group, but not others (e.g., individuals with stage I
breast cancers vs. those with nonstage I tumors).

Two  truisms  attach  to  these  issues.  First,  in
any proper comparison of 2 or more prognostic
factors  that  are  derived  from  different  patient-
cohorts,  the  cohort  compositions  and  measures
of “outcome” must be the same. Second, it must
be  understood  that  “surrogate”  measures  of
 outcome  are  not  the  same  as  “real”  outcomes.
Statements about surrogacy go as follows … fac-
tor “Z” forecasts well for lymph node metastasis,
and lymph node metastasis correlates well with
overall  prognosis,  so  therefore  factor  “Z”  also

forecasts overall prognosis. That form of logical
construction often falls into the “true-true-unrelated”
category  and  is  therefore  incorrect.  Regrettably,
surrogate  outcome  measures  have  become  very
common in the literature on anatomic pathology,
because of modern difficulties in obtaining infor-
mation  from  long-term  surveillance  of  patients.
Those problems stem from bureaucratic obstacles
to follow up – principally derived from the U.S.
Health Insurance Portability and Accountability
Act of 1996 [191] – and also the fact that patients
only uncommonly receive continuous care at any
one medical center.

The McGuire Criteria: Template
for Evaluation of “Prognostic” Tests

Dr. William L. McGuire was a professor and the
division  chief  of  Medical  Oncology  at  the
University of Texas-San Antonio for many years
before his untimely death in 1992, and an inter-
nationally renowned researcher on breast cancer
[192]. In the latter part of his career, Dr. McGuire
wrote a landmark editorial on prognostic and pre-
dictive  factors  in  oncology,  as  applied  to  breast
carcinoma  or  any  other  malignant  neoplasm
[193].  That  document  described  several  charac-
teristics of any effective test for clinical forecast-
ing,  which  have  since  become  known  as  the
McGuire criteria (MC) (Fig. 5.27). They not only
address  the  major  laboratory  problems  that  can

Fig. 5.27  The McGuire criteria for evaluation of proposed
prognostic and predictive tests in anatomic pathology and
oncology

82

M.R. Wick et al.

be associated with tests for “forecasting” factors,
but  also  require  that  proof  of  a  true  biological
effect on tumor growth be supplied for each new
marker. The final criterion centers on performing
“definitive”  clinical  studies  of  prognostic  and
predictive  markers.  That  stipulation  touches  on
the problems with study-group composition and
statistical analysis that were mentioned above.

In  applying  the  MC  to  currently  utilized
PPMTs,  where  do  we  stand?  Summaries  are
given for exemplary markers in Figs. 5.28–5.31.

In examining the details, the reader will note that
each of several common breast carcinoma-related
markers – including ERP/PRP, p53, HER-2, and
Ki-67  –  still  is  plagued  by  clinicopathologic
shortcomings vis-à-vis the MC.

HER-2 and Herceptin: An Historical
Review
In  1987,  Slamon  and  colleagues  discovered  a
possible therapeutic target in some breast carci-
nomas  [194].  Those  tumors  overexpressed  the

Fig. 5.28  The McGuire
criteria, applied to estrogen
receptor and progesterone
receptor proteins as detected
immunohistologically

Fig. 5.29  The McGuire
criteria, applied to HER-2
protein as detected
immunohistologically

5  Prognostication and Prediction in Anatomic Pathology

83

Fig. 5.30  The McGuire
criteria, applied to mutant
p53 protein as detected
immunohistologically

Fig. 5.31  The McGuire
criteria, applied to Ki-67
(a proliferation marker) as
detected immunohistologically

HER-2 gene, which codes for one of the epidermal
growth factor receptors (c-erbB-2) with tyrosine
kinase  activity.  Amplification  of  the  HER-2
gene  and  corresponding  overexpression  of  the
receptor  protein  were  found  to  cause  aberrant
intracellular  signaling  and  increased  cell  divi-
sion; that abnormality was present in 20–30% of
stage I UDAs.

Trastuzumab  (Herceptin  ©)  is  a  humanized
monoclonal  antibody,  developed  by  Genentech
Co.,  which  binds  to  the  [195]  extracellular  seg-
ment  of  HER-2  receptor,  blocking  its  coupling
with extracellular mitogens (Fig. 5.32). The result

is growth arrest in the G1 phase of the cell cycle.
Trastuzumab may suppress angiogenesis as well
through unrelated mechanisms, and it may serve
as  the  target  for  antibody-dependent  cellular
cytotoxicity by the host [137].

If it is determined that a breast cancer shows
c-erbB2  amplification  (HER-2+  status;  see
below), the patient is eligible for treatment with
trastuzumab.  Nevertheless,  the  actual  rates  of
success  with  that  agent  are  troubling;  70%  of
HER-2+ patients fail to respond, and resistance to
trastuzumab is developed rapidly in virtually all
cases that do show an initial benefit [196].

84

M.R. Wick et al.

Fig. 5.32  Illustrative diagram depicting how the therapeutic
biological agent, trastuzumab (Herceptin ©), is intended to
block  mitogens  (growth  factors)  that  bind  to  the  HER-2
receptor

Herceptin has been touted as having a “major
impact in the treatment of HER2-positive meta-
static breast cancer” [197]. In addition, the com-
bination  of
trastuzumab  with  conventional
chemotherapeutic agents has been said to increase
survival and response rate, in comparison to the
use of Herceptin alone [198]. Some clinical trials
have concluded that Herceptin reduced the risk of
relapse by 50% when given in the adjuvant set-
ting for 1 year [199, 200]. Nonetheless, the actual
case numbers are more sobering. In one study in
England,  9.4%  of  Herceptin-treated  breast  can-
cers relapsed compared with 17.2% of those who
were  not  given  trastuzumab.  Moreover,  almost
85% of the patients would not have developed a
recurrence  whether  or  not  they  received  trastu-
zumab, and roughly 10% relapsed despite getting
the  drug.  Only  8%  of  cases  showed  a  durable
response to Herceptin [201].

The actual benefits of Herceptin are also not
very  impressive  when  viewed  in  terms  of  all-
cause  mortality.  Large  studies  have  shown  that
one must treat between 25 and 100 patients with
breast cancer to prevent a single death during a
follow-up period of 4 years [202, 203]. For each
patient  who  benefits,  10–25  will  develop
Herceptin-mediated  cardiomyopathy,  and  some
of  those  individuals  will  die  from  congestive
heart  failure.  Finally,  it  is  worth  taking  special
note that the average cost to the healthcare sys-
tem  of  1  years’  treatment  with  trastuzumab  is
approximately $100,000 per patient [204–207].

Additional problems come to light when one
considers the laboratory methods that have been

Fig. 5.33  Immunohistologic staining for HER-2 protein
is predicated on the premise that HER-2 gene amplifica-
tion in breast carcinoma will produce an excess of the cell
membrane-bound protein target

used  to  define  HER-2  amplification  [165–177].
Early  on,  it  was  realized  that  the  humanized
monoclonal antibody, trastuzumab, did not func-
tion well as a diagnostic reagent in immunohis-
tochemical  studies.  Therefore,  alternatively,
heteroantisera  to  HER-2  were  utilized  in  an
immunohistologic assay that was marketed as the
“Herceptest  ©.”  That  evaluation  is  an  indirect
indicator of HER-2 gene amplification, which is
putatively manifest by causing a large amount of
HER-2-related protein to accumulate in the mem-
branes of tumor cells (Fig. 5.33). Problems that
have  been  encountered  with  the  Herceptest
include  fixation-related  variation  in  sensitivity,
suboptimal reproducibility between laboratories,
a subjective threshold for interpreting the test as
“positive,”  and  imperfect  correlation  with  ISH
studies  as  a  true  marker  of  gene  amplification
[208–214].  As  this  chapter  is  being  written  in
mid-2011  –  13  years  after  the  introduction  of
Herceptin  –  position  papers  are  still  being  pub-
lished on the “optimal” way of detecting HER-2
amplification in the laboratory [215–219].

Hence,  we  come  to  a  denouement  regarding
the use of HER-2 as a PPMT, and Herceptin, one
of the most championed single treatments of all
time. In the final analysis, HER-2 gene amplifica-
tion in breast carcinomas – which we believe is

5  Prognostication and Prediction in Anatomic Pathology

85

 trials.  This  averral  is  not  unique  to  the  authors;
indeed, the College of American Pathologists has
convened two multidisciplinary meetings on the
topic of PPMTs, with comparable conclusions to
ours  [228].  On  the  other  hand,  “old”  PPMTs,
when  properly  performed,  are  still  extremely
valuable and trustworthy with regard to clinical
forecasting  [53,  115].  In  specific  reference  to
breast  carcinoma,  these  have  been  enumerated
earlier  in  our  discussion,  including  factors  such
as  recognition  of  “special”  histologic  variants,
accurate measurement of tumor size, BSR grade,
mitotic rate, lymph node substage, and the pres-
ence of angiolymphatic invasion.

As  laboratory  methods  are  refined,  and  as
novel, potentially highly effective biological treat-
ments  become  available  and  are  tailored  to  spe-
cific neoplasms (e.g., imatinib for gastrointestinal
stromal tumors and chronic myelogenous leuke-
mia [1, 2], this situation may well change). At the
present time, however, pathologists must be sys-
tematic  and  critical  in  their  assessments  of  new
PPMTs, with a strict threshold for acceptance of
those  methods  as  “state-of-the-art”  procedures.
As explained in different chapters of this volume,
the evidence-based process starts by formulating
patient-related  questions,  such  as:  What  are  we
trying  to  achieve  with  the  new  laboratory  test?
What are the specific indications of this new test,
based  on  previous  knowledge  about  a  particular
disease? Are the findings obtained with the new
test  going  to  be  used  by  clinicians  to  select  the
treatment of a patient? Is the therapeutic interven-
tion indicated by the information provided by the
tests going to significantly affect the outcome of a
disease,  in  terms  of  survival,  quality  of  life,  or
other  indicators?  Are  there  other  less  expensive
tests  that  could  provide  similar  information?
Application  of  “evidence-based”  principles  will
be a crucial part of the process of realizing the full
potential of the personalized medicine paradigm
in a manner that optimizes the clinical applicabil-
ity of all relevant available information, and dis-
courages  the  use  of  laboratory  tests  and  other
diagnostic procedures that often add only incon-
venience, morbidity, false hopes, confusion, and/
or  unnecessary  cost  to  the  treatment  of  patients
with breast cancer and other diseases.

Fig.  5.34  Fluorescent  in-situ  hybridization  preparation
of ductal breast carcinoma, demonstrating several copies
of the HER-2 gene in each tumor cells and confirming the
presence of gene amplification

best  detected  using  ISH  (Fig.  5.34),  not  the
Herceptest – is treated with an agent that is very
expensive,  with  long-term  effectiveness  in  no
more  than  10%  of  cases  and  potentially  lethal
cardiotoxicity  [51,  220–226].  Elkin  et  al.  [227]
have concluded that the healthcare system “gets a
good  deal”  by  supporting  reflexive  testing  for
HER-2 amplification in all breast cancers. If that
is done by ISH, with an average cost of $350 per
assay, $73,500,000 would be expended yearly in
testing the 195,000 new cases of mammary carci-
noma in the U.S. [227]. Herceptin-related expen-
ditures  for  the  25%  of  new  UDAs  that  are
HER-2+,  with  1  full  year  of  treatment,  add
$3,400,000,000 to the tally for an annual total of
$3,473,500,000. With due respect, we are led to
disagree  with  Elkin  and  coworkers  regarding
their opinions on this topic.

Conclusions

Despite strong assertions to the contrary – both in
the lay press and in medical publications [204–
207]  –  the  current  status  of  “new”  PPMTs  for
human malignancies is a chaotic one with dubi-
ous  cost-effectiveness.  A  lack  of  uniformity
exists in how those tests are performed and inter-
preted, and their “meaning” is often obscured by
poorly  constructed  and  administrated  clinical

86

References

  1.  Waller CF. Imatinib mesylate. Recent Results Cancer

Res. 2010;184:3–20.

  2.  Arifi S, El-Sayadi H, Dufresne A, et al. Imatinib and

solid tumors. Bull Cancer. 2008;95:99–106.

  3.  Broders AC. Squamous cell epithelioma of the lip: a

study of 537 cases. JAMA. 1920;74:656–64.

  4.  Edmundson WF. Microscopic grading of cancer and
its  practical  implications.  Arch  Dermatol  Syphilol.
1948;57:141–50.

  5.  Eker  R,  Weyde  R.  The  significance  of  histological
grading in the prognosis of carcinomas in the true oral
cavity.  Acta  Pathol  Microbiol  Scand.  1949;26:
750–68.

  6.  Ringertz N. Grading of gliomas. Acta Pathol Microbiol

Scand. 1950;27:51–64.

  7.  Goyanna  R,  Torres  ET,  Broders  AC.  Histological
grading  of  malignant  tumors;  Broders’  method.
Hospital (Rio J). 1951;39:791–818.

  8.  Fahmy  A.  Histological  grading  of  urinary  bladder
tumors: a study of 411 urinary bladder biopsies. Urol
Int. 1963;15:358–77.

  9.  Pugh RC. The grading and staging of bladder tumors:
the  Institute  of  Urology  classification.  Br  J  Urol.
1957;29:222–5.

 10. Graham JB. Histologic grading of cancer of the uter-
ine cervix. Surg Gynecol Obstet. 1953;96:331–7.
 11. Price  CH.  The  grading  of  osteogenic  sarcoma.  Br

J Cancer. 1952;6:46–68.

 12. Broders AC, Hargrave R, Meyerding HW. Pathological
features of soft tissue fibrosarcoma with special refer-
ence to the grading of its malignancy. Surg Gynecol
Obstet. 1939;69:267–80.

 13. Denoix PF. Enquate permanent dans les centres anti-

cancereaux. Bull Inst Nat Hyg. 1946;1:70–5.

 14. Dukes CE. The classification of cancer of the rectum.

J Pathol Bacteriol. 1932;35:323–40.

 15. Mathews FS. The ten-year survivors of radical mas-

tectomy. Ann Surg. 1933;98:635–43.

 16. Enneking WF, Kagan A. The implications of “skip”
metastases  in  osteosarcoma.  Clin  Orthop  Relat  Res.
1975;111:33–41.

 17. Kim  TH,  Nesbit  ME,  D’Angio  GD,  Levitt  SH.  The
role of central nervous system irradiation in children
with acute lymphoblastic leukemia. Radiology. 1972;
104:635–41.

 18. Spiers  AS,  Booth  AE,  Firth  JL.  Subcutaneous  cere-
brospinal fluid reservoirs in patients with acute leuke-
mia. Scand J Haematol. 1978;20:289–96.

 19. Taylor  CR.  Immunoperoxidase  techniques:  practical
and theoretical aspects. Arch Pathol Lab Med. 1978;
102:113–21.

 20. Mori M, Ambe K, Adachi Y, et al. Prognostic value of
immunohistochemically-identified  CEA,  SC,  AFP,
and S100 protein positive-cells in gastric carcinoma.
Cancer. 1988;62:534–40.

 21. Kluftinger AM, Robinson BW, Quenville NF, Finley
RJ, Davis NL. Correlation of epidermal growth  factor

M.R. Wick et al.

receptor  and  c-erbB-2  oncogene  product  to  known
prognostic indicators of colorectal cancer. Surg Oncol.
1992;1:97–105.

 22. Rescher N. A philosophical introduction to the theory
of  risk  evaluation  and  measurement.  Washington:
University Press of America; 1983.

 23. Hubbard D. The failure of risk management: why it’s
broken  and  how  to  fix  it.  Baltimore:  John  Hopkins;
2009.

 24. Risk  and  uncertainty.  http://en.wikipedia.org/wiki/

Risk.

 25. Wolf  DC,  Mann  PC.  Confounders  in  interpreting
pathology  for  safety  and  risk  assessment.  Toxicol
Appl Pharmacol. 2005;202:302–8.

 26. Carter  BA,  Page  DL,  O’Malley  FP.  Usual  epithelial
hyperplasia  and  atypical  ductal  hyperplasia.  In:
O’Malley FP, Pinder SE, editors. Foundations in diag-
nostic  pathology  –  breast  pathology.  Churchill
Livingstone: Elsevier; 2006. p. 164–8.

 27. Marchevsky AM, Walts AE, Bose S, et al. Evidence-
based evaluation of the risks of malignancy predicted
by  thyroid  fine-needle  aspiration  biopsies.  Diagn
Cytopathol. 2010;38:252–9.

 28. Cibas ES, Ali SZ. The Bethesda system for reporting
thyroid cytopathology. Thyroid. 2009;19:1159–65.

 29. Prognosis.
References.

http://en.wikipedia.org/wiki/Prognosis#

 30. Hippocrates.  http://en.wikipedia.org/wiki/Prognosis#

References.

 31. Petosiris.  http://en.wikipedia.org/wiki/Petosiris_to_

Nechepso.

 32. Bellazzi R, Zupan B. Predictive data mining in clini-
cal medicine: current issues and guidelines. Int J Med
Inform. 2008;77:81–97.

 33. Breast  cancer  prognosis.  http://www.cancer.gov/can-

certopics/pdq/treatment/breast/Patient.

 34. Prediction. http://en.wikipedia.org/wiki/Prediction.
 35.  Copeland AH. Predictions and probabilities. Erkenntnis.

2007;6:1572–8420.

 36. Pepe  MS.  Evaluating  technologies  for  classifica-
tion  and prediction in medicine. Stat Med. 2005;24:
3687–96.

 37. Mahapatra A. Lung cancer – genomics and personal-
ized medicine. ACS Chem Biol. 2010;18:529–31.
 38. Bohr.  http://www.quotationspage.com/quote/26159.

html.

 39. Personalized  Medicine.  http://www.sciencedaily.com/
news/health_medicine/personalized_medicine/.

 40. Jain KK. Innovative diagnostic technologies and their
significance  for  personalized  medicine.  Mol  Diagn
Ther. 2010;14:141–7.

 41. U.S. Congressional Budget Office. The long-term out-
look for health care spending. http://www.cbo.gov/ftp-
docs/MainText.3.1.shtml. Accessed 12 June 2010.
 42. Traficant J: What 2 liver transplants taught me about
how  to  heal  health  care.  http://www.foxnews.com/
jim-traficant-healthcare. Accessed 12 June 2010.
 43. Drew  EB:  The  quiet  victory  of  the  cigarette  lobby:
how  it  found  the  best  filter  yet  –  Congress.  Atlantic
Monthly. September 1965.

5  Prognostication and Prediction in Anatomic Pathology

87

 44.  Deyo  RA,  Patrick  DL.  Hope  or  hype:  the  obsession
with  medical  advances  and  the  high  cost  of  false
promises. New York: AMACOM; 2005.

 45. Hanby  AM.  The  pathology  of  breast  cancer  and  the
role  of  the  histopathology  laboratory.  Clin  Oncol.
2005;17:234–9.

 46. Korkolis DP, Tsoli E, Fouskakis D, et al. Tumor histol-
ogy  and  stage  but  not  p53,  Her2-neu,  or  cathepsin-D
expression are independent prognostic factors in breast
cancer patients. Anticancer Res. 2004;24:2061–8.
 47.  Bilous M, Ades C, Armes J, et al. Predicting the HER2
status of breast cancer from basic histopathology data:
an  analysis  of  1500  breast  cancers  as  part  of  the
HER2000 International Study. Breast. 2003;12:92–8.
 48. Kim C, Taniyama Y, Paik S. Gene-expression-based
prognostic  and  predictive  markers  for  breast  cancer.
Arch Pathol Lab Med. 2009;133:855–9.

 49. Sandhu R, Parker JS, Jones WD, Livasy CA, Coleman
WB. Microarray-based gene expression profiling for
molecular classification of breast cancer and identifi-
therapy.  Lab  Med.
cation  of  new
2010;41:364–72.

targets  for

 50. Rettig RA, Jacobson PD, Farquhar CM, Aubry WM.
False  hope:  bone  marrow  transplantation  for  breast
cancer. New York: Oxford University Press; 2007.
 51. Gonzalez-Angulo AM, Morales-Vasquez F, Hortobagyi
GN.  Overview  of  resistance  to  systemic  therapy  in
patients  with  breast  cancer.  Adv  Exp  Med  Biol.
2007;608:1–22.

 52. Anonymous. Cancer Statistics, 2009. Oklahoma City:

American Cancer Society; 2009.

 53. Page DL, Jensen RA, Simpson JF. Routinely-available
indicators of prognosis in breast cancer. Breast Cancer
Res Treat. 1998;51:195–208.

 54. Klar M, Foeldi M, Markert S, Gitsch G, Stickeler E,
Watermann D. Good prediction of the likelihood for
sentinel lymph node metastasis by using the MSKCC
nomogram in a German breast cancer population. Ann
Surg Oncol. 2009;16:36–42.

 55. Rosen  PP,  Groshen  S,  Kinne  DW,  Norton  L.  Factors
influencing prognosis in node-negative breast carcinoma:
analysis of 767 T1N0M0./T2N0M0 patients with long-
term followup. J Clin Oncol. 1993;11:2090–100.

 56. Scawn R, Shousha S. Morphologic spectrum of estro-
gen receptor-negative breast carcinoma. Arch Pathol
Lab Med. 2002;126:325–30.

 57. Robertson  JF,  Ellis  IO,  Pearson  D,  Elston  CW,
Nicholson  RI,  Blamey  RW.  Biological  factors  of
prognostic  significance  in  locally-advanced  breast
cancer. Breast Cancer Res Treat. 1994;29:259–64.
 58. Houssami  N,  Ciatto  S,  Ellis  IO,  Ambrogetti  D.
Underestimation of malignancy in breast core-needle
biopsy: concepts and precise overall and category-spe-
cific estimates. Cancer. 2007;109:487–95.

 59. Houssami N, Ciatto S, Bilous M, Vezzosi V, Bianchi
S. Borderline breast core needle histology: predictive
values for malignancy in lesions of uncertain malig-
nant potential. Br J Cancer. 2007;96:1253–7.

 60. Ciatto S, Houssami N, Ambrogetti D, et al. Accuracy
and  underestimation  of  malignancy  of  breast  core

needle biopsy: the Florence experience of over 4000
consecutive  biopsies.  Breast  Cancer  Res  Treat.
2007;101:291–7.

 61.  Lee AH, Denley HE, Pinder SE, et al. Excision biopsy
findings  of  patients  with  breast  needle  core  biopsies
reported  as  suspicious  of  malignancy  or  lesion  of
uncertain  malignant  potential.  Histopathology.  2003;
42:331–6.

 62.  Bonnett  M,  Wallis  T,  Rossmann  M,  et  al.
Histopathologic analysis of atypical lesions in image-
guided  core  breast  biopsies.  Mod  Pathol.  2003;16:
154–60.

 63. Dillon MF, McDermott EW, Hill AD, O’Doherty A,
O’Higgins  N,  Quinn  CM.  Predictive  value  of  breast
lesions of “uncertain malignant potential” and “suspi-
cious  for  malignancy”  determined  by  needle  core
biopsy. Ann Surg Oncol. 2007;14:704–11.

 64. Margenthaler JA, Duke D, Monsees BS, Baraton PT,
Clark  C,  Dietz  JR.  Correlation  between  core  biopsy
and excisional biopsy in breast high-risk lesions. Am
J Surg. 2006;192:534–7.

 65. Simpson  JF,  Page  DL.  Pathology  of  preinvasive  and
excellent-prognosis  breast  cancer.  Curr  Opin  Oncol.
2001;13:426–30.

 66. Page DL. Special types of invasive breast cancer, with
clinical  implications.  Am  J  Surg  Pathol.  2003;27:
832–5.

 67.  Pia-Foschini M, Reis-Filho JS, Eusebi V, Lakhani SR.
Salivary gland-like tumours of the breast: surgical and
molecular pathology. J Clin Pathol. 2003;56:497–506.
 68. Weigel RJ, Ikeda DM, Nowels KW. Primary squamous
the  breast.  South  Med  J.

cell  carcinoma  of
1996;89:511–5.

 69. Van  Hoeven  KH,  Drudis  T,  Cranor  ML,  Erlandson
RA, Rosen PP. Low-grade adenosquamous carcinoma
of  the  breast.  A  clinicopathologic  study  of  32  cases
with  ultrastructural  analysis.  Am  J  Surg  Pathol.
1993;17:248–58.

 70. Toikkanen S. Primary squamous cell carcinoma of the

breast. Cancer. 1981;48:1629–32.

 71. Barnes  PJ,  Boutilier  R,  Chiasson  D,  Rayson  D.
Metaplastic  breast  carcinoma:  clinical-pathologic
characteristics  and  HER2/neu  expression.  Breast
Cancer Res Treat. 2005;91:173–8.

 72. Beatty  JD,  Atwood  M,  Tickman  R,  Reiner  M.
Metaplastic breast cancer: clinical significance. Am J
Surg. 2006;191:657–64.

 73. Foschini  MP,  Krausz  T.  Salivary  gland-type  tumors
of  the  breast:  a  spectrum  of  benign  and  malignant
tumors including “triple negative carcinomas” of low
malignant  potential.  Semin  Diagn  Pathol.  2010;27:
77–90.

 74. Ravdin  PM.  Should  HER2  status  be  routinely  mea-
sured  for  all  breast  cancer  patients?  Semin  Oncol.
1999;26(4 Supp 12):117–23.

 75. Yu JI, Choi DH, Park W, et al. Differences in prognos-
tic  factors  and  patterns  of  failure  between  invasive
micropapillary  carcinoma  and  invasive  ductal  carci-
noma  of  the  breast:  matched  case-control  study.
Breast. 2010;19:231–7.

88

M.R. Wick et al.

   76.  Pettinato  G,  Manivel  JC,  Panico  L,  Sparano  L,
Petrella G. Invasive micropapillary carcinoma of the
breast:  clinicopathologic  study  of  62  cases  of  a
poorly-recognized  variant  with  highly-aggressive
behavior. Am J Clin Pathol. 2004;121:857–66.
   77.  Wade PM Jr, Mills SE, Read M, Cloud W, Lambert
MJ  III,  Smith  RE:  Small-cell  neuroendocrine  (oat-
cell) carcinoma of the breast. Cancer. 1983;52:121–
5; Shin SJ, DeLellis RA, Ying L, Rosen PP. Small-cell
carcinoma  of  the  breast:  a  clinicopathologic  and
immunohistochemical study of nine patients. Am J
Surg Pathol. 2000;24:1231–8; Yamaguchi R, Tanaka
M, Otsuka H, et al. Neuroendocrine small cell carci-
noma  of  the  breast:  report  of  a  case.  Med  Mol
Morphol. 2009;42:58–61.

   78.  Richardson  RL,  Weiland  LH.  Undifferentiated
in  extrapulmonary  sites.

small-cell  carcinomas
Semin Oncol. 1982;9:484–96.

   79.  Moore  JM.  Undifferentiated  adenocarcinoma  of

breast. Tex State J Med. 1953;49:603–4.

   80.  Kirsten F, Chi CH, Leary JA, Ng AB, Hedley DW,
Tattersall MH. Metastatic adeno- or undifferentiated
carcinoma  from  an  unknown  site  –  natural  history
and guidelines for identification of treatable subsets.
Q J Med. 1987;62:143–61.

   81.  Soomro  S,  Shousha  S,  Taylor  P,  Shepard  HJ,
Feldmann M. c-erbB-2 expression in different histo-
logical  types  of  invasive  breast  carcinoma.  J  Clin
Pathol. 1991;44:211–4.

   82.  Martinazzi M, Crivelli F, Zampatti C, Martinazzi S.
Epidermal growth factor receptor immunohistochem-
istry  in  different  histological  types  of  infiltrating
breast carcinoma. J Clin Pathol. 1993;46:1009–10.
   83.  Miller WR, Ellis IO, Sainsbury J, Dixon JM. ABCs
of  breast  diseases:  prognostic  factors.  Br  Med  J.
1994;309:1573–6.

   84.  Mansour  EG,  Ravdin  PM,  Dressler  L.  Prognostic
in  early  breast  carcinoma.  Cancer.

factors
1994;74:381–400.

   85.  Seidman JD, Schnaper LA, Aisner SC. Relationship
of the size of the invasive component of the primary
breast carcinoma to axillary lymph node metastasis.
Cancer. 1995;75:65–71.

   86.  Carter CL, Allen C, Henson DE. Relation of tumor
size,  lymph  node  status,  and  survival  in  24,  740
breast cancer cases. Cancer. 1989;63:181–7.

   87.  Iwasa Y, Nowak MA, Michor F. Evolution of resistance
during clonal expansion. Genetics. 2006;172:2557–66.
   88.  Garcia SB, Norelli M, Wright NA. The clonal origin
and clonal evolution of epithelial tumors. Int J Exp
Pathol. 2000;81:89–116.

   89.  Flanagan  FL,  McDermott  MB,  Barton  PT,  et  al.
Invasive  breast  cancer:  mammographic  measure-
ment. Radiology. 1996;199:819–23.

   90.  Bloom  HJ,  Richardson  WW.  Histological  grading
and prognosis in breast cancer: a study of 1409 cases,
of which 359 have been followed for 15 years. Br J
Cancer. 1957;11:359–77.

   91.  Scarff RW, Torloni H. Histological typing of breast
tumors.  In:  International  histological  classification

of  tumours,  No.  2,  Vol.  2.  Geneva:  World  Health
Organization; 1968. p. 13–20.

   92.  Le Doussal V, Tubiana-Hulin M, Friedman S, Hacene
K, Spyratos F, Brunet M. Prognostic value of histo-
logic  grade  nuclear  components  of  Scarff-Bloom-
Richardson  (SBR):  an  improved  score  modification
based  on  a  multivariate  analysis  of  1262  invasive
ductal breast carcinomas. Cancer. 1989;64:1914–21.
   93.  Simpson JF, Page DL. The role of pathology in pre-
malignancy and as a guide for treatment and progno-
sis in breast cancer. Semin Oncol. 1996;23:428–35.
   94.  Simpson JF, Page DL. Status of breast cancer prog-
nostication based on histopathologic data. Am J Clin
Pathol. 1994;102(Suppl):S3–8.

   95.  Elston CW, Ellis IO. Pathological prognostic factors in
breast  cancer.  I.  The  value  of  histological  grade  in
breast cancer: experience from a large study with long-
term followup. Histopathology. 1991;19:403–10.
   96.  Frierson Jr HF, Wolber RA, Berean KW, et al. Inter-
observer reproducibility of the Nottingham modifi-
cation  of  the  Bloom  and  Richardson  histologic
grading  scheme  for  infiltrating  ductal  carcinoma.
Am J Clin Pathol. 1995;103:195–8.

   97.  Contesso  G,  Jotti  GS,  Bonadonna  G.  Tumor  grade
as a prognostic factor in primary breast cancer. Eur
J Cancer Clin Oncol. 1989;25:403–9.

   98.  Todd JH, Dowle C, Williams MR, et al. Confirmation
of  a  prognostic  index  in  primary  breast  cancer.  Br
J Cancer. 1987;56:489–92.

   99.  Imber  G.  Genius  on  the  edge.  New  York:  Kaplan;

2010.

 100.  Williams  BC.  The  history  of  mastectomy.  http://
www.ehow.com/about_5505904_history-mastectomy.
html. Accessed 19 June 2010.

 101.  Halsted WS. The results of radical operations for the
cure  of  carcinoma  of  the  breast  performed  at  the
Johns Hopkins Hospital from June 1889 to January
1894. Johns Hopkins Hosp Rep. 1894;4:297–327.
 102.  Bland CS. The Halsted mastectomy: present illness
and past history. West J Med. 1981;134:549–55.
 103.  Wick MR. Principles of evidence-based medicine as
applied  to  “sentinel”  lymph  node  biopsies.  Pathol
Case Rev. 2008;13:102–8.

 104.  Fisher  B,  Wolmark  N,  Redmond  C,  et al.  Findings
from  NSABP  Protocol  No.  B-04:  comparison  of
radical  mastectomy  with  alternative  treatments.  II.
The  clinical  and  biologic  significance  of  medial-
central breast cancers. Cancer. 1981;48:1863–72.
 105.  Sanghani M, Balk EM, Cady B. Impact of axillary
lymph node dissection on breast cancer outcome in
clinically node negative patients: a systematic review
and meta-analysis. Cancer. 2009;115:1613–20.
 106.  Collan  YU,  Eskelinen  MJ,  Nordling  SA,  et  al.
Prognostic studies in breast cancer – multivariate com-
bination  of  nodal  status,  proliferation  index,  tumor
size, and DNA ploidy. Acta Oncol. 1994;33:873–8.
 107.  Quiet CA, Ferguson DJ, Weichselbaum RR, Hellman
S. Natural history of node-positive breast cancer: the
curability of small cancers with a limited number of
positive nodes. J Clin Oncol. 1996;14:3105–11.

5  Prognostication and Prediction in Anatomic Pathology

89

 108.  Beal SH, Martinez SR, Canter RJ, Chen SL, Khatri
VP, Bold RJ. Survival in 12, 653 breast cancer patients
with extensive axillary lymph node metastasis in the
anthracycline era. Med Oncol. 2010;27(4):1420–4.
 109.  Sahin  AA,  Guray  M,  Hunt  KK.  Identification  and
biologic significance of micrometastases in axillary
lymph nodes in patients with invasive breast cancer.
Arch Pathol Lab Med. 2009;133:869–78.

 110.  Hansen  NM,  Grube  B,  Ye  X,  Turner  RR,  Brenner
RJ, Sim MS, et al. Impact of micrometastases in the
sentinel node of patients with invasive breast cancer.
J Clin Oncol. 2009;27:4679–84.

 111.  Viale  G,  Dell’Orto  P,  Biasi  MO,  et  al.  Comparative
evaluation of an extensive histopathologic examination
and a real-time reverse-transcription-polymerase chain
reaction  assay  for  mammaglobin  and  cytokeratin-
19 on axillary sentinel lymph nodes of breast carcinoma
patients. Ann Surg. 2008;247:136–42.

 112.  Douglas-Jones AG, Woods V. Molecular assessment
of  sentinel  lymph  nodes  in  breast  cancer  manage-
ment. Histopathology. 2009;55:107–13.

 113.  Karam AK, Hsu M, Patil S, et al. Predictors of com-
pletion  axillary  lymph  node  dissection  in  patients
with positive sentinel lymph nodes. Ann Surg Oncol.
2009;16:1952–8.

 114.  Pernas S, Gil M, Benítez A, et al. Avoiding axillary
treatment in sentinel lymph node micrometastases of
breast  cancer:  a  prospective  analysis  of  axillary  or
distant recurrence. Ann Surg Oncol. 2010;17:772–7.
 115.  Gurleyik G, Gurleyik E, Aker F, et al. Lymphovascular
invasion, as a prognostic marker in patients with inva-
sive breast cancer. Acta Chir Belg. 2007;107:284–7.

 116.  Nime FA, Rosen PP, Thaler HT, Ashikari R, Urban
JA. Prognostic significance of tumor emboli in intra-
mammary  lymphatics  in  patients  with  mammary
carcinoma. Am J Surg Pathol. 1977;1:25–30.
 117.  Rosen PP. Tumor emboli in intramammary lymphat-
ics in breast carcinoma: pathologic criteria for diag-
nosis  and  clinical  significance.  Pathol  Annu.
1983;18(Pt 2):215–32.

 118.  Lee AH, Pinder SE, Macmillan RD, Mitchell M, Ellis
IO, Elston CW, et al. Prognostic value of lymphovascu-
lar invasion in women with lymph node negative inva-
sive breast carcinoma. Eur J Cancer. 2006;42:357–62.

 119.  Trudeau  ME,  Pritchard  KI,  Chapman  JA,  et  al.
Prognostic  factors  affecting  the  natural  history  of
node-negative  breast  cancer.  Breast  Cancer  Res
Treat. 2005;89:35–45.

 120.  de  Mascarel

I,  MacGrogan  G,  Debled  M,
Sierankowski  G,  Brouste  V,  Mathoulin-Pélissier  S,
et al. D2-40 in breast cancer: should we detect more
vascular emboli? Mod Pathol. 2009;22:216–22.
 121.  Kahn  HJ,  Marks  A.  A  new  monoclonal  antibody,
D2-40,  for  detection  of  lymphatic  invasion  in  pri-
mary tumors. Lab Invest. 2002;82:1255–12557.
 122.  Arnaout-Alkarain A, Kahn HJ, Narod SA, Sun PA,
Marks  AN.  Significance  of  lymph  vessel  invasion
identified  by  the  endothelial  lymphatic  marker
D2-40 in node negative breast cancer. Mod Pathol.
2007;20:183–91.

 123.  Almholt  K,  Nielsen  BS,  Frandsen  TL,  et  al.
Metastasis of transgenic breast cancer in plasmino-
gen  activator
inhibitor-1  gene-deficient  mice.
Oncogene. 2003;22:4389–97.

 124.  Kilinc  N,  Yaldiz  M.  p53,  c-erbB-2  expression,  and
steroid  hormone  receptors  in  breast  carcinoma:
correlations  with  histopathological  parameters.  Eur
J Gynaecol Oncol. 2004;25:606–10.

 125.  Reed W, Hannisdal E, Boehler PJ, Gundersen S, Host
H, Marthin J. The prognostic value of p53 and c-erbB-2
immunostaining is overrated for patients with lymph
node-negative breast carcinoma: a multivariate analy-
sis  of  prognostic  factors  in  613  patients  with  a  fol-
lowup of 14-30 years. Cancer. 2000;88:804–13.
 126.  Chiu CG, Masoudi H, Leung S, et al. HER-3 overex-
pression  in  prognostic  of  reduced  breast  cancer
survival:  a  study  of  4046  patients.  Ann  Surg.
2010;251:1107–16.

 127.  Blows FM, Driver KE, Schmidt MK, et al. Subtyping
of breast cancer by immunohistochemistry to inves-
tigate a relationship between subtype and short and
long  term  survival:  a  collaborative  analysis  of  data
for  10,  159  cases  from  12  studies.  PLoS  Med.
2010;7(5):e1000279.

 128.  Putti TC, El-Rehim DM, Rakha EA, et al. Estrogen
receptor-negative  breast  carcinomas:  a  review  of
morphology  and
immunophenotypical  analysis.
Mod Pathol. 2005;18:26–36.

 129.  Rakha  EA,  El-Sayed  ME,  Green  AR,  Lee  AH,
Robertson JF, Ellis IO. Prognostic markers in triple-
negative breast cancer. Cancer. 2007;109:25–32.
 130.  Erdem O, Dursun A, Coskun U, Gunel N. The prog-
nostic value of p53 and c-erbB-2 expression, prolif-
erative  activity,  and  angiogenesis  in  node-negative
breast carcinoma. Tumori. 2005;91:46–52.

 131.  Horita K, Yamaguchi A, Hirose K, et al. Prognostic
factors affecting disease-free survival rate following
surgical  resection  of  primary  breast  cancer.  Eur
J Histochem. 2001;45:73–84.

 132.  Lialiaris TS, Georgiou G, Sivridis E, et al. Prognostic
and predictive factors of invasive ductal breast carci-
nomas. J BUON. 2010;15:79–88.

 133.  Lai P, Tan LK, Chen B. Correlation of HER-2 status
with estrogen and progesterone receptors and histo-
logic features in 3, 655 invasive breast carcinomas.
Am J Clin Pathol. 2005;123:541–6.

 134.  Cao XX, Xu JD, Liu XL, et al. RACK1: a superior
independent  predictor  for  poor  clinical  outcome  in
breast cancer. Int J Cancer. 2009;127(5):1172–9.
 135.  Haupt B, Ro JY, Schwartz MR. Basal-like breast car-
cinoma: a phenotypically distinct entity. Arch Pathol
Lab Med. 2010;134:130–3.

 136.  Mirza  M,  Shaughnessy  E,  Hurley  JK,  et  al.
Osteopontin-c is a selective marker of breast cancer.
Int J Cancer. 2008;122:889–97.

 137.  Sigurdsson H, Baldetorp B, Borg A, et al. Indicators
of prognosis in node-negative breast cancer. N Engl
J Med. 2990;322:1045–53.

 138.  Sasano  H.  Histopathological  prognostic  factors
in  early  breast  carcinoma:  an  evaluation  of  cell

90

M.R. Wick et al.

 proliferation  in  carcinoma  cells.  Expert  Opin
Investig Drugs. 2010;19 Suppl 1:S5–11.

 139.  Reis-Filho  JS,  Lakhani  SR.  Breast  cancer  special
types: why bother? J Pathol. 2008;216:394–8.
 140.  Weigelt B, Geyer FC, Natrajan R, et al. The molecu-
lar underpinning of lobular histological growth pat-
tern:  a  genome-wide  transcriptomic  analysis  of
invasive lobular carcinomas and grade- and molecu-
lar  subtype-matched  invasive  ductal  carcinomas  of
no special type. J Pathol. 2010;220:45–57.

 141.  Schnitt SJ. Classification and prognosis of invasive
breast  cancer:  from  morphology  to  molecular  tax-
onomy. Mod Pathol. 2010;23 Suppl 2:S60–4.
 142.  Schmidt C. Assays that predict outcomes make slow
progress  toward  prime  time.  J  Natl  Cancer  Inst.
2010;102:677–9.

 143.  Thuerigen O, Schneeweiss A, Toedt G, et al. Gene
expression signature predicting pathologic complete
response with gemcitabine, epirubicin, and docetaxel
in  primary  breast  cancer.  J  Clin  Oncol.  2006;24:
1839–45.

 144.  Végran F, Boidot R, Coudert B, et al. Gene expres-
sion profile and response to trastuzumab-docetaxel-
based  treatment  in  breast  carcinoma.  Br  J  Cancer.
2009;101:1357–64.

 145.  Bohn OL, Nasir I, Brufsky A, et al. Biomarker pro-
file in breast carcinomas presenting with bone metas-
tasis. Int J Clin Exp Pathol. 2009;3:139–46.

 146.  Nuyten DS, Kreike B, Hart AA, et al. Predicting a local
recurrence  after  breast-conserving  therapy  by  gene
expression profiling. Breast Cancer Res. 2006;8:R62.

 147.  Saal LH, Johansson P, Holm K, et al. Poor prognosis
in carcinoma is associated with a gene expression sig-
nature  of  aberrant  PTEN  tumor  suppressor  pathway
activity. Proc Natl Acad Sci USA. 2007;104:7564–9.
 148.  Karlsson  E,  Delle  U,  Danielsson  A,  et  al.  Gene
expression  variation  to  predict  10-year  survival  in
lymph-node-negative  breast  cancer.  BMC  Cancer.
2008;8:254.

 149.  Konstantinovsky S, Smith Y, Zilber S, et al. Breast
carcinoma  cells  in  primary  tumors  and  effusions
have  different  gene  array  profiles.  J  Oncol.
2010;2010:969084.

 150.  Staaf  J,  Ringnér  M,  Vallon-Christersson  J,  et  al.
Identification  of  subtypes  in  human  epidermal
growth  factor  receptor  2–positive  breast  cancer
reveals  a  gene  signature  prognostic  of  outcome.
J Clin Oncol. 2010;28:1813–20.

 151.  Charpin  C,  Secq  V,  Giusiano  S,  et  al.  A  signature
predictive of disease outcome in breast carcinomas,
immunocytochemical
identified  by  quantitative
assays. Int J Cancer. 2009;124:2124–34.

 152.  Kreipe  HH,  Ahrens  P,  Christgen  M,  Lehmann  U,
Langer F. Beyond staging, typing, and grading: new
challenges  in  breast  cancer  pathology.  Pathologe.
2010;31:54–9.

 153.  Giusiano  S,  Secq  V,  Carcopino  X,  et  al.
Immunohistochemical  profiling  of  node  negative
breast  carcinomas  allows  prediction  of  metastatic
risk. Int J Oncol. 2010;36:889–98.

 154.  Cox G, Jones JL, Andi A, Waller DA, O’Byrne KJ.
A biological staging model for operable non-small-
cell lung cancer. Thorax. 2001;56:561–6.

 155.  Li AR, Chitale D, Riely GJ, et al. EGFR mutations in
lung  adenocarcinomas:  clinical  testing  experience
and  relationship  to  EGFR  gene  copy  number  and
immunohistochemical  expression.  J  Mol  Diagn.
2008;10:242–8.

 156.  Sholl LM, Xiao Y, Joshi V, et al. EGFR mutation is a
better predictor of response to tyrosine kinase inhibi-
tors  in  non-small  cell  lung  carcinoma  than  FISH,
CISH, and immunohistochemistry. Am J Clin Pathol.
2010;133:922–34.

 157.  Anonymous.  Types  of  data.  http://www.changing-
minds.org/explanations/research/measurements/
types-data.htm. Accessed 19 June 2010.

 158.  Stroup  RM,  Pinkus  GS.  S100-immunoreactivity  in
primary  and  metastatic  carcinoma  of  the  breast:  a
potential source of error in immunodiagnosis. Hum
Pathol. 1988;19:949–53.

 159.  Wick  MR,  Patterson  JW.  Multimodal  pathologic
diagnosis  of  malignant  melanoma:  integration  of
morphology,  histochemistry,  immunohistology,  and
electron microscopy. J Histotechnol. 2003;26:253–8.
 160.  Wick MR, Lillemoe TJ, Copland GT, Swanson PE,
Manivel  JC,  Kiang  DT.  Gross  cystic  disease  fluid
protein-15 as a marker for breast cancer. Hum Pathol.
1989;20:281–7.

 161.  Miller  RT,  Swanson  PE,  Wick  MR.  Fixation  and
epitope retrieval in diagnostic immunohistochemistry:
a concise review with practical considerations. Appl
Immunohistochem Mol Morphol. 2000;8:228–35.
 162.  Idikio HA. Immunohistochemistry in diagnostic sur-
gical pathology: contributions of protein life-cycle,
use of evidence-based methods, and data normaliza-
immunohistochemical
tion  on
stains. Int J Clin Exp Pathol. 2010;3:169–76.

interpretation  of

 163.  Allred  DC,  Carlson  RW,  Berry  DA,  et  al.  NCCN
Task Force Report: estrogen receptor and progester-
one receptor testing in breast cancer by immunohis-
tochemistry. J Natl Compr Cancer Netw. 2009;Suppl
6:S1–21.

 164.  Canadian  Association  of  Pathologists-Association
canadienne  des  pathologistes  National  Standards
Committee, Torlakovic EE, Riddell R, Banerjee D,
et  al.  Best  practice  recommendations  for  standard-
ization  of  immunohistochemistry  tests.  Am  J  Clin
Pathol. 2010;133:354–65.

 165.  Jacobs TW, Gown AM, Yaziji H, Barnes MJ, Schnitt
SJ. Comparison of fluorescence in situ hybridization
and  immunohistochemistry  for  the  evaluation  of
HER-2/neu
in  breast  cancer.  J  Clin  Oncol.
1999;17:1974–82.

 166.  Kakar S, Puangsuvan N, Stevens JM, et al. HER-2/
neu  assessment  in  breast  cancer  by  immunohis-
tochemistry  and  fluorescence  in  situ  hybridization:
comparison of results and correlation with survival.
Mol Diagn. 2000;5:199–207.

 167.  Van de Vijver MJ. Assessment of the need and appro-
priate  method  for  testing  for  the  human  epidermal

5  Prognostication and Prediction in Anatomic Pathology

91

growth  factor  receptor-2  (HER2).  Eur  J  Cancer.
2001;37 Suppl 1:11–7.

 168.  McCormick SR, Lillemoe TJ, Beneke J, Schrauth J,
Reinartz J. HER2 assessment by immunohistochem-
ical analysis and fluorescence in situ hybridization:
comparison of HercepTest and PathVysion commer-
cial assays. Am J Clin Pathol. 2002;117:935–43.
 169.  Lal P, Salazar PA, Hudis CA, Ladanyi M, Chen B.
HER-2  testing  in  breast  cancer  using  immunohis-
tochemical analysis and fluorescence in-situ hybrid-
ization:  a  single-institution  experience  of  2,  279
cases and comparison of dual-color and single-color
scoring. Am J Clin Pathol. 2004;121:631–6.

 170.  Ross  JS,  Fletcher  JA,  Bloom  KJ,  et  al.  HER-2/neu
in  breast  cancer.  Am  J  Clin  Pathol.

testing
2003;120(Suppl):S53–71.

 171.  Mrozkowiak A, Olszewski WP, Piascik A, Olszewski
WT.  HER2  status  in  breast  cancer  determined
by  IHC  and  FISH:  comparison  of  the  results.  Pol
J Pathol. 2004;55:165–71.

 172.  Ellis  CM,  Dyson  MJ,  Stephenson  TJ,  Maltby  EL.
HER2 amplification status in breast cancer: a com-
parison between immunohistochemical staining and
fluorescence in situ hybridization using manual and
automated quantitative image analysis scoring tech-
niques. J Clin Pathol. 2005;58:710–4.

 173.  Dolan  M,  Snover  DC.  Comparison  of  immunohis-
tochemical  and  fluorescence  in  situ  hybridization
assessment of HER-2 status in routine practice. Am
J Clin Pathol. 2005;123:766–70.

 174.  Benohr  P,  Henkel  V,  Speer  R,  et  al.  HER-2/neu
expression in breast cancer – a comparison of different
diagnostic  methods.  Anticancer  Res.  2005;25(3B):
1895–900.

 175.  Egervari K, Szollosi Z, Nemes Z, Kaczur V. Comparison
of  immunohistochemical  and  fluorescence  in  situ
hybridization  assessment  of  HER-2  status  in  routine
practice. Am J Clin Pathol. 2006;125:155–6.

 176.  Sui W, Ou M, Chen J, et al. Comparison of immuno-
histochemistry  (IHC)  and  fluorescence  in  situ
hybridization (FISH) assessment for HER-2 status in
breast cancer. World J Surg Oncol. 2009;7:83.
 177.  Mayr D, Heim S, Weyrauch K, et al. Chromogenic in
situ hybridization for HER-2/neu-oncogene in breast
cancer: comparison of a new dual-color chromoge-
nic in situ hybridization with immunohistochemistry
and fluorescence in situ hybridization. Histopathology.
2009;55:716–23.

 178.  Krug LM, Crapanzano JP, Azzoli CG, et al. Imatinib
mesylate lacks activity in small cell lung carcinoma
expression  c-kit  protein:  a  phase  II  clinical  trial.
Cancer. 2005;103:2128–31.

 179.  Koch CA, Gimm O, Vortmeyer AO, et al. Does the
expression  of  c-kit  (CD117)  in  neuroendocrine
tumors represent a target for therapy? Ann NY Acad
Sci. 2006;1073:517–26.

 180.  Sharma  S.  Applied  multivariate

techniques.

Hoboken: Wiley; 1995.

 181.  Rasmussen BB, Thorpe SM, Norgaard T, Rasmussen
J,  Agdal  N,  Rose  C.  Immunohistochemical  steroid

receptor  detection  in  frozen  breast  cancer  tissue:  a
multicenter investigation. Acta Oncol. 1988;27:757–60.
 182.  Andersen J, Thorpe SM, King WJ, et al. The prog-
nostic  value  of
immunohistochemical  estrogen
receptor  analysis  in  paraffin-embedded  and  frozen
sections versus that of steroid-binding assays. Eur J
Cancer. 1990;25:442–9.

 183.  Wilbur DC, Willis J, Mooney RA, Fallon MA, Moynes
R,  di  Sant’Agnese  PA.  Estrogen  and  progesterone
receptor detection in archival formalin-fixed, paraffin-
embedded tissue from breast carcinoma: a comparison
of  immunohistochemistry  with  the  dextran-coated
charcoal assay. Mod Pathol. 1992;5:79–84.

 184.  Valgardsdottir R, Tryggvadottir L, Steinarsdottir M,
et al. Genomic instability and poor prognosis associ-
ated  with  abnormal  TP53  in  breast  carcinomas:
molecular  and
immunohistochemical  analysis.
APMIS. 1997;105:121–30.

 185.  Sjogren  S,  Inganas  M,  Norberg  T,  et  al.  The  p53
gene  in  breast  cancer:  prognostic  value  of  comple-
mentary DNA sequencing versus immunohistochem-
istry. J Natl Cancer Inst. 1996;88:173–82.

 186.  Thorlacius S, Thorgilsson B, Bjornsson J, et al. TP53
mutations  and  abnormal  p53  protein  staining  in
breast carcinomas related to prognosis. Eur J Cancer.
1995;31A:1856–61.

 187.  Umekita Y, Kobayashi K, Saheki T, Yoshida H. Nuclear
accumulation of p53 correlates with mutations in the p53
gene  on  archival  paraffin-embedded  tissues  of  human
breast cancer. Jpn J Cancer Res. 1994;85:825–30.
 188.  MacGeoch  C,  Barnes  DM,  Newton  JA,  et  al.  p53
protein detected by immunohistochemical staining is
not always mutant. Dis Markers. 1993;11:239–50.

 189.  Dunn  JM,  Hastrich  DJ,  Newcomb  P,  Webb  JC.
Maitland,  Farndon  JR:  Correlation  between  p53
mutations and antibody staining in breast carcinoma.
Br J Surg. 1993;80:1410–2.

 190.  Miles J: Getting the sample size right: a brief intro-
duction to power analysis. http://www.jeremymiles.
co.uk/misc/power/. Accessed 19 June 2010.

 191.  The Health Insurance Portability and Accountability
Act. http://en.wikipedia.org/wiki/Health_Insurance_
Portability_and_Accountability_Act/.  Accessed  19
June 2010.

 192.  Anonymous.  In-Memoriam:  William  L.  McGuire.

Breast Cancer Res Treat 1992;23:7–15.

 193.  McGuire WL. Breast cancer prognostic factors: eval-
uation guidelines. J Natl Cancer Inst. 1991;83:154–5.
 194.  Slamon DJ, Clark GM, Wong SG, Levin WJ, Ullrich A,
McGuire WL. Human breast cancer: correlation of
relapse and survival with amplification of the HER-2/
neu oncogene. Science. 1987;235:177–82.

 195.  Trastuzumab. http://en.wikipedia.org/wiki/trastuzumab/.

Accessed 19 June 2010.

 196.  Kute T, Lack CM, Willingham M, et al. Development
of  herceptin  resistance  in  breast  cancer  cells.
Cytometry. 2004;57A:86–93.

 197.  Tan  AR,  Swain  SM.  Ongoing  adjuvant  trials  with
trastuzumab in breast cancer. Semin Oncol. 2002;30
(5 Suppl 16):54–64.

92

M.R. Wick et al.

 198.  Nahta R, Esteva FJ. HER-2-targeted therapy: lessons
learned  and  future  directions.  Clin  Cancer  Res.
2003;9:5038–48.

 199.  Romond EH, Perez EA, Bryant J, et al. Trastuzumab
plus  adjuvant  chemotherapy  for  operable  HER-2-
positive  breast  cancer.  N  Engl  J  Med.  2005;353:
1673–84.

 200.  Piccart-Gebhart  MJ,  Procter  M,  Leyland-Jones  B,
et  al.  Trastuzumab  after  adjuvant  chemotherapy  in
HER-2-positive breast cancer. N Engl J Med. 2005;
353:1659–72.

 201.  Lewis R, Bagnall AM, Forbges C, et al. The clinical
effectiveness of trastuzumab for breast cancer: a sys-
tematic review. Health Technol Assess. 2002;6:1–71.
 202.  http://www.bpac.org/nz/magazine/2007/april/her-

ceptin.asp. Accessed 19 June 2010.

 203.  http://www.sws-pct.nhs.uk/PEC/2005/061205/

Enc_08.pdf. Accessed 19 June 2010.

 204.  Anonymous: Herceptin or trastuzumab: efficacy and
side  effects.  http://healthlifeandstuff.com/2009/12/
herceptin-or-trastuzumab-efficacy-side-effects/.
Accessed 19 June 2010.

 205.  Abelson J, Collins PA. Media hyping and the “hercep-
tin  access  story:”  an  analysis  of  Canadian  and  UK
newspaper coverage. Healthc Policy. 2009;4:e113–28.
 206.  Hedgecoe AM. It’s money that matters: the financial
context  of  ethical  decision-making  in  modern  bio-
medicine. Sociol Health Illn. 2006;28:768–84.
 207.  Williams  C,  Brunskill  S,  Altman  D,  et  al.  Cost-
effectiveness of using prognostic information to select
women with breast cancer for adjuvant systemic ther-
apy. Health Technol Assess. 2006;10:1–204.

 208.  Nakhleh  RE,  Grimm  EE,  Idowu  MO,  Souers  RJ,
Fitzgibbons  PL.  Laboratory  compliance  with  the
American  Society  of  Clinical  Oncology/college  of
American Pathologists guidelines for human epider-
mal  growth  factor  receptor  2  testing:  a  College  of
American  Pathologists  survey  of  757  laboratories.
Arch Pathol Lab Med. 2010;134:728–34.

 209.  Sauter G, Lee J, Bartlett JM, Slamon DJ, Press MF.
Guidelines  for  human  epidermal  growth  factor
receptor-2  testing:  biologic  and  methodologic  con-
siderations. J Clin Oncol. 2009;27:1323–33.

 210.  Turashvili G, Leung S, Turbin D, et al. Interobserver
reproducibility of HER2 immunohistochemical assess-
ment and concordance with fluorescent in situ hybrid-
ization  (FISH):  pathologist  assessment  compared  to
quantitative image analysis. BMC Cancer. 2009;9:165.
 211.  Jacobs  TW,  Prioleau  JE,  Stillman  IE,  Schnitt  SJ.
Loss of tumor marker-immunostaining intensity on
stored paraffin slides of breast cancer. J Natl Cancer
Inst. 1996;88:1054–9.

 212.  Mandrekar  SJ,  Sargent  DJ.  Predictive  biomarker
validation in practice: lessons from real trials. Clin
Trials. 2010;7(5):567–73.

 213.  Richter-Ehrenstein C, Muller S, Noske A, Schneider
A. Diagnostic accuracy and prognostic value of core
biopsy in the management of breast cancer: a series
of 542 patients. Int J Surg Pathol. 2009;17:323–6.

 214.  Nassar A, Radhakrishnan A, Cabrero IA, Cotsonis GA,
Cohen  C.  Intratumoral  heterogeneity  of  immunohis-
tochemical  marker  expression  in  breast  carcinoma:  a
tissue microarray-based study. Appl Immunohistochem
Mol Morphol. 2010;18(5):433–41.

 215.  Powell WC, Hicks DG, Prescott N, et al. A new rab-
bit monoclonal antibody (4B5) for the immunohis-
tochemical (IHC) determination of the HER2 status
in  breast  cancer:  comparison  with  CB11,  fluores-
cence in situ hybridization (FISH), and interlabora-
tory  reproducibility.  Appl  Immunohistochem  Mol
Morphol. 2007;15:94–102.

 216.  Wasielewski R, Hasselmann S, Ruschoff J, Fisseler-
Eckhoff A, Kreipe H. Proficiency testing of immu-
nohistochemical biomarker assays in breast cancer.
Virchows Arch. 2008;453:537–43.

 217.  Terry J, Torlakovic EE, Garratt J, et al. Implementation
of a Canadian external quality assurance program for
breast  cancer  biomarkers:  an  initiative  of  Canadian
Quality Control in immunohistochemistry (cIQc) and
Canadian Association of Pathologists (CAP) National
Standards  Committee/Immunohistochemistry.  Appl
Immunohistochem Mol Morphol. 2009;17:375–82.

 218.  Hanley  KZ,  Birdsong  GG,  Cohen  C,  Siddiqui  MT.
Immunohistochemical  detection  of  estrogen  recep-
tor,  progesterone  receptor,  and  human  epidermal
growth factor receptor 2 expression in breast carci-
nomas: comparison on cell block, needle-core, and
tissue  block  preparations.  Cancer  Cytopathol.
2009;117:279–88.

 219.  Liu  YH,  Xu  FP,  Rao  JY,  et  al.  Justification  of  the
change from 10% to 30% for the immunohistochem-
ical HER2 scoring criterion in breast cancer. Am J
Clin Pathol. 2009;132:74–9.

 220.  Davoli A, Hocevar BA, Brown TL. Progression and
treatment  of  HER2-positive  breast  cancer.  Cancer
Chemother Pharmacol. 2010;65:611–23.

 221.  Walker JR, Singal PK, Jassal DS. The art of healing
broken hearts in breast cancer patients: trastuzumab
and heart failure. Exp Clin Cardiol. 2009;14:e62–7.

 222.  Köninki  K,  Barok  M,  Tanner  M,  et  al.  Multiple
molecular mechanisms underlying trastuzumab and
lapatinib  resistance  in  JIMT-1  breast  cancer  cells.
Cancer Lett. 2010;294:211–9.

 223.  Tagliabue  E,  Balsari  A,  Campiglio  M,  Pupa  SM.
HER2  as  a  target  for  breast  cancer  therapy.  Expert
Opin Biol Ther. 2010;10:711–24.

 224.  Geiger S, Lange V, Suhl P, Heinermann V, Stemmler
HJ. Anticancer therapy-induced cardiotoxicity: review
of the literature. Anticancer Drugs. 2010;21:578–90.

 225.  Baselga  J.  Herceptin  alone  or  in  combination  with
chemotherapy  in  the  treatment  of  HER2-positive
metastatic  breast  cancer:  pivotal  trials.  Oncology.
2001;61 Suppl 2:14–21.

 226.  Dawood S, Broglio K, Buzdar AU, Hortobagyi GN,
Giordano SH. Prognosis of women with metastatic
breast cancer by HER2 status and trastuzumab treat-
ment:  an  institutional-based  review.  J  Clin  Oncol.
2010;28:92–8.

5  Prognostication and Prediction in Anatomic Pathology

93

 227.  Elkin  EB,  Weinstein  MC,  Winer  EP,  Kuntz  KM,
Schnitt SJ, Weeks JC. HER-2 testing and trastuzumab
therapy for metastatic breast cancer: a cost-effectiveness
analysis. J Clin Oncol. 2004;22:854–63.

 228.  Fitzgibbons PL, Page DL, Weaver D, et al. Prognostic
factors  in  breast  cancer.  College  of  American
Pathologists Consensus Statement 1999. Arch Pathol
Lab Med. 2000;124:966–78.

Principles of Classification
and Diagnosis in Anatomic
Pathology and the Inevitability
of Problem Cases

Michael Hendrickson

6

Keywords
Classification in anatomic pathology • Diagnostic principles in anatomic
pathology  •  Problem  cases  in  anatomic  pathology  •  Complexity  of
individual neoplasms • Neoplastic kinds as family resemblance groups
• Oncopathological reality

In this chapter, I set out a framework for thinking
critically  about  oncopathological   classification
and diagnosis (C&D), organizing the  discussion
around the central elements of the classification
process: (1) the individual cases being classified
(e.g.,  the  individual   neoplasm,  INeop),  (2)  the
groups  formed  by  aggregating   individual  cases
similar in relevant respects (the neoplastic kind,
KNeop), and (3) the classifier-diagnostician whose
essential contribution is  evident at every stage of
the process. Current research in molecular-genetic
oncology suggests that INeop’s are best regarded as
evolutionary processes, that the groups formed by
aggregating them with respect to their histogene-
sis are extensionally indeterminate family resem-
blance groups and that our view of the world of
neoplasms at any given time results both from the
way the world is and, equally, how we chose to
visualize and conceptualize it.

We  and  the  world  co-create  oncological
 reality  and  problem  cases  –  in-between  cases,
hybrid cases, and novel cases – are instructive

M. Hendrickson (*)
Department of Pathology, Stanford University Medical
Center, Stanford, CA 94305, USA
e-mail: hendrickson@stanford.edu

in  pointing  to  the  inevitable  failure  of  static
classificatory grids to do justice to the complex-
ity of the individual neoplasm. This perspective
has   fundamental  consequences  for  the  issues
of  concern  to  contemporary  evidence  based
pathology (EBP).

Evidence-Based Pathology
and Classification and Diagnostic
Practices in Anatomic Pathology

Evidence-based  medicine  (EBM)  is  a  conten-
tious topic with, for some, a problematic name.
It  is  presented  by  its  advocates  as  the  long-
needed  antidote  to  “clinical  judgment”  with
what  they  take  to  be  its  subjective,  anecdotal
character and its privileging of uncodified clini-
cal  expertise  over  published  population-based
experience. The antidote to anecdote offered by
EBM  is  the  statistical  analysis  of  populations.
The  fruits  of  this  approach  are  the  evidence
 provided  by  interventional   studies  (e.g.,  con-
trolled  clinical  trials)  and   observational  studies
(e.g.,  techniques  of  clinical  epidemiology).
Integration of such studies yields, among other
things,  clinical  guidelines  of  various  sorts  for

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_6, © Springer Science+Business Media, LLC 2011

95

96

M. Hendrickson

medical  conditions  and  a  critical  approach  to
biological  markers  used  either  for  diagnosis  or
for  hazarding  a  clinical  prediction  (i.e.,  risk,
prognostic or predictive markers).

the

On  the  other  hand,  EBM’s  detractors  draw
attention  to  the  fundamental  problem  of  popula-
tion-based studies – such studies tell us about pop-
ulations,  not  individuals.  Thus,  while  clinical
judgment  and  case-based  reasoning  (CBR)  self-
consciously attend to the particularity and unique-
ness  of
individual  under  consideration,
population-based studies scrupulously strip away
all  of  that  detail  replacing  it  with  a  handful  of
observed features. This is in service of generating
stable,  statistically  credible  population  averages.
Paradoxically,  the  use  of  EBM  techniques  still
requires  clinical  judgment  to  decide  whether  the
findings of a population-based study really apply
to an individual patient who is not an exact fit to
mythical “average patient” in the population stud-
ied; a special case is the patient with more than one
disease. Particularly annoying for some critics of
EBM  is  the  name  itself:  it  is  seen  as  ideological
and suggests, rather pointedly, that whatever phy-
sicians  had  been  using  to  make  diagnoses  and
decisions prior to the advent of EBM methodolo-
gies was not evidence-based. They argue that ele-
vating a particular way of thinking about clinical
medicine – statistical reasoning as instantiated in
EBM – to the exclusion of others is wrongheaded
and unrealistic. The polemic continues to this day.
EBP,  the  recruitment  of  EBM  principles  in
clinical and anatomic pathology, is in the process
of defining itself. What part of this methodology
has  relevance  to  pathology?  Certainly,  much  of
the methodological focus of EBM is irrelevant to
diagnostic and predictive pathology: profession-
ally, we have little to do with making decisions
about  alternative  therapies  given  a  particular
diagnosis.  Ours  is  largely  a  noninterventional,
nonexperimental  descriptive  literature  that  finds
itself in last place in the EBM quality ranking of
types of clinical research. In addition, anatomic
pathology faces unique difficulties in defining its
study  groups;  transforming  what  is  basically  a
complex,  primarily  visual  classificatory  experi-
ence  into  language  sufficiently  precise  to  be
 followed  by  other  patho logists  and  serve  as  the
basis for reproducible assignments.

I prefer to think of both EBM and, by implica-
tion,  EBP  in  less  polemical  terms.  Statistical
reasoning is one mode of thinking; CBR is another;
and taxonomic reasoning, the style that dominates
oncopathological  classification,  is  yet  another.
Navigating through the complexities of an individ-
ual  case  –  whether  it  be  the  clinical  details  of  a
patient or the histological particulars of that patient’s
tissue – requires the application of all three. There
are  no  non-ideological  reasons  to  privilege  one
mode over another; they all play a role.

The spirit of EBP is reformatory. Do our cur-
rent C&D practices in anatomic pathology need
fixing?  Before  I  answer  this  question,  I  need  to
take  an  unvarnished  look  at  oncopathological
classifications,  their  construction  and  evolution,
and the biological basis for the particular “messy”
structure  of  their  constituent  elements:  disease
entities  or  neoplastic  kinds  (KNeop’s).  It  is  the
purpose of this chapter to provide a twenty-first
century sketch of the situation.

This  is  not  an  easy  topic  as  the  foundational
problems  we  confront  in  oncopathological  C&D
are  widespread  in  the  natural  sciences.  Indeed,
much thought has been given to these topics in a
variety of disciplines. Our discussion draws upon
sources in contemporary molecular-genetic oncol-
ogy, biological systematics, the philosophy of biol-
ogy (and more broadly, the philosophy of science),
cognitive  and  judgmental  psychology,  and  statis-
tics.  Taking  our  problems  in  oncopathology  seri-
ously requires this kind of intellectual outreach.

Problem Cases in Anatomic Pathology

Efficient  day-to-day  diagnosis  is,  for  the  well-
trained  surgical  pathologist,  usually  straightfor-
ward. The majority of cases can be assigned without
much difficulty, using the classification de jour, to
established  diagnostic  categories.  In  this  chapter,
we will be concerned with the minority of problem
cases that challenge us. Prominent examples include
(1) in-between cases (“grey-zone” cases or border-
line  cases)  that  fall  into  the  apparently   seamless
morphological multivariate continuum that bridges
two kinds of neoplasms (KNeop’s); (2) hybrid cases
that present confusing combinations of distinct pat-
terns from two or more distinguishable KNeop’s; and

6  Principles of Classification and Diagnosis in Anatomic Pathology

97

(3) novel cases that combine features in a way that
have never before been encountered. Problem cases
are analogous to the patients with rare genetic met-
abolic defects that played a crucial role in develop-
ing  our  understanding  of  normal  metabolic
pathways; their analysis helps us understand how
all oncopathological classification works.

investigations are pursued in the spirit of biological
taxonomy  (the  Linnaean  classification  of  plants,
for example) and its associated mode of reasoning;
managerial investigations are in the spirit of clini-
cal epidemiology and its associated statistical and
decision analytic mode of reasoning.

Some Preliminaries

Scientific and Managerial Classifications
of Neoplasms

We  currently  have  two  general  strategies  for  the
classification of neoplasms in surgical pathology
and  cytopathology:
scientific  classifications
(S-classifications)  in  service  of  explanation  and
managerial  classifications  (M-classifications)  in
service  of  clinical  prediction.  S-classifications
answer questions like “why this particular shared
neoplastic  phenotype?”  Histogenetic  classifica-
tions (HG-classifications) are paradigmatic of (but
do  not  exhaust)  S-classifications.  By  contrast,
M-classifications  are  responsive  to  the  question
“What does the future hold for a patient suffering
from an individual neoplasm (INeop) with a particu-
lar phenotype?” M-classifications are fashioned to
forecast future biological events based on clinical
phenotype, FClin(t), such as the risk of developing
an invasive carcinoma given a particular histomor-
phologic  feature  (risk);  the  future  clinical  course
after no specific therapy – prognosis; and the likely
response to a specific therapy – prediction. Grading
systems for common adult malignancies are para-
digmatic instances of M-Classifications.

The canonical classifications in oncopathology
are hybrids of M-classification grids  superimposed
on HG-classifications. The image to have in mind
is that of a topological survey map with one set of
boundaries  marking  the  distribution  of  physical
features  such  as  peaks  (the  HG-classification)
superimposed upon which is a second set of prag-
matic (“political”) boundaries reflecting the various
discrete classes of an M-classification. The spirit of
these two classificatory activities is quite different
and  involves  very  different  types  of  taxonomic
models:  histogenetic  models  and  statistical  (or
probabilistic)  models,  respectively.  Histogenetic

Diagnostic Problems Related to Lack
of Expertise and Incomplete
Information about an Individual Case

Many of the “problem” cases encountered in day-
 to-day pathology practice are resolved by gather-
ing more information and/or by recruiting expert
opinion. There is much to be said about these two
strategies and when they should be employed; this
is not my concern here. I am interested in the limit-
ing case for which expertise and information are
not at issue. Consider the relevant expert in pos-
session  of  ‘compete’  information  concerning  a
 problematic case. A decision analytic device, the
Clairvoyant, sharpens this idea. This is an imagi-
nary figure with full knowledge, who can, and will,
answer truthfully and completely any question put
to her. [1] However, the Clairvoyant is temporally
constrained in two ways: she won’t tell you about
the future state of the patient harboring the prob-
lematic INeop nor will she tell you about results that
could be obtained employing technologies unavail-
able at the time of the expert’s interrogation. For
example, in 1950 it wouldn’t do to ask her about
the immunohistochemical findings for a particular
problematic case. Why problem cases persist for
the relevant expert with access to the Clairvoyant
is the subject matter of this chapter.

Why Problem Cases Persist Even
for the Relevant Expert with Access
to a Clairvoyant?

I  will  use  as  an  organizing  framework  for  this
discussion the principle players in C&D: (1) the
complexity  and  uniqueness  of  individuals,  the
INeop’s,  being  classified;  (2)  the  heterogeneity
of  groups  (the  KNeop’s)  formed  by  aggregating
INeop’s  similar  in  relevant  respects;  and  (3)  the
 classifier-diagnostician who puts it all together.

98

M. Hendrickson

Complexity and Uniqueness
of the Individual Neoplasm (INeop)

Complexity

As illustrated in Fig. 6.1, it is convenient to  discuss
the complexity of the individual neoplasm (INeop)
at  three  anatomic  levels:  the  neoplastic  cell,  the
neoplastic  tissue  (neoplastic  cells  embedded  in
the nonneoplastic  cells  that comprise  their  envi-
ronment), and the clinically detectable neoplastic
mass  (Table  6.1).  At  the  cellular  level,  the  INeop

inherits the functional and microanatomical com-
plexity of its normal counterpart. The function of
the normal cell is increasingly being framed in the
language of biological systems and discussions of
modules, pathways and global networks, nonlin-
ear interactions, emergent properties, and “down-
ward causation” fill the pages of molecular-genetics
journals and textbooks [2]. Additionally, there has
been a shift from an exclusive focus on the causal
roles of genes to one that recognizes the impor-
tance of epigenetic modifications – DNA methy-
lations and histone modifications. These conceptual
shifts  have  been  mirrored  in  cancer  molecular
genetics. Thus, in recent years exclusive focus on
single cancer genes has given way to talk of the
dysregulation of cancer cells at multiple levels of
cellular  control  including  epigenetic  alterations,
chromosome  copy  number  changes,  DNA  point
mutations, and inversions and translocations [3].
It is now clear that, in general, there is no gene or
handful of genes that are the cause of cancer, or
indeed, any particular kind of cancer [4–7]. As of
2009  at  least  350  (1.6%)  of  the  22,000  protein-
coding  genes  in  the  human  genome  have  been
reported to show recurrent somatic mutations in
cancer with strong evidence that these contribute
to cancer development [8]. Thus, the neoplastic
cells of the common adult cancers are genetically
highly  complex.  This  is  evident  both  from  low-
resolution cytogenetic studies and more recently
in highly refined  examinations  cataloging  sub-
microscopic  chromosomal  abnormalities.  The
Circos  diagrams  of  a  group  of  breast  cancers
shown in Fig. 6.2 provide a striking graphical rep-
resentation of this breathtaking complexity [9].

Fig. 6.1  Levels of organization. Cell and tissue levels are
at the waist of the hourglass; they lie at the organizational
midlevel. The molecular-to-cellular levels are split verti-
cally between structural (left) and functional (right) lev-
els;  Oltvai  and  Barabási’s  complexity  pyramid
is
represented  by  the  functional  branch  on  the  right  [66].
The phenotypes corresponding to various levels are indi-
cated on the left: FClin = clinical phenotype (e.g., present-
ing signs and symptoms); FClin(t) = future clinical course
(forecasts about risk, prognosis, prediction); FGross = naked
eye  phenotype  whether  seen  by  the  pathologist  in  the
gross room, by the surgeon intraoperatively or by the radi-
ologist with imaging techniques; FHist = light microscopic
phenotype; FCytoGen = cytogenetic phenotype. “Level-hopping”
is  indicated  on  the  right  both  -omic  (gene  expression
arrays, proteomics, etc.) and pre-omic

Table 6.1  Characteristics of INeop’s

Complexity

Cellular level
Tissue level
Mass level

Context sensitivity
Uniqueness
Summarizing Metaphors
Malignant gestation
Viral quasi-species

6  Principles of Classification and Diagnosis in Anatomic Pathology

99

Fig.  6.2  The  uniqueness  of  the  INeop:  Circos  diagrams;
molecular-genetic “train wrecks.” Circos plots of the somatic
rearrangements of 24 different invasive breast cancers make
clear  the  molecular-genetic  heterogeneity  of  this  group  of
neoplasms. Left: The symbolic conventions of the circos plot
[6]. Individual chromosomes are depicted on the outer circle
followed  by  concentric  tracks  for  point  mutation,  copy

number,  and  rearrangement  data  relative  to  mapping
position  in  the  genome.  Arrows  indicate  examples  of  the
various  types  of  somatic  mutation  present  in  this  cancer
genome.  Right:  The  Circos  plot  of  twenty  four  individual
breast cancers. Note: each Circos diagram is really the super-
position of the many diverse cytogenetic abnormalities of the
different clones comprising INeop [9]

The INeop viewed as a tissue exhibits another layer
of complexity. Neoplastic tissues have two constitu-
ents: neoplastic cells, typically arranged into paro-
dies  of  structures  normal  to  the  anatomic  site  of
origin,  and  nonneoplastic  cells.  The  construction,
evolution,  and  maintenance  of  a  neoplastic  tissue
involve communication among the tumor cells and
relevant  nonneoplastic  cell  types.  Well-studied
examples  include  the  vascularization  of  the  INeop
[10], the prominent role of the  macrophage in can-
cer  initiation  and  malignant  progression  [11],  and
participation of myoepithelial and  various stromal
cells in modulating the proliferation,  survival, polar-
ity,  differentiation  state,  and  invasive  capacity  of
breast cancer cells [12–14]. In conclusion, while it
is generally accepted that tumor  initiation and pro-
gression  are  predominantly  driven  by  acquired
genetic  alterations  of  neoplastic  cells,  the  crucial
importance  of  the  microenvironment  has  become
apparent in recent years. Taken together, neoplastic
cells and their nonneoplastic interactants constitute
a microecological system [15].

The INeop viewed as a clinically detectable mass
reveals yet another level of complexity: evolution-
ary complexity. The earliest radiologically detect-
able solid malignancy has typically gone through
at least 30 replications and consists of a billion or
more cells. Histologically, this mass appears as a

crazy  quilt  of  dozens  of  genealogically  related
neoplastic clones each mingled with nonneoplas-
tic constituents – cells and matrix – to form a com-
plex  of  multiple  microecologies.  Moreover,  the
crazy  quilt  of  patterns  in  a  tumor  evolves  over
time; the originally diagnosed INeop often has a dif-
ferent appearance than the recurrence.

What accounts for synchronic and diachronic
intratumor heterogeneity [16–18]? There are two
contributions:  hereditary
(inherited  somatic
mutations) and nonhereditary (phenotypic plas-
ticity). Since the 1970s, INeop’s have been regarded
as  Darwinian  evolutionary  processes  and  the
clinically detected cancer as a collection of gene-
alogically related clones, themselves the product
of a contingent, historical process [15, 18, 19].
Each  INeop  is  the  outcome  of  a  process  of
Darwinian evolution occurring among cell pop-
ulations  within  their  microenvironments.  The
heritable  variation  is  provided  by  the  genetic
instability of the cancer cell yielding a range of
phenotypes  and  their  associated  microenviron-
ments upon which selection can operate. Navin
has recently reviewed various models – includ-
ing stem cell variants – of somatic mutation gen-
erated  heterogeneity,  illustrated  in  Fig.  6.3
[16,  18]. The  second reason for tumor hetero-
geneity,  phenotypic  plasticity,  has  two  origins.

100

M. Hendrickson

Fig.  6.3   Tumor  progression  models  and  lineages.
Navin  and  Hicks  have  illustrated  five  models  of  tumor
progression  and  their  phenotypic  consequences  [18].
Green root nodes represent normal diploid cells, colored
nodes are different tumor clones. (a) Monoclonal evolu-
tion forms a monogenomic tumor. (b) Polyclonal evolution

forms  a  polygenomic  tumor.  (c)  Self-seeding  results  in
a  tumor  with  a  divergent  peripheral  subpopulation.
(d)  Mutator  phenotype  generates  a  tumor  with  many
diverse clones. (e) Cancer stem cell progression results in
a tumor with a minority of pink cancer stem cells

First, there is the  heterogeneity that can be attrib-
uted to phenotypic variations on a single speci-
fied  “cell  of  origin.”  This  is  exemplified  for  by
the spectrum of grades within a single phenotype
observed in the common adult cancer. The sec-
ond source of heterogeneity implicates the devel-
opmental  history  of  the  TIC.  For  example,  the
normal uterine cervix is populated by glandular,
squamous, and indifferent (or metaplastic) cells.
The  occurrence  of  confusing  mixtures  of  these
three phenotypes in an invasive cervical cancer
can  be  understood  as  the  TIC  inscribing  these
developmental  potentials  in  the  clonal  pheno-
types of the INeop it gives rise to. Müllerian neo-
plasia  offers  a  more  dramatic  example.
Commonly,  surface  epithelial  neoplasms  of  the
ovary  exhibit  more  than  one  phenotype.  When
this  is  striking,  we  call  them  “mixed.”  This
amounts to the TIC retracing the possible devel-
opmental  pathways  open  to  the  components  of

the  müllerian  ducts.  Of  course,  germ  cell  neo-
plasms exhibit the greatest degree of phenotypic
plasticity; this was dramatically demonstrated in
the  mouse  teratocarcinoma  studies  by  Mintz
et al. [20]. It is as if the neoplastic cell can, in its
confusion,  take  more  than  one  developmental
pathway; in other words, to follow Yogi Berra’s
advice:  “when  you  come  to  a  fork  in  the  road,
take it!”

There is one more layer of microecological
complexity.  There  is  growing  evidence  that
there are important interactions among the dis-
tinct  clones  that  make  up  an  INeop.  Here  the
clones  play  the  role  of  species  and  the  non-
neoplastic  cells,  the  role  of  the  environment
opening  the  way  to  an  ecological  analysis  of
neoplasia.  This  topic  is  reviewed  by  Marusyk
and Polyak [17].

There  are  practical

this
dynamic view of the INeop. The escape of an INeop

implications  of

6  Principles of Classification and Diagnosis in Anatomic Pathology

101

Fig. 6.4  The annotated dendrogram fingerprint (ADF). The
Nowell’s evolutionary trajectory is represented by the den-
drogram with its root in the tumor initiating cell, the normal
cell  that  has  undergone  malignant  transformation.  Time  is
represented vertically; an idealized summary of phenotype
horizontally. Each node in the dendrogram represents a neo-
plastic  clone;  each  node  consists  of  a  central  multicolored
rectangle  set  within  a  background  figure.  The  rectangle
symbolizes the multilevel genetic and epigenetic dysfunc-
tion  of  the  constituent  cells  of  the  clone;  different  shaped
rectangles  represent  different  patterns  of  cellular  dysfunc-
tion.  The  background  figure  represents  the  co-constructed
microenvironment  of  that  particular  clone.  The  top  panel
depicts a snapshot of the INeop at a particular time. Here the

horizontal axis again represents phenotype but now the verti-
cal axis represents the percentage contribution of each clone
(the nodes) to the composite tumor phenotype, or fingerprint.
For example, the first seven peaks go into the make-up of the
patient’s  primary  tumor;  the  labeled  peaks  to  metastatic
deposits in various sites. This is just one possible snapshot; a
slice across some other time would yield a different finger-
print. Thus, we have a representation of synchronic and dia-
chronic  tumor  heterogeneity.  To  summarize,  the  ADF
symbolizes  the  three  levels  of  complexity  of  the  INeop:  the
functional (epi)genetic complexity of the malignant cell by
the rectangle; the micro-ecological complexity of the malig-
nant tissue by its containing figure; and the evolutionary com-
plexity of the clinically detectable mass by the dendrogram

from previously effective chemotherapy appears
largely to have an evolutionary basis. Examples
include the development of imatinib resistance in
chronic  myelogenous  leukemia  [21–23]  and  in
gastrointestinal stromal tumors [24–26].

In  Fig.  6.4  I  introduce  some  symbolism,  a
modification of Nowell’s 1976 diagram that serves
to keep before us the multilevel complexity – cell,
tissue,  mass  –  of  the  INeop:  what  I  will  call  the
annotated dendrogram fingerprint (ADF).

Context Dependency

The  clinical  evolution  (clinical  phenotype)  of  a
particular INeop is context dependent. For example,
histomorphologically identical invasive squamous
carcinomas exhibit very different clinical behav-
iors depending upon their precise location in the
mouth and oropharynx. Similarly, the clinical pre-
sentation and the operability of a glioma of fixed
grade depends crucially on anatomic location.

102

M. Hendrickson

The complexity of neoplasms and their context
dependency reminds us that the implicit reductive
moves  made  in  oncopathological  classifications:
first,  the  reduction  of  the  patient  to  the  patient’s
INeop second, the reduction of the INeop to a small set
of gross, histological, immunological, and molec-
ular-genetic  characterizations  and,  third,  the
 further  reduction  of  these  characterizations  to  a
vector  of  categorical,  ordered  or  interval  values.
The unique particularities of each patient are inev-
itably  lost  in  this  process.  These  considerations
challenge  any  thoughts  of  strict,  context-free
 histological,  or  molecular-genetic  determinism.
There will always be, in the language of the epide-
miologist, confounding factors.

Uniqueness

It should be obvious from this discussion that each
INeop is nontrivially unique. The altered normal cell
from  which  it  arises  is  as  unique  as  the  patient’s
fingerprints.  Superimposed  on  this  baseline  indi-
viduality is the uniqueness imposed by the contin-
gencies  of  the  steps  leading  to  the  malignant
transformation of the normal cell to produce a tumor
initiating cell (e.g., the specific order in which can-
cer pathways are destabilized), the contingent inter-
action  of  those  malignant  cells  with  the  patient’s
unique physiologic microenvironment, the contin-
gency of the evolutionary pathways that constitute
tumor progression, and finally, the contingencies of
the  tumor’s  precise  location  and  time  of  clinical
detection.  All  of  these  factors  guarantee  that  the
clinical  behavior  of  groups  of  similar  INeop’s  will
only admit a statistical formulation. The Circos dia-
grams remind us of this uniqueness (see Fig. 6.2).

The INeop Is a Dynamic Process,
Not a Static Object

The foregoing discussion forces the conclusion that
INeop’s  are  difficult  to  conceptualize  and  more  use-
fully viewed as dynamic processes rather than static
objects. It is, on the one hand, a single entity (cer-
tainly in the sense of the single disease of the patient
who harbors it); on the other hand, it is a com-

plex collection of interacting, evolving,  physically
distinguishable  parts,  the  constituent  clones.  What
are suitable metaphors for the  individual neoplasm?
One  is  the  “malignant   gestation”;  a  metaphor  that
emphasizes the maldevelopmental character of the
process and its continuous spatiotemporal variation.
Microbiology is the source of another metaphor: the
viral quasi species as exemplified by hepatitis C and
HIV  [27–30].  Both  of  these  viral  infections  begin
with an inoculum having one genetic composition
but which then rapidly evolves into large numbers of
derivative  “ species”  under  the  selective  pressure
exerted  by  both  the  host’s  immune  response  and
therapy. This metaphor comes closest to capturing
the truth about the INeop. The  distinguishable clones
of an INeop are analogous to the species produced in
the course of terrestrial organismic evolution. That
is, each component of an INeop’s fingerprint is analo-
gous  to  a  species.  In  light  of  this  discussion,  we
anticipate  that  the  static  classifications  created  by
grouping relevantly similar INeop’s into kinds will be
a  problematic.  It  reminds  us  of  the  skepticism
expressed by Darwin in his Origins of the Species
about the reality of static Linnaean species.

Intrinsic Heterogeneity
of Neoplastic Kinds (KNeop’s)

Preliminaries

So  far  I  have  sketched  out  the  multilevel  com-
plexity of the INeop and emphasized its uniqueness.
How do we aggregate individually unique INeop’s
into  groups  based  on  relevant  similarities?  It
should be clear at the outset that the uniqueness
of the INeop’s guarantees the intragroup heteroge-
neity of the classes that comprise any classifica-
tion of INeop’s we can imagine.

Before we address the specifics of this process
we need to lay some groundwork by setting out some
preliminary  definitions  and  make  some  observa-
tions about the classification process in general.

Classification Contrasted with Diagnosis
These  two  terms  are  used  in  inconsistent
and  confusing ways. In this chapter, I will use

6  Principles of Classification and Diagnosis in Anatomic Pathology

103

 classification  to  denote  either  the  process  or
the  product of partitioning a particular domain
(e.g.,   epithelial  proliferations  of  the  breast)
into  a  set  of  mutually  exclusive  and  collec-
tively exhaustive kinds. Diagnosis denotes the
 process of assigning an as yet unexamined case
to  one  (or  more)  of  the  kinds  set  out  in  the
classification.

Classification Pluralism
It is a commonplace that there are many ways
to  classify  objects  in  Nature  depending  upon
one’s  interests.  Consider  the  many  classifica-
tions of plants: that of the curator of a botanical
garden,  that  of  the  green  grocer,  the  herbalist,
or  the  landscape  architect.  No  one  botanical
classification is privileged, they all serve differ-
ent purposes. This homely example prepares us
for  the  surprise  that  there  is  substantial,  often

 acrimonious  and  heated,  dispute  over  the
 scientific term “ species.” There are the phenetic
species, the  biological species, the ecospecies,
and the evolutionary species [31]. Each species
definition answers to the peculiarities of differ-
ent domains (viruses, bacteria, vertebrates) and
different research concerns (e.g., field identifi-
cation,  evolution,  ecology)  and  their  differing
organizing principles. Thus, there is no sense to
be  made  of  “the  one  correct  classification”
independent of the research community’s inves-
tigatory concerns.

Geometry of Classification:
The Phenospace
In the following discussion I will employ some of
the vocabulary of mathematical-statistical classi-
fication. The ideas are sketched out in the legend
accompanying Fig. 6.5.

Fig. 6.5  An  idealized  tumor  phenospace.  Phenospaces  for
INeop’s are constructed by reducing the objects of study (INeop’s
in our discussion) to some fixed number of ordered observa-
tions (say, tumor size, extent of necrosis, mitotic index, etc.)
and then representing each analyzed case as a point in a suit-
ably dimensioned space: a “feature space” or, synonymously,
a “phenospace.” It will meet our needs for this discussion to
confine  ourselves  to  two  continuous  features  and  use  the
third dimension, the height, to exhibit the number of cases

taking on a particular pair of values. In general, this process
yields peaks and valley and sparsely populated or unoccu-
pied  regions.  Several  questions  are  suggested  by  this  plot.
The existence of structure invites speculation about underly-
ing generative mechanisms: does the presence of seven peaks
suggest  seven  distinguishable  generative  mechanisms?  If
there are seven, where do the products of one leave off and
the  products  of  the  other  begin?  That  is,  are  there  natural
borders to these phenotypic clusters?

104

M. Hendrickson

Relational Kinds and Structural Kinds
Kinds that occur in nature can be divided into two
types: structural and relational (sometimes termed
historical) depending upon whether the defining
feature is a structural predicate or a relational one.
That I have a mass of 79.5 kg is a structural prop-
erty;  that  I  am  an  uncle  is  a  relational  one.
Structural  kinds  are  those  whose  defining  orga-
nizing  principle  is  intrinsic  to  the  objects  being
classified; it is to be found in each member of the
kind. The paradigmatic structural kinds are atomic
and molecular species (e.g., elemental gold, ben-
zene, isoleucine); to determine whether two pure
samples  are  of  the  same  kind,  one  has  only  to
examine the structure (physico-chemical proper-
ties etc.) of each sample. By contrast, relational
kinds are those whose definition appeals to a rela-
tionship to something external to the objects being
classified.  Paradigmatic  examples  of  relational
kinds are biological species. One definition of the
category species (of many possible definitions) is
the biological species: to be members of the same
species  is  to  be  a  member  of  a  naturally  inter-
breeding group. To be a tiger is to have tiger par-
ents. This is a definition that reaches beyond the
intrinsic properties of the individual under exami-
nation to its relationship to an external object, a
mother  and  a  father.  It  is  an  empirical  question
whether there are structural features of each tiger
(e.g., features of genomic organization) that pick
out tigers (and only tigers) from their mimics. So
far this does not appear to be the case. As I shall
see, histogenetic KNeop’s are relational kinds; what
binds them into a group is not a shared structural
“essence” but a shared cell of origin.

Oncopathological Taxonomic Models
I  would  like  to  re-frame  the  creation  of  onco-
pathological classifications as an exercise in tax-
onomic  model  building;  in  particular,  two  very
different kinds of models – histogenetic models
and statistical models.

Models  play  a  major  role  in  many  scientific
contexts.  Examples  include  the  billiard  ball
model of an ideal gas and its various elaborations,
the  Bohr  model  of  the  atom,  the  double  helix
model  of  DNA,  and  the  general  equilibrium

model of financial markets [32, 33]. There are a
number of advantages to talking in terms of mod-
els. First, it emphasizes that  models are the con-
structs  of  the  classifier.  Second,  it  shifts  the
discussion from metaphysics (distinction between
“real entities” and “pseudo-entities”) to consider-
ation  of  the  empirical  adequacy  of  competing
explanatory models. Finally, discussions of mod-
els  allow  us  to  distinguish  structurally  different
classes of models used in oncopathology: histo-
genetic and managerial.

Histogenetic Neoplastic Kinds
(HG-KNeop’s)

Histogenetic neoplastic kinds are collections of
INeop’s presumed to share an origin from a partic-
ular normal cell or population of cells committed
to  a  particular  line  of  differentiation  that  have
undergone  malignant  transformation.  The  plau-
sibility  of  this  theory  is  supported  less  by  the
direct  observation  of  this  temporal  progression
in any individual case but more by invoking the
heuristic: “looks like, therefore came from.”

The  following  discussion  provides  a  way  of
thinking about the formation of histogenetic neo-
plastic  kinds  (HG-KNeop)  and  has  this  sequential
structure: (a) the observations that invite explana-
tion; (b) a proposed model (Gouldian reruns); (c)
a description of the structure of the groups pre-
dicted by the model; and finally, (d) the problem
of conceptualizing and describing these groups.

The Observations: The Uninterpreted
Phenospace of the Domain

First, consider a particular domain’s phenospace,
say  invasive  breast  carcinoma.  Choose  1,000
invasive  breast  carcinomas,  each  from  a  differ-
ent patient. Characterize each INeop’s fingerprint.
Plot the fingerprints for each of the 1,000 cases
in a suitably dimensioned phenospace and struc-
ture  emerges.  Recall  that  in  the  phenospace,
proximity reflects similarity with respect to the
features that have been chosen by the investigator.

6  Principles of Classification and Diagnosis in Anatomic Pathology

105

The  resulting  phenospace  is  occupied  by  clus-
ters  separated  by  thinly  populated  or  unpopu-
lated gaps. The phenospace has structure. So far,
this is all description. How can we account for
clustering?  One  guiding  principle  is:  “Where
there is structure, there is an underlying genera-
tive  mechanism”;  some  mechanism  that  is
responsible  for  the  frequent  covariation  of  the
observed features in short, the clustering. Model
building begins at this point.

with  the  remarkable  image  of  the  INeop  as  a
 process, an evolutionary trajectory resulting in
the  production  of  numerous  genealogically
related  clones,  each  clone  being  analogous  to
an individual species. The Gouldian rerun idea
is simply this: a HG-KNeop is the superposition
of the ADFs of all INeop sharing a common gen-
erative  mechanism.  This  common  mechanism
is  usually  identified  with  the  “cell  of  origin.”
Figures  6.6  and  6.7  and  the  accompanying
legends elaborate this theme.

The Model: Gouldian Reruns

Steven  J.  Gould  in  his  book  on  the  Cambrian
Explosion, Wonderful Life, in making an argu-
ment  about  the  plausibility  of  human  intelli-
gence arising a second time in the history of the
planet, invites us to consider evolution run over
and over again from a common temporal start-
ing  point  [34].  I  want  to  recruit  this  powerful
image  as  a  way  to  conceptualize  HG-KNeop’s.
The  discussion  in  the  previous  section  left  us

The Model’s Consequences:
Extensionally Indeterminate
Core-Penumbra-Tierra Incognita
Structure (ExtnI-CoPeTI)

More generally, this process yields clusters sep-
arated by gaps in a suitably dimensioned pheno-
space.  Each  cluster  has  an  internal  substructure
consisting of one or more concentrations of typi-
cal (core) cases, a fringe of looser concentrations

a

Gouldian Re-runs

b

N

lim
∞

N

1U{       }

Smith

Jones

Stein

N=3

N=3

N=300

Fig. 6.6  Histogenetic neoplastic kinds: Gouldian reruns.
Each patient (Smith, Jones, and Stein) harbors an invasive
breast cancer, each with its unique ADF. Each one would
fill in patches in a suitably dimensioned breast carcinoma
phenospace. Each is thought to arise from a normal dif-
ferentiated cell, counterparts of which are present in the
breasts of all three. Fix that intersubjectively normal phe-
notype and now imagine each individual’s ADF “starting
off” from a malignant version of that common normal cell
type. Malignant transformation of that normal cell leads
to the corresponding tumor initiating cell (TIC). Think of
all  three  neoplasms  arising  from  that  common  root  and

then superimpose the three trajectories. This leads to the
filling out of our breast cancer phenospace with contribu-
tions  from  all  three  INeop’s.  Now  increase  the   number  to
300  and  we  get  something  like  what  is  pictured  at  the
extreme  right.  Panel  B  shows  a  contour  diagram  that
might be produced by this experiment. In more abstract
terms, we can think of a HG-KNeop arising from a specified
normal cell type “A” (HG-KNeop [A]) as the set theoretical
union  (what  we  have  been  calling  a  superposition)  of  a
large number of ADFs (“n”) and then let n increase indefi-
nitely. The increase in n amounts to gathering more expe-
rience about the range of variation of HG-KNeop [A]

106

M. Hendrickson

Fig.  6.7  The  histogenetic  taxonomic  model  schema.
Making sense of “Gold Standards” involves analyzing a
little  more  closely  the  relationship  between  the  pheno-
space and the taxonomic model we use to make sense of
the phenospace. Left-hand side: The Phenospace: this sil-
houette represents the observed phenospace (the result of
studying a large number of cases in the particular domain)
featuring  six  more  or  less  ill  defined  but  overlapping
peaks. The Mechanism space: we decided that there are
six generating histogenetic mechanisms. These individual
mechanisms  (μ’s)  correspond  to  the  postulated  mecha-
nisms that lead to the cluster in the phenospace; the pas-
sage,  for  example,  from  a  normal  cell  phenotype  to  the
KNeop;  the  complex  maldevelopmental  and  evolutionary
process that produces the crazy quilt of light microscopic
patterns  that  comprise  the  HG-INeop’s.  We  can  represent
these in another space, the mechanism space. Importantly,
we never directly observe the mechanisms represented in
this space; we infer their presence from the structure in the
observable phenospace. Given this taxonomic model, we
can  then  ask,  for  example,  whether  an  in-between  case
represents  an  instance  of  one  or  another  histogenetic

mechanism instantiated by the peaks on either side of the
problematic case. Right hand panel: The Surrogate space:
sometimes  we  decide  on  empirical  grounds  that  some
observable feature can be a stand-in for the, in principle,
unobservable mechanism. The interposed surrogate space,
Σ, is populated by the observable stand-ins for the corre-
sponding  set  of  postulated  but  nonobservable  mecha-
nisms.  An  example  would  be  the  SYT-SSX  gene  fusion
for synovial sarcoma. Sometimes these are referred to as
“Gold  Standards”  but  this  is  misleading.  If  the  require-
ment for a “Gold Standard” feature is that it is both neces-
sary and sufficient for the diagnosis of a particular KNeop,
then the fusion product is not one; the usual claim in the
case of synovial sarcoma is that 90% of cases show this
feature. Moreover, currently, the relationship between the
presence of a surrogate and the generative mechanism is
completely mysterious; the most that can be said is that
the  “Gold  Standard”  feature  and  the  mechanism  are
strongly correlated. Surrogates of this sort are more use-
fully regarded as features that are heavily weighted in an
overall assessment of all the clinicopathological features
used in diagnosis.

of atypical cases (penumbra) and cases that fade
off  in  a  diagnostically  problematic  way  into
“neighboring”  entities  (“terra  incognita”),  for
short,  CoPeTI  clusters.  Figures  6.8  and  6.9
illustrate  this  general  concept.  There  is  another
consequence of the model: since the number of
possible  trajectories  produced  by  a  particular
generative mechanism is, in principle, unlimited,
we  will  always  encounter  new  fingerprints.

Thus,  these  phenospace  clusters  have  the  addi-
tional  property  of  being  “open”  or,  more  preci-
sely, extensionally indeterminate; the boundaries
delimiting  a  parti- cular  KNeop  are,  according  to
this  model,  essentially  undefined.  In  summary,
the  Gouldian  rerun  model  predicts  relational
kinds  that  have  an  extensionally  indeterminate
CoPeTI  structure  (or  ExtnI-CoPeTI,  for  short).
The structure of KNeop’s is reminiscent of that of

6  Principles of Classification and Diagnosis in Anatomic Pathology

107

disease kinds in rheumatology, for example, sys-
temic lupus or rheumatic fever a KNeop is, in this
sense, a morphologic syndrome.

Phenospace structure, then, is a reflection of a
variety of factors that more or less constraint the
population  of  INeop’s  –  each  one  of  which  is  a
 contingent,  unique  evolutionary  trajectory  ––
originating  from  a  particular  TIC.  These  con-
straints  include  (1)  phenotypic   plasticity,  the
regenerative  and  developmental  potential  of  the
TIC,  and  (2)  the  contexts  (anatomic,  microana-
tomic,  humoral,  etc.)  of  the  INeop.  At  this  point,
three  questions  arise:  First,  how  many  scientifi-
cally credible mechanisms are suggested by the
structure? (How many KNeop’s are there in the par-
ticular domain?) Second, how are the  projections
of  these   mechanisms  into  the  phenospace  to  be
delimited?  That  is,  what  are  the  boundaries
 separating KNeop’s? Third, what attitude should I

Fig.  6.8  The  CoPeTI  group.  Biological  variability  in
whatever  domain  tends  to  produce  uni-modal  or  multi-
modal “bell-shaped” like curves; KNeop’s are no exception.
If, from our phenospace, I extract a peak and take a cross-
section through it and its immediate environment, I usu-
ally  obtain  something  like  this  figure.  Clustered  around
(the  core)  and  as
the  mean  are
I move away from the mean I encounter less typical cases
(the penumbra) and finally move into no-man’s land (the
terra incognita). For the sake of brevity and for us to keep
this structure in mind I will use the acronym: CoPeTI

typical  cases

Fig.  6.9  Two  representations  of  a  typical  patch  of  a
domain’s phenospace. The valley formed by four peaks in
a two dimensional phenospace is shown. On the right is a
three-dimensional  depiction  –  the  frequency  of  cases  is
indicated on the z-axis. On the left is a depiction of the
same phenospace but this time each case is represented by
a  point;  the  density  of  points  corresponds  with  the  fre-

quency of cases in a particular region of the phenospace.
The histogenetic taxonomic model assumes four mecha-
nism; the colors indicate the theoretical results of Gouldian
reruns starting off from four types of transformed normal
cell types. The CoPeTI  naming is employed here: 1 = core
cases;  2 = penumbral  cases;  3 = in-between  cases;  and
4 = terra incognita (“there dragons be”)

108

M. Hendrickson

have toward problem cases viewed from this new
perspective?

How many distinguishable histogenetic
mechanism can be supported by our data?
This can be thought of as a model parameter to
be  specified  by  the  investigator  in  much  the
same  way  that  in  cluster  analysis  one  has  to
specify  a  range  of  values  for  the  anticipated
number of clusters to be “discovered” in unsu-
pervised classification. “Lumpers” prefer a low
number  for  this  parameter;  “splitters”  prefer  a
higher number.

HG-KNeop, but some kinds are more probable than
others.  This  probabilistic  modeling  honors  the
ExtnI-CoPeTI  structure  of  KNeop’s  in  a  way  that
grids  do  not.  It  should  be  mentioned  that  the
machine-learning version of this approach has an
essentialist  cast:  the  multivariate  mean  is  inter-
preted as the “essence” of the KNeop and the varia-
tion  (represented  by  values  of  variances  and
covariances that make up the covariance matrix)
as reflecting random “noise.” Biological reality is
sacrificed in this model; much of the “noise,” far
from being random, may well be biologically rel-
evant  signal [35, 36].

How, if at all, are the KNeop’s to be delimited? To
grid or not to grid
A grid imposed on a phenospace is the geomet-
ric equivalent of a crisply defined classificatory
partition – the division of the phenospace into a
set  of  high-dimensional  volumes  that  are  non-
overlapping mutually exclusive regions that col-
lectively exhaust the  phenospace. The extensional
indeterminacy  of  the  Gouldian  rerun  model
guarantees  that  any  gridding  will  be  problem-
atic.  Any  partitioning  of  the  phenospace  in
unambiguous,  crisp  characterizations  of  the
observed  features  (the  grid)  will  fail  at  a  fine
enough level of partitioning. Indeed, both crisp
boundaries  and  necessary  and   sufficient  condi-
tions  for  membership  are  incompatible  with
ExtnI-CoPeTI  groups.  Further more,  there  is  no
refinement of a partition – whether using a more
nuanced treatment of light microscopic features
or employing thousands of molecular features –
that  will  escape  this  problem.  That  does  not
mean  that  the  current  grid  is  not  sufficient  for
most  diagnostic  work.  But  it  does  suggest  a
 different  attitude  toward  problem  cases;  prob-
lem cases are symptomatic of this fundamental
incompatibility.

One  approach  that  avoids  gridding  treats
KNeop’s  as  multivariate  probability  distributions
with ranges that include all possible values that
the  features  can  assume  in  the  phenospace.  For
any region of the phenospace, there is a nonzero
probability  that  any  of  the  posited  generative
mechanisms  (the  HG-KNeop’s)  could  take  on
values in that region. Anything is possible for the

How Are Problem Cases
to be Handled?
At this point, the reader may say: “All of this is
well and good but the practice of oncopathology
requires  some  manageable  partition  of  the  phe-
nospace.” The response is, of course, this is true
and  the  existing  systems  perform  surprisingly
well. What our analysis suggests is that grids are
pragmatic solutions and not to be taken too seri-
ously,  theoretically,  as  reflecting  our  current
understanding of KNeop’s (Fig. 6.10). So, from the
Gouldian  reruns  perspective,  problem  cases  are
guaranteed and draw attention to the limitations
of gridding. What to do? From a practical point
of  view,  if  there  is  no  managerial  distinction  at
issue, then forcing a problem case into one cate-
gory  or  another  seems  at  best,  of  academic
importance only, at worst, pointless. If there is a
managerial  gradient  involved,  then  the  discus-
sion  must  shift  into  a  totally  different  mode:
decision analytic and, as will be discussed, inde-
terminacy  of  histogenetic  assignment,  by  no
means,  paralyzes  clinical  decision  making  (see
Chap. 10).

Representing ExtnI-CoPeTI Structure
in Concepts and Language:

How do should we conceptualize and talk about
the continuous, multidimensional, spatio-temporal
variation characteristic of INeop’s on the one hand
and the ExtnI-CoPeTI groups (KNeop’s) into which

6  Principles of Classification and Diagnosis in Anatomic Pathology

109

PROBLEM CASES

“A”

“B”

“C”

Typical A

Typical B

Hybrid case

ADF

INeopfingerprint

TIC

Excluded

“A”

“B”

Excluded

“C”

In-between case

N = 1000

KNeopS: “A”, “B”, “C”

Novel case

Fig. 6.10  Problem cases. (a) A stylized ADF is simpli-
fied into its associated fingerprint, a snapshot of the INeop’s
heterogeneity at the moment of biopsy. (b) The relevant
domain is represented by a silhouette with labels “A,” “B,”

and “C”; for example, I call things in this region “KNeop(A).”
The  silhouette  represents  the  results  of  plotting  1,000
INeop’s from this domain. (C) The fingerprints of both typi-
cal cases and problem cases are depicted

they cluster? We have recourse to vague concepts
and vague language. It is important to emphasize
that  vagueness  is  a  property  of  concepts  and
predicates. The world is not vague; the world is,
well, what it is [37].

As an illustrative example, let us take synovial
sarcoma. Our Gouldian rerun model tells us that
I  should  expect  that  this  neoplastic  kind  should
have  an  ExtnI-CoPeTI  structure.  Consider  a  set
of instruction for diagnosing the HG-KNeop, syn-
ovial sarcoma and separating it from its mimics.
Many  of  the  phenotypic  features  that  we  are
advised to evaluate are imprecise. We have “syn-
ovial sarcomas are large” instead of providing a
numeric  size  range;  We  have  terms  like  “most”
(50%?, 95%?) and “cellular.” This vagueness of
feature  specification  extends  to  integer-valued,
countable features of the INeop. For example, the
difficulty  with  providing  a  mitotic  count  fre-
quency  (say,  maximum  number  of  mitotic  fig-
ures/ten high power fields) for a uterine smooth
muscle neoplasm is not in the counting part (sta-
tistical and sampling issues, though there are) but
in  deciding  whether  something  is  or  is  not  a
mitotic figure. In other words, the feature itself is
an extensionally indeterminant CoPeTI category.
Not  only  are  individual  criterial  features  vague,

criteria for membership in synovial sarcoma are
also  vague.  How  many  of  these  features  are
required? Should some be weighted more heavily
than others?

What Sort of Concept Is a KNeop?

Cognitive  psychologists  and  linguists  studying
concepts have written extensively on classes with
this  structure  beginning  in  the  1970s  with
the  work  of  Eleanor  Rosch  and  George  Lakoff.
Terms  used  for  these  groups  include  “family
resemblance groups,” “cluster concepts,” “proto-
type  groups,”  and  “polythetic  groups  [38–43].”
Common  features  include  a  high  level  of  intra-
group hetero geneity; a graded architecture (there
are better and worse examples in the class); pro-
totypic  examples;  and,  most  importantly,  an
absence of a defining set of individually neces-
sary and jointly sufficient (INJS conditions) for
membership.  The  last  amounts  to  the  assertion
that the groups have no essences. It became clear
in the 1970s that most nontechnical concepts and
their linguistic representation do not have a clas-
sical (i.e., satisfying INJS conditions) structure;
many  have  a  prototype  structure.  Traditionally,

110

M. Hendrickson

the kinds that occur in nature have been thought
to have a classical, essentialist structure. This is a
tradition that began with Aristotle in Hellenistic
Greece and was taken up wholesale by Linnaeus
in  the  18th  C.  and  informed  his  structuring  of
biological classification. It is only in the 20th C.
that the grip of essentialism has been relaxed. It
is  now  widely  accepted  that  biological  species
have no ‘essences.’ What about features that are
said  to  be  “Gold  Standard”  for  a  particular
KNeop’s?  These  are  more  usefully  thought  of  as
surrogates  for  histogenetic  mechanisms  (see
Fig. 6.7).

Managerial Neoplastic Kinds
(M-KNeop’s)

If histogenetic classifications have the flavor of
biological systematics, managerial  classifications
are more in the spirit of commercial risk analy-
sis,  say,  fashioning  risk  categories  for  credit
card  applicants  (good  risk,  intermediate  risk,
bad  risk)  using  applicant  characteristics  (age,
credit history, income, etc.). Managerial classi-
fications are formed by playing off a wide vari-
ety  of  descriptors  (features)  against  a  clinical
outcome of interest; in machine-learning terms,
they  are  exercises  in  supervised  classification
(see Chap. 7 and 10.) The basic ideas are illus-
trated  in  Fig.  6.11.  Paradigmatic  examples  are
grading  systems  for  common  adult  malignan-
cies;  these  are  managerial  classifications  that
 discretize a multivariate continuum into statisti-
cally credible, distinct FClin(t) groups or lotter-
(Fig.  6.12).  The  “benign-malignant”
ies
dichotomous  classification  and  its  expansions
can be regarded as “extended grading systems”
(Fig. 6.13).

Managerial  classifications  are  engrafted  on
underlying  histogenetic  classifications;  manage-
rial  classifications  both  inherit  the  diagnostic
problems of the underlying histogenetic classifi-
cation and lead to diagnostic difficulties of their
own.  Managerial  grey  zones  are  quite  different
and dealing with them involves a change in con-
ceptual register from histogenetic considerations

to  decision-analytic  ones.  Please  see  discussion
in Chap. 7 and 10.

The Human Element: The Classifier/
Diagnostician

The Pathologist and the World
Co-create Oncopathological Reality:
The Conceptual Fabric Defined

The true, insightful, and fundamental statement that
science, as a quintessentially human activity, must
reflect a surrounding social context does not imply
either  that  no  accessible  external  reality  exists,  or
that  science,  as  a  socially  constructed  institution,
cannot achieve progressively more adequate under-
standing of nature’s facts and mechanisms.

Stephen Jay Gould [44]

We co-construct our view of oncopathological real-
ity. I mean this, not is some spooky extreme post-
modernist  way  but  in  the  noncontroversial  sense
that classifications issue from our attempts concep-
tualize  and  describe  an  undifferentiated  world,  a
world  that  doesn’t  come  presorted  into  ‘natural
kinds.’  Construed  most  broadly,  classifications
embody our attempts to structure a world initially
experienced,  in  William  James  phrase,  “as  one
great  blooming,  buzzing  confusion.”  We  bring
our  current conceptual scheme and the methodolo-
gies (conceptual fabric) available to us at a particu-
lar time to bear on a particular domain (Table 6.2).
The  parsings  (or  classifications)  of  individuals  in
that  domain  have  changed  and  will  continue  to
change  as  we  acquire  new  experience  and  our
conceptual fabric changes. In other words, our clas-
sifications and their constituent kinds, the things we
count as “real,” change with the times (Table 6.2).

Coarse Grained Taxonomic Instability
(Macro-Revisions)

Both  nonmanagerial  and  managerial  classifica-
tions evolve under the pressure of both additional
experience and changes in the conceptual fabric.
In the process, old KNeop’s either disappear (or are
radically transformed) or new ones take their place.
Theory change is, of course, a standard topic in the

6  Principles of Classification and Diagnosis in Anatomic Pathology

111

MANAGERIAL TAXONOMIC MODELING

y
c
n
e
u
q
e
r
F

y
c
n
e
u
q
e
r
F

BLAND

MORPHOLOGIC CONTINUUM

HIGH GRADE

M-K_(1)

M-K_(2)

M-K_(3)

M-K_(4)

M-K_(5)

L1

L2

L3

L4

L5

PHENOSPACE

e
t
a
R
e
r
u

l
i
a
F

e
t
a
R
e
r
u

l
i
a
F

Sort of

LOTTERY
SPACE

Fig.  6.11  The  basic  ideas  behind  managerial  lotteries.
Upper panel: the phenospace depicting both a continuous
risk funtion and its discretized version. The x-axis repre-
sents some continuous composite measure of cytological
atypia and architectural complexity. The y-axis represents
two features: on the left, the frequency of cases having a
particular morphologic index value and, on the right, the
failure  rate  associated  with  a  given  morphologic  index
value. The top half of the panel depicts, in grey, a silhou-
ette of the phenospace against which is plotted a continu-
ous,  monotonically  increasing  risk  level.  In  the  bottom
half panel, the phenospace has been discretized into risk
categories;  the  step  function  represents  the  average  risk
for each of the newly formed categories. These manage-
rial neoplastic kinds are indicated in the bottom strip. For
example, managerial KNeop 1 or M-K-(1) etc. The lottery
space (bottom panel) makes explicit the distinct lotteries
associated with each managerial KNeop. The ‘eye’ reminds

us that we observe the morphology and associate the case
with a specific lottery. The ‘sort of’ reminds us that our
observations  of  the  lottery  characteristics  for  any  given
partition  evolve  over  time  with  the  acquisition  of  more
clinicopathological  experience.  This  evolution  is  one  of
the forces (among others) that drives the managerial clas-
sification macro-revisionary cycle (see discussion). Case
assignment is problematic at the boundaries of categories;
different  assignments  yield  different  predictions.  This  is
an artifact of the discretizing procedure; a more realistic
prediction would be that such a boundary case would have
a behavior intermediate between the two straddled lotter-
ies.  Until  recently,  it  was  conventional  to  employ  the
dichotomous  classification  –  “benign-malignant”;  clini-
cally more useful is the refined classification that recog-
nizes  additional  distinct  interpolated  between  ‘benign’
and  ‘malignant,’  for  example  ‘low  malignant  potential’
tumors

philosophy of science; its most famous expounder
in recent years was Thomas Kuhn [45, 46].

continuum be partitioned into “benign- malignant”
versus “benign-LMP-malignant [48–52]?”

Recall  the  lymphoma  histogenetic  classifica-
tion wars that roiled the world of  hemato pathology
in the 1970s [47]. Discussions surroun ding man-
agerial  revisions  can  be  equally  energetic.  The
debate  over  the  existence  of  a  “low  malignant
potential tumor” in the ovarian serous  neoplasia
spectrum is an example. Should the morphologic

The  dynamics  of  classification  change  can  be
represented as two evolutionary processes, one for
M-classifications (the clinicopathologic spiral) and
the other for S-classifications (the scientific spiral);
the two trajectories mutually inform one another as
they coevolve. Importantly, there is traffic between
the  two  sides;  some  landscape  features  begin  as

112

a
Classic Lobular
Carcinoma

n
o

i
t

a
m
r
o

f

d
n
a
G

l

3
<10%

2
10%-75%

1
>75%

7

6

5

6

5

4

5

4

3

Medullary
Carcinoma

3
2
1
3
2
1
3
2
1

3
2
1
3
2
1
3
2
1

9

8

7

8

7

6

7

6

5

3
2
1
3
2
1
3
2
1

8

7

6

7

6

5

6

5

3

Tubular
Carcinoma

1
mild

2
moderate

3
severe

Cytologic Atypia

Pleomorphic
Lobular
Carcinoma

Mitotic Index:
1 or 2 or 3

Total Score=
Aty+Gland+MI

M. Hendrickson

3

4

5

6

7

8

9

Overall Grade

Grade 3

Grade 2

Grade 1

4 patterns

13 patterns

10 patterns

27 patterns

• There are 27 patterns collapsed into 3 Groups
• Thus, each grade can be multiply realized

• Some combinations raise the issue of  special variant carcinomas

• Pattern vary in frequency; if  you don’t grade special variants,
  those cells typical of  specific special variant will be depleted

• Each feature constitues a mini-syndrome of  its own.

Pleo

Nuc Memb

N’ol

HyperChrom

b

The Lottery

Ms. Smith’s
breast cancer

“Match the pattern”

“Pick the corresponding
URN”

“URN II”
“Pick a ball”
“Red is bad”
“Black is good”

3

4
I

5

6

7

8

9

II

III

Color: Mitotic Index
Central hole: Gland Formation
Shape: Cytological Atypia

3

4
I

5

6

7

8

9

II

III

URN I

URN II

URN III

Fig. 6.12  Grading  infiltrating  ductal  carcinoma  of  the
breast – Nottingham Scarff-Bloom-Richardson (NSBR).
Grading systems are paradigmatic examples of manage-
rial classifications. The NSBR grading of invasive duc-
tal carcinoma (IDC) serves as an example [67, 68]. (a) A
representation of the IDC phenospace. Given a case of
IDC  one  makes  three  observations:  percentage  gland

formation,  degree  of  cytological  atypia,  and  mitotic
index.  Each  of  these  three  features  can  take  on  one  of
three values (1, 2, or 3). Add up the scores for the case
being examined (ranges from 3 to 9) and assign the case
a composite Grade using the scheme illustrated on the
right. (b) The IDC Lottery: an interpretation of the IDC
taxonomic model

6  Principles of Classification and Diagnosis in Anatomic Pathology

113

Y
C
N
E
U
Q
E
R
F

n
o
i
t
a
i
t
n
e
r
e
f
f
i
d
s
u
o
u
q
b
m
a
n
U

i

n
o
i
t
a
i
t
n
e
r
e
f
f
i
d
s
u
o
u
q
b
m
A

i

Focal
Proliferation

WORST AREA RULE

Benign
50% of Serous

LMP with microinvasion

S-LMP
15% of Serous

MPL

Carcinoma
35% of Serous

• Cystic, multicystic
or papillary

• Cystic, multicystic
or papillary

• Single layer without
Tufting

• Stratification (>4 cells)
and tufting

DESTRUCTIVE STROMAL
INVASION

• Atypia minimal

• Mild to moderate
cellular atypia

CYTOLOGIC ATYPIA
BEYOND THE PALE

• Mitotic figures
infrequent

• Mitotic figures findable
(up to 4 mf/10 hpf)

• Stromal component
usually prominent

• Complex papillae
with secondary
papillae

Fig. 6.13  The scheme applied to the ovarian serous neopla-
sia spectrum. In this figure the y-axis serves two purposes:
the total width, for any fixed value of x, represents frequency;
location below the x-axis indicate the extent to which serous

differentiation is easily recognized. For example, the serous
phenotype  becomes  increasingly  attenuated  as  one  moves
into  the  Grade  III  carcinoma  range.  S-LMP  serous  low
malignant potential neoplasm; MPL micropapillary lesion

Table 6.2  The oncopathological conceptual fabric

Methodologies available at a particular time
Global theories in the supporting sciences
Domain-specific theories (for example, theories about
the etiology and pathogenesis of neoplasia)
Domain-specific knowledge accumulated to date
Styles of scientific reasoning (for example, statistical
reasoning, case-based reasoning [65], taxonomic
reasoning, and experimental reasoning) [69–71]

S-distinctions and evolve into M-distinction. This
passage, for example, is the investigative focus for
researchers  validating  proffered  cancer  markers
(see Chap. 7 for a discussion of validation).

Transmission and Translation
of a Classification

One session over a multiheaded microscope with
an expert pathologist reviewing her cases is suffi-
cient to disabuse one of the idea that experts diag-
nose  using  explicit  criteria.  Recognition  comes
first,  criteria  to  justify  the  diagnosis,  later.  It  is
also  clear  that  substantial  nonhistopathologic

knowledge is recruited in arriving at a diagnosis.
Bartels sees this as an instance of “sensor fusion
– the combining of sensory data with data from
other sources such that the resulting information
is  in  some  sense  better  than  would  be  possible
when  these  sources  are  used  individually  [53].”
Both of these observations prompt my use of the
term  “classificatory  vision”
the
 pathologist’s complex interior sense of “the map
of the terrain”; the term reminds us that whatever
makes up the expert’s sense of some oncopatho-
logical  domain,  it  is  largely  nonlinguistic  and
draws widely on many elements of the conceptual
fabric. The later informs the way the expert navi-
gates around a slide; chooses which microscopic
fields  to  examine,  which  to  ignore;  in  short,  to
decide what is “signal” and what is “noise.”

to  denote

Under ideal circumstances, the transmission of
a  classificatory  vision  from  the  investigator  to  a
potential  user  involves  the  back  and  forth  com-
munication of the two over a mutiheaded micro-
scope.  It  is  an  exercise  in  iterative  ostensive
teaching: pointing, naming, and correcting. Even
under  these  ideal  circumstances,  there  is  an
ineliminable  indeterminacy  of  transmission.  We

114

M. Hendrickson

never know, at any stage of this process, whether
we have “gotten it” or not. Our misunderstandings
emerge only with time and the joint examination
of  additional  cases.  Importantly,  the  original
investigator’s  classificatory  vision  also  changes
with this new experience. The conversion of this
“sensory  fusion”  process  into  spoken  language,
the translation problem, is itself challenging; sum-
marizing that verbal formulation into a set of writ-
ten instructions is even more so. Published journal
articles  rely  on  photomicrographs  and  terse  tex-
tual  descriptions  inevitably  employ  ambiguous
language. As I indicated, quantitative features do
not escape this problem. These are anemic substi-
tutes  for  the  back  and  forth  of  a  microscope
session.

Diagnostic  decision  making  aids  are  largely
dedicated to facilitating this communicative task.
They attempt to recapture the originary scene of
ostensive  classification  transmission  by  making
available  extravagant  numbers  of  images  and
modeling the “expert’s” intuitions with rule based
or  probabilistic  computer  models.  This  topic  is
expanded upon in Chaps. 7 and 10.

Fine-Grained Taxonomic Instability
(Micro-Revisions)

I  discussed  public  large-scale  revisions  above;
there  is  another  kind  of  classificatory  revision,
“micro-revisions.” By this I mean the ongoing,
daily adjustments in the classificatory vision of
the  pathologist,  the  expert  in  particular,  that  is
occasioned by her confronting a novel case and
assimilating it to one or another KNeop’s by, for
example,  using  criteria  that  go  beyond  those
extracted from the literature. If memory serves,
the next time she sees another case like this one,
she will make the same diagnosis. In these situ-
ations,  the  expert  is  both  classifying  and  diag-
nosing at the same time. The way to understand
the  expert  reporting:  “I  communed  privately
with the case for a long time and decided it was
KNeop(A) rather than KNeop(B) or KNeop(C)” is as a
classificatory  act.  Returning  to  the  map  meta-
phor,  we  can  think  of  this  as  the  ongoing

adjustment  and  renegotiation  of  details  of  the
expert’s grid – whether scientific or managerial.
Micro-revisions  provide  a  framework  for
understanding  expert  disagreement,  which  is
notoriously  widespread  in  anatomic  pathology.
Over time, micro-revisionary cycles lead inevita-
bly  to  the  noncongruence  of  the  private  maps
(classificatory visions) of different experts. Their
maps are usually congruent over “core” cases but
become increasingly noncongruent as one moves
progressively away from the “core” through the
“penumbra” and slides into the “terra incognita.”

Boyd Kinds, an Alternative
to Essentialism

Here  is  a  puzzle  that  raises  important  issues:  Is
the KNeop, synovial sarcoma of 1950, the same or
different  from  the  KNeop  synovial  sarcoma  of
2010? If not how are they related? Synovial sar-
coma was first described about 90 years ago. The
extension (the INeop’s included under the term) of
the KNeop synovial sarcoma has changed over the
years  with,  for  example,  the  acceptance  of  a
monophasic variant. In the late 1980s, a consis-
tent,  specific  translocation  involving  chromo-
somes  X  and  18  was  discovered  to  be  widely
distributed in synovial sarcomas as then defined
[54].  The  fusion  product  of  this  translocation,
SYT-SSX  chimeric  RNA,  can  be  detected  by
reverse-transcriptase  polymerase  chain  reaction
and  this  procedure  is  now  used  in  routine
 diagnostic test. It now has become customary to
talk of the presence of the fusion product SYT-
SSX as the “Gold Standard” for the diagnosis of
synovial  sarcoma,  despite  the  fact  that  not  all
“classic” synovial sarcomas exhibit this feature.
In recent years, the extension of synovial sarcoma
has been expanded, using the SYT-SSX criterion,
to  include  a  variety  of  sarcomas  that,  on  light
microscopy examination, either possess a distinc-
tive  phenotype  more  characteristic  of  another
type of sarcoma or are undifferentiated [55, 56].
The  history  of  synovial  sarcoma  and,  in  par-
ticular,  after  the  acceptance  of  SYT-SSX  as  the
“Gold  Standard”  traces  a  general  pattern.  First,
there is an early impression of distinctive H&E

6  Principles of Classification and Diagnosis in Anatomic Pathology

115

histomorphological similarities justifying a group-
ing; I dub it “KNeop(A).” Then, I posit some under-
lying  generative  mechanism.  Next,  I  refine  the
initial characterization in light of new observations
or reconceptualization under the pressure of changes
in  theory.  Throughout  this  process,  the  KNeop  (A)
retains the same name and I have the sense that I am
approaching  asymptotically  the  “true”  KNeop(A)
with each cycle. This is the historical and contingent
process of classificatory evolution. What happens
to  the  KNeop(A)  during  this  process?  Clearly  its
extension changes. What remains constant? It can-
not  be  anything  like  a  classical  “essence,”  (i.e.,
“Gold Standard,” set of INJS conditions) as I have
seen. These are subtle and difficult issues and space
only permits hints at a solution.

The traditional conception of “natural kinds”
(i.e., groupings that occur in nature independent
of our interest) has involved INJS conditions. It
turns out that almost none of the categories inves-
tigated in biology, nor in most of the other special
sciences  –  such  as  psychology,  meteorology,
astronomy,  economics,  or  linguistics  –  involve
shared intrinsic characteristics that are necessary
and sufficient for membership [57–59].

The philosopher, Richard Boyd, has proposed
an alternative understanding of natural kinds that
does  not  involve  necessary  and  sufficient  mem-
bership  conditions;  he  calls  these  “homeostatic
property  clusters  natural  kinds  [60–62].”  They
feature  Wittgensteinian  families  of  properties
that tend to be nonaccidentally coinstantiated, in
that something that possesses some of the proper-
ties in the cluster makes it more likely that it will
also  possess  the  other  properties  in  the  cluster.
Boyd  has  argued  that  biological  species,  higher
taxa and many of the categories studied in eco-
nomics  and  geology,  have  this  character.  Thus,
categories can occur in nature prior to our clas-
sificatory schemes without any intrinsic charac-
teristics  or  “essences”  that  all  members  of  the
category  have  in  common.  I  think  KNeop’s  with
their  ExtnI-CoPeTI  structure  are  instances  of
Boyd  kinds.  The  model  also  effectively  deals
with both what has been termed macro-revisions
and  micro-revisions.  Chiong  provides  a  medi-
cally oriented summary in the context of defining
“brain death” [63].

Conclusions: The Mythology of
Classificatory and Diagnostic
Pathology (Table 6.3)

We can summarize the arguments of this chapter
by setting out the major conclusions as a collec-
tion of myths. I have already discussed the myths
of  the  homogeneous,  static  INeop’s  and  of  histo-
pathologic or molecular-genetic determinism.

Naïve Realism

My guess is that I have a folk theory of categoriza-
tion itself. It says that things come in well-defined
kinds,  that  the  kinds  are  characterized  by  shared
properties, and that there is one right taxonomy of
the kinds [64].

It  is  easier  to  show  what  is  wrong  with  a
 scientific  theory  than  with  a  folk  theory.  A  folk
theory defines common sense itself. When the folk
theory  and  the  technical  theory  converge,  it  gets
even tougher to see where that theory gets in the
way – or even that it is a theory at all [39], p. 33.

Naïve realism in oncopathology takes roughly
this  form:  There  are  the  histogenetic  neoplastic
kinds  “out  there”  waiting  to  be  discovered.  The
attuned  investigator  by  careful  examination  can
identify  these  kinds  in  an  unmediated  way.  The
oncopathological taxonomist is like the field biolo-
gist  venturing  forth  into  the  rainforest  to  identify
and describe all the species of orchids encountered.

Essentialism

Naive realism is essentialist in that it asserts that
while  the  individual  neoplasms  comprising  a
neoplastic kind show great variation, behind that
variation there is an essence that is shared by all
of the members of the kind. This essence amounts
to a set of necessary and sufficient conditions for
membership in the kind; I have referred to these
as criterial features. Furthermore, this “essence”
can  be  approximated  by  the  averages  of  all  the
criterial features of the examined members of the
group; in telecommunication jargon, the average
is the “signal”; the variation is the “noise.”

116

M. Hendrickson

Table 6.3  Some myths of oncopathology

The myth

Opposed to the myth

The myth of the homogeneous, static neoplasm

The myth of histological determinism
The myth of molecular-genetic determinism
(“smallism”, i.e., privileging the causal role of lower
levels of the organizational pyramid over higher levels)
The myths of naïve realism about KNeop’s

•  Realism (we have unmediated direct access to the

way the World is structured)

•  Essentialism (all members of a KNeop’s share a set
of properties that are both necessary and sufficient
for membership; i.e., they share an ‘essence.’)
•  Classification monism (there is one correct and

true way to classify natural individuals into natural
kinds)

•  Experts in the relevant domain have access to the
‘true’ diagnosis

The myth of the disappearance of problem cases in
the fullness of time

The ADF and the histomorphologic crazy quilt; the
individual neoplasm as a multiplicity of evolving clones
Anatomic context dependency
Levels of organization and complexity; emergent properties
of integrated systems; context dependency

We have no direct access to the ‘real’; our interactions with
the World are mediated by a ‘conceptual fabric’; we
co-create oncological reality
KNeop’s possess no essences; KNeop’s are ExtnI CoPeTI
groups; histo-morphologic syndromes

Classification pluralism; the form and structure of a
classification depends upon the background questions being
asked. The coexistence of S-classifications and
M-classifications instantiate this principle in oncopathology
Classificatory macro-revisions
The problem of expert disagreement
For the expert when confronting problem cases (in
possession of ‘complete’ information) the normally
separate acts of diagnosis and classification collapse into a
single activity
Pathology experts are the ‘language police’ of the
oncopathological community
Each INeop is non-controversially unique
There are fundamental limitations to imposing a static
essentialist grid onto an evolutionary process. This is true
whether one is dealing with biological organisms or INeop’s.
Aristotle meets Darwin

This metaphysical outlook pervades our onco-
pathological literature; it is our folk theory of clas-
sification and is encouraged by daily contact with
case material that is easily and nonproblematically
diagnosed  using  the  vague  guidelines  available.
Using  any  half-way  functional  classification,  the
ADF of most cases, of course, will be located near
the center of some CoPeTI group. This pragmatic
fact about an evolved classification is insufficient to
warrant a belief in oncopathological essentialism.

The Role of the Expert

This  myth  amounts  to  the  belief  that  the  expert,
examining a problematic case, can see through the
troublesome variation of the individual neoplasm
to  its  essence  and,  in  possession  of  this  insight,
make the ‘correct’ assignment.

Conclusions

Classification Monism

The myth of classification monism suggests that
in the “Recording Angel’s Book” is inscribed the
one true classification of neoplasms. Our terres-
trial efforts, over time, gradually converge on this
true order.

The analysis of clinically vivid defects of metabo-
lism  (e.g.,  alkaptonuria)  led,  historically,  to  an
understanding  of  normal  metabolic  pathways.
Similarly, an analysis of problem cases led us to
reflections  on  how  classification  and  diagnosis
usually  proceeds  in  oncopathology  and,  ulti-
mately,  the  sketch  of  C&D  presented  in  this
chapter. This perspective has it that these “naïve

6  Principles of Classification and Diagnosis in Anatomic Pathology

117

realist”  positions  are  wrong  in  just  about  every
respect. Noncontroversially, there is nothing more
real than the individual cancer afflicting a patient.
The realist stance has it that the neoplastic kind to
which the individual cancer belongs is as real as
Ms.  M’s  cancer.    There  are  the  neoplastic  types
out  there  to  be  discovered;  decades  of  accumu-
lated ‘field’ experience has produced the current
canonical list of the named neoplastic kinds dis-
covered to date.  When talking about them we use
locutions like: “most cases of X” or “sometimes
X’s  can  be  confused  with  Y’s  because...”  or  “it
can be very difficult to tell an X from a Y” or “X’s
never  have  feature  a...”  Other  realist  discourse
includes: “It used to be thought that X was a real
entity, but now we know it not to be, it is only a
phenotype”  or  “We  report  59  cases  of  a  previ-
ously unrecognized vulvar soft tissue neoplasm..”
or  “Undifferentiated  sarcoma:  does  it  exit?”
Opposed to naïve realism is the idea that a clas-
sification reflects not only what the world has to
offer but also the conceptual fabric in which the
investigator  is  embedded.  In  other  words,  onco-
pathological  classifications  are  a  coconstruction
of investigator and the world. KNeop’s do not have
essences  any  more  than  biological  species  or
medical  genetic  disorders  have  essences.  I  have
argued that KNeop’s have an ExtnI-CoPeTI struc-
ture; they are open-ended and not defined by any
set of necessary and sufficient conditions. “Gold
Standard” for the diagnosis of a KNeop is always
talk about privileged surrogates. I have suggested
that Boyd’s perspective is a  promising alternative
to essentialism. First, it frees us of a conceptual
structure that has not worked in, for example, bio-
logical  systematics.  It  realistically  reflects  what
actually  goes  on  in  biological  classification  by
accommodating: (1) groups that are faithful to the
continuous  spatial  and  temporal  variation  of
KNeop’s;  and  (2)  the  dynamics  of  both  public
macro-revisions  and  private  micro-revisions  so
characteristic  of  oncopathological  classification
and diagnosis.

Opposed  to  classification  monism  is  classifi-
cation pluralism; the commonplace that, even in
biological  systematics,  we  parse  a  particular
domain in many different ways depending upon
our interests. The managerial and nonmanagerial

classifications
oncopathology.

instantiate

this  principle

in

Against  this  background,  what  is  the  role  of
the expert pathologist in a particular oncological
domain?  To  answer  this  question  we  need  to
move beyond the naïve realist view of the expert
as a trained but neutral observer reading off the
structure  of  the  world  in  a  theory  neutral  way.
This is all wrong. Oncological classification and
diagnosis is a community activity and the expert
plays an essential regulatory role in that commu-
nity. Experts determine the correct usage of neo-
plastic  kind  terms;  they  are  the  arbiters  of  the
taxonomic boundary disputes I alluded to above.
Thus, the only Gold Standard is Expert Consensus
and in the absence of that consensus, the ‘right’
answer is undefined. The expert is accomplished
in many ways, but one of them is not the impos-
sible  task  of  identifying  essences.  When  the
expert says: “I have never seen an case of ‘A’ that
had feature ‘x’…” this is not to be construed as a
claim about his special access to essences; it is to
be taken as an convoluted expression of his taxo-
nomic conventions. A more realistic claim is that
the  expert  has  refashioned  the  ‘boundaries’  of
the entity (in some principled way, it is hoped) to
accommodate the problematic case. The expert’s
classificatory vision has changed; he is both clas-
sifying and diagnosing at the same time.

Eventual Disappearance
of Problem Cases

Naïve  realism  encourages  the  belief  that  with
further work problem cases will eventually dis-
appear.  The  perspective  that  this  analysis  pro-
vides, on the contrary, insures the persistence of
hybrid,  in-between, and unique cases; indeed, at
a fine enough level of examination, all cases are
problem  cases.  We  can  think  of  each  type  of
problem case as an exaggeration of features cen-
tral to typical INeop’s. Hybrid cases are, taxonomi-
cally  speaking,  embarrassingly  heterogeneous
either because they have reached back into their
developmental  history  or,  in  their  neoplastic
maldevelopment, have taken all the forks in the

118

M. Hendrickson

road; novel cases are embarrassingly unique; in-
between cases have tapped more shallowly into
their  developmental  history  in  a  way  that  has
them phenotypically bridge two or more devel-
opmentally  related  standard  trajectories  for  tis-
sues in that anatomic site. Think of the neoplastic
counterparts  of  the  uterine  cervical  cells  that
have  both  glandular  and  squamous  features.
There is little hope that the flood of molecular-
genetic data generated by the rapidly proliferat-
ing  high-throughput  technologies  will  change
these  facts  of  diagnostic  and  classificatory  life.
The  central  obstacle  to  this  project  is  summa-
rized  by  the  historian  Forrester:  “The  ideal  of
science  as  certain  knowledge  is  of  course
Aristotle’s ideal. One version of how Aristotle’s
vision  was  finally  contested  and  overthrown
focuses  on  Darwinian  evolution.  The  pre-Dar-
winian Aristotelian theory of the natural world is
founded, it is argued, on the category or species,
arranged  hierarchically  in  order  of  generality.
Darwin’s fundamental break with the Aristotelian
tradition was to see classes or species as consti-
tuted  by  populations  of  individuals  which  vary
along an indefinite number of axes. … the claim
is that it is populations of independently varying
individuals that constitute the base matter of the
natural  and  human  worlds.  All  categories  or
species  are  artificial,  imprecise  and  ultimately
misleading attempts to portray in the outmoded
Aristotelian language of predication [that is, in
crisp,  unambiguous  criteria]  a  fundamental
dynamic  reality  which  can  be  represented  only
statistically.” [65]

What Does Our Analysis Mean
for Evidence-Based Pathology?

EBP, whatever it turns out to be, must address the
issues  raised  in  this  essay:  the  complexity  of
INeop’s; the ExtnI-CoPeTI structure of KNeop’s and
the  creative  role  of  the  classifier-diagnostician.
To the extent that EBP is chiefly concerned with
managerial distinctions, EBM has much to teach
us.  While  there  are  certainly  no  essences  and
extensional  indeterminacy  is  a  reality,  continua
can be discretized, for managerial purposes, in an

arbitrary but principled ways. This theme is fur-
ther  elaborated  in  Chaps.  7  and  10  discussing
validation and decision analysis.

Acknowledgment  This chapter represents a precis of a
book-length work in preparation expanding on these top-
ics. I am indebted to Prof Charitini Douvaldzi (Stanford
University) for invaluable discussions of this material.

References

  1.  Howard  RA.  Foundations  of  professional  decision
analysis:  a  Manuscript  in  process.  Stanford  Course
Notes; 1998.

  2.  Klipp  E,  Liebermeister  W,  Wierling  C,  Kowald  A,
Lehrach H, Herwig R. Systems biology: a textbook.
Wiley-VCH; 2009.

  3.  Wang E, editor. Cancer systems biology. Boca Raton:

CRC Press; 2010.

  4.  Hanahan  D,  Iinberg  RA.  The  hallmarks  of  cancer.

Cell. 2000;100(1):57–70.

  5.  Vogelstein B, Kinzler KW. Cancer genes and the path-
ways they control. Nat Med. 2004;10(8):789–99.
  6.  Ledford H. Big science: the cancer genome challenge.

Nature. 2010;464(7291):972–4.

  7.  Iinberg  RA.  Biology  of  cancer.  New  York:  Garland

Science; 2006.

  8.  Stratton  MR,  Campbell  PJ,  Futreal  PA.  The  cancer

genome. Nature. 2009;458(7239):719–24.

  9.  Stephens  PJ,  McBride  DJ,  Lin  ML,  et  al.  Complex
landscapes  of  somatic  rearrangement  in  human
breast  cancer  genomes.  Nature.  2009;462(7276):
1005–10.

 10. Folkman  J.  Role  of  angiogenesis  in  tumor  growth
and  metastasis.  Semin  Oncol.  2002;29(6  Suppl
16):15–8.

 11. Qian BZ, Pollard JW. Macrophage diversity enhances
tumor progression and metastasis. Cell. 2010;141(1):
39–51.

 12. Polyak K. Breast cancer: origins and evolution. J Clin

Invest. 2007;117(11):3155–63.

 13. Polyak  K,  Kalluri  R.  The  role  of  the  microenviron-
ment  in  mammary  gland  development  and  cancer.
Cold Spring Harb Perspect Biol. 2010;2(11):a003244.
Epub 2010 Jun 30.

 14. Iigelt B, Bissell MJ. Unraveling the microenvironmen-
tal influences on the normal mammary gland and breast
cancer. Semin Cancer Biol. 2008;18(5):311–21.

 15. Merlo LM, Pepper JW, Reid BJ, Maley CC. Cancer as
an  evolutionary  and  ecological  process.  Nat  Rev
Cancer. 2006;6(12):924–35.

 16. Navin N, Krasnitz A, Rodgers L, et al. Inferring tumor
progression  from  genomic  heterogeneity.  Genome
Res. 2010;20(1):68–80.

 17. Marusyk  A,  Polyak  K.  Tumor  heterogeneity:  causes
and  consequences.  Biochim  Biophys  Acta.  2010;
1805(1):105–17.

6  Principles of Classification and Diagnosis in Anatomic Pathology

119

 18.  Navin  NE,  Hicks  J.  Tracing  the  tumor  lineage.  Mol

 38.  Beckner M. The biological way of thought. New York:

Oncol. 2010;4(3):267–83.

Columbia Univ Press; 1959.

 19. Nowel PC. The clonal evolution of tumor cell popula-

 39. Lakoff  G.  Women,  fire,  and  dangerous  things.

tions. Science. 1976;194(4260):23–8.

 20. Mintz  B,  Welmensee  K.  Normal  genetically  mosaic
mice produced from malignant teratocarcinoma cells.
Proc Natl Acad Sci U S A. 1975;72(9):3585–9.

 21. Hofmann WK, Komor M, Wassmann B, et al. Presence
of the BCR-ABL mutation Glu255Lys prior to STI571
(imatinib) treatment in patients with Ph+ acute lym-
phoblastic leukemia. Blood. 2003;102(2):659–61.
 22.  Roche-Lestienne C, Soenen-Cornu V, Grardel-Duflos N,
et al. Several types of mutations of the Abl gene can
be found in chronic myeloid leukemia patients resis-
tant to STI571, and they can pre-exist to the onset of
treatment. Blood. 2002;100(3):1014–8.

 23. Shah NP, Nicoll JM, Nagar B, et al. Multiple BCR-
ABL kinase domain mutations confer polyclonal resis-
tance  to  the  tyrosine  kinase  inhibitor  imatinib
(STI571)  in  chronic  phase  and  blast  crisis  chronic
myeloid leukemia. Cancer Cell. 2002;2(2):117–25.

 24. Corless CL, Heinrich MC. Molecular pathobiology of
gastrointestinal stromal sarcomas. Annu Rev Pathol.
2008;3:557–86.

 25. Gramza AW, Corless CL, Heinrich MC. Resistance to
tyrosine  kinase  inhibitors  in  gastrointestinal  stromal
tumors. Clin Cancer Res. 2009;15(24):7510–8.

 26. Liegl B, Kepten I, Le C, et al. Heterogeneity of kinase
inhibitor  resistance  mechanisms  in  GIST.  J  Pathol.
2008;216(1):64–74.

 27. Solé RV, Deisboeck TS. An error catastrophe in can-

cer? J Theor Biol. 2004;228(1):47–54.

 28. Solé R, Goodwin B. Signs of life: how complexity per-
vades  biology.  New  York:  HarperCollins  Publishers;
2002.

 29. Nowak  MA.  Evolutionary  dynamics.  Exploring  the
equations  of  life.  Cambridge:  The  Belknap  Press  of
Harvard University Press; 2006.

 30. Mas  A,  Lopez-Galindez  C,  Cacho  I,  Gomez  J,
Martinez  MA.  Unfinished  stories  on  viral  quasispe-
cies  and  Darwinian  views  of  evolution.  J  Mol  Biol.
2010;397(4):865–77.

 31. Ereshefsky  M.  Species.  Stanford:  Stanford

Encyclopedia of Philosophy; 2010.

Chicago: University of Chicago Press; 1987.

 40. Margolis E, Laurence S. Concepts and cognitive sci-
ence. Concepts: core readings. Cambridge: The MIT
Press; 1999. p. 3–81.

 41. Rosch  E.  Principles  of  categorization.  Cognition  and
categorization.  Hillsdale:  Lawrence  Erlbaum  Associ-
ates; 1978. p. 312–22.

 42. Vineis P. Definition and classification of cancer: mono-
thetic or polythetic? Theor Med. 1993;14(3):249–56.
 43.  Wittgenstein L. Philosophical investigations. New York:

Macmillan; 1953.

 44. Gould SJ. The Hedgehog, the Fox, and the Magister’s
Pox:  Mending  the  Gap  Between  Science  and  the
Humanities: Harmony; 2003.

 45. Kuhn  TS.  The  structure  of  scientific  revolutions.
Chicago: The University of Chicago Press; 1962.
 46. Kuhn TS. The Copernican revolution: planetary astron-
omy in the development of western thought. Cambridge,
Massachusetts: Harvard University Press; 1976.

 47. Dorfman  RF.  Classifications  of  the  malignant  lym-

phomas. Am J Surg Pathol. 1977;1(2):167–70.

 48. Hendrickson  MR,  Kempson  RL.  Reply:  the  citadel
defended-The  counterattack.  Hum  Pathol.  2000;
31(11):1440–2.

 49. Kempson RL, Hendrickson MR. Ovarian serous bor-
derline tumors: the citadel defended [editorial; com-
ment]. Hum Pathol. 2000;31(5):525–6.

 50. Seidman  JD,  Kurman  RJ.  Ovarian  serous  borderline
tumors: a critical review of the literature with empha-
sis  on  prognostic  indicators  [see  comments].  Hum
Pathol. 2000;31(5):539–57.

 51. Seidman  JD,  Soslow  RA,  Vang  R,  et  al.  Borderline
ovarian tumors: diverse contemporary viewpoints on
terminology  and  diagnostic  criteria  with  illustrative
images. Hum Pathol. 2004;35(8):918–33.

 52. Kurman  RJ,  Seidman  JD.  Ovarian  serous  borderline
tumors:  the  citadel  defended.  Hum  Pathol.  2000;
31(11):1439–42.

 53. Bartels  PH,  Montironi  R.  Quantitative  histopathol-
ogy:  the  evolution  of  a  scientific  field.  Anal  Quant
Cytol Histol. 2009;31(1):1–4.

 32. Frigg  R,  Hartmann  S.  Models  in  science.  Stanford:

 54. Fisher  C.  Synovial  sarcoma.  Ann  Diagn  Pathol.

Stanford Encyclopedia of Philosophy; 2006.

1998;2(6):401–21.

 33. Giere  RN.  Scientific  perspectivism.  Chicago:

University Of Chicago Press; 2006.

 34. Gould SJ. Wonderful Life: the Burgess Shale and the
nature of history. New York: W. W. Norton; 1989.
 35. Bartels  PH.  Future  directions  in  quantitative  pathol-
ogy: digital knowledge in diagnostic pathology. J Clin
Pathol. 2000;53(1):31–7.

 36. Mehta  T,  Tanik  M,  Allison  DB.  Towards  sound
epistemological  foundations  of  statistical  methods
biology.  Nat  Genet.
for
2004;36(9):943–7.

high-dimensional

 37. Russell B. Vagueness. In: Keefe R, Smith P, editors.
Vagueness: a reader Cambridge. MA: The MIT Press;
1999.

 55. van  de  Rijn  M,  Barr  FG,  Xiong  QB,  Hedges  M,
Shipley J, Fisher C. Poorly differentiated synovial sar-
coma: an analysis of clinical, pathologic, and molecu-
lar  genetic  features.  Am  J  Surg  Pathol.  1999;23(1):
106–12.

 56. Krane  JF,  Bertoni  F,  Fletcher  CD.  Myxoid  synovial
sarcoma:  an  underappreciated  morphologic  subset.
Mod Pathol. 1999;12(5):456–62.

 57. Hull  DL.  The  effect  of  essentialism  on  taxonomy  –
two  thousand  years  of  stasis  (I).  Br  J  Philos  Sci.
1964;61:314–26.

 58. Hull  DL.  The  effect  of  essentialism  on  taxonomy  –
two  thousand  years  of  stasis  (II).  Br  J  Philos  Sci.
1965;16(61):1–18.

120

M. Hendrickson

 59. Hull D, Ruse M. The philosophy of biology. Oxford:

Oxford University Press; 1998.

 60. Boyd  R.  Homeostasis,  species,  and  higher  taxa.
Species:  new  interdisciplinary  essays.  Cambridge:
The MIT Press; 1999. p. 141–86.

 61. Boyd  R.  Scientific  realism.  Stanford:  Stanford

Encyclopedia of Philosophy; 2002.

 62. Keller RA, Boyd R, Wheeler QD. The illogical basis
of phylogenetic nomenclature. Bot Rev. 2003;69(1):
93–111.

 63. Chiong W. Brain death without definitions. Hastings

Cent Rep. 2005;35(6):20–30.

 64. Wilson  RA.  Genes  and  the  agents  of  life.  The  indi-
vidual  in  the  fragile  sciences  –  biology.  Cambridge:
Cambridge University Press; 2005.

 65. Forrester J. If p, then what? Thinking in cases. Hist

Human Sci. 1996;9(3):1–25.

 66. Oltvai ZN, Barabási AL. Systems biology. Life’s com-
plexity pyramid. Science. 2002;298(5594):763–4.

 67. Dalton  LW,  Pinder  SE,  Elston  CE,  et  al.  Histologic
grading of breast cancer: linkage of patient outcome
with  level  of  pathologist  agreement.  Mod  Pathol.
2000;13(7):730–5.

 68.  Elston CW, Ellis IO. Pathological prognostic factors in
breast  cancer.  I.  The  value  of  histological  grade  in
breast cancer: experience from a large study with long-
term follow-up. Histopathology. 1991;19(5):403–10.
 69. Crombie  AC.  Styles  of  Scientific  Thinking  in  the
European  Tradition.  The  history  of  argument  and
explanation  especially  in  the  mathematical  and  bio-
medical sciences and arts. London: Duckworth; 1994.
 70. Hacking  I.  Style  for  historians  and  philosophers.
Historical  ontology.  Cambridge:  Harvard  University
Press; 2002. p. 159–77.

 71. Hendrickson M. Exorcizing Schrödinger’s ghost: reflec-
tions on what is life? And its surprising relevance to can-
cer  biology.  In:  Gumbrecht  HU,  Harrison  R,  editors.
Schrodinger. Palo Alto: Stanford University Press; 2010.

Evaluating Oncopathological
Studies: The Need to Evaluate
the Internal and External Validity
of Study Results

Michael Hendrickson and Bonnie Balzer

7

Keywords
Evaluation  of  oncopathological  studies  •  Prognostic  classification  rules
• Evidence-based pathology • Validity of study results • Gene expression
arrays

Published oncopathological studies purport to tell
us not only how the world appears to the investi-
gator but makes the stronger additional claim that
the  world  will  look  the  same  way  to  us.  What
guarantees are there that the data presented by the
investigator  justifies  his  view  of  the  world?  The
term  ‘internal  validity’  captures  this  worry.
Threats  to  internal  validity  can  be  grouped  into
problems related to either chance or bias.  Chance
issues: did the investigators look at a large enough
sample  to  develop  an  accurate  picture  of  the
domain  they  are  describing?  This  is  the  statisti-
cian’s problem of Type I/II error, study power and
sample size calculations. Worries of this sort are
allayed by maximizing the sample size. Another
question:  is  their  picture  of  the  world  more
nuanced  and  detailed  than  their  data  warrants?
This is the problem of ‘over-fitting’ and has at its
heart the ‘curse of dimensionality.’ This problem
is particularly acute for the high dimensional data
produced  by  –omics  research.  The  simple,  but
sometimes  unrealistic,  cure  for  this  worry  is  the
validation of the study’s conclusions using a com-

M. Hendrickson (*)
Department of Pathology, Stanford University Medical
Center, Stanford, CA 94305, USA
e-mail: hendrickson@stanford.edu

pletely different set of cases drawn from the rele-
vant domain.  In the absence of such an independent
sample,  there  are  a  number  of  cross-validation
techniques that can partially address this problem.
Bias refers to the systematic erroneous association
of some characteristic with a group in a way that
distorts a comparison with another group.  Bias is
directly addressed through the appropriate design
of experimental studies and by randomization in
clinical  interventional  trials;  there  are  no  such
safeguards  in  non-experimental  observational
research.  Investigator  intra-observer  and  inter-
observer agreement: No two pathologists have (in
the  language  of  Chap.  6)  an  identical  classifica-
tory vision. Are there important differences among
the  investigators  in  their  agreement  on  the  mor-
phologic evaluations presented in the study?

External validity: What guarantees are there
that the investigator’s view of the world will be
ours? The study may pass internal validity tests
but  the  vision  it  provides  may  have  little  to  do
with the world as we will perceive it. This is a
question  about  generalizability,  or  ‘external
validity.’ For example, one may question whether
valid  conclusions  drawn  from  a  study  of  cases
extracted  from  the  expert  pathologist’s  files
have  much  to  do  with  community  pathology
practice.

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_7, © Springer Science+Business Media, LLC 2011

121

122

M. Hendrickson and B. Balzer

Table 7.1  Evaluation of an oncopathological study

Overall study design

Managerial claim study?
Scientific claim study?
Unsupervised or supervised classification

Internal validity
Chance

Is sample size adequate?
Is the data overfitted?

Biases

Missing data bias
Short follow-up bias
Referral (selection) bias
Spectrum bias
Confounding factor bias
Verification bias

External validity

Can the results of this study be generalized to other
cases?

Communicability

Observer agreement among the authors
Communication  of  classificatory  vision  to  potential
users

Were the morphologic distinctions described in the
study communicated successfully?

Relevance to the reader

Are the study results of practical significance to my
practice?

Another  problem  central  to  oncopathological
studies is the communication of the investigator’s
‘classificatory vision’.  As discussed in Chapter 6,
surgical  pathology  C&D  is,  obviously,  a  highly
visual,  impressionistic  activity  and  passing  from
the  visual  to  the  conceptual  and  verbal  poses
 challenges not faced by other clinical disciplines.
Failure to communicate the relevant morphological
criteria can occur at several levels and may under-
mine the impact of an otherwise valid study.

In  this  chapter  we  will  survey  these  topics
(Table 7.1).  For narrative convenience we depart
from  the  strict  outline  of  the  table  at  various
points. We will draw upon several Stanford gyne-
cologic  pathology  studies  to  make  these  points.
We  do  this,  not  to  slight  other  workers,  but
because  these  are  the  problems  with  which  we
have most hands on experience. We finish with a
brief  overview  of  the  substantial  informatics
problems faced by genomic studies.

The Overall Design of an
Oncopathological Study

What Is the Goal of the Study?
Managerial Classification
or Something Else?

As discussed in Chap. 6, it is important in oncopa-
thology to distinguish between those studies whose
purpose is to make serious risk/prognostic/predic-
tive (RPP) claims – future clinical course or clinical
phenotype, FClin(t) for short – and those that do not.
Histogenetic classifications, an example of a scien-
tific classification and managerial classifications, are
quite different on a number of counts. Histogenetic
modeling involves postulating a number of plausible
mechanism that produce the observed phenotypes of
the  neoplasms  in  a  given  domain  (see  Fig.  6.7  and
related  discussion).  For  managerial  classifications
the  taxonomic  modeling  exercise  now  takes  the
form of fashioning statistically credible, distinct lot-
teries by dividing, in a suitable way, a multivariate
continuum (Figs. 6.12–6.14).

The first step in analyzing a oncopathological
report is to have a clear idea of the investigator’s
intent and the type of classification modeling in
which the authors are engaged. Is the study present-
ing  an  interesting  new  neoplastic  type  or  kind
(KNeop) (“stroll through the phenospace”) or per-
haps a new KNeop with some comments on clinical
outcome but, with no serious managerial claim?
Or,  is  the  study  making  a  serious  managerial
claim?  These  usually  conclude  with  something
like: “It’s essential that you make this distinction
or patients will be disadvantaged!”

Supervised and Unsupervised
Classification Models
The managerial/nonmanagerial distinction can be
sharpened by turning to the machine learning con-
trast between supervised and unsupervised classi-
fication. Supervised learning is the task of inferring
a classification rule from a “supervising” training
set. That is, the observed features are partitioned
into  “predictors”  (typically,  individual  gross  or
histomorphologic features) and “outcomes” (some
aspect of FClin(t)). The training set consists of a

7  Evaluating Oncopathological Studies

123

set of cases with known outcomes. A supervised
learning algorithm analyzes the training data and,
with one eye on the outcome, uses the predictors to
group cases that concentrate, for example, “good
actors” and “bad actors”; that is, it produces a clas-
sification  rule,  almost  invariably  with  some  mis-
classification rate. The hope is that the classification
rule will predict the correct outcome for any col-
lection of unexamined cases test sets in the domain.
Realization of this hope requires the learning algo-
rithm  to  accurately   generalize  from  the  training
data  to  unseen  situations  encountered  in  the  test
set. Again, there is always a misclassification rate,
and  almost  always  the  misclassification  rate  is
higher for the test set than for the training set.

In  unsupervised  classification,  on  the  other
hand, all observed features are taken as an unpar-
titioned ensemble and a search is undertaken for
“natural” grouping or clustering in the data. Despite
its apparent objectivity (‘letting the observations
speak for themselves’) this is not, by any means,
a  theory-free  process.  These  techniques  require
substantial  input  from  the  investigator:  among
other things, the selection of the individuals to be
studied, the features to be examined (or not), the
scales used to evaluate those features, statistical
pre-processing  of  those  measurements,  to
normalize  them  or  not,    a  choice  of  similarity
metric (e.g., Eulidean, Mahalanobis), a choice of
clustering techniques, a specification of the num-
ber of clusters the investigator thinks is present in
the data, a specification of a threshold for form-
ing groups, the kind of intracluster structure one
is looking for (e.g., Gaussian), etc. Cluster analy-
sis  in  its  various  forms  is  the  tool  employed  in
unsupervised classification [1–3].

Recasting in these terms, our original question
about the oncopathological study under examina-
tion,  then,  is:  “Is  this  study,  structurally,  some
version of supervised or unsupervised classifica-
tion and, if supervised, is the supervising feature
some FClin(t)?”

Exploratory and confirmatory statistics
and computers
The  advent  of  high-speed  computation,  made
exploratory  data  analysis  possible.  Exploratory
data  analysis  (EDA)  is  an  approach  to  analyzing

data  for  the  purpose  of  suggesting  hypotheses
worth testing. EDA complements the tools of clas-
sical statistics designed to test hypotheses. No lon-
to
ger  were  statisticians’  analyses  confined
hypothesis  testing  using  mathematically  tractable
parametric  techniques  (e.g.,  normal  distribution
theory), but they could examine high-dimensional
data using nonparametric computer intensive tech-
niques.  Exploratory  data  analysis  of  high  dimen-
sional data sets brought with it the need for directly
visualizing this data in a perspicuous and convenient
way [4]. Through the use of rotating scatter plots,
color coding, the use of a variety of symbols, high-
dimensional data could be examined and manipu-
lated. These capabilities are now standard in laptop
statistical programs like JMP or StatView and plots
of this sort appear routinely in the -omic literature.

We have employed these techniques in several
Stanford  studies  since  the  1980s.  The  graphics
used in our study of problematic uterine smooth
muscle  neoplasms  (Fig.  7.1)  and  our  study  of
serous low malignant potential tumors (Fig. 7.2)
illustrate this point.

Diagnostic and Predictive Components
of Oncopathological Studies

Oncopathological  observational  studies  inherit
all the methodological complications of clinical
observational  studies  [5,  6]  but  have  the  added
special  problems  of  reproducibly  making  and
communicating histopathological distinctions.

For  the  purposes  of  this  discussion,  we  can
distinguish the predictive components of the study
(internal and external validity) and the diagnostic
component (investigator observer agreement and
the  translation  and  transmission  problems  pecu-
liar to oncopathological communications).

The Predictive Component: Internal
and External Validity

Anatomic surgical pathology is a largely regulation-
free discipline; we do our own policing. There
is no FDA oversight of conventional light micro-
scopic  distinctions  that  are  employed  routinely  in

124

a

HYALIN NECROSIS

No Necrosis or Only Hyalin Necrosis

M. Hendrickson and B. Balzer

All unspecified
numbers represent
follow-up time in
months

Rare

1≤ MI ≥ 2

2 ≤ MI ≥ 5

5 ≤ MI ≥ 10

10 ≤ MI ≥ 20

20 ≤ MI

NO NECROSIS

Diffuse
Severe
Atypia

Diffuse
Moderate
Atypia

Focal or
Multifocal
Moderate or
Severe
Atypia

Mild
Atypia

No
Atypia

Case 10
I-NED 26

Case 13
I-AWD 60

116 106

38

137

38

52

41

35

121

60

146 67

24

I-NED 66
Case 11
Case 12
I-NED 56

27 96

60

31

31 44

94

I-NED 147
Case 3

55 60

35

44

I-DID 120
Case 2

I-NED 29
Case 1

I-AWD 156
Case 4

51 54 40

57 86 93

33 64 52 104

56

68

98
I-DOD-23

I-DOD-64

73

45

38

70

58

27

40

47

120

141

66

54

69

39

56

132

100

24

63

I-DOD 96
Case 5

51

89

38

34

43

I-NED 31
Case 6

I-NED 58
Case 7

IV-DOD-4

I-DOD-14

58
Case 14

36

102
Case 8

78
Case 9

Group II

Group V

Group I

Rare

1≤ MI ≥ 2

2 ≤ MI ≥ 5

5 ≤ MI ≥ 10

10 ≤ MI ≥ 20

20 ≤ MI

“ATYPICAL LEIOYOMA WITH RECURRING POTENTIAL”

“LEIOMYOSARCOMA”

“LEIOMYOMA WITH LIMITED EXPERIENCE”

“LEIOMYOMA”

= NED or DID

   (

age ≥ 50 yrs)

= Unusual cases (see legend)

= High stage presentation or recurrent extra-uterine neoplasm

(      age ≥ 50 yrs)

 AWD = alive with disease

 DOD = dead with disease

Fig.  7.1  Uterine  smooth  muscle  charts.  Taking  the  two
plots  together,  six  dimensions  of  data  are  depicted;  our
assignments (color coding of regions of the phenospace)
are  a  seventh.  Graphical  displays  preserving  the  covari-
ance structure of the data set and are easier to assimilate
than entries in a complex table [71]. We employed contour
lines to make clear important exclusion from this study. In
the no-necrosis/hyaline-necrosis plot the bottom left contour

lines remind us of the Mt. Everest of mitotically inactive leio-
myomas not included in the study that serve to put “benign
metastasizing leiomyoma” in its proper context. In the coag-
ulative tumor cell necrosis plot, the bottom left reminds the
reader of the large number of acutely infarcted leiomyomas
not included in the study and the top right contour lines, the
larger number of diagnostically nonproblematic leiomyo-
sarcomas that were not part of the study group

patient  care  decision-making  or,  for  that  matter
(as  the  disclaimer  on  all  our  surgical  pathology
reports  makes  explicit),  immunohistochemical
tests. Treatment recommendations are often made
on the basis of what, in the cancer marker world,
would be Level 4–5 (Level 1, being the best) evi-
dence.  By  contrast,  prognostic  and  predictive
gene  expression  array  (GEA)  markers  have
received  intense  scrutiny.  In  the  service  of  evi-
dence-based reasoning in pathology, it  is useful
to start thinking of proffered FClin(t) claims within
the framework of cancer marker studies and clini-
cal prediction rules.

The evidentiary rules for cancer marker studies
have been extensively discussed in the recent lit-
erature by Hayes and his group and others [7–11].
Table 7.2 provides a list designed for GEA mark-
ers. How would these apply to oncopathological
classification using conventional light microscopic
features?  A  variant  of  breast  cancer  is  reported:
“A distinctive clinicopathological entity … with a
particularly good (or bad) prognosis: report of 54
cases.” This can be framed as a prognostic cancer
marker claim. Questions to ask: (1) really? – validity
issues;  and  (2)  am  I  better  off  with  this  marker
than the  -existing ones? What is the incremental

7  Evaluating Oncopathological Studies

Focal or Extensive Coagulative Tumor Cell Necrosis
N = 41 Failure Rate = 68%
5 ≤ MI ≥ 10

10 ≤ MI ≥ 20

2 ≤ MI ≥ 5

1≤ MI ≥ 2

Rare

III-DOD 21 m
Case 16

I-DOD 67 m
Case 15

I-DOD 204 m
Case 17

I-DOD 60 m
Case 18

25 m

93 m

70 m

29 m

60 m

I-DOD 27m

I-DOD 8 m

71 m

I-DOD 6 m

I-DOD 11 m

49 m

120 m

49 m

Case 19
I-CR 183 m

Case 20
I-AWD 10 m

III-CR 21 m

III-CR 4 m

b

COAGULATIVE
TUMOR CELL
NECROSIS

Diffuse
Severe
Atypia

Diffuse
Moderate
Atypia

Focal or Multifocal
Moderate or Severe
Atypia

Mild
Atypia

No
Atypia

• Rule out
infarcted
submucous
myomas

• Rule out
phase of hyalin
necrosis before
‘organization’
has occured

Case 21
I-AWD 84 m

29 m

46 m

III-AWD 53 m
Case 22

165 m

62 m

125

IV-DOD 18 m
III-DOD 9 m

I-DOD 192 m
I-CR 133 m
I-DOD 84 m
I-DOD 48 m
I-DOD 19 m
I-DOD 6 m
I-DOD 5 m

Group III

Group IV

20 ≤ MI

III-CR 24 m

I-AWD 34 m

I-DOD 20 m

I-DOD 17 m
Case 23

I-CR 8 m
Case 24

Rare

1≤ MI ≥ 2

2 ≤ MI ≥ 5

5 ≤ MI ≥ 10

10 ≤ MI ≥ 20

20 ≤ MI

“LMP WITH LIMITED
  EXPERIENCE”

“LEIOMYOSARCOMA”

= No evidence of disease (NED) or died of intercurrent disease (DID)

(      age ≥ 50 yrs)

= Presentation at high stage presentation or recurrent extra-uterine neoplasm

(      age ≥ 50 yrs)

 AWD = alive with disease

 DOD = dead of disease

 CR = clinical remission

Fig. 7.1  (continued)

FClin(t)  insight  from  this  classification  over  and
above the  classifications I usually employ? Would
the  usual  grading  scheme  have  picked  up  this
 difference? These are the basic questions addressed
in the cancer marker analytic literature.

Clinical  prognostic  models  use  a  variety  of
patient descriptors to fashion a multivariate clas-
sification rule that assigns the patient to an out-
come  category,  for  example,  low,  intermediate,
and high risk. Examples include the Nottingham
prognostic index to estimate the long-term risk of
cancer  recurrence  or  death  in  breast  cancer
patients [12]. The notions of training set/test set,
overfitting,  validation,  curse  of  dimensionality,
etc. permeate the analytic literature in this area.
Several  brief,  accessible  introductions  to  prog-
nostic  models  have  appeared  recently  [13–18].
An  introduction  to  multivariate  statistics  is  pro-
vided by Katz [19, 20].

The  histopathologic  version  of  this  takes  as
predictors  a  variety  of  gross  and  histological
 features  and  plays  them  off  against  a  specified
FClin(t). For example, the Stanford study attempting
to  fashion  a  clinically  relevant  morphologic
definition  of  well-differentiated  endometrial
adenocarcinoma was cast in the format of a prog-
nostic  model  using  myoinvasion  as  a  surrogate
for clinically relevant disease [21]. All promising
H&E features were recorded and, with the aid of
CART feature selection and validation, a subset
was selected that optimally concentrated myoin-
vasive  positive/negative  cases.  Other  examples
are provided by various multivariate classification
rules  using  gross  and  histological  features  to
separate adrenal cortical neoplasms into clinically
benign  and  malignant  groups  [22]  and  sorting
thymomas  into  prognostically  relevant  histo-
pathological groups [23–26].

126

M. Hendrickson and B. Balzer

Serous LMPS:276 Patients with at least 5 years follow-up

Stage I
N = 163 (59%)

Stage II-IV
N = 113 (41%)

288

A

50

26

46

94

190

22

51

310

A
98

A

13

227

64

A

7

A

A

A

A

A

A

A

A

A

A

A

37

L
A
A

A

A

A

A

A

A

A

A

A

Rec = 22 (13%)

DOD = 4 (2%)

p<.001

p<.02

Rec = 39 (35%)

DOD = 10 (9%)

L

158

60

73

227

Implants
Invasive
Indeterminate
(Invasive vs.
non-invasive)
Indeterminate
(No slide available)

Microinvasive

Micropapillary

months

LMP    Ca
Time of transformation

Unresectable

A

Autoimplantation

Recurrent S-LMP

S-LMP at 2nd Look

Clinical Status

NED

DID

AWD

DWD

DOD

M

L=Leukemia
M=”Mesothelioma”

Fig. 7.2  Ovarian serous low malignant potential tumors
(S-LMPs). In this figure, the abscissa and ordinate serve
nonnumeric  functions:  low  stage  vs.  high  stage  and,
roughly, “interesting” and “uninteresting case.” All of the
cases in the study are represented; most of the cases are,
from the perspective of our study goals, relevant only in
making clear important denominators. The top part of the
plot is used to spread out cases of interest: those exhibit-
ing  microinvasion,  micropapillary  features,  those  that
transitioned to well-differentiated carcinoma, the  character

of  the  implants,  etc.  These  are  represented  by  symbols;
color  codes  for  clinical  outcome.  Again,  we  have
preserved the covariance structure of the data; for example,
the covariation of micropapillary features and microinvasion.
An  additional  advantage  of  this  sort  of  representation  is
that the reader can query our database. A glance provides
the reader with the distribution of our cases and an idea of
the confidence one can have in statistics relating to specific
subgroups of cases. A picture is, it turns out, worth more
than a thousand words [72]

Critical Evaluation of the Validity
of Oncopathological Studies
The  critical  evaluation  of  an  oncopathological
study involves answering five questions: (1) What
was the role of chance in producing the claimed
results  (issues  of  sample  size  and  overfitting)?
(2) Are biases implicated in producing the results
(e.g., selection bias, spectrum bias, confounding
factors)?  (3)  Can  the  results  of  this  study  be
 generalized to other cases? (4) Were the morpho-
logic distinctions described in the study commu-
nicated successfully? (5) Are the study results of
practical significance?

Sampling Issue: What Has Been
Included in the Study? Carving Out the
Study Group from the Larger Domain

Using  the  metaphor  of  the  phenospace  devel-
oped in Chap. 6, we can think of the investiga-
tor’s study group as being formed by carving a
(high  dimensional)  patch  out  of  the  pheno-
space. This patch will include cases exhibiting
features  over  a  certain  multivariate  range  and
will exclude cases falling outside those ranges.
Fig. 6.12 of invasive ducal carcinomas conveys
this image.

7  Evaluating Oncopathological Studies

127

Table 7.2  Study design

What is the goal of the
study?

Type of data collection

Sample size

Sampling method

Study material

Descriptive (“stroll through
the phenospace”)?
Correlated feature?
Managerial claim?
Retrospective
Prospective
Is the sample sufficiently large to
detect in a statistically credible
way the difference claimed (or,
alternatively) not found?
Sample of convenience
Stratified sampling
Random sampling
What has been included?
What is the spectrum of cases?
What has been excluded?

Evaluation of the phenospace being evaluated
in the study raises the following general questions:
What is the spectrum of cases included in the study?
What did the investigators’ cases look like? What
features  were  regarded  as  criterial?  Were  some
features more important than others? What were
salient but noncriterial features? What is required
is a multivariate representation that preserves the
covariance structure of the case data and, in particu-
lar, links the clinical outcome with each case.

Another Sampling Issue: What Was
Excluded from the Study?
As  discussed  above,  the  entity  the  investigator  is
reporting is embedded in a study is typically only
part of a larger phenospace. The question of what
was  left  out  is  particularly  important  when  the
investigators are making a managerial claim. How
was the cut made along the boundaries delimiting
“good actors” and “bad actors” within this sample?
These distinctions are usually found in the differ-
ential diagnosis section in the discussion section of
the paper. That discussion should go beyond report-
ing  the  typical  features  of  the  contrasted  entity;
rather, it is more helpful to discuss the resolution of
problem cases at that boundary and how the authors
resolved them. The uterine smooth muscle scatter
plot makes these cuts explicit in Fig. 7.1. Venn dia-
grams  provide  another  tool  for  depicting  high-
dimensional  data  in  two  dimensions  and   seldom
represent  more  than  three  dichotomous  variates.
The  British  mathematician  A.  W.  F.  Edwards

developed a simple method of generalizing Venn
diagrams  to  higher  dimensions  [27, 28].  Fig. 7.3
illustrates one use.

Yet Another Sampling Issue:
Is the Sample Large Enough to Support
the Study’s Conclusions?
More  experience  is  better  than  less  experience.
This simple thought elaborated in the hypothesis
testing  framework  yields  the  mathematically
sophisticated  apparatus  of  sample  size  calcula-
tions;  the  number  of  cases  required  to  detect  a
specified difference between two groups [29–31].
The  statistical  hypothesis  model  is  set  out  in
Fig. 7.4. The behavioral psychologist have iden-
tified inattention to the importance of sample size
as the belief in the “law of small numbers”; that,
for example, the averages calculated from small
samples are as good as those derived from large
samples  [32].  The  essentialism  discussed  in
Chap. 6 appears to ground this belief.  After all,
says  the  confirmed  essentialist,  you  don’t  need
many  cases  to  identify  the   clinicopathologic
essence  of  a  particular  KNeop.  Symptomatic  of
small sample size problems are the outcome sta-
tistics of series with small number of cases – rare
diseases  especially – yield unstable measures of
clinical  outcome;  “survival  ranges  from  20  to
80%”: translation, “you almost certainly will be
cured  of  this  disease,”  or  “you  almost  certainly
will die of this disease.” For example, the large
and  conflicting  literature  about  the  prognostic
relevance of heterologous elements in malignant
mixed  tumors  of  the  uterus  is  based  on  studies
with few subjects. Prognoses, from these under-
powered studies, for tumors with various types of
heterologous elements studies ranged from “very
bad”  to  “irrelevant”  to  “good.”  It  took  a  large
GOG  study  of  clinical  stage  I  cases  to  begin  to
sort this out [33].

The lesson: if a serious claim is made about dif-
ferences in prognosis between two tumor types, it
should  be  against  the  background  of  sample  size
calculations.  How  many  cases  would  need  to  be
studied to establish, in a statistically credible way,
the claimed RPP difference? A related issue is the
problem  of  testing  multiple  hypotheses;  this  is
particularly a problem for high-dimensional data.

128

M. Hendrickson and B. Balzer

FIBROMUSCULAR STROMA ABSENT

FIBROMUSCULAR STROMA PRESENT

Well Differentiated Carcinoma

Carcinosarcoma

FOCAL

Well Differentiated
Endometrioid
Carcinoma of Cervix

Architectual
INDEX

H
I
G
H

MORULES/SQUAMOUS

Well  Differentiated
Carcinoma

with

APA - LMP
N = 1

Squamous
Elements

Focal Well
Differentiated Carcinoma

Focal Well
Differentiated
Carcinoma with
Squamous Elements

APA - LMP
N = 23

Focal Complex
Hyperplasia

Focal Carcinoma with
Low Architectural Index
but High Grade Cytology

Endometrial Polyps
Endocervical Polyps

Well Differentiated
Endometrioid
Carcinoma of Cervix

FOCAL

Complex Hyperplasia

Focal Complex
Hyperplasia
with Squamous
Elements

APA
N = 29

PA
N = 1

Complex
Hyperplasia
with Squamous
Elements

MORULES/SQUAMOUS

Architectual
INDEX

L
O
W

Architectual
INDEX

H
I
G
H

FOCAL

Fragments of Myoinvasive
Carcinoma
N = 19

Carcinofibroma

Well Differentiated
Endometrioid
Carcinoma of Cervix

APA - LMP
N = 1

APA
N = 1

PA
N = 1

Fragments of Myoinvasive
Carcinoma
N = 3

Adenofibroma/
Adenosarcoma

Endometrial Polyps
Endocervical Polyps

Well Differentiated
Endometrioid
Carcinoma of Cervix

FOCAL

Adenofibroma/Adenosarcoma

Architectual
INDEX

L
O
W

FIBROMUSCULAR STROMA ABSENT

FIBROMUSCULAR STROMA PRESENT

PA                 = Polypoid Adenomyofibroma
APA              = Atypical Polypoid Adenomyofibroma
APA - LMP    = Atypical Polypoid Adenomyofibroma of Low Malignant Potential

Fig.  7.3  The  atypical  polypoid  adenomyofibroma  study:
differential diagnosis Venn diagram. We used a four vari-
ates  Venn  diagram  to  depict  the  differential  diagnosis  of
atypical  polypoid  adenomyofibroma.  In  each  of  the  cells
defined by this partition, we list the differential diagnostic
possibilities.  Four  morphologic  features  are  depicted:
(a) architectural index (top half, high; bottom half, low); (b)
the  presence  of  a  prominent  fibromuscular  stroma  (right
half of rectangle) or its absence (left half of rectangle); (c)
the focality of the process (inside dumbbell, focal; outside

dumbbell, diffuse) manifest in the hysterectomy specimen
as focality and in a sampling as dimorphism; and (d) the
presence  of  squamous  or  morular  differentiation  (inside
central oval) or its absence (outside central oval). The pres-
ence or absence of each of these four features defines 16
different  morphological  combinations.  The  diagnostic
possibilities that correspond to these patterns are set out in
the appropriate overlap regions. In summary, this diagram
both  indicates  differential  diagnostic  possibilities  and  the
‘carving out’ process that resulted in our study group [73]

Is the Level of Detail of the Conclusions
Unrealistic Given the Sample Size?
Less well appreciated than underpowered studies
is  the  problem  of  overfitting.  More  information
about a fixed number of cases may not be better
when  it  comes  to  forecasting  a  FClin(t).  That  is,
the  addition  of  refinements  (new  features)  to  a
 classification rule recorded from a fixed number
of cases may be unhelpful. Recall the discussion
of  training  and  test  sets  above.  The  problem  is
with “overfitting” the training set; that is, provid-
ing too elaborate a characterization of the study
group used in training the classification rule. This
would  be  fine  if  the  world  was  exactly  like  the

sample,  sadly,  it  is  not  [34].  This  is  a  serious
problem  for  high  dimensional  biology  (HDB)
(see  below)  but  also  a  problem  for  the  lower
dimensional biology of histopathological predic-
tion rules.

The simple (but often unrealistic) remedy for
overfitting is the validation of the classifier using
a completely different set of cases. Some protec-
tion  against  overfitting  is  provided  by  cross-
validation.  The  method  involves  sequentially
leaving out parts of the original sample (“split-
sample”) and conducting a classifier; the process
is  repeated  until  the  entire  sample  has  been
assessed.  The  results  are  combined  into  a  final

7  Evaluating Oncopathological Studies

129

Neyman-Pearson
Hypothesis Testing

Reject

Accept

H True

H False

Power
1−β

.80

β

1−α

α
.05

Conditional
Probabilities so
have to sum to 1

α

1−β

Usually fix at some specified level
eg. .05 or ,01

Power is a function of the size of the sample

Fig. 7.4  Type I and Type II errors in a 2 × 2 table. The
hypothesis  is  that  a  difference  exists  between  two
groups.  Type  I  a  error  amounts  to  the  erroneous  con-
clusion  that  there  is  a  difference  between  compared
groups when no difference exists. We can think of it as
the false discovery rate. Similarly, type II error (b error)
is  the  false-negative  conclusion  that  there  is  no  differ-
ence  when,  in  fact,  a  difference  does  exist.  Power  is
defined as (1−b); the probability of correctly identifying
a  difference.  The  probabilities  of  these  two  kinds  of
error  are  parameters  that  are  set  by  the  investigator.
Typical  choices  are:  a = 0.05  and  (1−b) = 0.80.  While
neither error can ever entirely be avoided, a simple method
to  decrease  their  probability  is  to  increase  the  sample
size.  Interestingly  (and  controversially),  Ioannidis  by
analyzing  the  logic  behind  hypothesis  testing  and  the
usual choices for the size of type I and II errors (contro-
versially)  concluded  that  most  published  studies  pro-
duced false conclusions [36]

model  that  is  the  product  of  the  training  step
[34].  CART  (classification  and  regression  tree
analysis) incorporates cross-validation as it con-
structs  an  optimal  decision  tree  [35].  We  used
CART  in  our  study  of  endometrial  carcinoma
[21]. The exploratory tree (constructed using all
of the data in the training set in all its particular-
ity)  was  very  elaborate  and  contains  dozens  of
nodes; cross-validation typically prunes the tree
down to three or four nodes.

Effect of Missing Data
Missing data can be fatal to the conclusions of the
study, or not. Certainly, studies that make serious
FClin(t)  claims  and  are  missing  much  of  the  fol-
low-up information are suspect. This also applies

to important potentially confounding factors that
would  bear  on  FClin(t):  size  of  tumor,  location,
resectability, etc.

Bias
Bias refers to the systematic erroneous associa-
tion  of  some  characteristic  with  a  group  in  a
way  that  distorts  a  comparison  with  another
group. Ioannidis defines bias as “the combina-
tion of various design, data, analysis, and pre-
sentation factors that tend to produce research
findings  when  they  should  not  be  produced
[36].” Vineis defines bias as “results that are the
consequence  of  an  erroneous  study  design
[37].”  There  is  a  substantial  epidemiology  lit-
erature  on  dozens  of  forms  of  bias;  most  of
these  are  not  directly  relevant  to  biases  in  the
oncopathological literature [38].

Bias  is  directly  addressed  through  the  appro-
priate  design  of  experimental  studies  and  by
 randomization in clinical interventional trials, but
there are no such safeguards in nonexperimental
observational research.

Short  Follow-up  Bias:  This  is  a  particularly
important source of bias, obviously, for studies of
neoplasms  that  have  a  long  clinical  course.
Examples  from  gynecologic  pathology  include
serous  borderline  surface  epithelial  neoplasms,
endometrial  stromal  neoplasms,  and  granulosa
cell  tumors.  For  example,  the  initial  impression
of granulosa tumors was that they were clinically
benign, but longer follow-up studies from Britain
and Scandinavia disabused us of this notion. So,
short  follow-up  of  such  neoplasms  yields  mis-
leadingly high relapse-free survival estimates.

Referral  (Selection)  Bias  and  Spectrum  Bias:
It is a commonplace that university practice dif-
fers  substantially  from  community  practice.  It
follows that emptying the consultants’ files at a
university center will yield a different set of cases
than  a  comparable  emptying  of  the  community
pathologist’s files. The bias reflects that fact that
consultants tend not to get straightforward cases
(so  atypical  cases  are  overrepresented)  and
university oncology units tend to get therapeuti-
cally challenging cases (so bad actors tend to be
overrepresented)

130

M. Hendrickson and B. Balzer

In  general,  there  is  an  overrepresentation  of
difficult cases and clinically malignant cases. For
example, the natural history of leiomyosarcoma as
depicted in the Stanford study is completely atypi-
cal;  the  number  of  young  women  is  way  out  of
proportion to national age distributions for this dis-
ease. Reason: pathologists send in cases from young
women for verification. In summary, the spectrum
of  cases  reported  reflects  the  accrual  practices  of
the  institution  (either  the  clinical  services  or  the
consultation  practice  of  the  pathologist)  both  in
terms of recruitment into the study and the spec-
trum of clinical outcomes in the study group.

Confounding Factors: We discussed in Chap. 6 the
myth  of  histopathologic  determinism.  Another
way  of  thinking  about  this  is  in  terms  of  con-
founding  factors.  One  has  the  impression  from
some of our morphological literature that the only
phenotypic feature that matters for a patient is the
histopathologic  phenotype  of  her  INeop.  The  huge
success of the TNM staging system reminds us of
the  importance  of  nonhistopathological  features  in
determining FClin(t).
Verification Bias: There is a straightforward ques-
tion  to  be  asked  of  a  study:  Were  the  cases  all
diagnosed  in  the  same  way?  Did  the  reviewing
pathologist  see  all  the  cases?  If  immunohis-
tochemistry  played  a  role  in  case  assignment,
was this performed on all cases?

However, there are deeper issues at play here
that  we  can  illustrate  with  the  example  of
marker studies. We need to distinguish two dif-
ferent  scenarios.  First,  studies  that  promote  a
marker  for  distinguishing  two,  in  principle,
separable but phenotypically overlapping clus-
ters  (say,  distinguishing  primary  from  meta-
static mucinous carcinomas of the ovary). There
is a fact of the matter determined in a method-
ologically independent way; there is, or is not,
a primary in the place predicted by the marker.
Here,  talk  of  test  characteristics:  sensitivity,
specificity,  etc.  make  sense.  The  second  situa-
tion,  concerns  markers,  claimed  to  clear  up
some  muddled  region  of  a  phenospace,  for
example,  poorly  (or  undifferentiated)  mesen-
chymal  neoplasms  of  the  uterus.  Here  we  find
ourselves  dealing  with  issues  (discussed  in

Chap. 6) of classification revision and theoreti-
cal stipulations, over which the relevant experts
may or may not agree. Diagnostic test discourse
is weirdly out of place here.

External Validity

The study may pass internal validity tests, but the
vision  it  provides  may  have  little  to  do  with  the
world as we will perceive it. This is a question about
generalizability, or “external validity.” For example,
one may question whether valid conclusions drawn
from  a  study  of  cases  extracted  from  the  expert
pathologist’s files have much to do with community
pathology practice.

Relevance
Assume  that  the  chance  and  bias  hurdles  have
been satisfactorily addressed. We are left with the
question of the relevance of the study to general
practice.  Would  my  patient’s  tumor  have  been
included  in  this  study  and  do  the  summary
 statistics  reported  in  the  study  apply  to  my
patient?  Oncopathological  studies  should,  and
usually do, include relevant nonhistopathological
features:  age,  gender,  comorbidity,  symptoms,
gross features, details of treatment, etc.

Clinicopathology is a work in progress
Typically, in the course of delineating the feature
of a KNeop over time, initial studies have had lim-
ited generalizability. In time a fuller picture of the
KNeop’s neighborhood in the phenospace emerges,
and  the  morphologic  spectrum  of  the  KNeop
becomes  clearer.  It  may  be  that  the  clinical
aggressiveness  of  a  KNeop  is  overestimated  (e.g.,
aggressive angiomyxoma), or the diagnostic sig-
nificance of a particular pattern is overestimated
(the myxoid pattern for uterine myxoid leiomyo-
sarcoma)  by  a  failure  to  attend  to  KNeops  in  the
neighborhood.  Thus,  external  validity  is  incre-
mental; later studies typically review earlier stud-
ies  and  modify  their  conclusions  accordingly.
Explorations of the phenospace are always works
in progress. This is very reminiscent of the decay
of marker test characteristics over time [7–11].

7  Evaluating Oncopathological Studies

131

Other Diagnostic Issues
in Oncopathological Studies:
The Communicative Component

Reproducibility of the assessment of
features or classifications employed in
the study

How do we know that the investigators agreed on
the  morphological  evaluations  detailed  in  the
study? Intraobserver and interobserver disagree-
ment  is  common  in  daily  diagnostic  life  –  the
great scandal of diagnostic anatomic pathology –
and well documented in our literature. The assess-
ment  of  cytological  atypia
in  endometrial
hyperplasia, an important managerial distinction,
is a notorious example [39].

conceal the fact that it is a gorilla. The question is
how  effectively did the study under consideration
study  deal  with  the  gorilla?  A  diagram  of  the
study’s  phenospace  is  one  way  of  partially
addressing this problem. Gleason pioneered this
technique with his ubiquitous grading chart and,
following his example, we employed diagrams to
convey architectural patterns in our endometrial
cancer study (Fig. 7.5) [40]. Additional assurances

High bias
Low bias
Low variance High variance

Expected error (100 samples)

Expected error (1,000 samples)

Variance

r
o
r
r
e

n
o
i
t
a
c
i
f
i
s
s
a
c

l

i

e
v
i
t
c
d
e
r
P

Translation and Transmission of the
Investigator’s Classificatory Vision

Another important question is how effectively did
the  paper  communicate  the  classificatory  vision
of the authors? What compromises my ability to
imitate  the  investigators  in  my   diagnostic  work
when confronted with a case that would fall in the
domain  of  the  study  group?  Published  journal
articles  rely  on  photomicrographs  and  terse  tex-
tual descriptions inevitably employing imprecise
language. Quantitative features do not escape the
problem of vagueness as discussed in Chap. 6.

Both these concerns have their roots in issues
discussed in Chap. 6: (1) the extensionally inde-
terminant  CoPeTI  structure  of  the  classes  being
considered; (2)  the  inevitable linguistic  impreci-
sion that attaches to both the characterization of
the  features  used  to  define  these  unruly  groups
and the characterization of the groups themselves;
(3) the observer’s ongoing classificatory, private,
micro-revisions prompted by the examination of
problem cases; and (4) the difficulties of translat-
ing and transmitting a classificatory vision.

This last problem lies at the heart of our onco-
pathological enterprise; it is the gorilla sitting in
the  middle  of  the  drawing  room;  we  can  put  a
negligee of statistics on it but the gown does not

Dimension of input

Bias

Fig. 7.5  The bias-variance dilemma. What makes for a
good  classifier?  Much  has  been  written  about  this  by
researchers in the machine-learning and pattern recogni-
tion fields. The performance of a classifier, as measured
by its misclassification rate, depends on the interrelation-
ship among (1) sample size, (2) the dimensionality of the
data (how many features are evaluated), and (3) the com-
plexity of the model – how many parameters have to be
estimated  within  tolerable  error  limits.  Imagine  that  we
have several different, but equally good, training data sets.
A classification rule is biased for a particular set of train-
ing sets if, when trained on each of these data sets, it is
systematically  incorrect when predicting the correct out-
come.  This,  for  example,  occurs  when  the  classification
rule is too simple; univariate rules typically have this char-
acter; they “under-fit” the data. A classification rule has
high  variance  if  it  predicts  different  outcomes  when
trained  on  different  training  sets.  This  occurs  when  the
classification  rule  is  too  complex;  complex  multivariate
rules typically have this character; they “overfit” the data.
The misclassification rate of a classifier is related to the
sum of the bias and the variance of the classification rule.
In the diagram, the expected error curve is the sum of the
bias  cure  and  the  variance  curve.  Generally,  the  rule
designer must negotiate a trade-off between bias and vari-
ance as a function of the dimensionality of the data; thus,
the  error  curves  have  a  minimum.  Two  expected  error
curves are shown: the larger the number of cases the lower
the  expected  error.  A  classifier  with  low  bias  must  be
“flexible” so that it can fit the data well. But if the classi-
fier is too flexible, it will fit each training data set differ-
ently, and hence have high variance [3]

132

M. Hendrickson and B. Balzer

that  the  translation-transmission  problem  was
addressed by the investigators are provided by an
assessment of the level of agreement among the
investigators [21].

Sources of Communication Failures

Effective transmission of information can fail at a
number of levels:
 1.  The investigator is not really describing what

he/she actually does:

Assuming that we have assurances that the
investigators  can  reliably  make  the  distinc-
tions they describe, are they really following
their  own  rules?  Or  is  the  investigator  doing
something  else;  for  example,  is  he  making  a
gestalt assignment and then justifying it with a
story about explicit criteria? This phenomenon
is well documented [41].

 2.  The descriptions employ insufficiently precise

language:

Ambiguous language travels with the vague
predicates  and  vague  categories  required  to
conceptualize  and  verbalize  continuously
varying features and extensionally indetermi-
nant  CoPeTI  groups.  “Confluent  growth,”
“papillary growth”; “large” vs. “small” cells;
“mitotic  figure.”  In  our  study  of  endometrial
carcinoma,  we  spent  many  hours  trying  to
understand what Kurman and Norris, in their
excellent and thorough study, meant by “con-
fluent growth” and “fibrous stroma” [21, 42].

 3.  The descriptions are incomplete:

Common omissions include: unstated crite-
ria  or  feature-weighting  strategies;  failure  to
address unanticipated combinations of criterial
features; and a failure to address the ubiquitous
problem of tumor heterogenity

 4.  The authors have not provided instructions for
dealing with the expected problem cases in the
domain they are describing:

What  help  is  provided  for  problem  cases  –
hybrids, in-between cases, and novel cases? As
Chap. 6 suggests, completely anticipating such
cases is impossible. That said, typical problem
cases in the investigator’s experience should be
presented.

Genomic Studies

Genomics,  GEA  particularly,  currently  domi-
nate our literature. Our journals are filled with
articles promoting expression array patterns as
cancer  markers,  as  the  basis  for  revising  con-
ventional  light  microscopic  classifications  of
neoplasms, as prognostic and predictive markers
or  –  more  in  the  basic  science  literature  –  as
ways  elucidate  cell  signaling  pathways.  The
mood has been upbeat. In 2005, Ioannidis ironi-
cally summed up the prevailing optimistic per-
spective  of  GEAs  in  a  2005  Lancet  editorial
entitled, “Microarrays and molecular research:
noise discovery?”

The promise of microarrays has been of apocalyp-
tic dimensions. As put forth by one of their inven-
tors,  “all  human  illness  can  be  studied  by
microarray analysis, and the ultimate goal of this
work is to develop effective treatments or cures for
every human disease by 2050 [43].” All diseases
are to be redefined, all human suffering reduced to
gene-expression  profiles.  Cancer  has  been  the
most  common  early  target  of  this  revolution  and
publications in the most prestigious journals have
heralded  the  discovery  of  molecular  signatures
conferring  different  outcomes  and  requiring  dif-
ferent treatments [44].

This  editorial  was  occasioned  by  a  pessimistic
“forensic statistics” analysis of several published
prognostic GEA signatures for a variety of can-
cers in the same issue. These authors concluded:

…the list of genes included in a molecular signa-
ture  …  depends  greatly  on  the  selection  of  the
patients in training sets. Five of the seven largest
published studies addressing cancer prognosis did
not classify patients better than chance. This result
suggests that these publications were overoptimis-
tic.  [-----]  Studies  with  larger  sample  sizes  are
needed  before  gene  expression  profiling  can  be
used in the clinic [45].

In  the  same  vein,  Dupuy  and  Simon  reported  a
detailed  account  of  42  peer-reviewed  studies
 published in 2004. Fifty percent of them  contained
at least one of the following three basic flaws:

1)  in  outcome-related  gene  finding,  an  unstated,
unclear, or inadequate control for multiple testing;
2) in class discovery, a spurious claim of correla-
tion between clusters and clinical outcome, made
after  clustering  samples  using  a  selection  of  out-
come-related differentially expressed genes; or 3) in

7  Evaluating Oncopathological Studies

133

supervised  prediction,  a  biased  estimation  of  the
prediction  accuracy  through  an  incorrect  cross-
validation procedure [46].

function.  Clarke  et  al.  refer  to  these  issues  as
the “confound of multimodality” (COMM):

These are more than just quibbles; these failures
fatally compromise the usefulness of such results
[46].  Why  is  this  not  working?  What  are  the
problems? Some of them – multiple testing and
validation  –  are  familiar  from  the  discussion
above. Others are more complicated. First, what is
not addressed by the majority of GEA studies?

The Biological Perspective

Let us locate these worries within the context that
was  sketched  out  in  Chap.  6.  The  material  for
most  expression  array  studies  is  a  convenience
sample – banked tissue of some sort (frozen, par-
affin blocks, etc.) from which mRNA is extracted
and  from  which  cDNA  is  prepared.  Studies  of
such  material  do  not  directly  address  several
essential aspects of the INeop.
 1.  INeop heterogeneity and evolution: The sampled
INeop is caught in a moment of time, a snapshot;
the  “signal”  represents  the  average  of  the
genetically  and  epigenetically  heterogeneous
cells and populations in the sample. For exam-
ple, in Chap. 6, the Circos plots of “individual”
breast cancers are really graphical summaries
of  all  of  the  cytogenetic  abnormalities  of
individual cells present in the sample.

 2.  Context  dependency  of  the  neoplastic  cell:
it  is  a  commonplace  that  cells  behave  dif-
feren-tly  in  different  micro-environments.
Deciding whether a gene or the elements of
a genetic pathway are inappropriately upre-
gulated  or  downregulated  requires  knowl-
edge of the context; precisely the thing that
is  lost  in  the  homogenization  required  for
GEA studies.

 3.  INeop microenvironment: there is the problem
of  separating  the  signal  from  the  nonneo-
plastic elements in the sample from the sig-
nal  of  the  neoplastic  elements.  In  recent
years, microdissection techniques and single
cell GEA have begun to address this problem
[47, 48].

 4.  INeop cellular complexity – functional and micro-
anatomic – and the context dependency of cellular

problems that are associated with extracting truth
[read, an empirically adequate model] from com-
plex systems. … COMM refers to the potential that
the  presence  of  multiple  interrelated  biological
processes  will  obscure  the  true  relationships
between a gene or gene subset and a specific process
or  outcome,  and/or  create  spurious  relationships
that may appear statistically or intuitively correct
and yet may be false [3].

Clarke  et  al.  provide  a  number  of  illuminating
examples: the multiple functions of transforming
growth  factor  b1  and  the  transcription  factors
tumor necrosis factor a and estrogen receptor a
(ERa)  [3].  Whether  these  are  up  or  downregu-
lated  depends  upon  a  context,  again,  precisely
what is lost in GEA studies.

Methodological Problems

What are the issues peculiar to GEA publications?
There are four basic problems: (1) The confusion
between an observational study and an experimen-
tal  study;  (2)  High  dimensional  biology  (HDB)
and  the  “small  sample  scenario”;  (3)  Fishing
expeditions and the role of modeling in biology;
(4) Noisy experimental data.
 1.  Observation studies vs. experimental studies:
One  recurrent  theme  is  the  failure  of  many
genomic  researchers  to  distinguish  between
an observational and experimental design. We
can  frame  our  discussion  in  terms  of  “level-
hopping.”  Sotiriou  and  Pusztai  distinguish
between “top-down” and “bottom-up” studies
[49].  In  the  “top-down”  approach,  gene-
expression data from cohorts of patients with
known  clinical  outcomes  are  compared  to
identify  genes  that  are  associated  with  prog-
nosis without any a priori biologic  assumption.
In short, the jump is from a molecular profile
to  a  FClin(t).  Genomic  techniques  inherit  all
the problems of correlating conventional light
microscopic features with FClin(t) and another
substantial  set  of  problems  involved  in  mov-
ing  the  starting  point  back  to  the  molecular
level.  Bypassed  in  this  additional  trajectory
are,  respectively,  molecular  motifs,  signaling
pathways, and cell-wide networks. This structure

134

M. Hendrickson and B. Balzer

of  such  a  correlative,  level-hopping  study  is
observational and, the fact that genes are the
predictors notwithstanding, not experimental.
In the “bottom-up” approach, gene-expression
patterns that are associated with a specific bio-
logic  phenotype  or  a  deregulated  molecular
pathway  are  first  identified  and  then  subse-
quently correlated with the clinical outcome.
In  the  candidate-gene  approach,  selected
genes  of  interest  on  the  basis  of  existing
biologic  knowledge  are  combined  into  a
multivariate  predictive  model.  Both  of  these
designs  discipline  the  study  with  certain  a
priori modeling assumptions and, as we shall
see, the results are crucially dependent on the
truth of those assumptions [49].
Potter  describes  the  consequences  of  this
shift  from  an  experimental  to  an  observational
perspective:

When  a  cancer  sample  is  compared  with  normal
tissue, attributing differences in gene expression to
differences in disease state is entirely inappropriate
in  the  absence  of  data  regarding  the  age,  sex,
genetic profile, histology and treatment of the per-
son  from  whom  the  sample  came.  This  involves,
not the failure to control confounding, but often the
failure  even  to  measure  any  of  the  other  relevant
exposures.  If  unaffected  tissue  from  the  same
patient is used as a comparison, there are still the
problems  of  the  existence  of  field  effects  and  of
selection bias [50].

Potter  suggests  education  as  the  culprit  for  this
common misapprehension:

The reason for this failure to distinguish between
observational  and  experimental  designs  might
be  that,  although  observational  scientists  are
trained  in  experimental  methods,  the  reverse  is
seldom true. Furthermore, making the observa-
tions with new and powerful technology seems
to induce amnesia as to the original nature of the
study  design.  It  is  as  though  astronomers  were
to ignore everything they knew both about how
to  classify  stars  and  about  sampling  methods,
and instead were to point spectroscopes haphaz-
ardly  at  stars  and  note  how  different  and  inter-
esting  the  pattern  of  spectral  absorption  lines
were [50].

experimental research anymore but rather is obser-
vational  epidemiology,  with  its  own  rules  of  evi-
dence, in which molecular biology simply provides
a measuring tool [51].

I think there is a deeper issue of ideology involved
here. Recall my discussion of histological deter-
minism, the unstated background belief that what
drives  prognosis  are  the  histological  features  of
the  INeop.  This,  as  I  discussed,  encourages  an
inattention to other known determinants of prog-
nosis.  I  believe  something  similar  –  molecular
determinism  –  is  responsible  for  the  failure  to
appropriately  frame  GEA  studies  as  observa-
tional  studies  vulnerable  to  all  the  biases  well
known to epidemiologists.
 2.  Mathematical-statistical problems in HDB:

Toto, I’ve a feeling we’re not in Kansas anymore

– Wizard of Oz

The mathematical-statistical issues involved in
high-dimensional spaces are formidable. Passing
from the mathematics of t-tests, chi-squared test
and linear regression – the conventional, and very
important, biostatistical topics – to the mathemat-
ics  and  statistics  of  high- dimensional  spaces  is
like moving from reading a bestselling detective
novel to tackling James Joyce’s Finnegans Wake.
Thus, a critical reading of gene expression litera-
ture  is  challenging,  even  for  the  statistician;
indeed,  a  cottage  industry  of  “forensic  statisti-
cians” has been prompted by the mathematical-
statistical difficulties presented by what has come
to be known by many workers as “genomic signal
processing.” [52] The lesson for anatomic pathol-
ogists: the first thing to check on any paper that
employs genomic techniques is whether a statis-
tician is among the authors.

The  “tall,  skinny  matrix”  or  “small  sample
scenario” problem: The problems arise because
of  the  peculiar  topology  of  high-dimensional
spaces and the relative paucity of data points in
those spaces. Clarke et al. summarizes the basic
problem [3, 53]:

This theme is also picked up on by Ransohoff

The culture of laboratory medicine does not appre-
ciate that, when the tools of molecular biology are
applied to heterogeneous groups of people, it is not

Most  univariate  and  multivariate  probability
theories  were  derived  for  data  space  where  N
(number  of  samples)  >  D  (number  of  dimen-
sions). Expression data are usually very different
(D>>>N).  A  study  of  100  mRNA  populations

7  Evaluating Oncopathological Studies

135

(one  from  each  of  100  tumors)  arrayed  against
10,000  genes  can  be  viewed  as  each  of  the  100
tumors  existing  in  10,000-D  space.  This  data
structure  is  the  inverse  of  an  epidemiological
study  of  10,000  subjects  (samples)  for  which
there  are  data  from  100  measurements  (dimen-
sions),  yet  both  data  sets  contain  100  data
points.

By way of contrast, a widely used rule of thumb
in the pattern recognition field is to have at least
ten training samples per feature dimension [54].
In microarray studies, this ratio is often closer to
0.01 samples per dimension [55].

Curse of dimensionality: The performance of a
statistical  model  (classifier)  depends  upon  the
interrelationship of three things: (1) sample size,
(2) data dimensionality, and (3) model (classifi-
cation  rule)  complexity.  The  “curse  of  dimen-
sionality”  refers  to  the  breakdown  of  optimal
model  fitting  using  statistical  learning  tech-
niques  in  high  dimensions.  The  ability  of  an
algorithm to converge to a “true” model degrades
rapidly  as  the  data  dimensionality  increases.
The number of training cases required to main-
tain  optimality  goes  up  exponentially  with  the
dimensionality  (the  number  of  features  exam-
ined per case) of the feature space [2, 3, 53, 56].
Fig.  7.6  and  the  accompanying  legend  have
more details.

These observations can be reframed in terms
of  the  “bias/variance  dilemma  [54].”  Simple
models may be biased but will have low variance.
More  complex  models  have  greater  representa-
tion power (low bias) but overfit to the particular
training  set  (high  variance).  Thus,  the  large
 variance  associated  with  using  many  features
(including  those  with  modest  discrimination
power) defeats any possible classification benefit
derived from these features. With severe limits on
 available samples in microarray studies, complex
models  using  high-feature  dimensions  will
severely overfit, greatly compromising classifica-
tion performance [53].

Some  form  of  the  curse’s  reach,  manifest  as
overfitting, extends to a wide variety of applica-
tions:  classifiers  using  conventional
light
 microscopic  features,  multivariate  regression
techniques, and artificial neural networks (ANN).

+

C
H

I

/–+

–

Light Microscopy

Fig. 7.6  A simple example of the “curse of dimensionality.”
The  x-axis  depicts  a  typical  conventional  light  micro-
scopic  (H&E)  morphological  continuum  ranging  from
“core” cases of Blues through in-between cases (shades
of  purple)  to  “core”  cases  of  Red.  In  the  language  of
Chap. 6 we have two overlapping ExtnI CoPeTI clusters.
An immunohistochemical test is performed that can result
in  a  negative,  a  positive  or  an  inconclusive,  result.  The
possible results are displayed in a 3 × 3 matrix. The size of
the circles corresponds to the proportion of cases in each
category.  Most  “core”  cases  of  Blue  are  negative;  most
core cases of Red are positive. Some of the light micro-
scopically in-between cases travel with the Blues, some
with the Reds, and some remain indeterminate. The usual
diagnostic  interpretation  is  that  the  IHC  test  has  disam-
biguated the purple region into true Blues and true Reds.
What about the other cells? Some typical Blue cases are
positive; some typical Red cases are negative. It is a clas-
sificatory decision to continue to call B/+ cases “B” and
R/− cases “R”; similar decisions are required for the other
possibilities.  There  are  33  possible  diagnostic/classifica-
tory decisions to make choices. It is easy to see that with
the addition of new features, each of which can take on
three  values,  the  possibilities  will  go  up  exponentially
with the number of features; 3n for n features. It is also
clear  that,  if  the  number  of  investigated  cases  remains
constant,  the  possible  combinations  outstrip  the  number
of cases. This is another version of the “curse of dimen-
sionality.”  Immunohistochemical  panels  present  us  with
this alarming vista

A  related  counterintuitive  property  of  high-
dimensional space is the following: the investigator
is  often  in  the  position  of  finding  a  data  point’s
nearest neighbor in the feature space. Here is the awk-
ward fact: the distance to a point’s farthest neighbor
approaches  that  of  its  nearest  neighbor  when  the

136

M. Hendrickson and B. Balzer

dimensionality of the space increases to as few
as 15 [3]. This has implications for case-based
reasoning (CBR) (see Chap. 10).

Richard  Simon’s  group  has  provided  many
accessible  introductions  to  these  mathematical-
statistical  problems  [57–59].  The  “curse  of
dimensionality” can be glimpsed using a simple
example  that  pathologists  confront  on  a  daily
basis (Fig. 7.7).
 3.  Epistemological concerns – is hypothesis-free
data mining really science at all? Genomics
and  data  mining  have  raised  a  number  of
deeper issues about what constitutes science.
These  are  worries  about  epistemology,  that
branch  of  philosophy  that,  among  other
things,  attempts  to  understand  what  consti-
tutes  the  scientific  method.  A  good  place  to
begin is with the critique of Sydney Brenner,
the  2004  Nobel  Laureate  for,  among  many
other  things,  his  C.  elegans  work.  Sydney
Brenner, with his characteristic talent for get-
ting to the heart of the matter, frames the data
mining problem in broad mathematical terms,
as an ill-posed inverse problem. The generic
forward  problem  involves  positing  a  model,
deriving  predictions  from  that  model,  and
then comparing predictions with the data. The
generic inverse problem involves deducing a
model  from  the  data  without  any  a  priori
assumptions  about  the  model.  Data  mining
amounts to an ill-posed (theory-poor) inverse
problem. His argument is worth quoting in its
entirety:

I want to show here that this approach is bound to
fail, because even though the proponents seem to
be unconscious of it, this claim of systems biology
is that it can solve the inverse problem of physiol-
ogy by deriving models of how systems work from
observations  of  their  behavior.  It  is  known  that
inverse  problems  can  only  be  solved  under  very
specific conditions. A good example of an inverse
problem is the derivation of the structure of a mol-
ecule from the X-ray diffraction pattern of a crys-
tal.  This  cannot  be  achieved  because  information
has been lost in making the measurements. What is
measured is the intensity of the reflection, which is
the square of the amplitude, and since the square of
a negative number is the same as that of its positive
counterpart, phase information has been lost. There
are three ways to deal with this. The obvious way
is to measure the phase; the question then becomes

well-posed and can be answered. The other is to try
all combinations of phases. There are 2n possible
combinations, where n is the number of reflections;
this  approach  might  be  feasible  where  n  is  small
but  is  not  possible  where  n  is  in  the  hundreds
or  thousands,  when  we  will  exceed  numbers  like
the  total  number  of  elementary  particles  in  the
Universe. The third method is to inject new a priori
knowledge; this is what Watson and Crick did to
find the right model. That a model is correct can be
shown by solving the forward problem, that is, by
calculating the diffraction pattern from the molec-
ular structure. The universe of potential models for
any complex system like the function of a cell has
very large dimensions and, in the absence of any
theory of the system, there is no guide to constrain
the choice of model. In addition, most of the obser-
vations made by systems biologists are static snap-
shots and their measurements are inaccurate; it will
be impossible to generate nontrivial models of the
dynamic processes within cells, especially as these
occur  over  an  enormous  range  of  time  scales—
from milliseconds to years. Any nonlinearity in the
system  will  guarantee  that  many  models  will
become unstable and will not match the observa-
tions. Thus, as Tarantola [60] has pointed out in a
perceptive article on inverse problems in geology,
which every systems biologist should read, the best
that  can  be  done  is  to  invalidate  models  (in  the
Popperian sense) by the observations and not use
the observations to deduce models since that cannot
be successfully carried out [61].

An engaging expansion of this argument is avail-
able  online:  Sydney  Brenner’s  lecture:  “Much
ado  about  nothing:  systems  biology  and  the
inverse problem [61].”

Brenner is concerned with hypothesis-free data
exploration.  A  more  detailed  argument  along
these same lines has been made by systems biolo-
gist Dougherty and coworkers in a series of publi-
cations (See Dougherty, 2008 for references [62]).
After  an  extensive  review  of  the  history  of  the
scientific method, they conclude that studies that
depart  from  the  model-data  interaction  schema
(i.e., hypothesis-driven research) shouldn’t count
as science at all. They summarize their dismissal
of data mining by citing Immanuel Kant’s famous
dictum:  “A  concept  without  a  percept  [observa-
tion]  is  empty;  a  percept  without  a  concept  is
blind.”

So,  there  are  fatal  downsides  to  sifting
through massive amounts of data in a theory-free
way. There are also downsides, in this data rich
environment, of having partial theories. Clarke

7  Evaluating Oncopathological Studies

137

Fig.  7.7  Endometrial  cancer  chart  [40].  Schematic  of
endometrial  glandular  proliferations  with  small  budding
glands,  macroglands,  and  exophytic  papillae.  The  lower
one-half of the chart represents proliferations with very low
risk for myometrial invasion in the hysterectomy specimen
(<0.05%)  and  are  designated  as  “complex  endometrial
hyperplasia” (with or without atypia), whereas the upper
one-half of the chart represents proliferations with a suf-
ficiently high risk for myometrial invasion in the hysterec-
tomy specimen to warrant diagnosis as “well-differentiated
endometrial  adenocarcinoma.”  Proliferations  with  inter-

mediate degrees of complexity are depicted immediately
above  the  solid  horizontal  line  on  the  lower  one-half  of
the chart and are designated as “borderline, cannot exclude
well-differentiated  endometrial  adenocarcinoma.”  These
latter lesions have an intermediate risk for myoinvasion
in  the  hysterectomy  specimen  (approximately  5%).  For
example, the circles denote low, intermediate, and high-risk
exophytic papillary patterns. Similar circles can be drawn
for the macroglandular and small budding glandular pat-
terns. They are correlated with photomicrographs of cor-
responding cases

138

M. Hendrickson and B. Balzer

et  al.  warn  us  against  the  self-fulfilling  proph-
esy. If you are committed to a model, in HDB
you  can  usually  confirm  it;  “seek  and  ye  shall
find.” The  problem is that what you find may be
erroneous. “With thousands of measurements and
the concurrent presence of multiple sub-pheno-
types, intuitively logical but functionally incor-
rect  associations  may  be  implied  between  a
signal’s  (gene  or  protein)  perceived  or  known
function  in  a biological system or phenotype of
interest [3].”

Finally,  at  a  more  technical,  statistical  level,
epistemological issues intrude. Mehta et al. cau-
tion that many papers aimed at the HDB commu-
nity  describe  the  development  or  application  of
statistical techniques whose validity is question-
able, and betray a misunderstanding of the episte-
mological foundations of statistics. For example,
there is sometimes a confusion of measurement
uncertainty with biological variation [52].
 4.  Noisy data: Noisy data is a major problem for
HDB.  Important  biological  information  may
have a very low signal, and separating this sig-
nal  from  measurement  noise  is  highly  prob-
lematic.  GEA  data  are
typically  highly
correlated: this correlation could either repre-
sent  “signal”  (true  correlations  of,  for  exam-
ple,  elements  of  an  activated  pathway)  or
measurement “noise” in the data. Indeed, spu-
rious  correlations  are  a  property  of  high-
dimensional,  noisy  data  sets  and,  obviously,
are  a  problem  for  statistical  approaches  that
seek to define a data set solely by its correla-
tion  structures.  Although  data  normalization
can  remove  spurious  correlation  (and  also,
unfortunately, real correlations) the results are
sensitive to the particular technique employed;
in other words, the same data set can yield dif-
ferent models using different techniques.

The Marker Study Perspective

GEA studies designed to make M-Class distinc-
tion  (risk,  prognosis,  prediction)  are  properly
evaluated  within  the  cancer  marker  frame-
work.  There  are  many  excellent  surveys  of  this
 evaluation process as applied to proffered GEA

markers and classifiers. Many of these focus on
the most intensively studied field, breast cancer
[9, 34, 49, 63–70].

Relevance to Evidence-Based Pathology

In this chapter, we have suggested applying the
framework of tumor marker studies and prognos-
tic  classification  rules  to  histopathologic  claims
of managerial relevance and emphasized the par-
ticular need for this in GEA results.

Evidence-based pathology, at the very least, will
involve acquiring the conceptual tools to deal with
the issues discussed in the Genomics section. This
is a formidable task; remember the forensic statisti-
cians. What anatomic pathologists can provide is a
measure of morphologic “common sense.”

References

  1.  Everitt  B,  Landau  S,  Leese  M.  Cluster  analysis.

London: Arnold; 2001.

  2.  Hastie T, Tibshirani R, Friedman J. The elements of
statistical  learning.  Data  mining,  inference,  and  pre-
diction. New York: Springer; 2001.

  3.  Clarke R, Ressom HW, Wang A, et al. The properties
of  high-dimensional  data  spaces:  implications  for
exploring gene and protein expression data. Nat Rev
Cancer. 2008;8(1):37–49.

  4.  Breiman  L.  Statistical  modeling:  two  cultures.  Stat

Sci. 2001;16(3):199–231.

  5.  Grimes  DA,  Schulz  KF.  An  overview  of  clinical
land.  Lancet.  2002;

lay  of

the

the

research:
359(9300):57–61.

  6.  Grimes  DA,  Schulz  KF.  Descriptive  studies:  what
they  can  and  cannot  do.  Lancet.  2002;359(9301):
145–9.

  7.  Hayes DF, Bast RC, Desch CE, et al. Tumor marker
utility grading system: a framework to evaluate clini-
cal  utility  of  tumor  markers.  J  Natl  Cancer  Inst.
1996;88(20):1456–66.

  8.  Hayes  DF.  Do  we  need  prognostic  factors  in  nodal-
negative  breast  cancer?  Arbiter.  Eur  J  Cancer.
2000;36(3):302–6.

  9.  Henry NL, Hayes DF. Uses and abuses of tumor mark-
ers in the diagnosis, monitoring, and treatment of pri-
mary  and  metastatic  breast  cancer.  Oncologist.
2006;11(6):541–52.

 10. Harris  L,  Fritsche  H,  Mennel  R,  et  al.  American
Society of Clinical Oncology 2007 update of recom-
mendations  for  the  use  of  tumor  markers  in  breast
cancer. J Clin Oncol. 2007;25(33):5287–312.

7  Evaluating Oncopathological Studies

139

 11. Weigelt  B,  Horlings  HM,  Kreike  B,  et  al.
Refinement  of  breast  cancer  classification  by
molecular  characterization  of  histological  special
types. J Pathol. 2008;216(2):141–50.

 12. Galea  MH,  Blamey  RW,  Elston  CE,  Ellis  IO.  The
Nottingham Prognostic Index in primary breast can-
cer. Breast Cancer Res Treat. 1992;22(3):207–19.
 13. Altman  DG,  Vergouwe  Y,  Royston  P,  Moons  KG.
Prognosis and prognostic research: validating a prog-
nostic model. BMJ. 2009;338:b605.

 14. Moons  KG,  Altman  DG,  Vergouwe  Y,  Royston  P.
Prognosis  and  prognostic  research:  application  and
impact of prognostic models in clinical practice. BMJ.
2009;338:b606.

 15. Moons  KG,  Royston  P,  Vergouwe  Y,  Grobbee  DE,
Altman DG. Prognosis and prognostic research: what,
why, and how? BMJ. 2009;338:b375.

 16. Royston  P,  Moons  KG,  Altman  DG,  Vergouwe  Y.
Prognosis  and  prognostic  research:  Developing  a
prognostic model. BMJ. 2009;338:b604.

 17. Justice  AC,  Covinsky  KE,  Berlin  JA.  Assessing  the
generalizability of prognostic information. Ann Intern
Med. 1999;130(6):515–24.

 18. Mallett  S,  Royston  P,  Waters  R,  Dutton  S,  Altman
DG. Reporting performance of prognostic models in
cancer: a review. BMC Med. 2010;8:21.

 19. Katz MH. Multivariable analysis: a primer for readers
of  medical  research.  Ann  Intern  Med.  2003;138(8):
644–50.

 20. Concato  J,  Feinstein  AR,  Holford  TR.  The  risk  of
determining  risk  with  multivariable  models.  Ann
Intern Med. 1993;118(3):201–10.

 21. Longacre  TA,  Chung  MH,  Jensen  DN,  Hendrickson
MR. Proposed criteria for the diagnosis of well-differ-
entiated endometrial carcinoma. A diagnostic test for
myoinvasion. Am J Surg Pathol. 1995;19(4):371–406.
 22. Lau SK, Weiss LM. The Weiss system for evaluating
adrenocortical neoplasms: 25 years later. Hum Pathol.
2009;40(6):757–68.

 23. Gupta  R,  Marchevsky  AM,  McKenna  RJ,  et  al.
Evidence-based pathology and the pathologic evalua-
tion of thymomas: transcapsular invasion is not a sig-
nificant  prognostic  feature.  Arch  Pathol  Lab  Med.
2008;132(6):926–30.

 24. Marchevsky  AM,  Gupta  R,  McKenna  RJ,  et  al.
Evidence-based  pathology and  the pathologic evalua-
tion of thymomas: the World Health Organization clas-
sification can be simplified into only 3 categories other
than thymic carcinoma. Cancer. 2008;112(12):2780–8.
 25. Marchevsky AM, McKenna Jr RJ, Gupta R. Thymic
epithelial  neoplasms:  a  review  of  current  concepts
using an evidence-based pathology approach. Hematol
Oncol Clin North Am. 2008;22(3):543–62.

 26. Begg  CB,  Cramer  LD,  Venkatraman  ES,  Rosai  J.
Comparing  tumour  staging  and  grading  systems:
a  case  study  and  a  review  of  the  issues,  using  thy-
moma as a model. Stat Med. 2000;19(15):1997–2014.
 27. Edwards AWF. Cogwheels of the mind. The story of
Venn Diagrams. Baltimore: Johns Hopkins University
Press; 2004.

 28. Stewart I. Another fine math you’ve got me into. New

York: W.H. Freeman and Company; 1992.

 29. Hennekens  CH,  Buring  JE.  Epidemiology  in  medi-
cine. Boston: Little, Brown and Company; 1987.
 30. Matthews DE, Farewell VT. Using and understanding
medical statistics. 4 Rev Enl edition ed. Switzerland:
S. Karger AG; 2007.

 31. Florey  CD.  Sample  size  for  beginners.  BMJ.

1993;306(6886):1181–4.

 32. Tversky A, Kahneman D. Belief in the law of small
numbers. Judgment under uncertainty: heuristics and
biases.  Cambridge:  Cambridge  University  Press;
1982. p. 23–31.

 33. Silverberg  S,  Major  F,  Blessing  J,  et  al.  Carcino-
sarcoma  (malignant  mixed  mesodermal  tumor)  of
the  uterus.  A  Gynecologic  Oncology  Group  patho-
logic  study  of  203  cases.  Int  J  Gynecol  Pathol.
1990;9(1):1–19.

 34. Ransohoff DF. Rules of evidence for cancer molecu-
lar-marker discovery and validation. Nat Rev Cancer.
2004;4(4):309–14.

 35.  Breiman L, Friedman JH, Olshen RA. Classification and
regression trees. Belmont: Wadsworth International
Group; 1984.

 36. Ioannidis  JP.  Why  most  published  research  findings

are false. PLoS Med. 2005;2(8):e124.

 37. Vineis  P.  History  of  bias.  Soz  Praventivmed.

2002;47(3):156–61.

 38. Chavalarias D, Ioannidis JP. Science mapping analysis
characterizes  235  biases  in  biomedical  research.
J Clin Epidemiol 2010;63:1205–15.

 39. Kendall  BS,  Ronnett  BM,  Isacson  C,  et  al.
Reproducibility  of  the  diagnosis  of  endometrial
hyperplasia, atypical hyperplasia, and well-differen-
tiated  carcinoma.  Am  J  Surg  Pathol.  1998;22(8):
1012–9.

 40. McKenney JK, Longacre TA. Low-grade endometrial
adenocarcinoma:  a  diagnostic  algorithm  for  distin-
guishing  atypical  endometrial  hyperplasia  and  other
benign  (and  malignant)  mimics.  Adv  Anat  Pathol.
2009;16(1):1–22.

 41. Einhorn HJ. Expert judgment: some necessary condi-
tions and an example. Judgment and decision making:
an
interdisciplinary  reader.  Vol  2.  Cambridge:
Cambridge University Press; 2000. p. 336–47.

 42. Kurman R, Norris H. Evaluation of criteria for distin-
guishing atypical endometrial hyperplasia from well-
differentiated  carcinoma.  Cancer.  1982;49(12):
2547–59.

 43. Schena  M.  Microarray  analysis.  New  York:  Wiley-

Liss; 2003.

 44. Ioannidis  JP.  Microarrays  and  molecular  research:
noise discovery? Lancet. 2005;365(9458):454–5.
 45. Michiels S, Koscielny S, Hill C. Prediction of cancer
outcome with microarrays: a multiple random valida-
tion strategy. Lancet. 2005;365(9458):488–92.

 46. Dupuy  A,  Simon  RM.  Critical  review  of  published
microarray studies for cancer outcome and guidelines
on  statistical  analysis  and  reporting.  J  Natl  Cancer
Inst. 2007;99(2):147–57.

140

M. Hendrickson and B. Balzer

 47. Navin N, Krasnitz A, Rodgers L, et al. Inferring tumor
progression  from  genomic  heterogeneity.  Genome
Res. 2010;20(1):68–80.

 48. Marusyk  A,  Polyak  K.  Tumor  heterogeneity:  causes
and  consequences.  Biochim  Biophys  Acta.  2010;
1805(1):105–17.

 49. Sotiriou C, Pusztai L. Gene-expression signatures in
breast cancer. N Engl J Med. 2009;360(8):790–800.
 50. Potter JD. At the interfaces of epidemiology, genetics
and genomics. Nat Rev Genet. 2001;2(2):142–7.
 51. Ransohoff DF. Discovery-based research and fishing.

Gastroenterology. 2003;125(2):290.

 52. Mehta T, Tanik M, Allison DB. Towards sound episte-
mological foundations of statistical methods for high-
dimensional biology. Nat Genet. 2004;36(9):943–7.
 53. Wang Y, Miller DJ, Clarke R. Approaches to working
in  high-dimensional  data  spaces:  gene  expression
microarrays. Br J Cancer. 2008;98(6):1023–8.

 54. Jain AK, Duin RPW, Mao J. Statistical pattern recog-
nition: a review. IEEE Trans Pattern Anal Mach Intell.
2000;22:4–37.

 55. Allison DB, Cui X, Page GP, Sabripour M. Microarray
data analysis: from disarray to consolidation and con-
sensus. Nat Rev Genet. 2006;7(1):55–65.

 56. Duda RO, Hart PE, Stork DG. Pattern classification.

New York: Wiley; 2001.

 57. McShane LM, Radmacher MD, Freidlin B, Yu R, Li
MC, Simon R. Methods for assessing reproducibility
of clustering patterns observed in analyses of microar-
ray data. Bioinformatics. 2002;18(11):1462–9.

 58. Simon  R.  Interpretation  of  genomic  data:  questions
and answers. Semin Hematol. 2008;45(3):196–204.
 59. Simon  RM,  Korn  EL,  McShane  L,  Radmacher  M,
Wright  GW,  Zhao  Y.  Design  and  analysis  of  DNA
microarray investigations. New York: Springer; 2003.
 60. Tarantola A. Popper, Bayes and the inverse problem.

Nat Phys. 2006;2:492–4.

 61.  Brenner  S.  Much  ado  about  nothing:  systems  biology
and  the  inverse  problem.  Reading  the  human  genome
with  Sydney  Brenner.  2009.  http://thesciencenetwork.
org/programs/reading-the-human-genome-with-
sydney-brenner/much-ado-about-nothing-systems-biol-
ogy-and-inverse-problems. Accessed on April 5, 2011.

 62. Dougherty  ER.  On  the  epistemological  crisis  in

genomics. Curr Genomics. 2008;9(2):69–79.

 63. Ioannidis  JP.  Is  molecular  profiling  ready  for  use  in
clinical  decision  making?  Oncologist.  2007;
12(3):301–11.

 64. Pusztai L. Lost in translation – prognostic signatures
for  breast  cancer.  Nat  Clin  Pract  Oncol.  2008;
5(7):363.

 65. Pusztai  L.  Current  status  of  prognostic  profiling  in

breast cancer. Oncologist. 2008;13(4):350–60.

 66. Pusztai L, Iwamoto T. Breast cancer prognostic mark-
ers in the post-genomic era. Breast Cancer Res Treat.
2011;125:647–50.

 67. Pusztai L, Mazouni C, Anderson K, Wu Y, Symmans
WF. Molecular classification of breast cancer: limi-
tations  and  potential.  Oncologist.  2006;11(8):
868–77.

 68. Ransohoff DF. Bias as a threat to the validity of cancer
molecular-marker  research.  Nat  Rev  Cancer.  2005;
5(2):142–9.

 69. Ross  JS,  Hatzis  C,  Symmans  WF,  Pusztai  L,
Hortobagyi  GN.  Commercialized  multigene  predic-
tors of clinical outcome for breast cancer. Oncologist.
2008;13(5):477–93.

 70. Sotiriou C, Wirapati P, Loi S, et al. Gene expression
profiling in breast cancer: understanding the molecu-
lar  basis  of  histologic  grade  to  improve  prognosis.
J Natl Cancer Inst. 2006;98(4):262–72.

 71. Bell SW, Kempson RL, Hendrickson MR. Problematic
uterine  smooth  muscle  neoplasms.  A  clinicopatho-
logic  study  of  213  cases.  Am  J  Surg  Pathol.
1994;18(6):535–58.

 72. Longacre TA, McKenney JK, Tazelaar HD, Kempson
RL, Hendrickson MR. Ovarian serous tumors of low
malignant  potential  (borderline  tumors):  outcome-
based  study  of  276  patients  with  long-term  (>  or  =
5-year)  follow-up.  Am  J  Surg  Pathol.  2005;29(6):
707–23.

 73. Longacre  TA,  Chung  MH,  Rouse  RV,  Hendrickson
MR.  Atypical  polypoid  adenomyofibromas  (atypical
polypoid  adenomyomas)  of  the  uterus.  A  clinico-
pathologic  study  of  55  cases.  Am  J  Surg  Pathol.
1996;20(1):1–20.

Power Analysis and Sample Sizes
in Pathology Research

8

Robin T.  Vollmer

Keywords
Power analysis • Sample size in pathology research • Pathology research
• Experimental design in pathology research • Statistical power in  pathology
research

For decades research in pathology has occurred
with  little  attention  paid  to  the  formalities  of
experimental  design  and  issues  of  sample  size
and statistical power, and it still happens this way.
Pathology research often begins with an intuition
or  a  question  arising  from  cumulative  observa-
tions made on tissue specimens, followed by the
more  formal  step  of  collecting  exploratory  data
for study and analysis. However, this process is
now evolving as a result of changes in the research
environment.  In  most  academic  institutions,
research in pathology is increasingly being con-
trolled by institutional review boards (IRBs) and
their  statisticians.  Plans  for  pathology  research
are now expected to follow known experimental
designs and include analyses of sample size and
statistical  power.  Nevertheless,  in  spite  of  the
foregoing there will always be a role for explor-
atory studies, which require minimal attention to
the details of formal experimental design. In fact,
analysis of sample size and statistical power can-
not be done until preliminary exploratory studies
are completed.

R.T. Vollmer ()
Department of Laboratory Medicine, VA Medical Center,
508 Fulton Street, Durham, NC 27705, USA
e-mail: Robin.Vollmer@va.gov

Effect of Sample Size on Testing
of Hypothesis

Most of us know that studies of few patients do not
produce statistically significant results. Less intui-
tive  are  the  occasional  observations  that  studies
with large numbers of patients can yield low p val-
ues  on  effects  that  eventually  prove  of  limited
importance [1]. Consider a hypothetical example.
Let T symbolize a laboratory test, which is positive
in  25%  of  patients.  In  preliminary  observations,
35% of patients negative for T had the disease,
D, and 45% of those with positive T had D. The
change  in  prevalence  of  D  from  35  to  45%  is
known as the size of the effect, or simply the effect
size.  How  many  patients  are  required  to  demon-
strate  that  T  and  D  are  significantly  associated?
The following plot shows the relationship between
the p value for a chi-square test of independence
between T and D vs. the number of study patients.
Figure  8.1  shows  that  using  fewer  than  200
patients yields high p values, that over 400 patients
produce p values less than 0.05 and that large num-
bers of study patients can yield very low p values
even  when  the  size  of  the  test  effect  is  modest.
Scrutiny of this plot leads to two alternative conclu-
sions. First, for the test T any study with less than

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_8, © Springer Science+Business Media, LLC 2011

141

142

e
u
l
a
v
p

0.4

0.3

0.2

0.1

0

200 400

R.T.  Vollmer

Table 8.1  Information necessary to calculate sample sizes:
the general case

a

b

The p value one wants to meet, or
probability of type I error
The probability of type II error
(power is 1 − b )

Effect size The magnitude of result one wants to find

p=.05

p=.01

Estimating Sample Sizes and Power:
The General Case

600

800
Number of patients

1000 1200 1400

Fig. 8.1  Plot of p value obtained from a chi-square test of
independence  vs.  the  number  of  patients  studied.  The
results come from the binomial model to be discussed in
subsequent sections. All plots and results in this chapter
were generated from S-PLUS software or programs writ-
ten in C language by the author using the algorithms listed
in the chapter and in the references

400 patients is of insufficient size to reach a statisti-
cally significant result, that is, it is “underpowered.”
Now, alternatively, consider a preliminary study of
just 100 patients for this test. Clearly, such a study
should result in a high p value but in doing so will
tell us that the effect size of T for D is small and
perhaps too small to be of practical interest.

Statistical Errors Types I and II:
Definition of Power

In classical statistical analysis, the investigator for-
mulates what is called the null hypothesis. This is
the hypothesis that there are no significant effects
between  two  or  more  populations  of  interest  and
that all observed results are due to randomness. In
this setting, one can make two errors. The first is the
type I error, which is the rejection of the null hypoth-
esis, when in fact the null hypothesis is true. For a
given experiment, the probability of making a type I
error  is  the  same  as  the  p  value  for  the  statistical
test. The second error, a type II, develops when we
accept the null hypothesis although it is false. The
probability of a type II error is commonly symbol-
ized as b, and power equals 1 − b. Thus, the lower
the probability of type II error, the higher is the sta-
tistical power for the study and its statistical test.

Fortunately, statistical software packages in com-
mon use readily calculate both sample sizes and
power  for  most  common  experimental  designs.
These
include  S-PLUS  (www.spotfire.tibco.
com),  SAS  (www.sas.com),  SPSS  (www.spss.
com), and NCSS (www.ncss.com). All one need
do is to select three key pieces of information: the
p value one hopes to meet, the power level for the
test,  and  the  minimal  size  of  the  experimental
effect (Table 8.1).

Common  choices  for  a  are  0.05  or  0.01.
Common choices for b are 0.2 and 0.1. Because
power  equals  1 − b,  the  choosing  0.2  for  b  is
equivalent to a choosing 0.8 for the power (some-
times expressed as 80%). Choice of the minimal
effect size to be detected depends on the nature of
the random variables used. For example, in sur-
vival analysis one might want to detect a differ-
ence  in  survival  of  as  little  as  2  months  in  a
disease that is rapidly fatal. For other studies, the
choice of 2 months would be too small to matter.
Regardless, the choices of a, b, effect size, and
sample size are made by the investigator and the
values used by the software to estimate b and the
power.  What  follows  are  several  examples  for
common types of studies.

Estimating Sample Sizes and Power:
Two Binary Random Variables

For  this  type  of  study,  the  motivation  is  to  dis-
cover  an  association  between  a  binary  outcome
and  a  binary  explanatory  variable.  A  common
example in pathology is the study of how a spe-
cific diagnosis relates to an immunohistochemi-
cal  stain  (IHC).  The  outcome  is  the  diagnosis,

8  Power Analysis and Sample Sizes in Pathology Research

143

Table 8.2  Information necessary to calculate sample sizes:
the binomial case

a
b

Effect size

fp

The p value one wants to meet
The probability of type II error
(power is 1 − b )
The change in frequency of outcome one
wants to detect
The frequency of a positive result for the
explanatory variable

and the explanatory variable is the IHC result. In
this situation, the binomial model provides esti-
mates of either sample size or power. The model
requires one to select values for a and b as well
as at least two additional variables (Table 8.2).

Finding a value for fp, the fraction of patients
whose tissue will stain positive, requires prelimi-
nary studies or a search of prior literature. Then
one  must  select  the  size  of  the  effect  to  be
detected. For example, if we expect that the fre-
quency of the diagnosis to be 0.25 of patients and
want to see if a positive IHC stain increases the
frequency  of  diagnosis  to  0.45,  then  the  effect
size  would  be  an  increase  from  0.25  to  0.45.
If preliminary studies indicated that fp was approx-
imately  0.25,  then  the  binomial  model  with
a = 0.05 and b = 0.2 would estimate that the sam-
ple size should be 295 patients. Now suppose that
for the above study the investigator has just 125
patients. What would be the power of his study
be  to  detect  the  same  effect  with  a = 0.05?  The
binomial  model  indicates  that  the  power  would
be 0.4 (in percentages 40%). In other words with
just 125 patients, the probability of making a type
II error would be 0.6.

Figure  8.2  summarizes  the  strong,  positive
relationship between power (expressed as a frac-
tion) and the number of patients studied and was
designed for the above study and its choices of a, b,
and effect size.

Estimating Sample Sizes and Power:
Means of Continuous Random
Variables

If  the  random  variable  of  interest  is  continuous,
like a clinical chemistry test result, and the  outcome

r
e
w
o
P

1.0

0.8

0.6

0.4

0.2

0

100

200

300

400

500

Number of patients

Fig. 8.2  Plot of calculated power vs. number of patients
studied for associating a IHC stain with a diagnosis, when
a = 0.05 and the size of effect is a change in frequency of
diagnosis from 0.25 to 0.45

Table 8.3  Information necessary to calculate sample sizes:
the case for means

a
b

Effect size

sd’s

The p value one wants to meet
The probability of type II error
(power is 1 − b )
The change in mean value one wants to
detect
The standard deviations of the dependent
continuous variable for the populations
studied

variable  is  binary  like  the  presence  of  a  disease,
then a common research question is whether the
mean value of the result differs for those with and
without the disease. For example, in 2008 Zhang
et al. published mean values of several biomarkers
measured in cerebral spinal fluid (CSF) in patients
with  either  Alzheimer  or  Parkinson  disease  [2].
Although the authors’ primary motivation was not
to  test  for  differences  in  means  of  biomarkers
between  these  two  diseases,  their  data  provide  a
useful example of sample size and power applied
to chemical tests. As before, one must select val-
ues for a and b. However, for such a study select-
ing  the  size  of  the  effect  implies  selecting
differences in mean values of the biomarkers for
the two diseases (Table 8.3).

Finally, one must know approximate values for
the standard deviations of each continuous variable
to be tested, and this information must come from

144

R.T.  Vollmer

Table 8.4  Example of power calculation for testing differences in mean values of CSF biomarkers in Alzheimer and
Parkinson diseases

Biomarker
t Amyloid (pg/mL)
BDNF (pg/mL)
IL-8 (pg/mL)
b42 Amyloid (pg/mL)
b2-Microglobulin (mg/mL)
VDPB (mg/mL)
ApoAll (mg/mL)
ApoE (mg/mL)
ApoAl (mg/mL)
Haptoglobin (mg/mL)

Mean values
Alzheimer
1,425

202.3
37.4
371.8
1.4
1.1
0.9
2.5
2.3
2.3

Parkinson
387.8
184
36.3
510.6
1.6
1.2
0.8
2.3
2.4
3.8

Power
1.0
0.7
0.05
1.0
0.3
0.1
0.07
0.1
0.06
0.2

b
0
0.3
0.95
0
0.7
0.9
0.93
0.9
0.94
0.8

BDNF stands for brain-derived neurotrophic factor. IL-8 stands for interleukin 8. b Amyloid stands for amyloid [A]
b 42. VDPB stands for vitamin D binding protein. Apo stands for apolipoprotein. Sample size was fixed at 48 patients
in the Alzheimer group and 40 in the Parkinson group, and for this analysis a was fixed at 0.05. Mean values come from
Table 2 of Zhang et al. [2]

preliminary studies. With these choices made, one
can  then  estimate  sample  sizes.  Alternatively,  if
one knows the sample sizes, then the software can
estimate the power (and b) for detecting the effect.
For example, let us examine the power avail-
able  to  detect  the  differences  in  mean  values
reported by Zhang et al. (Table 8.4). The authors
had a total of 88 patients, and they found that the
means for both t amyloid and b42 amyloid were
significantly  different  between  the  two  diseases.
By  contrast,  they  did  not  find  significant  differ-
ences  in  mean  values  for  BDNF,  IL-8,  b2-
microglobulin, VDBP, ApoAll, ApoE, ApoAl, and
haptoglobin. Table 8.4 also shows the power avail-
able for detecting significant differences in means
for the ten biomarkers. For each of the eight non-
significant  biomarkers,  the  power  for  detecting
the  observed  difference  in  means  was  less  than
0.8, also implying that the probability of type II
errors was relatively high (range from 0.3 to 0.95).
By contrast, the power for finding significant dif-
ferences in means for t amyloid and b42 amyloid
between the two diseases was essentially 1.

Estimating Sample Sizes
for the Logistic Model

The logistic regression model allows us to exam-
ine  the  relationship  between  a  single,  binary

outcome  random  variable  and  one  or  more
explanatory random variables. For example, the
logistic regression model is ideal when we want
to see if a combination of explanatory variables
can predict the presence or absence of a disease.
The  binary  outcome  could  alternatively  be  the
presence of absence of a response to treatment or
the  presence  or  absence  of  failure  after  treat-
ment. The greatest strength of the logistic model
comes  when  there  are  multiple  or  continuous
explanatory  variables.  In  this  circumstance  of
multiple explanatory variables, estimating sam-
ple  sizes  and  power  for  the  logistic  model
requires the information listed in Table 8.5.

For example, consider the situation for just
one  continuous,  explanatory  variable,  x1.
First,  some  preliminary  data  are  necessary.
From this data, one calculates the mean, mx1,
and standard deviation, s1, of x1 as well as the
overall probability of a positive outcome, Po.
Po  also  approximates  the  conditional  proba-
bility of a positive outcome when the x1 vari-
able equals mx1, so that Po can be written as
P(y = 1  |  x1 = mx1).  Next,  we  use  either  the
preliminary data to estimate the probability of
a  positive  outcome  when  x1 = mx1 + s1  or  we
select some threshold probability we wish to
detect.  This  second  probability  is  the  condi-
tional  probability  P(y = 1
|  x1 = mx1 + s1),
which for simplicity we will symbolize as P1.

8  Power Analysis and Sample Sizes in Pathology Research

145

Table 8.5  Information necessary to calculate sample sizes:
the logistic regression model

a
b

Po

Effect size
nx

x details

The p value one wants to meet
The probability of type II error
(power is 1 − b )
The baseline probability of positive
outcome
The odds ratio one wants to detect
The number of explanatory x variables to
be used
Details about the x variables such as
mean and standard deviations

4000

3000

2000

1000

e
z
i
s

e
l
p
m
a
S

0

0

0.1

0.2
Po

0.3

0.4

Finally, we must consider the odds ratio, OR,
defined as:

OR

=

P

1 / (1

P
1)
Po / (1 Po)

-
-

.

If higher levels of x1 increase the chance of
a positive outcome, then the OR will exceed 1.0.
If  higher  levels  of  x1  decrease  the  chance  of  a
positive outcome, then the OR will be less than
1.0.  Thus,  by  estimating  or  selecting  a  value  of
P1 that we want to detect, we also select an OR.
With values chosen for a and b as before, then
one  can  use  the  tables  published  by  Hsieh  or
available  software  packages  to  estimate  the
sample size [3].

Common sense tells us that if we are trying to
study uncommon outcomes, then we need larger
sample sizes. This is true for the logistic model,
where the value of Po can dramatically affect the
size  of  the  sample  needed  to  detect  a  particular
OR effect. The following figure demonstrates this
relationship  with  a  plot  of  sample  size  vs.  the
value of Po.

Figure 8.3 shows that when a positive outcome
is uncommon, over 1,000 patients are needed to
detect an OR of 1.5. By contrast, when the out-
come is as common as 0.2 (i.e., 20% of patients),
then just 274 patients are needed.

Similarly, the value of the OR to be detected
dramatically  affects  the  size  of  the  sample.
Figure  8.4  demonstrates  this  affect  for  fixed
 values of Po = 0.2, a = 0.05, and b = 0.2.

The plot demonstrates that for fixed values of
Po, a and b, the closer the projected value of OR

Fig. 8.3  Plot of required sample size vs. Po, the underlying
probability of a positive outcome for a logistic regression
analysis  at  a  fixed  OR  of  1.5.  The  relationship  between
sample  size  and  Po  comes  from  equations  given  by
Hosmer and Lemshow and is determined for a single con-
tinuous x variable with a = 0.05 and b = 0.2

e
z
i
s

e
l
p
m
a
S

5000

4000

3000

2000

1000

0

1.0

1.5

2.0

2.5

OR

Fig. 8.4  Plot  of  sample  size  vs.  OR,  the  odds  ratio  of  a
positive outcome for an x variable at a value one standard
deviation  above  its  mean  and  for  a  fixed  Po  of  0.2.  The
relationship between sample size and OR comes from equa-
tions given by Hosmer and Lemshow and is determined for
a  single  continuous  x  variable  with  a = 0.05  and  b = 0.2
 (values of OR between 0.9 and 1.05 were excluded)

is to 1.0, the larger will be the required sample
size.  By  contrast  when  the  projected  OR  is  as
low  as  0.7,  just  350  patients  are  needed,  and
when the OR is as high as 1.5, just 274 patients
are needed.

If there is more than one x variable, then an
additional  step  is  necessary  and  consists  of

146

R.T.  Vollmer

calculating  the  multiple  correlation  coefficient
for  each  x  and  its  relation  to  the  others.  The
result is then used to estimate the final sample
size. Thus, estimating sample sizes and power
for  the  logistic  model  often  requires  prelimi-
nary  data  and  sophisticated  software  like  the
PASS package (www.ncss.com). The issues and
equations involved are summarized by Hosmer
and Lemeshow [4].

Finally, the number of explanatory variables
one examines affects the sample size needed for
the logistic model, and this is also true for the
Cox survival model. If the number of x variables
is large and the number of patients with observed
positive outcomes is small, multivariate logistic
regression analyses can yield unreliable results
due  to  overestimated  and  underestimated  vari-
ances.  Logistic  models  in  this  situation  may
overfit the data and then not validate well with
new  data.  Hosmer  and  Lemeshow  suggest  the
following  guidelines  [4].  First,  let  n1  be  the
number of patients with a y = 1 outcome and n0
be  the  number  with  y = 0  outcome.  Pick  the
lower of these two and label it nL. Next, let the
number  of  x  variables  be  nx.  Hosmer  and
Lemeshow  suggest  that  nx  and  nL  should  be
chosen such that:

nx

+ 1



nL

/ 10.

In other words, nL should exceed more than 10
times  the  number  of  x  variables.  This  result
implies that the total sample size should be even
larger. In the author’s experience, it is most often
the  number  of  patients  with  a  positive  outcome
that  will  be  smaller  and  therefore  of  greatest
importance  for  comparing  with  the  number  of  x
variables.

Consider  an  example.  Suppose  we  plan  a
study with five explanatory × variables (nx = 5)
and suppose that the fraction of patients with
a  positive  outcome  is  0.2.  Then  nL  must  be
such that

nL

10 (5 1) 60

+ =

´

and the total number of patients needed (n) will
be at least

n

60 / 0.2

=

300.

Hosmer and Lemeshow also caution that con-
tingency  tables  of  outcome  by  values  of  the  x
variables should contain at least ten patients per
cell.  Because  current  studies  of  either  nucleic
acid  microarrays  or  serum  proteonomics  often
include thousands of x variables and just several
hundreds  of  total  patients,  the  above  consider-
ations suggest that it is possible such studies may
not validate well with new patients.

Estimating Sample Sizes for Survival
Analysis

Estimating  sample  size  and  power  for  survival
analysis is more complex than for other analyses,
because  the  outcome  in  survival  analysis  is  a
composite of two random variables: time and sta-
tus at the last time. To understand the process, let
us  consider  the  survival  times  of  two  groups  of
patients,  A  and  B.  Groups  A  and  B  might  be
defined by the presence or absence of a molecular
marker  or  stain.  Alternatively,  groups  A  and  B
might be defined by values of a continuous vari-
able x below or above a cutpoint. For this kind of
study, the information needed to estimate sample
sizes is given in Table 8.6.

The  effect  size  to  be  detected  is  the  hazard
ratio, which in turn relates directly to the change
in survival one wants to detect. For the two groups

Table 8.6  Information necessary to calculate sample sizes:
survival analysis

a
b

PA
PB
Td

Pd
Effect size
nx

x details

The p value one wants to meet
The probability of type II error
(power is 1 − b )
Proportion of patients in group A
Proportion of patients in group B
Planned duration in time for the study
patients
The overall probability of death at Td
The hazard ratio, hr, one wants to detect
The number of explanatory x variables to
be used
Details about the x variables such as
mean and standard deviations

8  Power Analysis and Sample Sizes in Pathology Research

147

A

B

y
t
i
l
i

b
a
b
o
r
p

l

i

a
v
v
r
u
S

1.0

0.8

0.6

0.4

0.2

0

0

5

Years

10

15

Fig.  8.5  Plot  of  survival  probability  for  two  simulated
patient groups, A and B

of  patients,  A  and  B,  the  ratio  of  their  hazard
functions would be hr = hA/hB, and the relation-
ship  between  their  survival  functions,  SA  and
SB, relates to this ratio as follows:

SA SB .

=

(hr)

Now  consider  the  following  plot  of  survival
probabilities  for  patient  groups  A  and  B
(Fig. 8.5).

The survival probabilities at 10 years for groups
A and B are respectively 0.90 and 0.78. Suppose,
we plan a study with a control group whose sur-
vival is equal to group A and we want to detect a
drop in survival equivalent to that of group B. The
proportional hazard model implies that the hazard
ratio, hr, for group B relative to group A is:

(Here,  ln  stands  for  the  natural  logarithm.)
Thus,  the  hr  to  be  detected  would  be  approxi-
mately 2.4. To put this in perspective, Therneau
and Grambsch suggest that many clinical studies
are designed to detect much more modest values
of hr – for example, values ranging from 1.15 to
2.00.  On  the  other  hand,  detecting  lower  hr
requires  more  study  patients.  Having  selected
values for the variables in Table 8.6, we use the
software  to  determine  sample  sizes  needed  for
survival studies (see, e.g, Therneau and Grambsch
and Scheoenfeld).

Consider a specific example. Recently Marotti
et  al.  reported  the  results  of  estrogen  receptor-b
(ER-b)  expression  in  invasive  breast  cancer  [5].
They found that in ER-a positive tumors, the pres-
ence of ER-b implied an improvement in propor-
tion  surviving  at  30  years  from  approximately
0.64–0.74. This difference in survival corresponds
to  a  hazard  ratio  of  approximately  0.68.  In  their
study, the p value for this result was 0.10, so that
they concluded that the difference in survival was
not  significant.  For  this  subset  of  their  data,  the
authors  had  470  patients  with  ER-a  positive
tumors. Now consider how many would be required
to  show  that  the  effect  of  ER-b  on  survival  was
significant at a p value of 0.05 and with a power of
0.8?  The  answer  is  222  patients  with  observed
times of death. Because most of the study patients
were  living  at  the  last  time  of  observation,  the
authors had fewer than 150 with observed times of
death.  Thus,  their  study  did  not  have  sufficient
power to detect this small change in survival.

hr

=

hB / hA ln(SB) / ln(SA)

=

=

ln(0.78) / ln(0.90)

=

2.4.

Issues Relevant to Follicular
Variant of Papillary Carcinoma
of the Thyroid

One of the more controversial topics in anatomic
pathology concerns the definition of the follicular
variant  of  papillary  carcinoma  of  the  thyroid
(PTC) [6–10]. This tumor has come to be recog-
nized  as  a  variant  of  PTC  even  when  encapsu-

lated and when papillary structures are absent. In
fact, the defining morphological features of PTC
have come to comprise several nuclear phenom-
ena, about which there is much debate and docu-
mented  disagreements.
If  an  encapsulated
follicular tumor is not a follicular variant of PTC,
then what would it be? The answer is a follicular
adenoma.  Thus,  for  encapsulated  follicular
tumors  the  major   distinction  to  be  made  is
between follicular variant of PTC and follicular

148

R.T.  Vollmer

adenoma.  Clearly,  the  prognosis  for  follicular
adenoma should be excellent. By contrast, there
are well-documented cases of follicular variant of
PTC  with  recurrence  and  metastases  after
 sufficiently  long  follow-up.  Thus,  some  experts
now suggest that to resolve the dilemmas about
definition of PTC and its distinction from follicu-
lar  adenoma  what  are  needed  are  studies  with
long-term follow-up. Let us consider this issue in
further detail.

Experts  agree  that  the  long-term  prognosis
for most patients with PTC is good. The AJCC
lists the 5-year survival of stage I PTC as 0.971
and  the  5-year  relative  survival  as  0.998  [11].
This  implies  that  the  baseline  5-year  survival
for  a  group  without  PTC  should  be  approxi-
mately  0.9729.  Now  let  us  suppose  that  the
5-year  survival  for  an  encapsulated  follicular
adenoma  should  approximate  the  baseline  for
the  population  (i.e.,  0.9729).  Next,  let  us  sup-
pose  that  the  5-year  survival  for  encapsulated
PTC  should  approximate  that  for  stage  I  PTC
(i.e., 0.971). How many patients would it take
to detect a significant difference in survival for
encapsulated  PTC  vs.  follicular  adenoma?  If
the  total  follow-up  were  to  be  10  years  with
a = 0.05 and b = 0.2, then the answer is approxi-
mately  120,000  patients,  or  1,200,000  patient-
years of follow-up. These numbers suggest that
it  will  be  unlikely  that  studies  of  survival  in
these  tumors  will  ever  answer  the  question  of

whether   survival  in  encapsulated  PTC  differs
 significantly from follicular adenoma.

References

  1.  Miller  I,  Miller  M,  John  E.  Freund’s  mathematical
statistics  with  applications.  7th  ed.  Upper  Saddle
River: Pearson Prentice Hall; 2004.

  2.  Zhang J, Sokal I, Peskin ER, et al. CSF multianalyte
profile  distinguishes  Alzheimer  and  Parkinson  dis-
eases. Am J Clin Pathol. 2008;129:526–9.

  3.  Hsieh FY. Sample size tables for logistic regression.

Stat Med. 1989;8:795–802.

  4.  Hosmer  DW,  Lemeshow  S.  Applied  logistic  regres-

sion. 2nd ed. New York: Wiley; 2000.

  5.  Marotti JD, Collins LC, Hu R, Tamimi RM. Estrogen
receptor-b expression in invasive breast cancer in rela-
tion to molecular phenotype: results from the Nurses’
Health Study. Mod Pathol. 2010;23:197–204.

  6.  Rosai  J.  Papillary  thyroid  carcinoma:  a  root-and-
branch rethink. Am J Clin Pathol. 2008;130:683–6.
  7.  Baloch ZW, LiVolsi VA. Follicular-patterned lesions
of the thyroid. The bane of the pathologist. Am J Clin
Pathol. 2002;117:143–50.

  8.  Hunt JL, Dacic S, Barnes EL, Bures JC. Encapsulated
follicular variant of papillary thyroid carcinoma. Am
J Clin Pathol. 2002;118:602–3.

  9.  Baloch  Z,  LiVolsi  VA,  Henricks  WH,  Sebak  BA.
Encapsulated  follicular  variant  of  papillary  thyroid
carcinoma. Am J Clin Pathol. 2002;118:603–4.
 10. Chan  JKC.  Encapsulated  follicular  variant  of  papil-
lary  thyroid  carcinoma.  Am  J  Clin  Pathol.  2002;
118:605.

 11. Edge SB, Byrd DR, Compton CC, Fritz AG, Greene
FL, Trotti III A, editors. AJCC cancer staging manual.
7th ed. New York: Springer; 2010. p. 87–96.

Meta-Analysis: A Statistical Method
to Integrate Information Provided
by Different Studies

9

Eleftherios C. Vamvakas

Keywords
Meta-analysis  for  evaluation  of  therapies  •  Statistical  methodology  for
medical  literature  review  •  Epidemiology  of  study  results  •  Evidence-
based pathology • Diagnostic test accuracy

Meta-analysis  or  statistical  overview  is  the
 structured and systematic integration of informa-
tion from different studies of a given problem [1].
It refers to the disciplined synthesis of previous
research  findings  where  the  results  of  multiple
reports on the effect of an exposure or treatment
are compared, contrasted, and reanalyzed. When
the  results  are  discrepant,  the  purpose  of  the
meta-analysis  is  to  investigate  reasons  for  dis-
agreements among the studies. When the results
are  concordant,  the  goal  of  an  overview  is  to
derive,  through  the  application  of  a  number  of
quantitative methods, a measure of the effect of
the  exposure  or  treatment  across  the  combined
investigations. This measure is referred to as the
“average” or “summary” effect of the exposure or
treatment under study [1–5].

Meta-analysis differs from the traditional nar-
rative reviews of the literature in that: (1) all com-
pleted investigations on the effect of an exposure
or treatment that meet specific eligibility criteria

E.C. Vamvakas ()
Department of Pathology and Laboratory Medicine,
Cedars-Sinai Medical Center, 8700 Beverly Blvd.,
Los Angeles, CA 90048, USA
e-mail: vamvakase@cshs.org

are retrieved and considered for inclusion in the
overview; (2) the quality of the retrieved studies
is  assessed  systematically;  (3)  the  degree  of
agreement among the studies is evaluated – both
conceptually  and  based  on  statistical  criteria  –
and the synthesis of the findings proceeds if the
variation in reported results is sufficiently modest
to  be  attributed  to  chance;  and  (4)  quantitative
methods are used to calculate the “average” effect
of  the  intervention  across  the  available  studies,
and  to  test  that  effect  for  statistical  significance
[1–5].  If  a  meta-analysis  is  conducted  in  accor-
dance  with  these  principles,  it  can  provide  the
reader with “an objective view of the research lit-
erature,  unaffected  by  the  sometimes  distorting
lens of individual experience and personal prefer-
ence that can affect a less structured review” [6].
Meta-analysis  has  two  generally  accepted
applications  and  one  controversial  use.  It  can
serve  to  integrate  the  findings  of  studies  which
report  a  treatment  effect  operating  in  the  same
direction, but varying substantially in size between
reports. The purpose of the synthesis is to provide
a more precise estimate of the most likely magni-
tude  of  the  treatment  effect,  so  that  a  definitive
randomized  controlled
trial  (RCT)  can  be
designed, enrolling as many patients as are needed

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_9, © Springer Science+Business Media, LLC 2011

149

150

E.C. Vamvakas

to  establish
that  effect.
the  existence  of
Alternatively,  meta-analysis  can  be  used  to
 investigate  reasons  for  disagreements  among
studies  which report treatment effects operating in
 opposite directions or differing markedly in size
when they all point in the same direction. The aim
of the analysis is to explain discrepancies among
published  results,  based  on  relevant  characteris-
tics of the patients who were included in the avail-
able studies, the treatments that were administered,
or the quality of study design and analysis.

The  third,  and  controversial,  application  of
meta-analysis is to integrate the findings of stud-
ies that report a treatment effect operating in the
same  direction,  but  not  attaining  statistical  sig-
nificance  in  any  study,  perhaps  because  of  the
small  sample  size  and  inadequate  statistical
power  of  each  report.  Here,  the  purpose  of  the
synthesis is to establish the existence of a treat-
ment effect by combining the patient populations
enrolled in separate studies. This proposed use of
statistical overviews makes meta-analysis appear
as a possible alternative to an RCT undertaken to
establish  the  efficacy  of  a  therapeutic  interven-
tion. Overviews, however, best serve as a supple-
ment (as opposed to an alternative) to RCTs.

This is because – even when a definitive RCT
for establishing the efficacy of a therapeutic inter-
vention is, eventually, conducted – its findings may
not necessarily apply to all patients. Trialists use
highly  restrictive  inclusion/exclusion  criteria  for
patient enrollment, because they strive to include a
patient population as homogeneous as possible, to
make it easier to detect a treatment effect untainted
by  confounding  factors.  The  results  of  an  RCT
may  therefore  not  apply  to  patients  who  do  not
meet  the  eligibility  criteria  of  the  study.  Thus,
RCTs  constitute  the  gold  standard  for  evaluating
the efficacy of therapeutic interventions, but need
to be supplemented by meta-analyses in order to
broaden the applicability of their findings [1–7].

A meta-analysis integrates the findings of sep-
arate studies, which usually differ in many aspects
of their design. This variation in the design attri-
butes  of  reports  included  in  a  meta-analysis
results in greater generalizability of the findings
of a statistical overview, compared to the results
of an RCT, because – by combining studies with
disparate design characteristics – a meta-analysis

permits examination of the effect of an interven-
tion in many different situations. If the treatment
effect  is  consistent  in  all  the  studies,  this
 consistency favors a true treatment effect, rather
than one due to chance, or some systematic error,
or  uncontrolled  factor  that  may  have  compro-
mised the results of all completed investigations.
To appreciate the contribution of meta-analyses
in the medical literature, it is appropriate to think
of an overview as an original report consisting of
two parts: a qualitative component and a quanti-
tative one [1, 6, 7]. According to Jenicek [1], the
first phase of a statistical overview should be a
“qualitative” meta-analysis, which must precede
the “quantitative” phase of the report. An assess-
ment of the quality of all retrieved studies should
be  made,  and  studies  of  unacceptable  quality
should  be  excluded  from  the  overview.  In
Goodman’s  opinion,  a  meta-analysis  should
“raise  research  and  editorial  standards,  by  call-
ing attention to the strengths and weaknesses of
the body of research in an area” [6]. O’Rourke
and Detsky assert that the major contribution of
a meta- analysis lies in the attention that it draws
to  flaws  in  the  design  and  conduct  of  previous
studies [7]. When all published studies are sub-
jected to a detailed review of the methods – with
a  focus  on  the  impact  of  the  methods  on  the
validity  of  the  results  –  inadequacies  can  be
identified  and
their  resolution  encouraged.
Recognized shortcomings can be thus avoided in
future  individual  research  efforts,  so  that  more
valid results are produced.

For  their  initially-intended  purpose  (i.e.,  for
the  investigation  of  the  effects  of  therapeutic
interventions),  meta-analyses  were  limited  to
RTCs  [8–10].  The  rationale  and  tools  for  con-
ducting a meta-analysis were thus developed for
RCTs,  that  is,  the  controlled  clinical  experi-
ments undertaken to establish that a new treat-
ment  achieves  a  better  clinical  outcome  than
standard therapy; and which can be presumed to
be free of the effects of selection bias and con-
founding factors, as well as free of the effect of
observation  bias  when  they  are  double-blind
[11]. Meta-analysis was used in pathology (i.e.,
for the study of diagnostic-test accuracy) several
years after the method had been widely used in
clinical medicine.

9  Meta-Analysis: A Statistical Method to Integrate Information Provided by Different Studies

151

Studies of diagnostic-test accuracy conducted
at different centers often produce estimates of the
sensitivity and specificity of a test that vary widely.
Such  variation  may  be  due  to  random  sampling
variation, differences in study quality, differences
in the characteristics of the test and the enrolled
patients,  and/or  differences  in  the  cutoff  points
used to calculate the published estimates of sensi-
tivity and specificity [12–17]. The wide variation
across studies in the reported accuracy of a labo-
ratory test limits the value of the information pro-
vided  by  traditional  reviews  of  the  literature
presenting the range of available estimates. What
is  often  needed,  instead  of  this  range,  is  insight
into the reasons for the differences in the reported
estimates of accuracy, and – if possible – a sum-
mary estimate of the sensitivity and specificity of
the test based on all available data.

Meta-analysis has thus been used to: (1) pro-
duce valid summary estimates of the diagnostic
accuracy of laboratory tests; (2) explain the varia-
tion  in  the  results  of  published  reports;  and  (3)
improve  the  quality  of  the  primary  studies  by
identifying  their  methodologic  shortcomings.
However,  there  are  differences  between  RCTs
and studies of diagnostic-test accuracy, as well as
methodologic obstacles to the use of meta-analy-
sis in diagnostic pathology [16], which must be
carefully  addressed  by  meta-analysts  of  studies
of diagnostic-test accuracy.

The purpose of this chapter is to provide prac-
ticing pathologists with the necessary background
for  reading  and  evaluating  published  reports  of
meta-analyses  of  RCTs  as  well  as  overviews  of
studies of diagnostic-test accuracy. The first part of
the  chapter  describes  the  applications  of  meta-
analysis in the domain of RCTs. The second part
discusses how these same concepts and (appropri-
ately modified) methods can be used to integrate
results of studies of diagnostic-test accuracy. The
rationale  for  quantitative  research  synthesis  is
 presented  and  the  component  parts  of  a  meta-
analysis are described. A recommended approach
to the medical interpretation of overviews is then
presented. All concepts are illustrated using as an
example  the  meta-analysis  [18]  of  the  RCTs  of
white-blood-cell (WBC) reduction of red-  blood-
cell  (RBC)  components,  by  means  of  prestor-
age  or  poststorage   filtration,  to  prevent  the

deleterious

purportedly
immunomodulatory
effects of WBC-containing allogeneic blood trans-
fusion  (ABT)  [19].  Transfusion-related  immuno-
modulation may  predispose patients to an increased
risk of bacterial infection, and perhaps also mortality,
during or shortly after a hospitalization [20].

The Unit of Observation
in Meta-Analysis

Meta-analysis is the epidemiology of study results.
Clinical  studies  use  the  individual  patient  as  the
unit of observation. In contrast, the unit of obser-
vation in meta-analysis is either the adverse effect
of an exposure or the beneficial effect of an inter-
vention, as calculated from each individual origi-
nal  report.  For  example,  published  RCTs  of  the
deleterious effect of exposure to WBC-containing
ABT or, alternatively, of the efficacy of the inter-
vention of WBC reduction of RBC components to
prevent this purported ABT adverse effect, used
the  individual  patient  as  the  unit  of  observation
[21–34].  On  the  contrary,  the  meta-analyses  of
these reports [18, 35, 36] used as the unit of obser-
vation one or more measures of the adverse ABT
effect  as  calculated  from  within  each  reported
study.  More  specifically,  the  odds  ratio  (OR)  of
either  bacterial  infection  [21–32]  or  all-cause
mortality [21, 23–29, 31, 33, 34] represented the
clinical effect of the deleterious immunomodula-
tory or pro-inflammatory effects of ABT [19, 20]
as calculated from within each RCT [21–34].

Each  study  [21–34]  thus  contributed  to  the
meta-analysis [18] one or more estimates of the
effect  of  WBC-containing  ABT  in  increasing
the risk of either infection or mortality. Figures 9.1
and 9.2 show the results of these individual RCTs
[21–34]  as  calculated  from  intension-to-treat-
analyses. The OR of bacterial infection or short-
term  (up  to  3-month  posttransfusion)  mortality
(Figs.  9.1  and  9.2,  respectively)  was  calculated
from  each  RCT  if  the  authors  had  reported  the
table
four  counts  of  a  2 × 2  contingency
(Table  9.1).  Two  RCTs  [30,  32]  had  presented
only “as-treated” analyses of patients transfused
with non-WBC-reduced vs. WBC-reduced RBCs,
vs. subjects randomly allocated preoperatively to
receive either non-WBC-reduced or WBC-reduced

152

E.C. Vamvakas

Fig.  9.1  RCTs  investigating  the  association  of  WBC-
containing  ABT  with  bacterial  infection  ranked  in  the
order  of  magnitude  of  the  ABT  effect  that  they  reported
[21–32]. For each RCT, the figure shows the OR of bacte-
rial infection in subjects randomized to receive non-WBC-
reduced  vs.  WBC-reduced  allogeneic  RBCs  or  whole
blood,  as  calculated  from  an  intention-to-treat  analysis
(Table 9.2). Each OR is surrounded by its 95% CI. If the
95% CI of the OR includes the null value of 1, the ABT
effect is not statistically significant (p > 0.05). A deleteri-
ous ABT effect is indicated by an OR > 1, provided that the
associated 95% CI does not include the null value of 1

RBCs who had not needed ABT  perioperatively.
For  these  two  studies,  which  reported  only  on
bacterial  infection  as  an  outcome  (Fig.  9.1  and
Table  9.2),  the  minimal  number  of  infections
recorded in the third comparison group of patients
not  needing  perioperative  transfusion  was  allo-
cated [18] to the two randomization arms to pro-
duce approximate 2 × 2 contingency tables for an
intention-to-treat  analysis.  One  RCT  [23]  fol-
lowed-up  postrandomization
the
adverse events of infection and/or death) only the
transfused patients. Figures 9.1 and 9.2 show the
results of these RCTs [21–34] ranked in the order
of  magnitude  of  the  ABT  effect  on  bacterial
infection  or  mortality  calculated  from  within
each study.

(recording

Fig.  9.2  RCTs  investigating  the  association  of  WBC-
containing ABT with short-term (up to 3-month posttrans-
fusion),  all-cause  mortality  ranked  in  the  order  of
magnitude of the ABT effect that they reported [21, 23–
29, 31, 33, 34]. For each RCT, the figure shows the OR of
mortality  in  subjects  randomized  to  receive  non-WBC-
reduced vs. WBC-reduced allogeneic RBCs, as calculated
from an intention-to-treat analysis

A  meta-analysis

integrates  exposure  or
 intervention  (treatment)  effects  calculated  from
separate studies, such as the deleterious effects of
WBC-containing  ABT  shown  in  Figs.  9.1  and
9.2.  It  is  important  to  appreciate  that  a  meta-
analysis  integrates  the  exposure  or  treatment
effects  calculated  from  individual  studies,  as
opposed to “pooling” the data on the individual
patients  enrolled  in  each  RCT.  Thus,  the  ABT
effect  from  each  RCT,  as  incorporated  into  the
analysis  of  an  overview  [18,  35,  36],  is  based
exclusively on the outcomes of the recipients of
non-WBC-reduced  vs.  WBC-reduced  ABT
within each individual study. In Table 9.1, patients
preoperatively  randomized  to  receive  WBC-
containing ABT (in the event that they need peri-
operative  transfusion)  constitute  the  treatment
arm,  because  –  by  receiving  non-WBC-reduced
RBCs – they are exposed to the immunomodulatory

9  Meta-Analysis: A Statistical Method to Integrate Information Provided by Different Studies

153

Table 9.1  2 × 2 Contingency table counts for the randomized controlled trials investigating the relationship between
WBC-containing ABT and bacterial infection or short-term (up to 3-month posttransfusion) all-cause mortality

Subjects developing infection b
Subjects remaining free of infection c

Treatment arm (subjects randomized to
receive non-WBC-reduced ABT a)
a
c

Control arm (subjects randomized
to receive WBC-reduced ABT a)
b
d

Odds of infection or mortality in treated patients

=

a
c

/ (
/ (

a c
)
+
a c
)
+

=

a
c

Odds of infection or mortality in controls

=

b
d

/ (
/ (

b d
+
b d
+

)
)

=

b
d

odds ratio (OR) =

odds of infection or mortality in treated patients
odds of infection or mortality in controls

=

a c
/
b d
/

=

ad
bc

a = count of patients randomized to receive non-WBC-reduced ABT developing infection b
b = count of patients randomized to receive WBC-reduced ABT developing infection b
c = count of patients randomized to receive non-WBC-reduced ABT not developing infection c
d = count of patients randomized to receive non-WBC-reduced ABT not developing infection c
a In the event that they needed perioperative transfusion
b Or dying within 3 months posttransfusion
c Or surviving up to 3 months posttransfusion (to the completion of each study’s follow-up period)

or pro-inflammatory effects of allogeneic WBCs
[19,  20].  Patients  preoperatively  randomized  to
receive  non-WBC-reduced  ABT  constitute  the
control arm, that is, the unexposed subjects.

Controls from one RCT cannot serve as con-
trols  for  patients  being  exposed  to  allogeneic
WBCs  in  another  RCT.  Such  a  comparison
would  be  invalid,  because  the  various  studies
included  in  a  meta-analysis  differ  from  one
another in various characteristics of the enrolled
patients,  the  exposure(s)  received,  as  well  as
attributes  relating  to  study  design  and  analysis
(Table 9.2). These differences among the studies
are often likely to affect the outcome of interest
(e.g., the odds of infection (Fig. 9.1) or all-cause
mortality (Fig. 9.2)). Thus, because of multiple
differences  among  the  available  studies,  it  is
invalid  to  compare  directly  the  experience  of
individual patients from one study to that of sub-
jects from another RCT. An overview compares
the effect of an exposure or intervention in one
study  with  the  effect  of  that  exposure  or  inter-
vention in other RCTs.

For example, Table 9.2 lists some of the differ-
ences  between  the  RCTs  [21–34]  investigating
the  association  of  WBC-containing  ABT  with
bacterial infection or all-cause mortality recorded
up  to  3  months  posttransfusion.  Among  other

 differences  between  the  studies,  these  RCTs
 differed  in  the  RBC  product  transfused  to  the
non-WBC-reduced arm, the RBC product trans-
fused to the WBC-reduced arm, and/or the clini-
cal setting. All but four RCTs, including all RCTs
published  after  1998,  transfused  to  the  WBC-
reduced  arm  allogeneic  RBCs  filtered  before
storage  (Table  9.2).  Thus,  for  patients  in  the
WBC-reduced  arm,  these  RCTs  abrogated  both
any ABT effects mediated by immunologically-
competent allogeneic mononuclear cells [37–39]
and any ABT effects mediated by WBC-derived
soluble  mediators  that  progressively  accumulate
in  the  supernatant  fluid  of  RBCs  during  storage
[40–43].  In  contrast,  three  RCTs  published
between 1992 and 1998 [30–32], as well as one of
three randomization arms employed in the RCT of
van de Watering et al. [28], transfused to the WBC-
reduced  arm  allogeneic  RBCs  or  whole  blood
 filtered  after  storage.  For  patients  in  the  WBC-
reduced  arm,  these  RCTs  [28,  30–32]  prevented
effects  mediated  by  immunologically-competent
allogeneic  mononuclear  cells  [37–39],  but  not
effects mediated by WBC-derived soluble media-
tors that accumulate during storage [40–43].

Five RCTs [21, 23, 25, 28, 29] were conducted
in cardiac surgery and five [22, 26, 30–32] in gas-
trointestinal  surgery.  The  ABT  effect  may  be

e
h
t

h
c
i
h
w
n
i

y
r
t
n
u
o
C

d
e
t
c
u
d
n
o
c

s
a
w
y
d
u
t
s

c
i
e
n
e
g
o
l
l
a

d
e
c
u
d
e
r
-

C
B
W

o
t

n
e
v
i
g

t
n
e
n
o
p
m
o
c
C
B
R

c
i
e
n
e
g
o
l
l
a
d
e
c
u
d
e
r
-

C
B
W
-
n
o
N

e
h
t

o
t

n
e
v
i
g
t
n
e
n
o
p
m
o
c
C
B
R

f
o

r
a
e
Y

m
r
a

l
o
r
t
n
o
c

e
h
t

m
r
a

t
n
e
m
t
a
e
r
t

e
z
i
s

e
l
p
m
a
S

g
n
i
t
t
e
s

l
a
c
i
n
i
l

C

n
o
i
t
a
c
i
l
b
u
p

s
e
c
n
e
r
e
f
e
R

s
d
n
a
l
r
e
h
t
e
N
e
h
T

s
C
B
R
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
e
r
P

s
C
B
R
d
e
c
u
d
e
r
-
t
a
o
c
-
y
f
f
u
B

S
U

s
C
B
R
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
e
r
P

s
C
B
R
d
e
fi
i
d
o
m
n
U

S
U

s
C
B
R
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
e
r
P

s
C
B
R
d
e
fi
i
d
o
m
n
U

3
4
4

7
9
6

b
2
6
5

n
o
i
t
c
e
s
e
r

r
e
c
n
a
c

l
a
t
c
e
r
o
l
o
C

y
r
e
g
r
u
s

c
a
i
d
r
a
C

y
r
e
g
r
u
s

c
a
i
d
r
a
C

2
0
0
2

4
9
9
1

4
0
0
2

a
]
2
2
[

.
l
a

t
e

s
r
e
i
b
u
o
H

]
3
2
[

.
l
a

t
e

v
o
k
h
s
o
B

]
1
2
[

.
l
a

t
e

y
e
c
a
r
B

s
d
n
a
l
r
e
h
t
e
N
e
h
T

s
C
B
R
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
e
r
P

s
C
B
R
d
e
c
u
d
e
r
-
t
a
o
c
-
y
f
f
u
B

2
5
0
,
1

)
3
1
4
=
n
(

e
v
i
t
c
e
l
e

r
o

)
9
7
=
n
(

e
t
u
c
A

4
0
0
2

]
4
2
[

.
l
a

t
e

n
e
t
l
i

H
n
a
v

y
t
i
l
a
t
r
o
m
e
s
u
a
c
-
l
l
a

,

m
r
e
t
-
t
r
o
h
s

r
o
/
d
n
a

n
o
i
t
c
e
f
n
i

l
a
i
r
e
t
c
a
b

h
t
i

w
n
o
i
s
u
f
s
n
a
r
t

d
o
o
l
b

c
i
e
n
e
g
o
l
l
a

g
n
i
n
i
a
t
n
o
c
-
C
B
W

f
o

n
o
i
t
a
i
c
o
s
s
a

e
h
t

g
n
i
t
a
g
i
t
s
e
v
n
i

s
T
C
R

f
o
n
g
i
s
e
D

.

2
9
e
l
b
a
T

n
o
i
t
c
e
s
e
r

;
r
i
a
p
e
r

m
s
y
r
u
e
n
a

c
i
t
r
o
a

y
c
n
a
n
g
i
l
a
m

l
a
n
i
t
s
e
t
n
i
o
r
t
s
a
g

f
o

m
o
d
g
n
i
K
d
e
t
i
n
U

s
C
B
R
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
e
r
P

r
o

)
4
0
2
=
n
(
d
e
c
u
d
e
r
-
t
a
o
c
-
y
f
f
u
B

s
C
B
R

)
8
9
1
=
n
(

c
d
e
c
u
d
e
r
-
a
m
s
a
l
p

k
r
a
m
n
e
D

S
U

s
C
B
R
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
e
r
P

s
C
B
R
d
e
c
u
d
e
r
-
t
a
o
c
-
y
f
f
u
B

s
C
B
R
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
e
r
P

r
o
f

d
e
r
o
t
s

s
C
B
R
d
e
fi
i
d
o
m
n
U

s
y
a
d

5
2
<

s
C
B
R
d
e
r
e
t
l
fi
)
3
0
3
=
n
(

s
d
n
a
l
r
e
h
t
e
N
e
h
T

e
g
a
r
o
t
s
t
s
o
p

r
o

)
5
0
3
=
n
(

e
g
a
r
o
t
s
e
r
P

s
C
B
R
d
e
c
u
d
e
r
-
t
a
o
c
-
y
f
f
u
B

s
d
n
a
l
r
e
h
t
e
N
e
h
T

s
C
B
R
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
e
r
P

s
C
B
R
d
e
c
u
d
e
r
-
t
a
o
c
-
y
f
f
u
B

k
r
a
m
n
e
D

S
U

s
C
B
R
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
t
s
o
P

s
C
B
R
d
e
fi
i
d
o
m
n
U

s
C
B
R
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
t
s
o
P

c
i
e
n
e
g
o
l
l
a
d
e
c
u
d
e
r
-
t
a
o
c
-
y
f
f
u
B

s
C
B
R

k
r
a
m
n
e
D

d
o
o
l
b

e
l
o
h
w
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
t
s
o
P

d
o
o
l
b

e
l
o
h
w
d
e
fi
i
d
o
m
n
U

k
r
a
m
n
e
D

S
U

s
C
B
R
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
e
r
P

s
C
B
R
d
e
fi
i
d
o
m
n
U

s
C
B
R
d
e
r
e
t
l
fi
-
e
g
a
r
o
t
s
e
r
P

s
C
B
R
d
e
c
u
d
e
r
-
t
a
o
c
-
y
f
f
u
B

7
9
5

9
7
2

d
4
2
3

4
1
9

4
7
4

1
2
2

6
8
5

4
2

7
9
1

0
8
7
,
2

y
r
e
g
r
u
s

l
a
t
c
e
r
o
l
o
C

s
t
n
e
i
t
a
p

a
m
u
a
r
T

1
0
0
2

6
0
0
2

]
6
2
[

.
l
a

t
e

d
a
t
s
e
l
t
i

T

]
7
2
[

.
l
a

t
e

s
n
e
h
t
a
N

y
r
e
g
r
u
s

c
a
i
d
r
a
C

8
9
9
1

g
n
i
r
e
t
a

W

e
d

n
a
v

y
r
e
g
r
u
s

c
a
i
d
r
a
C

2
0
0
2

]
5
2
[

.
l
a

t
e

s
i
l
l
a

W

y
r
e
g
r
u
s

l
a
n
i
t
s
e
t
n
i
o
r
t
s
a
G

y
r
e
g
r
u
s

l
a
t
c
e
r
o
l
o
C

y
r
e
g
r
u
s

c
a
i
d
r
a
C

s
t
n
e
i
t
a
p

d
e
z
i
l
a
t
i
p
s
o
h

l
l

A

s
t
n
e
i
t
a
p

a
m
u
a
r
t

n
r
u
B

y
r
e
g
r
u
s

l
a
t
c
e
r
o
l
o
C

4
0
0
2

8
9
9
1

6
9
9
1

2
9
9
1

2
0
0
2

9
9
9
1

]
9
2
[

.
l
a

t
e

n
i
g
l
i

B

a
]
0
3
[

.
l
a

t
e

r
e
t
t
r
a
T

]
1
3
[

.
l
a

t
e

n
e
s
n
e
J

]
8
2
[

.
l
a

t
e

a
]
2
3
[

.
l
a

t
e

n
e
s
n
e
J

e
]
3
3
[

.
l
a

t
e

k
i
z
D

e
]
4
3
[

.
l
a

t
e

n
e
s
l
e
i
N

n
o
i
t
c
e
f
n
i

f
o

s
i
s
y
l
a
n
a

e
h
t

n
i

d
e
s
u

e
r
e
w

)
8
6
2
=
n
(

t
n
e
s
n
o
c

e
s
u
f
e
r

t
o
n

d
i
d

o
h
w
s
t
n
e
i
t
a
p

;
y
t
i
l
a
t
r
o
m

f
o
s
i
s
y
l
a
n
a

e
h
t

n
i

d
e
s
u

e
r
e
w

)
4
2
3
 =
n
(

s
t
n
e
i
t
a
p

d
e
z
i
m
o
d
n
a
r

l
l

A

)
n
o
i
t
c
e
f
n
i

t
o
n

t
u
b
(

y
t
i
l
a
t
r
o
m
n
o

a
t
a
d

g
n
i
t
r
o
p
e
r

s
T
C
R

d

e

s
C
B
R

)
d
e
fi
i
d
o
m
n
u

,
.
e
.
i
(

d
e
c
u
d
e
r
-
t
a
o
c
-
y
f
f
u
b
n
o
n

o
t

t
n
e
l
a
v
i
u
q
e

s
i

t
n
e
n
o
p
m
o
c

s
i
h
t

,
t
n
e
t
n
o
c
C
B
W

s
t
i

f
o

s
m
r
e
t

n
I
c

)
s
t
n
e
i
t
a
p

d
e
z
i
m
o
d
n
a
r

l
l
a

o
t

d
e
s
o
p
p
o
s
a
(

y
l
n
o

s
t
n
e
i
t
a
p

d
e
s
u
f
s
n
a
r
T

)
y
t
i
l
a
t
r
o
m

t
o
n
t
u
b
(

n
o
i
t
c
e
f
n
i

l
a
i
r
e
t
c
a
b

n
o

a
t
a
d

g
n
i
t
r
o
p
e
r

s
T
C
R

a

b

9  Meta-Analysis: A Statistical Method to Integrate Information Provided by Different Studies

155

enhanced in the setting of cardiac surgery, because
WBC-derived soluble mediators and/or allogeneic
mononuclear cells may act as a second inflamma-
tory  insult,  compounding  the  diffuse  inflamma-
tory  response  to  the  extracorporeal  circuit  and
predisposing to postoperative complications [44].
Alternatively, the ABT effect may be enhanced in
the  “unclean”  setting  of  gastrointestinal  surgery.
Either  way,  it  is  possible  for  a  deleterious  ABT
effect to become manifest only in the presence of
cofactors, such as the special conditions that exist
in cardiac or gastrointestinal surgery.

In  addition,  there  was  great  variation  among
the RCTs in the amount of blood transfused and
the  frequency  of  a  diagnosis  of  postoperative
infection.  As  few  as  26.7%  of  randomized  sub-
jects  needed  perioperative  transfusion  in  some
gastrointestinal surgery studies [26]; in contrast,
as many as 94.7% of randomized subjects needed
perioperative transfusion in some cardiac-surgery
studies [28]. In gastrointestinal surgery, the fre-
quency  of  postoperative  infection  ranged  from
8.1% [32] to 33.4% [22]. The differences in the
proportion of transfused patients reflected patient-
related  selection  factors  (severity  of  underlying
illness)  as  well  as  setting-  and  surgeon-related
selection factors (subjective application of liberal
or  conservative  transfusion  criteria  during  an
operation – when objective laboratory indicators
of the need for transfusion are unavailable). The
differences  in  the  frequency  of  postoperative
infection  reflected  differences  in  the  patients’
severity  of  illness  and  the  employed  diagnostic
criteria for infection, differences in the types of
infections  evaluated  in  each  study,  and  perhaps
also  the  effects  of  observation  and/or  selection
bias (since not all RCTs were double-blind and,
in  most  cases,  the  details  of  the  randomization
procedure[s] were not reported).

Thus, it is most unlikely that all RCTs targeted
an increase in the risk of postoperative infection
or all-cause mortality mediated by a deleterious
ABT effect that was biologically the same in all
the  cases.  Instead,  these  RCTs  [21–34]  most
likely targeted effects of non-WBC-reduced ABT
that  differed  both  in  magnitude  and/or  nature  –
being mediated by either allogeneic mononuclear
cells [37–39] or WBC-derived soluble mediators

accumulating  during  storage  [40–43]  or  both;
and being compounded (or not) by other cofac-
tors (such as a diffuse inflammatory response to
the extracorporeal circuit used in cardiac surgery
[44]).  Accordingly,  a  meta-analysis  integrating
the  results  of  all  available  studies  would  not
establish an effect attributed to a specific biologic
mediator  or  mechanism.  Stated  in  other  words,
the medical heterogeneity of the available RCTs
made it inappropriate to combine the results of all
available RCTs in a meta-analysis [45].

Assessment of the Eligibility
of Original Reports for Inclusion
in a Meta-Analysis

Meta-analysis is based on the assumption that all
studies  evaluating  the  effect  of  an  exposure  or
intervention are retrieved. Some of these reports
are  then  selected  for  inclusion  in  the  analysis,
based on eligibility criteria specified in advance.
Exclusions are initially determined by the medi-
cal scope of the overview. If the medical question
asked  is  a  general  one,  broad  selection  criteria
may be used; if it is a more specific one, the cri-
teria are stricter. The hypothesis under investiga-
tion must be defined in precise terms, so that the
selection of studies for analysis can be made in
an objective and reproducible manner. Additional
criteria  for  exclusion  may  include  the  date  of
publication  (because  a  study  may  no  longer  be
clinically  relevant),  the  language  of  publication
(if  reports  not  published  in  English  cannot  be
properly  evaluated),  the  length  of  follow-up  (if
this  is  considered  too  short  for  a  meaningful
assessment of the outcome under study), and the
completeness of the presented information (if the
four  counts  of  a  contingency  table  (Table  9.1)
cannot be extracted from an abstract, letter to the
editor,  or  other  summary  report  of  a  study).
Excluded studies should be listed in the report of
the meta-analysis, and the reasons for their exclu-
sion should be explicitly stated.

When subjects are randomly allocated preop-
eratively to receive non-WBC-reduced vs. WBC-
reduced  ABT  in  the  event  that  they  need
perioperative  transfusion  (Table  9.2),  patients

156

E.C. Vamvakas

from either arm of the RCT should have the same
baseline probability of developing bacterial infec-
tion, of dying within 3 months of the transfusion,
and of needing perioperative ABT. Provided that the
number  of  the  enrolled  subjects  is  very  large,
the play of chance will distribute equally between
the  treatment  and  control  arms  all  prognostic
 factors for mortality or development of bacterial
infection  other  than  the  receipt  of  non-WBC-
reduced  (as  opposed  to  WBC-reduced)  ABT.
Therefore,  in  the  absence  of  any  intervention
(such  as  WBC  reduction  of  the  administered
RBCs),  the  same  proportion  of  patients  from
either randomization arm should be expected to
develop infection or die within 3 months of the
transfusion from any cause. For this reason, any
difference  in  the  odds  of  infection  or  mortality
between  the  two  randomization  arms  can  be
ascribed to the receipt of non-WBC-reduced (vs.
WBC-reduced) ABT.

The  intent  to  investigate  the  existence  of  a
causal relationship is the reason why – when meta-
analyses were initially introduced in  medicine –
only RCTs used to be eligible for inclusion in an
overview. When results from observational stud-
ies  are  also  available,  the  findings  of  observa-
tional  studies  are  either  not  considered  at  all  or
integrated  separately  from  the  results  of  RCTs.
As it will be discussed later, however, investiga-
tions of diagnostic-test accuracy are, in their vast
majority, observational studies.

Assessment of the Quality
of Randomized Controlled Trials
Included in a Meta-Analysis

The assessment of the quality of studies meeting
the  eligibility  criteria  for  inclusion  in  a  meta-
analysis was listed as a necessary part of any sta-
tistical  overview  in  the  early  guidelines  for
meta-analysis  in  clinical  research  [1,  2,  6–8].
Formal  instruments  for  assessing  the  quality  of
RCTs have been (and continue to be) developed
[46–50]. Chalmers et al. [46] developed a detailed
list of items to be used for scoring the quality of
published RCTs on a scale from 0 to 1. Guidelines
for evaluating observational studies were initially

presented by Lichtenstein et al. [51] and Feinstein
[52] and several more scales followed.

A simple instrument was developed by Jadad
et  al.  [53]  for  use  by  all  readers  of  RCTs.  The
maximum  quality  score  that  can  be  given  to  a
study based on this instrument is 5. Two points
are  given  to  a  report  for  random  assignment  of
subjects to treatment and control groups; 2 points
are  granted  for  blinding  both  investigators  and
patients;  and  1  point  is  added  if  the  number  of
patients  excluded  from  the  analysis,  along  with
the reasons for all dropouts and withdrawals, are
presented in the report of the RCT. With regard to
the randomization procedure, 1 point is given if a
study is designated as “randomized,” but the ran-
domization procedure is not described; 0 point is
given if the randomization procedure is described,
but is judged to be inappropriate; and 2 points are
given if the randomization procedure is described,
and is appropriate. In regard to the blinding tech-
nique, 1 point is given if a study is designated as
“double-blind,”  but  the  procedure  for  blinding
investigators and patients is not described; 0 point
is  given  if  the  blinding  procedure  is  described,
but is judged to be inappropriate; and 2 points are
given if the blinding procedure is described, and
is appropriate.

Moher et al. [54] used the instrument of Jadad
et al. [53] to measure the quality of 127 RCTs
from  the  medical  literature.  Few  RCTs  had
reported either the method used to generate the
randomization  sequence  (15%),  or  the  method
used to conceal this sequence until the point of
randomization  occurred  (14%).  RCTs  that  had
not adequately described the measures taken to
conceal  the  treatment  allocations,  exaggerated
the  effect  of  the  intervention  under  study  by
37% (p < 0.01), compared to RCTs that had ade-
quately  reported  the  method(s)  used  for  con-
cealment.  Furthermore,  RCTs  receiving  a  low
total quality score (£2) exaggerated the estimate
of  the  effect  of  the  intervention  by  34%,  com-
pared  with  high-quality  trials  (>2)  (p < 0.001).
Moher et al. [54] concluded that the pooling of
the  findings  of  low-quality  RCTs  results  in  a
clinically important and statistically significant
exaggeration  of  the  efficacy  of  an  intervention
under study.

9  Meta-Analysis: A Statistical Method to Integrate Information Provided by Different Studies

157

Therefore,  after  a  quality  score  has  been
assigned to each study that is eligible for inclu-
sion in a meta-analysis, meta-analysts must con-
front  the  contentious  issue  whether  studies  of
inferior quality are to be included in the calcula-
tion of the “average” treatment effect. The main
argument for including studies that are not of the
best quality is that a larger number of studies per-
mits examination of the effect of the intervention
in more situations. However, this advantage must
be balanced against the disadvantage of including
questionable results. There is a general consensus
that, if studies of poor quality are to be included,
the  differences  in  quality  must  be  taken  into
account  in  the  analysis  [55,  56].  Computational
methods used in meta-analysis assign weights to
each study that are proportional to a study’s sam-
ple  size  [57,  58].  In  theory,  the  quality  scores
could  also  be  incorporated  into  the  weights
assigned  to  each  report,  so  that  the  calculated
“average”  treatment  effect  can  depend  more
heavily on the findings of investigations of supe-
rior quality [55]. Alternatively, the studies could
be stratified by quality score, so that an “average”
treatment effect can be calculated separately for
each stratum of quality. If the effects differ across
strata, the “average” effect calculated from stud-
ies of superior quality can be considered to be the
valid one.

Overviews in the health field have sometimes
adjusted for the quality of the combined studies
by statistical techniques [59]. Some experts have
recommended that minimal quality standards be
set in advance, in the form of criteria for inclu-
sion,  and  that  studies  that  do  not  meet  them  be
excluded.  Others  have  proposed  that  only  the
“best” of the available studies be used [60, 61].
Quality scores have been criticized, however, as
being based on the report of a study, which is not
necessarily an accurate measure of the truth about
some  elements  of  quality.  The  standards  for
reporting details of the methods used have become
more stringent over the last decade [62, 63], and
studies  published  more  recently  tend  to  attain
higher quality scores for that reason.

Rationale for Quantitative Research
Synthesis

An overview compares the effect of an exposure
or treatment in one study with the effect of that
exposure  or  treatment  in  other  studies.  A  meta-
analysis  by  the  fixed-effects  method  [57,  58]
combines  a  series  of  2 × 2  contingency  table
counts  (Table  9.1),  as  though  the  tables  were
strata of patients enrolled in the same study (and
stratified according to the level of a confounding
factor  in  an  epidemiologic  investigation  or  by
admitting  hospital  in  a  multicenter  RCT).  The
findings  from  these  individual  strata  are  inte-
grated,  according  each  stratum  a  weight  com-
mensurate with its sample size. An assumption is
made that there is a uniform or “fixed” treatment
effect in all of the strata (or in all of the studies
included  in  the  meta-analysis).  Studies  are
thought to have generated different estimates of
this  fixed  effect  solely  because  of  random  sam-
pling variation. The results of a meta-analysis by
the fixed-effects method are thus valid only if this
is a reasonable assumption to make.

This assumption cannot be reasonably made if
the combined studies differ with respect to impor-
tant design attributes (Table 9.2). If current medi-
cal  knowledge  suggests  that  the  effect  of  an
intervention  should  differ  in  various  situations
(such as those shown in Table 9.2), it is probably
unreasonable to assume that the exposure or treat-
ment under study has had the same effect in all the
reported studies. A meta-analysis by the random-
effects method [66] is advocated for these circum-
stances.  The  assumption  of  a  random-effects
analysis is that the effect of the exposure or treat-
ment varies from study to study, being randomly
positioned about some central value. This value is
the summary or “average” effect of the exposure
or treatment across the combined studies.

In a fixed-effects analysis, only within-studies
variation  influences  the  uncertainty  of  the  sum-
mary effect across the combined studies that are
calculated  by  the  overview.  “Within-studies”
variation refers simply to random sampling vari-
ation  from  study  to  study,  that  is,  the  variation
that  results  each  time  that  a  study  sample  is

158

E.C. Vamvakas

drawn, at random, from the target population of
all  eligible  patients.  This  sampling  variation  is
inversely proportional to the sample size of each
report. No between-studies variation is presumed
to  exist  when  a  fixed-effects  analysis  is  con-
ducted,  as  all  included  studies  are  assumed  to
measure the same (fixed) effect of the exposure
or  treatment.  Therefore,  the  differences  among
the studies in the magnitude and direction of the
reported  treatment  effect  do  not  influence  the
uncertainty  that  surrounds  the  summary  effect
calculated by the meta-analysis.

On the contrary, in a random-effects analysis,
both within-studies and between-studies variation
influence  the  uncertainty  surrounding  the  calcu-
lated summary effect. The uncertainty associated
with the measured estimate of the effect increases
if  the  sample  size  of  the  combined  studies  is
small, because small sample sizes result in large
within-studies variation. The uncertainty increases
further if the combined studies differ in important
design  characteristics  (such  as  those  shown  in
Table  9.2),  because  such  differences  among  the
reports imply that the individual studies should be
expected to measure different exposure or treat-
ment effects. The more the combined studies dif-
fer in important design characteristics, the greater
the  expected  differences  in  the  estimates  of  the
effect(s)  calculated  by  these  studies;  therefore,
the  greater  also  the  between-studies  variation,
and  the  greater  the  uncertainty  surrounding  the
summary effect calculated by the meta-analysis.
The 95% confidence interval (CI) of the sum-
mary effect measures this uncertainty which sur-
rounds the “average” treatment effect calculated
by  the  meta-analysis.  The  95%  CI  calculated
from a fixed-effects analysis is an estimate of the
within-studies variation in the combined studies.
In contrast, the 95% CI calculated from a random-
effects analysis is an estimate of both the within-
and between-studies variation, thus being, wider
than the 95% CI calculated from a fixed-effects
analysis. The difference between these two 95%
CIs is proportional to the between-studies varia-
tion, or the magnitude of the differences in study-
design attributes as well as in reported results.

Small  studies  have  more  of  an  impact  on  the
calculated  “average”  effect  when  a  random-  (as

opposed to fixed-) effects analysis is undertaken.
When the literature eligible for analysis consists
of  one  (or  a  few)  large  investigations  and  many
small studies, a single large report may dominate
the findings of an overview conducted by a fixed-
effects method. This analysis would take only the
within-studies  variation  into  account,  and  would
thus  weigh  studies  with  large  sample  sizes  (and
small  within-studies  variation)  more  favorably
than small reports. Therefore, the conclusions of
the overview could, for the most part, reflect the
results  of  these  few  large  studies,  as  opposed  to
the composite evidence from all completed stud-
ies. In contrast, the findings of a meta-analysis by
the random-effects method would reflect the com-
bination of within- and between-studies variation.
The more the combined studies differ in important
design attributes, the more important the between-
studies  variation  becomes,  as  compared  to  the
within-studies variation. As a result, the more the
influence of a single large study diminishes, and
the  more  the  stated  conclusions  of  the  overview
accomplish  the  purpose  of  the  meta-analysis,
which is to examine the effect of an exposure or
treatment in many, different situations.

If  there  are  no  important  design  differences
between  the  combined  studies,  random-  and
fixed-effects analyses will produce similar results.
On  the  contrary,  if  there  are  substantive  differ-
ences  among  the  studies,  the  two  methods  of
analysis  will  produce  disparate  results  [64].
Fixed- and random-effects analyses are based on
different  conceptions  of  the  proper  role,  scope,
and meaning of meta-analysis. Despite the differ-
ences in assumptions delineated above, there are
strong opinions about the appropriateness of both
lines of analysis [65–68].

Assessment of the Combinability
of the Reports Included
in the Meta-Analysis

Results  of  separate  studies  should  be  combined
by the methods of meta-analysis only when the
estimates  of  effect  size  that  they  have  reported
are sufficiently close to one another. This prereq-
uisite  is  referred  to  as  homogeneity  of  effects.

9  Meta-Analysis: A Statistical Method to Integrate Information Provided by Different Studies

159

The opposite situation – that is, when sizable dif-
ferences  exist  between  investigations  in  study
attributes and the reported estimates of effect – is
known as heterogeneity of effects. A discussion
of  the  homogeneity  (or  heterogeneity)  of  the
studies must precede any integration of studies in
a meta-analysis. Study results should not be inte-
grated in the presence of unexplained heteroge-
neity,  although  this  principle  is  very  often  not
adhered to in published meta-analyses. Statistical
reviewers  of  the  U.S.  Food  and  Drug  Adminis-
tration  have  denigrated  as  mere  computational
exercises  all  overviews  that  had  combined  het-
erogeneous reports [69–71].

Homogeneity  is  assessed  statistically  by  the
Q test statistic, which examines whether the vari-
ation in the findings of the studies is sufficiently
modest to have arisen by chance [1–7]. If p < 0.05
for the Q test statistic, there is a smaller than 5%
probability that the variation in the results of the
available studies might have arisen by chance. In
this  situation,  the  hypothesis  of  homogeneity  is
rejected, and the results of the studies should not
be combined. Such statistical heterogeneity gen-
erally  reflects  the  medical  heterogeneity  of  the
studies.  For  example,  in  Fig.  9.1,  the  effect  of
WBC-containing ABT varies from a 20% reduc-
tion to a 7.3-fold increase in the risk of infection.
This  extreme  statistical  heterogeneity  (p < 0.001
for  the  Q  test  statistic,  that  is,  a  smaller  than
1/1,000 probability that the variation in the find-
ings of the studies [21–32] might have arisen by
chance) reflects the considerable medical hetero-
geneity of the studies (Table 9.2).

The  Q  test  statistic  is  a  chi-square  test  with
n − 1  degrees  of  freedom  (where  n = number  of
studies  included  in  the  overview).  Because  the
Q test statistic depends on the number of studies –
being  less  sensitive  to  heterogeneity  when  the
number of studies available for meta-analysis is
small – an alternative test (I2) has been proposed
to  assess  the  extent  of  heterogeneity  among
studies  [72,  73].  I2  does  not  inherently  depend
on the number of studies included in the analy-
sis, and it is expressed as a percentage (i.e., the
percentage  of  total  variation  across  studies
attributed  to  heterogeneity).  For  this  reason,  it
has intuitive meaning to the reader, and it can be

directly compared between meta-analyses. Low
 heterogeneity corresponds to I2 values of <25%,
while high heterogeneity is reflected in I2 values
of >75% [72]. However, Higgins et al. [72] did
not  indicate  any  specific  cutoff  value  (e.g.,
>75%) past which it is inappropriate to integrate
studies  owing  to  heterogeneity.  Instead,  they
suggested that quantification of heterogeneity is
only one component of a wider investigation of
variability across studies; and that the interpre-
tation  of  a  given  degree  of  heterogeneity  will
differ  according  to  whether  the  estimates  of
effect  from  the  various  studies  show  the  same
direction of effect.

When  studies  are  heterogeneous,  instead  of
integrating results, meta-analysts should present
an analysis of the possible reasons for variation
in  the  findings  of  the  available  studies  [74–78].
The simplest method for explaining heterogene-
ity is a stratification of the eligible studies based
on  design,  quality,  and/or  characteristics  of
enrolled  patients  and/or  administered  interven-
tions or exposures. Providing that the hypothesis
of homogeneity is not rejected within each stra-
tum following such stratification of the available
studies, the calculated stratum-specific “average”
treatment effects may help explain the disagree-
ments among the available reports.

In  the  case  of  the  RCTs  of  WBC-containing
ABT and infection (Fig. 9.1 and Table 9.2), meta-
analyses  of  clinically-homogeneous  subsets  of
RCTs by a random-effects method [79] produced
results  diametrically  opposed  to  the  findings
expected from the theory that attributes the effect
of non-WBC-reduced ABT to WBC-derived sol-
uble mediators [40–43]: there was a reduction in
the risk of postoperative infection in association
with  poststorage  (as  opposed  to  prestorage)
WBC  reduction  [18].  More  specifically,  across
nine relatively homogeneous RCTs [21–29] that
transfused allogeneic RBCs filtered before stor-
age to the WBC-reduced arm, no increase in the
risk  of  postoperative  infection  was  detected  in
association with non-WBC-reduced ABT (sum-
mary  OR = 1.06,  95%  CI,  0.91–1.24;  p > 0.05  –
middle panel in Fig. 9.3). If the ABT effect were
mediated  by  WBC-derived  soluble  mediators,
prestorage  filtration  should  have  abrogated  an

160

E.C. Vamvakas

nents  given  to  the  WBC-reduced  arm  of  each
study before the WBCs could release any signifi-
cant  amounts  of  mediators  into  the  supernatant
fluid of the components.

In  contrast,  across  four  RCTs  [28,  30–32]
that transfused RBCs filtered after storage to the
WBC-reduced arm, there was a more than two-
fold increase in the risk of infection in associa-
tion with non-WBC-reduced ABT (middle panel
in Fig. 9.3). If the ABT effect were mediated by
WBC-derived soluble mediators, poststorage fil-
tration  should  not  have  abrogated  an  increased
infection risk associated with non-WBC-reduced
ABT,  because  it  would  not  have  removed  such
mediators  from  the  supernatant  fluid  of  the
stored RBCs given to the WBC-reduced arm of
the  studies.  Yet,  this  was  the  only  clinically-
homogeneous  subset  of  studies  in  which  an
adverse  effect  of  WBC-containing  ABT  was
detected (Fig. 9.3).

Investigation of the sources of variation in the
results  of  RCTs  of  WBC-containing  ABT  and
all-cause  mortality  did  generate  a  clinically-
meaningful result, however (Fig. 9.4). Across the
(also statistically-homogeneous) cardiac-surgery
studies, there was a 72% increase in mortality in
association  with  non-WBC-reduced  (compared
with WBC-reduced) ABT. This result conforms
to  what  would  have  been  expected  from  the
immunomodulation theory [44] (that there would
be more of an immunomodulatory ABT in car-
diac  surgery  where  the  pro-inflammatory  effect
of  the  extracorporeal  circuit  acts  as  a  cofactor
than in other settings), and it is also calculated in
adherence  to  the  rule  of  integrating  only  medi-
cally- and statistically-homogeneous studies.

Regression  techniques  offer  a  more  elegant
method for explaining heterogeneity among stud-
ies  [80].  If  ten  or  more  original  reports  on  the
effect of WBC-containing ABT on increasing the
risk  of  bacterial  infection  or  all-cause  mortality
were available per explanatory variable included
in the model, the variation in the results of those
RCTs might be explained by the following regres-
sion model:

investigating

Fig. 9.3  Possible  sources  of  variation  in  the  findings  of
RCTs
the  association  between  WBC-
containing  ABT  and  bacterial  infection.  Stratified  meta-
analyses  are  presented  of  studies  that  administered  the
same  RBC  product  to  their  treatment  or  control  arm  or
were conducted in the same clinical setting. The summary
ORs  calculated  across  the  studies  that  transfused  buffy-
coat-reduced allogeneic RBCs to the treatment arm (seven
studies; see Table 9.2) or were conducted in gastrointesti-
nal surgery (five studies) are shown solely for the purpose
of illustration, because these study subgroups were hetero-
geneous (p < 0.01 and <0.001, respectively, for the Q test
statistic),  precluding  a  medically-meaningful  integration
of their findings. Only the subgroup analysis of the studies
that administered poststorage-filtered allogeneic RBCs or
whole  blood  to  the  control  arm  produced  a  statistically
significant  (p < 0.05)  ABT  effect  (summary  OR = 2.25;
95% CI, 1.12–4.25). Thus, the purported deleterious effect
of WBC-containing ABT appeared to be prevented by the
transfusion of WBC-reduced RBCs filtered after – but not
before – storage. The subgroup of four studies [28, 30–32]
transfusing poststorage-filtered allogeneic RBCs [28, 30,
31] or whole blood [32] to the control arm was the small-
est  (n = 1,616)  of  all  subgroups  shown  in  the  figure  and
consisted  of  early  studies  published  before  1999
(Table 9.2). Transfusion of poststorage-filtered allogeneic
RBCs or whole blood is now rarely (if ever) used in the
US. BCR buffy-coat-reduced; WB whole blood

increased  infection  risk  associated  with  non-
WBC-reduced  ABT,  because  it  would  have
removed the allogeneic WBCs from the compo-

ln (odds of infection or mortali

ty

)

=

a

+

b
1

X
1

+

b

2

X

2

+

b

3

X

3

+

b

T

T

,

9  Meta-Analysis: A Statistical Method to Integrate Information Provided by Different Studies

161

b1, b2, b3 = partial regression coefficients for the
variables thought to represent possible sources
of variation in the results of available studies
(i.e.,  partial  regression  coefficients  for  the
study  descriptors  [or  explanatory  variables]
X1, X2, X3 above)

bT = partial  regression  coefficient  for  being  ran-
domly  allocated  to  the  WBC-containing  arm
of an RCT, that is, corresponding to the expo-
sure or treatment under study

If some of the calculated estimates of the par-
tial regression coefficients for the predictor vari-
ables in this model (b1, b2, b3) differed from zero
to a statistically significant extent, the heteroge-
neity  among  studies  might  be  explained  by  the
corresponding  study  descriptor(s).  For  example,
if b1, b2, and b3 all differed significantly from zero,
the conclusion of the analysis would be that the
RBC product given to the treatment and control
arm,  as  well  as  the  cardiac-surgery  (vs.  noncar-
diac surgery) clinical setting, could be responsible
for  the  extreme  variation  in  the  results  of  the
reported studies. With the sources of heterogene-
ity  thus  explained,  the  effect  of  random  assign-
ment  to  the  receipt  of  non-WBC-reduced  (vs.
WBC-reduced) ABT (bT) could then be  calculated,
as well as tested for statistical significance.

The Medical Interpretation
of Overviews

In overviews addressing medical issues, research
questions must be stated with no less thorough a
biologic discussion than would appear in a tradi-
tional review, and the findings must be discussed
in  the  context  of  a  review  of  pathophysiologic
principles and results of basic laboratory research
and individual RCTs [78]. Most importantly, the
relevance of the findings to patient care must be
explained  to  the  reader.  In  addition,  readers  of
overviews must be reminded that meta-analyses
use  historical  material  from  studies  published
over a considerable period, because this histori-
cal  nature  of  the  material  may  influence  the
applicability  of  the  findings  to  contemporary
clinical practice.

investigating

Fig. 9.4  Possible  sources  of  variation  in  the  findings  of
RCTs
the  association  between  WBC-
containing ABT and short-term (up to 3-month posttrans-
fusion),  all-cause  mortality.  Stratified  meta-analyses  are
presented  of  studies  that  administered  the  same  RBC
product to their treatment arm or control arm or were con-
ducted in the same clinical setting. The summary ORs cal-
culated across the studies that transfused poststorage-filtered
allogeneic RBCs (two studies [28, 31]) or were conducted
in gastrointestinal surgery (two studies [26, 31]) are shown
solely for the purpose of illustration, because these study
subgroups were heterogeneous (p = 0.02 for the Q test sta-
tistic in both cases). Only the subgroup analysis of studies
conducted in cardiac surgery [21, 23, 25, 28, 29] produced
a statistically significant (p < 0.05) ABT effect (summary
OR = 1.72; 95% CI, 1.05–2.81). Across the six remaining
studies conducted at other settings [24, 26, 27, 31, 33, 34],
the summary OR was 0.99 (95% CI, 0.73–1.33). Both the
cardiac-surgery  and  the  noncardiac-surgery  studies  were
homogeneous (p > 0.10 and p = 0.20, respectively, for the
Q test statistic). Cardiac-surgery studies enrolled a total of
2,990 patients; noncardiac-surgery studies a total of 5,045.
BCR buffy-coat-reduced

where
ln = natural logarithm
a = intercept (i.e., a constant)
X1 = RBC component given to the treatment arm
(nonbuffy-coat-reduced or buffy-coat-reduced
RBCs or whole blood)

X2 = RBC  component  given  to  the  control  arm
(WBC-reduced RBCs or whole blood filtered
before or after storage)

X3 = clinical setting (cardiac surgery, gastrointes-

tinal surgery, or other)

162

E.C. Vamvakas

For example, the only adverse effect detected
with WBC-containing ABT vis-à-vis the develop-
ment  of  bacterial  infection  derived  from  RBC
components (middle panel in Fig. 9.3) no  longer
used in the US. Regardless of any methodologic
reasons that may have produced this finding across
four studies [28, 30–32], the only analysis that is
clinically relevant today is the comparison between
prestorage-filtered WBC-reduced and non-WBC-
reduced  RBCs.  In  this  latter  comparison,  no
adverse effect of WBC-containing ABT on bacte-
rial infection was detected.

The  finding  that  WBC-containing  ABT  may
be related to increased all-cause mortality in car-
diac-surgery (lower panel in Fig. 9.4) is most rel-
evant  today,  however,  because  not  all  blood
transfusion services in the US administer WBC-
reduced  components  to  patients  undergoing
open-heart surgery.

The  value  of  meta-analysis  in  combining
patient  populations  enrolled  in  separate  studies
for the purpose of documenting the existence of
an exposure or treatment effect is not universally
accepted  [80–82].  The  reason  is  that  meta-
analyses and large RCTs disagree 10–35% of the
time, that is, more often than would be expected
by chance [83–86]. This is probably because the
findings  of  meta-analyses  are  susceptible  to  the
effects  of  selection  and  observation  bias,  in  a
manner similar to the results of traditional obser-
vational original reports.

An  observational  study  conducted  at  a  single
institution and investigating the effect of an expo-
sure  or  treatment  on  a  disease  must  enroll  all
patients who are sequentially admitted to that hos-
pital  or  service  with  a  specific  diagnosis.  If  sub-
jects are missed or excluded, selection bias could
result.  Similarly,  the  validity  of  a  meta-analysis
depends on the complete sampling of all the stud-
ies performed on a particular topic. Validity can be
preserved  if  a  representative  sample  is  obtained,
but any incomplete sample is a potentially biased
one [87]. Unfortunately, meta-analysts may not be
able to locate all published studies, because com-
puterized data bases do not cover all periodicals,
search  algorithms  often  fail  to  identify  relevant
articles,  and  the  indexing  of  studies  is  imperfect
[88]. Even if the literature is optimally searched,
studies  published  as  government  reports,  book

chapters,  dissertations,  conference  proceedings,
etc.,  may  not  be  captured,  while  unpublished
 studies will not be identified. Published trials differ
systematically from unpublished ones, in that they
are more likely to have a larger sample, and to have
generated statistically significant results [89]. The
systematic exclusion of small and negative studies
from a meta-analysis that conditions eligibility on
achievement of publication status is known as pub-
lication bias [90].

There is ample evidence of publication bias in
the medical literature. Easterbrook et al. [91] doc-
umented a 3.8-fold increase in the odds of publi-
cation (95% CI, 1.5–9.8) for observational studies
reporting  statistically  significant  findings,  as
compared to studies with null results. Multivariate
analysis showed that the better odds of publica-
tion could not be explained by the quality of study
design. On the contrary, there was a trend towards
a greater number of statistically significant results
with poorer quality studies [91].

Selection  bias  can  arise  not  only  during
retrieval  of  studies  from  the  literature,  but  also
during assessment of the eligibility of the retrieved
reports. In evaluating the quality of investigations,
analysts may be influenced by knowledge of the
study results or journal of publication. They may
even be inclined to modify eligibility criteria, so
as to include in the overview reports from presti-
gious  journals.  According  to  Felson  [87],  selec-
tion  bias  is  the  principal  reason  for  discrepant
results in meta-analyses. Different teams of ana-
lysts may base their conclusions on alternate sets
of  original  reports,  generating  either  statistically
significant or null (i.e., statistically insignificant)
findings. This was also the explanation for the dis-
crepant  results  between  the  meta-analysis  [18]
whose results are depicted in Figs. 9.3 and 9.4 and
the earlier overviews [35, 36] that had included a
smaller number of RCTs whose findings had been
made available through 2002 [92].

Meta-Analyses of Studies
of Diagnostic-Test Accuracy

Development  of  new  fields  often  requires  the
development of new methods [93]. The cardinal
difference  between  RCTs  and  studies  of  the

9  Meta-Analysis: A Statistical Method to Integrate Information Provided by Different Studies

163

Table 9.3  Diagnostic accuracy of a laboratory test

Positive test results
Negative test results
Totals

Disease status
Present
True-positives (TP)
False-negatives (FN)
TP + FN

Accuracy

=

Number of correct test results
Number of people tested

Sensitivity

=

Number of true - positive test results
Number of people with disease tested

=

Specificity

=

Number of true - negative test results
Number of people without disease tested

TP
TP FN
+
TN
TN FP
+

=

Absent
False-positives (FP)
True-negatives (TN)
FP + TN

Totals
TP + FP
FN + TN
All individuals tested

Positive predictive value

=

Negative predictive value

=

Number of true - positive test results
Number of all positive test results
Number of true - negative test results
Number of all negative test results

=

TP
TP FP
+
TN
TN FN
+

=

 accuracy  of  diagnostic  tests  is  that  all  patients
enrolled  in  RCTs  have  the  disease  of  interest;
whereas  studies  of  diagnostic-test  accuracy
include a mixed population of subjects with and
without  disease.  Moreover,  RCTs  measure  one
quantity, that is, the effect of a therapeutic inter-
vention in treated patients vs. controls; whereas
studies  of  diagnostic-test  accuracy  measure  two
quantities,  that  is,  the  sensitivity  and  specificity
of a test. These two quantities are interdependent,
and they also depend on the cutoff point used in
each study for judging the results of the test to be
positive; sensitivity can be increased by decreas-
ing the cutoff point and decreasing the specificity,
or vice versa. Sensitivity and specificity are thus
negatively correlated.

In combining the findings (Table 9.3) of stud-
ies of diagnostic-test accuracy, an assumption is
made that the published estimates of the sensitiv-
ity  and  specificity  of  a  test  are  likely  to  vary
among  studies,  because  of  differences  between
the studies in the cutoff point used for judging the
results  of  the  test  to  be  positive.  Accordingly,
methods  for  integrating  the  findings  of  these
reports must address the interdependence between
sensitivity  and  specificity,  and  the  influence  of
the cutoff point used in each study on the corre-
sponding estimate of accuracy. To meet the for-
mer objective, the estimates of sensitivity are not
combined independently of the estimates of spec-
ificity,  but  the  two  components  of  accuracy

Table 9.4  Interrelationships between the sensitivity and
specificity of a diagnostic test

True-positive proportion (TPP) = Sensitivity
True-negative proportion (TNP) = Specificity
False-positive proportion (FPP) = 1 – Specificity
False-negative proportion (FNP) = 1 – Sensitivity
TPP + FNP = (Sensitivity) + (1 – Sensitivity) = 1
TNP + FPP = (Specificity) + (1 – Specificity) = 1
Sensitivity = P (T+/D+)a = TPP
Specificity = P (T−/D−)b = TNP
1 – Specificity = P (T+/D−)c = FPP
1 – Sensitivity = P (T−/D+)d = FNP

a Probability of the test’s being positive given the presence
of disease
b Probability of the test’s being negative given the absence
of disease
c Probability of the test’s being positive given the absence
of disease
d Probability  of  the  test’s  being  negative  given  the  pres-
ence of disease

obtained from each study are considered jointly.
To remove the effect that the variation of the cut-
off point has on accuracy, the diagnostic accuracy
of a laboratory test across the available studies is
summarized in the form of a summary receiver-
operating  characteristic  (ROC)  curve  that  plots
the  “average”  true-positive  (TP)  proportion  cal-
culated  for  the  test  against  the  “average”  false-
positive  (FP)  proportion  (Table  9.4)  [12–17].
If this initial analysis shows that the accuracy of
the laboratory test is constant within a range of

164

E.C. Vamvakas

clinically relevant cutoffs, a point estimate of the
summary  accuracy  of  the  test  –  the  summary
OR – is also calculated, in lieu of (or in addition
to)  the  summary  OR  curve.  Statistical  methods
for  integrating  data  on  laboratory  test  accuracy
have both been developed and are under develop-
ment [94, 95].

To  control  for  the  interdependence  between
the estimates of sensitivity and specificity derived
from each primary study, meta-analyses combine
these  two  quantities  into  an  OR,  and  they  then
use the OR as the overall measure of the accuracy
of  the  laboratory  test  as  calculated  from  each
study (Table 9.5). The OR is defined as the odds
of a TP test result, divided by the odds of a FP
test result, that is, the odds of obtaining a positive
test result as calculated from a person with dis-
ease, divided by the odds of obtaining a positive
test result in a person without disease. The natu-

ral logarithm of the odds of a TP test result (i.e.,
ln[TPP ÷ (1 − TPP)]) is designated as logit (TPP).
The  natural  logarithm  of  the  odds  of  a  FP  test
result  (i.e.,  ln[FPP ÷ (1 − FPP)])  is  designated  as
logit  (FPP).  The  natural  logarithm  of  the  OR,
designated  as  D,  equals  the  difference  between
these logits (i.e., D = logit [TPP] – logit [FPP]). D
is a logodds ratio that measures how well the test
discriminates between subjects with and without
the  disease.  S  is  a  measure  of  the  threshold  for
classifying  a  test  result  to  be  positive,  and  it
equals  the  sum  of  the  logits  (i.e.,  S = logit
[TPP] + logit  [FPP]);  it  is  large  and  positive  if
both the TPP and the FPP are large, and it is nega-
tive when they are small [95, 96].

The calculations involved in estimating the posi-
tion of a summary ROC curve across studies included
in  a  meta-analysis  are  summarized  in  Table  9.6.
From each primary study, the meta-analysts extract

Table 9.5  Overall diagnostic accuracy of a laboratory test, as determined by the odds ratio

Odds ratio

=

odds of a true - positive test result
odds of a false - positive test result

or

Odds ratio

=

odds of obtaining a positive test result in a person with disease
odds of obtaining a positive test result in a person without disease

or

Odds ratio

=

(true - positive proportion) / 1 (true - positive proportion)
(false - positive proportion) / 1 (false - positive proportion)

-
-

or

Odds ratio

=

(sensitivity) / 1 (sensitivity)
1 (specificity) / (specificity)

-

-

Sensitivity (%)
99.9
99.8
99.5
99.0
98.0
95.0
90.0
80.0
70.0
60.0
50.0
a Measure of the overall accuracy of a laboratory test used in the meta-analyses of studies of the diagnostic accuracy of
laboratory tests

Natural logarithm of odds ratioa
13.8
12.4
10.6
9.2
7.8
5.9
4.4
2.8
1.7
0.8
0.0

Specificity (%)
99.9
99.8
99.5
99.0
98.0
95.0
90.0
80.0
70.0
60.0
50.0

Odds ratio
999,000.00
249,500.00
39,800.00
9,802.00
2,402.00
361.00
81.00
16.00
5.44
2.25
1.00

9  Meta-Analysis: A Statistical Method to Integrate Information Provided by Different Studies

165

Table 9.6  Calculation of a summary receiver-operating characteristic (ROC) curvea

A. Calculate the quantities Di and Si for each study included in the analysis
1. Extract 2 × 2 contingency table counts from the report of each study:

Test results
Positive
Negative

Disease status
Present
True-positives (TP)
False-negatives (FN)

Absent
False-positives (FP)
True-negatives (TN)

2. Calculate the TPP, FPP, and ORi for each study:

True - positive proportion (TPP)

=

False - positive proportion (FPP)

=

TP
TP FN
+

FP
FP TN
+

Odds ratio (OR )
i

=

TPP (1 TPP)
¸ -
FPP (1 FPP)
¸ -

3. Calculate the quantities Di and Si for each study:
logit (TPP)

logit (FPP)

D ln(OR )
i

-

=

=

i

S
i

=

logit (TPP)

+

logit (FPP)

where ln is the natural logarithm, logit (TPP) is the natural logarithm of the odds of a true-positive test result;
and logit (FPP) is the natural logarithm of the odds of a false-positive test result
B. Fit a simple linear regression model using the quantities Di and Si from each study

D = a + b S

C. Transform this model back to the conventional axes of TPP vs. FPP, and draw a summary ROC curve over the

range of the data

aSee Littenberg and Moses [95] and Moses et al. [96]

data  for  a  2 × 2  contingency  table  showing  the
reported  TP,  FP,  true-negative  (TN),  and  false-
negative (FN) results of the laboratory test under
evaluation.  They  then  compute  a  true-positive
proportion  (TPP = TP ÷ [TP + FN])  and  a  false-
positive  proportion  (FPP = FP ÷ [FP + TN])  for
each  contingency  table,  and  calculate  the  logit
(TPP) and the logit (FPP), as well as the differ-
ence  between  the  logits  (D)  and  the  sum  of  the
logits (S). Having summarized the data from each
primary study with these two quantities (e.g., Di
and Si for the ith study), they fit a simple linear
regression model using D as the dependent vari-
able and S as the predictor variable: [95, 96]

D

=

+
a b

S

.

By  employing  this  logarithmic  transforma-
tion, it is possible to use a line to represent a cur-
vilinear  relationship.  The  fitted  regression  line
can then be transformed back to the conventional

axes of an ROC curve (i.e., a plot of the TPP vs.
the  FPP),  and  depict  a  summary  ROC  curve
across the combined studies [95, 96].

The intercept of the model (a) is the estimated
logodds  ratio  when  the  accuracy  of  the  test
remains constant as the cutoff point varies from
study to study. The regression coefficient or slope
(b) provides an estimate of the extent to which the
logodds ratio depends on the cutoff used. When b
is  near  zero  (or,  at  least,  if  −0.5 < b < +0.5),  the
shape of the curve calculated by the transformed
model  approximates  that  of  a  traditional  ROC
curve. Also if b does not differ significantly from
zero, the accuracy of the test does not depend on
the particular cutoff point used in each study, and
the accuracy of the test across the combined stud-
ies can be summarized by the logodds ratio given
by the intercept a. The larger this intercept is, the
closer  the  curve  is  positioned  to  the  upper  left
corner in the ROC space, which indicates a greater
diagnostic accuracy for the test [95, 96].

166

E.C. Vamvakas

Obstacles to the Use of Meta-
Analysis for the Integration of
Findings of Studies of Diagnostic-
Test Accuracy

As discussed previously in the context of RCTs:
(1) low-quality studies should be either excluded
from a meta-analysis or weighed in proportion to
their quality; and (2) available studies should be
combined only if the variation in reported results
is sufficiently modest to be attributed to chance.
In the case of RCTs, the Q test statistic quantifies
the probability that the variation in the results of
the available studies is sufficiently modest to per-
mit  their  integration  by  the  methods  of  a  meta-
analysis.  The  meta-analytic  methods  are  still
being  refined,  however,  for  use  with  studies  of
diagnostic-test accuracy, and the lack of as fully
developed  methods  can  be  somewhat  of  an
impediment to the use of meta-analysis in pathol-
ogy. The calculations presented in Table 9.6 are
sometimes  deemed  to  be  too  complicated,  and
many investigators succumb to the temptation of
integrating estimates of sensitivity independently
of the corresponding estimates of specificity; as
well as estimates of specificity independently of
the  corresponding  estimates  of  sensitivity.
Although it is easy to do so with the meta-analysis
software made widely available for RCTs, such a
simplistic approach to the  analysis is valid only
when  the  meta-analysts  have   demonstrated  a
lack of dependence of the  diagnostic-test accu-
racy  on  the  cutoff  employed  in  each  retrieved
study.

The most important obstacle, however, to the
use  of  meta-analysis  for  integrating  results  of
studies of diagnostic-test accuracy are the subop-
timal technical or scientific merits of the studies
available  for  analysis.  Several  aspects  of  study
design and analysis [97–99] need thus be consid-
ered by analysts in judging the quality of studies
of  diagnostic-test  accuracy  (Table  9.7).  These
issues  have  been  discussed  in  detail  by  Sackett
et al. [100].

When the accuracy of an index laboratory test
is  investigated,  an  assumption  is  made  that

Table  9.7  Questions  to  be  considered  in  assessing  the
quality of studies of the diagnostic accuracy of laboratory
tests

Was the “gold standard” used definitive?
Was the “gold standard” used independent of the test
under evaluation?
Were all individuals included in the study – or a
random subset of individuals – tested by the “gold
standard?”
Did the individuals included in the study represent a
consecutive series or a randomly selected study
population?
Were any individuals withdrawn from the analysis
following their inclusion in the study, because of
equivocal test results or any other reason?
Did the performance of the test under evaluation and
the “gold standard” conform to the standard of
practice?
Was the uncertainty surrounding the calculated
estimates of sensitivity and specificity of the test
quantified?
Was the cutoff point used for interpreting the results of
the test as positive clinically appropriate, and/or was it
varied within a clinically relevant range?
Was the clinical setting in which the test was evaluated
adequately described?

the  employed  “gold  standard”  can  definitively
 discriminate  between
individuals  with  and
 without disease. If the available “gold standard”
is imperfect, there will be an error in the initial
estimation of the accuracy of the laboratory test
[101–103]. Available tests with established diag-
nostic  accuracy  rarely  meet  the  definition  of  a
“gold  standard”;  however,  those  evaluating  the
diagnostic accuracy of new laboratory tests must
strive to use the best available method for ascer-
taining  the  presence  of  disease  in  their  study
population.

Ideally,  all  enrolled  patients  should  undergo
complete  diagnostic  work-ups  without  knowl-
edge  of  the  results  of  the  laboratory  test  under
evaluation.  However,  if  the  “gold  standard”
requires an invasive procedure, it may be desir-
able  to  restrict  its  use  to  a  subset  of  the  study
population.  This  approach  is  acceptable  only  if
the  selection  of  patients  for  verification  by  the
“gold  standard”  is  random.  If  the  patients  who
undergo the invasive procedure are selected based
on abnormal results from other tests, or because

9  Meta-Analysis: A Statistical Method to Integrate Information Provided by Different Studies

167

they  have  risk  factors  for  disease,  etc.,  verifica-
tion bias is introduced which can seriously distort
the  results  of  the  study.  The  effects  of  this  sys-
tematic  error  on  the  diagnostic  accuracy  of  the
test  are  unpredictable,  and  they  cannot  be  cor-
rected statistically [103–105].

The  accuracy  of  a  laboratory  test  should  be
assessed  in  consecutive  patients,  or  patients
selected randomly for inclusion in the study, and
all  enrolled  patients  should  be  included  in  the
analysis. No withdrawals of patients can be per-
mitted  following  their  inclusion  in  the  study,
because of equivocal test results or any other rea-
son. The methods for carrying out the test under
evaluation  and  the  gold  standard  should  be
described in sufficient detail, and the cutoff point
used for judging either test to be positive should
be specified. Moreover, the uncertainty surround-
ing  the  calculated  estimates  of  sensitivity  and
specificity should be communicated to the reader,
by  reporting  95%  CIs  for  these  proportions.  To
remove the influence of an arbitrary cutoff point
on the reported estimates of sensitivity and speci-
ficity,  the  cutoff  for  the  test  should  be  varied
within  a  clinically  relevant  range,  and  the  diag-
nostic accuracy of the test should be reported in
the form of an ROC curve. Alternatively, if a sin-
gle cutoff point is used, the clinical appropriate-
ness  of  the  chosen  cutoff  point  should  be
discussed. The clinical setting in which the test is
evaluated  should  be  stated  explicitly,  and  a
description of the characteristics of the enrolled
patients (e.g., age, gender, symptoms, results of
other diagnostic tests, etc.) should be provided.

Once  all  studies  of  diagnostic-test  accuracy
conform to the STARD guidelines [106], perhaps
the most important obstacle to the use of meta-
analysis  for  integrating  the  results  of  studies  of
diagnostic-test  accuracy  will  have  been  over-
come.  Further  guidelines  (for  both  the  conduct
and the reporting of studies) are provided in the
STROBE  statement  [107],  which  applies  to
observational studies in general.

When the available studies of the accuracy of
a laboratory test differ in important study charac-
teristics (such as the employed cutoff point or the
disease  prevalence  in  the  included  population),

the  diagnostic  accuracy  of  the  test  should  be
assumed to differ from study to study, according
to  the  varying  characteristics  of  each  study.
Therefore,  in  such  a  situation,  a  random-effects
method should be used to integrate the findings
of  the  available  studies.  Random-effects  meth-
ods  that  take  into  account  the  dependence  of
 sensitivity and specificity on the cutoff point used
in  each  study  continue  to  be  developed.  The
fixed-effects method delineated in Table 9.6 for
the calculation of a summary ROC curve [95, 96]
can be modified to conform to the assumptions of
a random-effects analysis.

Another impediment to the use of meta-analysis
in pathology is that individuals included in stud-
ies of diagnostic-test accuracy are not allocated
randomly by the investigators to have (or to not
have) the disease of interest. The validity of sta-
tistical tests is guaranteed only if the allocation
of  subjects  to  comparison  arms  is  random.  In
RCTs, randomization makes it possible to ascribe
a probability distribution to the difference in out-
come  between  two  arms  that  receive  equally
effective  treatments  under  the  null  hypothesis.
Knowledge  of  this  distribution  is  a  prerequisite
for assigning significance levels to any observed
differences. If the allocation of subjects to groups
is not random, the validity of tests of significance
depends  on  additional  assumptions  about  the
comparability of the groups and the appropriate-
ness  of  the  statistical  models.  The  veracity  of
these latter assumptions is difficult to establish.

Finally,  in  the  case  of  RCTs,  there  is  ample
evidence of publication bias in the medical litera-
ture,  and  meta-analyses  of  RCTs  consider  the
possible effect(s) of this bias on the stated con-
clusions. The effect of publication bias on studies
of  diagnostic-test  accuracy
is  not  as  well
researched,  but  many  investigators  suspect  that
the published studies of diagnostic-test accuracy
are a biased subset that tends to overestimate the
diagnostic accuracy of the test under evaluation.
Since the studies of diagnostic-test accuracy are –
in their vast majority – observational, it is likely
that  publication  bias  may  have  an  even  larger
impact on meta-analyses of studies of diagnostic-
test accuracy than on meta-analyses of RCTs.

168

E.C. Vamvakas

Conclusions

References

the  applicability  of

Meta-analyses are a supplement to RCTs, which
remain the gold standard for evaluating the effi-
cacy of therapeutic interventions, but need to be
supplemented  by  meta-analyses  in  order  to
broaden
the  findings.
Furthermore, meta-analyses can be an important
research tool for the systematic evaluation of the
quality  of  published  studies  and  for  the  disci-
plined investigation of reasons for disagreements
among  reports.  Overviews  can  identify  errors
and shortcomings in completed studies and may
be able to explain why trial results differ. In the
past,  narrative  reviews  of  the  literature  served
these  functions  in  a  less  formal  manner.  Meta-
analyses  are  much  better  suited  for  these  pur-
poses, because of their objective and quantitative
nature.

Since  meta-analysis  is  a  technical  statistical
method, many clinicians find themselves unable
to  appreciate  its  nuances  and  limitations  in  the
same way that they can appreciate those of a tra-
ditional original report. However, readers of the
medical  literature  need  to  be  familiar  with  the
definitions  and  assumptions  of  fixed-  vs.  ran-
dom-effects analyses, as well as with the mean-
ing of the results of the Q test statistic or other
tests  for  homogeneity  [108].  This  is  because
health-policy guidelines – as well as recommen-
dations  for  patient  management  –  are  already
being  based,  and  are  likely  to  be  increasingly
based  in  the  future,  on  input  from  statistical
overviews.

Meta-analysis has several potential uses when
the  diagnostic  accuracy  of  laboratory  tests  is
investigated, but it is a rather new field of study
that needs to refine its tools and establish its cred-
ibility.  Validated  instruments  for  assessing  the
quality of studies, statistical tests for judging the
combinability  of  studies,  and  random-effects
methods for integrating the findings of studies are
well-developed  for  RCTs,  and  continue  to  be
developed for studies of diagnostic-test accuracy.
In  the  future,  meta-analysis  should  become  as
powerful  a  tool  for  technology  assessment  in
pathology as it is for clinical medicine.

  1.  Jenicek M. Meta-analysis in medicine: where we are
and  where  we  want  to  go.  J  Clin  Epidemiol.  1989;
42:35–44.

  2.  L’Abbé KA, Detsky AS, O’Rourke K. Meta-analysis
in  clinical  research.  Ann  Intern  Med.  1987;107:
224–33.

  3. Cooper  H,  Hedges  LV,  editors.  The  handbook  of
research synthesis. New York: Russell Sage Foundation;
1994.

  4.  Cook TD, Cooper H, Gordray DS, et al. Meta-analysis
for explanation: a casebook. New York: Russell Sage
Foundation; 1992.

  5.  Glasziou  P,  Irwig  L,  Bain  C,  Colditz  G.  Systematic
reviews in health care: a practical guide. Cambridge:
Cambridge University Press; 2001.

  6.  Goodman SN. Have you ever met a meta-analysis you

didn’t like? Ann Intern Med. 1991;114:244–6.

  7.  O’Rourke  K,  Detsky  AS.  Meta-analysis  in  medical
research: strong encouragement for higher quality in
individual research efforts. J Clin Epidemiol. 1989;42:
1021–4.

  8.  Chalmers TC, Levin H, Sacks HS, Reitman D, Berrier J,
Nagalingam R. Meta-analysis of clinical trials as a sci-
entific discipline. I. Control of bias and comparison with
large co-operative trials. Stat Med. 1987; 6:315–25.
  9.  Peto R. Why do we need systematic overviews of ran-

domized trials? Stat Med. 1987;6:233–40.

 10. Yusuf  S.  Obtaining  medically  meaningful  answers
from  an  overview  of  randomized  clinical  trials.  Stat
Med. 1987;6:281–6.

 11. Vamvakas EC. Statistical associations and cause-and-
effect  relationships.  In:  Vamvakas  EC.  Evidence-
based  practice  of  transfusion  medicine.  Bethesda:
AABB Press; 2001. p. 21–64.

 12. Irwing L, Tosteson AN, Gatsonis C, et al. Guidelines
for  meta-analyses  evaluating  diagnostic  tests.  Ann
Intern Med. 1994;120:667–76.

 13. Irwig  L,  Macaskill  P,  Glasziore  P,  Fahey  M.  Meta-
analytic methods for diagnostic test accuracy. J Clin
Epidemiol. 1995;48:119–30.

 14. Vamvakas EC. Meta-analyses of studies of the diag-
nostic  accuracy  of  laboratory  tests:  a  review  of  the
concepts  and  methods.  Arch  Pathol  Lab  Med.
1998;122:675–86.

 15. Boissel  JP,  Cucherat  M.  The  meta-analysis  of  diag-

nostic test studies. Eur Radiol. 1998;8:484–7.

 16. Vamvakas E. Applications of meta-analysis in pathol-
ogy  practice.  Am  J  Clin  Pathol.  2001;116  Suppl
1:S47–64.

 17. Glasziou P, Irwing L, Bain C, Colditz G. Diagnostic
tests. In: Systematic reviews in health care: a practical
guide. Cambridge: Cambridge University Press; 2001.
p. 74–89.

 18. Vamvakas E. White-blood-cell containing allogeneic
blood  transfusion  and  postoperative  infection  or

9  Meta-Analysis: A Statistical Method to Integrate Information Provided by Different Studies

169

 mortality: an updated meta-analysis. Vox Sang. 2007;
92: 224–32.

 19. Blajchman MA, Bordin JO. Mechanisms of transfu-
immunosuppression.  Curr  Opin

sion-associated
Hematol. 1994;1:457–61.

 20. Vamvakas  E,  Blajchman  MA.  Transfusion-related
immunomodulation: an update. Blood Rev. 2007;21:
327–48.

 21. Bracey  AW,  Radovancevick  R,  Nussimeier  NA,
LaRocco  M,  Vaughn  WK,  Cooper  JR.  Leukocyte-
reduced blood in open-heart surgery patients: effects
on outcome. Transfusion. 2002;42(Suppl):5S.

 22. Houbiers JGA, Brand A, van de Watering LMG, et al.
Randomized controlled trial comparing transfusion of
leucocyte-depleted  or  buffy-coat-depleted  blood  in
surgery  for  colorectal  cancer.  Lancet.  1994;344:
573–8.

 23. Boshkov  LK,  Furnary  A,  Morris  C,  Chien  G,  van
Winkle  D,  Reik  R.  Prestorage  leukoreduction  of
red  cells  in  elective  cardiac  surgery:  results  of  a
 double-blind  randomized  controlled  trial.  Blood.
2004;104: 112a.

 24. van Hilten JA, van de Watering LMG, van Bockel JH,
et al. Effects of transfusion with red cells filtered to
remove  leukocytes:  randomized  controlled  trial  in
patients  undergoing  major  surgery.  BMJ.  2004;328:
1281–4.

 25. Wallis JP, Chapman CE, Orr KE, Clark SC, Forty JR.
Effect  of  WBC  reduction  of  transfused  RBCs  on
 postoperative  infection  rates  in  cardiac  surgery.
Transfusion. 2002;42:1127–34.

 26. Titlestad  IL,  Ebbesen  LS,  Ainsworth  AP,  Lillevang
ST,  Quist  N,  Georgsen  J.  Leukocyte-depletion  of
blood  components  does  not  significantly  reduce  the
risk of infectious complications: results of a double-
blind, randomized study. Int J Colorectal Dis. 2001;16:
147–53.

 27. Nathens  AB,  Nester  TA,  Rubenfeld  GD,  Nirula  R,
Gernsheimer TB. The effects of leukoreduced blood
transfusion on infection risk following injury: a ran-
domized controlled trial. Shock. 2006;26:342–7.
 28. van  de  Watering  LMG,  Hermans  J,  Houbiers  JGA,
et al. Beneficial effect of leukocyte depletion of trans-
fused  blood  on  post-operative  complications  in
patients  undergoing  cardiac  surgery:  a  randomized
clinical trial. Circulation. 1998;97:562–8.

 29.  Bilgin YM, van de Watering LMG, Eijsman L, et al.
Double-blind, randomized controlled trial on the effect
of leukocyte-depleted erythrocyte transfusions in car-
diac valve surgery. Circulation. 2004;109:2755–60.
 30. Tartter PI, Mohandas K, Azar P, Endres J, Kaplan J,
Spivack M. Randomized trial comparing packed red
cell  blood  transfusion  with  and  without  leukocyte
depletion  for  gastrointestinal  surgery.  Am  J  Surg.
1998;176:462–6.

 31.  Jensen  LS,  Kissmeyer-Nielsen  P,  Wolff  B,  Quist  N.
Randomized comparison of leukocyte-depleted versus
buffy-coat-poor  blood  transfusion  and  complications
after colorectal surgery. Lancet. 1996;348:841–5.

 32. Jensen  LS,  Andersen  AJ,  Christiansen  PM,  et  al.
Postoperative infection and natural killer cell function
following  blood  transfusion  in  patients  undergoing
elective  colorectal  surgery.  Br  J  Surg.  1992;79:
513–6.

 33. Dzik  WH,  Anderson  JK,  O’Neill  EM,  Assman  SF,
Kalish  LA,  Stowell  CP.  A  prospective,  randomized
clinical trial of universal WBC reduction. Transfusion.
2002;42:1114–22.

 34. Nielsen  HJ,  Hammer  J,  Kraup  AL,  et  al.  Prestorage
leukocyte  filtration  may  reduce  leukocyte-derived
bioactive substance accumulation in patients operated
for burn trauma. Burns. 1999;25:162–70.

 35. Fergusson  D,  Khanna  MP,  Tinmouth  A,  Hébert  PC.
Transfusion  of  leukoreduced  red  blood  cells  may
decrease postoperative infections: two meta-analyses
of  randomized  controlled  trials.  Can  J  Anaesth.
2004;51:417–24.

 36. Blumberg N, Zhao H, Wang H, Messing S, Heal JM,
Lyman GH. The intention-to-treat principle in clinical
trials and meta-analyses of leukoreduced blood trans-
fusions  in  surgical  patients.  Transfusion.  2007;47:
573–81.

 37. Blajchman  MA,  Bardossy  I,  Carmen  R,  Sastry  A,
Singal  DP.  Allogeneic  blood  transfusion-induced
enhancement  of  tumor  growth:  two  animal  models
showing amelioration by leukodepletion and passive
transfer using spleen cells. Blood. 1993;81:1880–2.
 38. Mincheff  MS,  Meryman  HT,  Kapoor  V,  Alsop  P,
Wotzel  M.  Blood  transfusion  and  immunomodula-
tion:  a  possible  mechanism.  Vox  Sang.  1993;65:
18–24.

 39. Kao  KJ.  Induction  of  humoral  immune  tolerance  to
major  histocompatibility  complex  antigens  by  trans-
leukocytes.  Blood.
irradiated
fusions  of  UV-B
1996;88:4375–82.

 40. Ghio M, Contini P, Mazzei C, Brenci S, Barbaris G,
Filaci G, et al. Soluble HLA Class I, HLA Class II,
and Fas ligand in blood components: a possible key to
explain the immunomodulatory effects of allogeneic
blood transfusion. Blood. 1999;93:1770–7.

 41. Bordin  JO,  Bardossy  L,  Blajchman  MA.  Growth
enhancement  of  established  tumors  by  allogeneic
blood  transfusion  in  experimental  animals  and  its
amelioration  by  leukodepletion:  the  importance  of
timing  of
leukodepletion.  Blood.  1994;84:
344–8.

the

 42. Nielsen HJ, Reimert CM, Pedersen AN, Brunner N,
Edvarsen L, Dybkjaer E, et al. Time-dependent spon-
taneous release of white cell- and platelet-derived bio-
active
stored  human  blood.
substances
Transfusion. 1996;36:960–5.

from

 43. Innerhofer  P,  Luz  G,  Spotl  L,  Hobish-Hagen  P,
Schobersberger  W,  Fischer  M,  et  al.  Immunologic
changes following transfusion of autologous or allo-
geneic  buffy-coated-poor  versus  leukocyte-depleted
blood  in  patients  undergoing  arthroplasty.  I.  Pro-
liferative T-cell responses and T-helper/T-suppressor
cell balance. Transfusion. 1999;39:1089–96.

170

E.C. Vamvakas

 44.  Fransen  E,  Maessen  J,  Denterner  M,  Senden  N,
Buurman W. Impact of blood transfusion on inflam-
matory  mediator  release  in  patients  undergoing  car-
diac surgery. Chest. 1999;116:1233–9.

 45. Diversity  and  heterogeneity  [monograph  on  the
Internet]. Oxford: The Cochrane Collaboration. 2002.
http://www.cochrane-net.org/openlearning/HTML/
mod13-5.htm.

 46.  Chalmers TC, Smith Jr H, Blackburn B, et al. A method
for  assessing  the  quality  of  a  randomized  controlled
trial. Control Clin Trials. 1981;2:31–49.

 47. Detsky  AS,  Naylor  CD,  O’Rourke  K,  McGeer  AJ,
L’Abbe KA. Incorporating variations in the quality of
individual randomized trials into meta-analysis. J Clin
Epidemiol. 1992;45:255–65.

 48. Zelen M. Guidelines for publishing papers on cancer
clinical trials: responsibilities of editors and authors. J
Clin Oncol. 1983;1:164–9.

 49. Moher D, Jadad AR, Nichol G, Penman M, Tugwell P,
Walsh  S.  Assessing  the  quality  of  randomized  con-
trolled trials: an annotated bibliography of scales and
checklists. Control Clin Trials. 1995;16:62–73.

 50. Moher D, Jadad AR, Tugwell P. Assessing the quality
of reports of randomized trials. Int J Technol Assess
Health Care. 1996;12:195–208.

 51. Lichtenstein MJ, Mulrow CD, Elwood PC. Guidelines
for  reading  case-control  studies.  J  Chronic  Dis.
1987;40:893–903.

 52. Feinstein  AR.  Twenty  scientific  principles  for  trohoc
research.  In:  Feinstein  AR,  editor.  Clinical  epidemiol-
ogy: The architecture of clinical research. Philadelphia:
Saunders; 1985. p. 543–7.

 53. Jadad AR, Moore RA, Carroll D, et al. Assessing the
quality  of  reports  of  randomized  trials:  is  blinding
necessary? Control Clin Trials. 1996;17:1–12.

 54. Moher D, Jones BA, Cook DJ, et al. Does quality of
reports of randomized trials affect estimates of inter-
vention  efficacy  reported  in  meta-analyses?  Lancet.
1998;352:609–13.

 55. Steinberg  KK,  Thacker  SB,  Smith  SJ,  et  al.  A  meta-
analysis of the effect of estrogen replacement therapy on
the risk of breast cancer. JAMA. 1991;265:1985–90.
 56. Berlin  JA,  Colditz  GA.  A  meta-analysis  of  physical
activity  in  the  prevention  of  coronary  heart  disease.
Am J Epidemiol. 1990;142:612–28.

 57. Mantel N, Haenszel W. Statistical aspects of the anal-
ysis  of  data  from  retrospective  studies  of  disease.
J Natl Cancer Inst. 1959;22:719–48.

 58. Yusuf S, Peto R, Lewis J, Collins R, Sleight P. Beta
blockade  during  and  after  myocardial  infarction:  an
overview  of  the  randomized  trials.  Prog  Cardiovasc
Dis. 1985;27:335–71.

 59. Klein  S,  Simes  J,  Blackburn  GL.  Total  parenteral
nutrition  and  cancer  clinical  trials.  Cancer.  1986;58:
1378–86.

 60. Slavin  RE.  Best-evidence  synthesis:  an  alternative
approach  to  traditional  and  meta-analytic  reviews.
Educ Res. 1986;15(9):5–11.

 61. Slavin RE. Best-evidence synthesis: why less is more.

Educ Res. 1987;16(5):15–6.

 62. Begg  C,  Cho  M,  Eastwood  S,  et  al.  Improving  the
quality  of  reporting  of  randomized  controlled  trials:
the CONSORT statement. JAMA. 1996;276:637–9.
 63. Mahoer D, Schulz KF, Altman DG. The CONSORT
statement:  revised  recommendations  for  improving
the  quality  of  reports  of  parallel-group  randomized
trials. J Am Podiatr Med Assoc. 2001;91:437–42.
 64. Berlin  JA,  Laird  NM,  Sacks  HS,  Chalmers  RC.  A
comparison  of  statistical  methods  for  combining
event  rates  from  clinical  trials.  Stat  Med.  1989;8:
141–51.

 65. Thompson  SG,  Pocock  SJ.  Can  meta-analysis  be

trusted? Lancet. 1991;338:1127–30.

 66. Demets  DL.  Methods  for  combining  randomized
 clinical  trials:  strengths  and  limitations.  Stat  Med.
1987;6:341–8.

 67. Meier P. Commentary on “Why do we need system-
atic  overviews  of  randomized  trials?”.  Stat  Med.
1987;6:329–31.

 68. Bailey KR. Inter-study differences: how should they
influence  the  interpretation  and  analysis  of  results?
Stat Med. 1987;6:351–8.

 69. Huque  MF.  Experiences  with  meta-analysis  in  FDA
submissions.  Proc  Biopharm  Sect  Am  Stat  Assoc.
1988;2:28–33.

 70. Dubey S. Regulatory considerations on meta-analysis
and  multicenter  trials.  Proc  Biopharm  Sect  Am  Stat
Assoc. 1988;2:18–27.

 71. Stein  RA.  Meta-analysis  from  one  FDA  reviewer’s
perspective.  Proc  Biopharm  Sect  Am  Stat  Assoc.
1988;2:34–8.

 72. Higgins  JPT,  Thompson  SG,  Deeks  JJ,  Altman  DG.
Measuring  inconsistency  in  meta-analyses.  BMJ.
2003;327:557–60.

 73. Altman DG, Bland MJ. Interaction revisited: the dif-
ference between two estimates. BMJ. 2003;326:219.
 74. Abramson  JH.  Meta-analysis:  a  review  of  pros  and

cons. Public Health Rev. 1990/91;18:1–47.

 75. Greenland S, Longnecker MP. Methods for trend esti-
mation  from  summarized  dose-response  data,  with
applications  to  meta-analysis.  Am  J  Epidemiol.
1992;135:1301–9.

 76. Greenland S, Salvan A. Bias in the one-step method
for pooling study results. Stat Med. 1990;9:247–52.
 77. Cook TD, Cooper H, Cordray DS, et al. Meta-analysis
for explanation: a casebook. New York: Russell Sage
Foundation; 1992.

 78. Ellenberg SS. Meta-analysis: the quantitative approach
to research review. Semin Oncol. 1988;15:472–81.
 79. DerSimonian R, Laird N. Meta-analysis in clinical tri-

als. Control Clin Trials. 1986;7:177–88.

 80. Greenland  S.  Quantitative  methods  in  the  review
of  epidemiologic  literature.  Epidemiol  Rev.  1987;
9:1–30.

 81. Bailar  III  JC.  The  promise  and  problems  of  meta-

analysis. N Engl J Med. 1997;337:559–61.

 82. Meta-analysis under scrutiny. Lancet. 1997;350:675.
 83. Villar J, Carrolli G, Belizan JM. Predictive ability of
meta-analyses of randomized controlled trials. Lancet.
1995;345:772–6.

9  Meta-Analysis: A Statistical Method to Integrate Information Provided by Different Studies

171

  84.  Cappelleri  JC,  Ioannidis  JPA,  Schmid  CH,  et  al.
Large trials versus meta-analyses of small trials: how
do their results compare? JAMA. 1996;276:1332–8.
  85.  LeLorier  J,  Grégoire  B,  Benhaddad  A,  et  al.
Discrepancies  between  meta-analyses  and  subse-
quent  large  randomized,  controlled  trials.  N  Engl
J Med. 1997;337:536–42.

  86.  Ioannidis JPA, Cappelleri JC, Lau J. Issues in com-
parisons  between  meta-analyses  and  large  trials.
JAMA. 1998;279:1089–93.

  87.  Felson  DT.  Bias  in  meta-analytic  research.  J  Clin

Epidemiol. 1992;45:885–92.

  88.  Haynes  RB,  McKibbon  KA,  Walker  CJ,  et  al.
Computer  searching  of  the  medical  literature:  an
evaluation of MEDLINE search systems. Ann Intern
Med. 1985;103:812–6.

  89.  Begg CB, Berlin JA. Publication bias and dissemina-
tion  of  clinical  research.  J  Natl  Cancer  Inst.
1989;81:107–14.

  90.  Dickersin K, Chan S, Chalmers TC, et al. Publication
bias and clinical trials. Control Clin Trials. 1987;8:
343–53.

  91.  Easterbrook  PJ,  Berlin  JA,  Copalan  R,  Matthews
DR.  Publication  bias  in  clinical  research.  Lancet.
1991;337:867–72.

  92.  Vamvakas EC. Why have meta-analyses of the ran-
domized controlled trials of the association between
non-white-blood-cell
reduced  allogeneic  blood
transfusion  and  postoperative  infection  produced
discordant results? Vox Sang. 2007;93:196–207.
  93.  Sackett DL. Discussion of the paper “Meta-analytic
methods  for  diagnostic  test  accuracy”  presented  at
the  Potsdam  International  Consultation  on  Meta-
analysis  (Potsdam,  Germany;  March  1994).  J  Clin
Epidemiol. 1995;48:131–2.

  94.  Midgette AS, Stukel TA, Littenberg B. A meta-an-
alytic method for summarizing diagnostic test perfor-
mances:  receiver-operating-characteristic  summary
point estimates. Med Decis Making. 1993;13: 253–7.
  95.  Littenberg  B,  Moses  LE.  Estimating  diagnostic
accuracy  from  multiple  conflicting  reports:  a  new
meta-analytic method. Med Decis Making. 1993;13:
313–21.

  96.  Moses  LE,  Shapiro  D,  Littenberg  B.  Combining
independent  studies  of  a  diagnostic  test  into  a

 summary ROC curve: data-analytic approaches and
some additional considerations. Stat Med. 1993;12:
1293–316.

  97. Arroll B, Schechter MT, Sheps SB. The assessment of
diagnostic tests; a comparison of the recent medical
literature  –  1982  versus  1985.  J  Gen  Intern  Med.
1988;3:443–7.

  98.  Cooper  LS,  Chalmers  TC,  McCally  M,  et  al.  The
poor quality of early evaluations of magnetic reso-
nance imaging. JAMA. 1988;259:3277–80.

  99.  Bean CA, Sostman HD, Zheng JY. Status of clinical
MR evaluations, 1985–1988: baseline and design for
further assessments. Radiology. 1991;180:265–9.
 100.  Sackett DL, Straus SE, Richardson WS, Rosenberg
W,  Haynes  RB.  Evidence-based  medicine.
Edinburgh: Churchill Livingstone; 2000.

 101.  Valenstein  PN.  Evaluating  diagnostic  tests  with
imperfect  standards.  Am  J  Clin  Pathol.  1990;93:
252–8.

 102.  Phelps  CE,  Hutson  A.  Estimating  diagnostic  test
accuracy using a “fuzzy gold standard”. Med Decis
Making. 1995;15:44–57.

 103.  Begg  CB.  Biases  in  the  assessment  of  diagnostic

tests. Stat Med. 1987;6:411–23.

 104.  Begg  CB,  Greenes  RA.  Assessment  of  diagnostic
tests when disease verification is subject to selection
bias. Biometrics. 1983;39:207–15.

 105.  Gray  R,  Begg  CB,  Greenes  RA.  Construction  of
receiver  operating  characteristic  curves  when  dis-
ease  verification  is  subject  to  selection  bias.  Med
Decis Making. 1984;4:151–64.

 106.  Bossuyt PM, Reitsma JB, Burns DE, et al. Towards
complete and accurate reporting of studies of diag-
nostic accuracy: the STARD explanation and elabo-
ration. Ann Intern Med. 2003;138:W1–12.

 107.  Vandenbroucke  JP,  von  Elm  E,  Altman  DG,  et  al.
Strengthening
the  Reporting  of  Observational
Studies  in  Epidemiology  (STROBE):  explanation
and  elaboration.  Ann  Intern  Med.  2007;147:
W163–94.

 108.  Moher D, Liberati A, Tetzlaff J, Altman DG, and the
PRISMA Group. Preferred reporting items for sys-
tematic  reviews  and  meta-analyses:  the  PRISMA
statement.  PLoS  Med.  2009;6:e1000097,  p.  1–6.
Available at: www.plosmedicine.org.

Decision Analysis and Decision
Support Systems in Anatomic
Pathology

Michael Hendrickson and Bonnie Balzer

10

Keywords
Decision  analysis  •  Anatomic  pathology  decision  support  systems
• Evidence-based pathology

In  this  chapter,  we  discuss  two  aspects  of
 decision-making in anatomic pathology: decision
analysis  (DA)  and  decision  support  systems
(DSS).  As  background  information  for  our  dis-
cussion of DA, we will distinguish two kinds of
ignorance:  vagueness  and  probabilistic  uncer-
tainty and then discuss the two dominant interpre-
tations of probability – stable relative frequencies
(frequentist) and degrees of belief (Bayesian).

Subjective Probability and Decision
Analysis

Subjective probability is the language of DA. DA
provides the conceptual machinery to integrate a
suitable characterization of the basic elements of
the  decision  –  actions,  information,  and  prefer-
ences – to produce the best alternative. In consid-
ering ways in which these concepts can be usefully
deployed in day-to-day anatomic diagnostics, we
can ask several questions. We start by asking the
question: is there a clinically relevant decision to

M. Hendrickson ()
Department of Pathology, Stanford University Medical
Center, Stanford, CA 94305, USA
e-mail: hendrickson@stanford.edu

be  made?  Other  questions  involve  the  important
decision  analytic  concepts  of  the  value  of  infor-
mation,  Bayesian  updating,  sensitivity  analysis,
and  cost  functions.  Finally,  we  will   discuss
 strategies for dealing with diagnostic assignment
uncertainty that persists after information gather-
ing strategies have been exhausted.

Decision Support Systems

DSS  are  computer-based  classifiers  that  evalu-
ate  evidence  and  classify  a  situation  either  in
service  of  morphological  diagnosis  or  the  pre-
diction of some aspect of a patient’s future, such
as the risk for developing invasive cancer, prog-
nosis or prediction. Diagnostic systems are basi-
imitative.  Rule-based  expert  systems
cally
attempt to model the diagnostic performance of
expert  pathologists.  They  have  limitations  that
will be discussed against the background of top-
ics  discussed  in  Chap.  6.  Prognostic  systems
employ  what  is  termed  “case-based  reasoning
(CBR),”  matching  a  set  of  patient  predictors
with those of a large number of database patients
for  whom  predictors  and  clinical  outcomes  are
known. The use of this term has little to do with
clinical  CBR  and,  in  mathematical-statistical

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_10, © Springer Science+Business Media, LLC 2011

173

174

M. Hendrickson and B. Balzer

terms,  amounts  to  a  nearest-neighbor  search.
The  “curse  of  dimensionality”  discussed  in
Chap. 7 again intrudes.

DSS  are  basically  classifiers;  they  evaluate
evidence and classify a situation either in service
of  diagnosis  or  prediction  of  some  aspect  of  a
patient’s  future  clinical  phenotype,  FClin(t).  In
this discussion, we are interested less in the spe-
cifics of DSSs and more in locating them within
the  classificatory  and  diagnostic  framework
explained in Chap. 6. From that perspective, we
ask  what  can  we  expect  from  diagnostic  DSSs
given the problems presented to expert systems
by the translation-transmission problem and the
private, microrevisionary changes that constantly
shift the individual expert’s landscape and bound-
aries.  In  short,  that  an  expert’s  opinion  is  a
 moving target and that experts’ mental “maps” of
the classificatory terrain are almost always, par-
ticularly  in  the  neighborhood  of  boundaries.
noncongruent.

Scope-Side Decision Analysis

DA  and  modern  mathematical  probability  were
born at the same time in France in the seventeenth
century [1, 2]. These ideas have been elaborated
and systematized over the ensuing 300 years and,
in  the  twenty-first  century,  subjective  expected
utility  (SEU)  theory  informs  the  way  we  think
about everything from economics to public health
policy to personal decision-making. Its principles
have  become  today’s  common  sense  and  they
form the core of EBM. In the past 50 years, DA
has become a discipline in its own right with its
own  journals  and  academic  departments.  The
current  mathematical  formulations  of  DA  are
daunting, but the core ideas are easily stated and
quite intuitive. Sox provides a detailed account,
with worked examples, of DA in a medical set-
ting [3].

We  are  interested  here  in  setting  out  the
basic ideas of DA and believe that the quality
of both the teaching and practice of diagnostic
surgical  pathology  would  be  improved  by
informing  day-to-day  decision-making  with
these principles.

The Basic Ideas of Decision Analysis:
The Decision Basis

Decisions  involve  choosing  among  alternatives
that will yield uncertain futures, for which we have
preferences. There are three elements of any deci-
sion: (1) what you can do or the alternative actions
that can be taken; (2) what you know, the informa-
tion  you  have;  and  (3)  what  you  want,  your
 preferences. Collectively, these three represent the
decision  basis,  the  specification  of  the  decision.
DA provides the logic that operates on the decision
basis – actions, information, preferences – to pro-
duce the best alternative. Crucial to this process is
a clear description by the decision maker of pre-
cisely what decision is under consideration at the
time – the framing of the decision. This frame will
inform all elements of the decision basis [4].

Vagueness vs. Probabilistic Uncertainty

DA deals with uncertainty of a very specific type
and not with other types; it is crucial to understand
the difference. There are two kinds of uncertainty:
vagueness  and  probabilistic  uncertainty  [5].  As
discussed in Chap. 6, vagueness is a characteristic
of verbal descriptions of both the features that fig-
ure in the morphologic definition of an entity and
the delimitation of a neoplastic kind (KNeop’s) from
its neighbors in the phenospace; vagueness gives
rise to assignment uncertainty. Probabilistic uncer-
tainty is predicated of either unexamined features
of members of a particular precisely defined class
or their future behavior. We may be in doubt about
whether a particular individual neoplasm (INeop) is
A or B (an in-between case) after our exhaustive
examination – that’s vagueness. On the other hand,
we  may  be  certain  that  the  INeop  is  an  “A,”  but
uncertain  whether  that  patient  will  experience  a
recurrence or not – that’s probabilistic uncertainty.
This is what motivated my use of the lottery meta-
phor  to  model  managerial  KNeop’s.  Vagueness  is
uncertainty  about  which  lottery  the  patient  is  in;
probabilistic uncertainty is intrinsic to the lottery.

DA  banishes  vagueness  –  assignment
 uncertainty  –  from  the  modeling  process  by
insisting that predicates (feature descriptors, like

10  Decision Analysis and Decision Support Systems in Anatomic Pathology

175

“large,” “crowded,” “atypical”) and category des-
ignations (“complex atypical endometrial hyper-
plasia”)  pass,  what  the  decision  analyst,  Ron
Howard, calls a “clarity test.”

“Consider a clairvoyant who knew the future,
who  had  access  to  all  future  newspapers,  read-
ings of physical devices, or any other determin-
able  quantity.  The  clarity  test  asks  whether  the
clairvoyant  [literal  at  the  level  of  Asperger’s
 syndrome] would be able to say whether or not
the event in question occurred or, in the case of a
variable, the value of the variable.” He exempli-
fies this with the term “technical success” “[this]
would have to be defined in such terms as ‘able to
operate × hours  under  conditions  y  without  fail-
ure’ […]” [6]. This insistence is appropriate for
decision analytic modeling: probabilistic reason-
ing is predicated on crisp, unambiguously defined
categories; the foundation of mathematical prob-
ability is classical set theory. Something is either
“A” or “not A,” there is nothing in-between; this
is Aristotle’s law of the excluded middle.

For  most  situations

involving  vagueness
(Chap.  6),  this  requirement  seems  forced  and
arbitrary and has led to the development of alter-
native, more flexible logics. Fuzzy theory (fuzzy
set  theory,  fuzzy  logic,  fuzzy  categories)  was
developed  in  the  1960s  in  by  a  U.C.  Berkeley
engineer, Lotfi Zadeh, in response to these prob-
lems  [7].  There  are  several  accessible  introduc-
tions [8–10]. The medical applications have been
explored  in  a  series  of  publications  by  Sadegh-
Zadeh [11]. Some of these techniques have been
incorporated in DSS [12].

Interpretations of Mathematical
Probability

Mathematical probability refers to an axiomatic
branch  of  mathematics  and  is  used,  noncontro-
versially,  to  model  games  of  chance  (classical
probability). Controversy arises in extending the
model to real-world situations outside the casino.
There are two main schools of thought (and many
variants): frequentist and subjectivist (or person-
alist).  The  frequency  interpretation  holds  that
probability should model long-run stable empiri-

cal frequencies. For example, repeated tosses of a
coin yields a relative frequency of 0.5 for a “fair”
coin.  In  many  real-world  applications,  long-run
frequencies  are  commonly  never  achieved.  For
example, we talk about the probability of it rain-
ing on a particular day or the probability of a suc-
cessful space craft launching (e.g., the Challenger
space shuttle). A different notion of probability is
required to handle these situations. One general
response to these situations is to view probability
as a measure of belief. People who interpret prob-
ability in this way are called subjectivists or per-
sonalists.  Formally,  for  them,  a  probability  is
cashed out for a willingness to bet on one possi-
bility over another; DA is committed to this view
of  probability.  The  probabilist  Spiegelhalter
describes  an  experiment  that  helps  to  fix  these
concepts. He is addressing a lay audience.

I hold a coin and ask, “What is the chance this will
come  up  heads?”  They  cheerfully  say  something
like “50%” or “half-and-half.” I then toss the coin,
catch it, flip it onto the back of my hand without
revealing it, and ask, “What is the probability this
is heads?” Pause. Then someone, less confidently,
mumbles  “50%.”  I  reveal  the  coin  to  myself,  but
not to them, and ask, “What is your probability that
this is heads?” Very grudgingly they might eventu-
ally admit “50%.” In this experiment I have gone
from pure aleatory [games of chance, or frequen-
tist  interpretation]  uncertainty  to  pure  epistemo-
logical  [subjectivist]  uncertainty,  showing  (1)
epistemological  uncertainty  is  “in  the  eye  of  the
beholder”  (my  probability  was  eventually  0%  or
100%, whereas theirs was still 50%), (2) that the
language of probability applied to both forms, and
(3) that these different types of uncertainty may be
perceived differently [13].

Much  has  been  written  in  the  statistical  and
machine  learning  literature  on  these  often  con-
tentious  issues  of  interpretation;  Hacking  and
Hájek are good places to start [2, 14, 15].

The Clinician’s Lament

Equipped with these distinctions, we can now exam-
ine  a  common  complaint  about  pathologists.  The
disgruntled clinician points out: “I have to have a
certain diagnosis in order to proceed with my clini-
cal work.” There is, of course, no question of elimi-
nating probabilistic uncertainty; most of us discover

176

M. Hendrickson and B. Balzer

this shortly after emerging from the womb. Voltaire
observed:  “Doubt  is  not  a  pleasant  condition,  but
certainty  is  absurd.”  What  are  we  to  make  of  the
clinician’s  insistence  on  certainty  from  patholo-
gists? When clinicians insist on certainty, it is usu-
ally assignment uncertainty they are worrying about.
Their therapies are indexed by our assignments; cli-
nicians  are  usually  completely  comfortable  with
probabilistic  uncertainty given a fixed assignment;
in other words, as long as they know what lottery
they  are  dealing  with.  Again,  the  macho  surgical
pathologist’s response – “May be wrong, but never
in doubt” – is about assignment uncertainty.

Applying the Basic Intuitions
of Decision Analysis to Diagnostic
Pathology

as a example) and M-classifications. I commented
there  that  S-classifications  are  fine-grained  and
mark all the myriad salient phenotypic distinctions
that  can  be  made;  M-classifications  are  coarse-
grained and codify the much fewer clinically rele-
vant   distinctions.  The  public  working  out  of  this
 sentiment is seen in the several attempts to group
HG-KNeop’s  into  managerially  relevant  categories.
Examples  include  the  soft  tissue   neoplasms  and
gynecologic mesenchymal proliferations [16, 17].
We  are  now  confronted  with  a  new  challenge  –
both practical and pedagogical; the mapping of the
many  HG-KNeop’s  into  a  handful  of  managerial
classes.  In  other  words,  there  has  been  a  gradual
move toward the diagnosis that is “good enough to
get on with clinical management” and away from
the “histogenetically right diagnosis.”

How does DA help in oncopathological diagno-
sis?  The  underlying  strategy  here  is  to  locate  a
particular diagnostic problem in the patient’s spe-
cific clinical context and ask: “What information
does  the  clinician  require  to  move  the  patient’s
clinical management along to the next step?” In
this first section, we discuss some generalizations
and guiding principles useful in day-to-day diag-
nosis in surgical pathology.

Our heritage from the legendary surgical pathol-
ogists of the mid-twentieth century was admission
to the clinical decision-making process. Pathologists
throughout the world emerged from their hospital
basements and became full participants in patient
care  management.  This  activist  tradition  empha-
sized the importance of locating anatomic diagno-
ses within a clinical decision-making framework.
Exhilarating, though this was, it came with a price;
the exposure of an elaborate nineteenth and early
twentieth century tumor taxonomy to the minimal-
ism  of  pragmatic  oncopathology.  For  example,
there is the taxonomically unglamorous truth that
the status of the excision margins and tumor size
are  more  important  than  which  of  five  different
subspecies of tumor “A” (all, currently, calling for
the same therapy) might be afflicting the patient.
There was  a growing appreciation of the distinc-
tion  one  of  us  (MH)  made  in  Chap.  6  between
S-classifications (I used histogenetic classifications

The “Future Utility of the Distinction”
Argument

Elaborate histological, cytogenetic, and molecular-
genetic workups are often justified on the grounds
that something useful will turn up that will be of use
in the future. “How are we ever going to know if a
distinction is important if we don’t record it?” The
problem  with  unsponsored  research  efforts  of  this
sort is that they will ultimately have to be repeated in
a disciplined way (see internal validity discussion in
Chap. 7) and it may well interfere with cost- effective,
efficient, patient management. So, the liberal use in
our literature of locutions like “It is important to dis-
tinguish ‘A’ from ‘B,’ ‘C’ and ‘D’ have to be criti-
cally examined; the obligatory follow-up questions:
‘Why?’ ‘Important for whom?’ It may well be that
the distinction is one that can, in principle, be made,
but should a clinical manager be willing to pay for
this distinction? What is the evidence that something
different should be done in light of this new infor-
mation? The problem is compounded when the cli-
nician, innocent of our classificatory ways, assumes
that because we have a name for something, it is a
distinction  he  should  worry  about.  Of  course,  this
problem can run the other way. In the absence of any
convincing evidence, clinicians, in their role of the
final  decision  maker,  coerce  the  pathologist  to
engage in many empty rituals. Searching for keratin

10  Decision Analysis and Decision Support Systems in Anatomic Pathology

177

positive  cells  in  sentinel  lymph  node  biopsies  and
performing CD117 examinations on random malig-
nancies come to mind [18].

To  date,  discussions  of  histopathology
employing   EBM  principles  have  concentrated
 exclusively on managerial distinctions. Let’s take
a look at some guiding principles and how they
play out in day-to-day diagnostics.

Background Principles to be
Considered in Decision Analysis

Good Decisions and Good Outcomes

It  is  important  to  distinguish  between  good/bad
 decisions and good/bad outcomes. Uncertainty in
medicine  is  ineliminable;  good  decision-making
consists  in  “taming  chance,”  in  employing  a
coherent decision-making strategy that uses one’s
best  guess  about  uncertain  quantities  in  light  of
current information. Good decision analytic tech-
nique doesn’t guarantee a favorable outcome; the
well  thought-out  model  of  a  particular  situation
doesn’t  guarantee,  obviously,  that  the  outcome
will be the one you want. It is also true that faulty
decision-making may be followed by a favorable
outcome. What DA promises is that if you follow
its precepts, you will maximize your chances of
the outcome you favor.

Uncertainty is Inescapable But
Shouldn’t Paralyze Decision-Making

If diagnostic assignments are crisp, there are no
problems; the business of DA is uncertainty man-
agement.  How  might  DA  handle  assignment
uncertainty? One way to finesse this problem is
to settle upon a taxonomic model; for example,
that the region between peaks in the phenospace
is  populated  by  atypical  cases  from  one  or  the
other population and assign a probability to the
two possibilities [5]. Other models are possible,
for example, including a third population of “in-
between” cases, a separate and distinct lottery (in
the language of Chapter 6), and assigning proba-
bilities to three possibilities.

Some Crucial Questions and Other
Issues Related to Decisions
in Pathology (Table 10.1)

Is There a Clinically Relevant Decision
To Be Made?

The  Stanford  professor,  Ron  Howard,  who  first
coined  the  term  “decision  analysis”  in  the  1960s,
has  often  said  that  the  most  difficult  part  of
 consulting  work  in  DA  is  discovering  whether  or
not the client really does have a decision to make?

“If  you  have  only  one  alternative,  then  you
have no choice in what you do. If you do not have
any information linking what you do to what will
happen  in  the  future,  then  all  alternatives  serve
equally  well  because  you  do  not  see  how  your
actions will have any effect. If you have no pref-
erences regarding what will happen as a result of
choosing any alternative, then you will be equally
happy choosing any one” [4].

Howard was referring to the difficulty in expos-
ing this structure in the typically complex details of
the client’s specific situation. Often what is required
is someone to take a bird’s eye view of the situation
and point out the obvious. Another correlative point:
DA has nothing to tell us about nonmanagerial dis-
tinctions; there is no clinical decision to be made.
Scientific classification disputes are discussed in an
entirely different framework. Histogenetic classifi-
cation  issues,  for  example,  involve  the  scientific
plausibility of competing embryological theories in
a particular domain, issues quite remote from clini-
cal decision making.

Table 10.1  Important questions to ask about a problematic
case

Question 1: Is there a clinically relevant decision to be made?
Question 2: Do I need more information to make this
clinically relevant decision? The value of information
Question 3: What have I learned from my new informa-
tion? Bayesian updating
Question 4: Would I make a different clinically relevant
decision if my probabilities were slightly adjusted?
Sensitivity analysis
Question 5: What’s at stake for this particular patient?
Cost functions
Question 6: What do I do in the face of assignment
uncertainty when it makes a clinical difference?

178

M. Hendrickson and B. Balzer

The Problem of the Burgeoning
Oncopathological Zoo

Anatomic  pathologists  face  a  situation  similar  to
that of Howard’s client. The complexity of our diag-
nostic task stems from the hybrid character of onco-
pathological  classification  –  a  managerial  overlay
on a much more complicated histogenetic classifi-
cation. We have hundreds of named entities but, in
any particular domain, these entities fall into only a
few managerial categories. Many of the named cat-
egories  are  associated  with  vague  FClin(t)  claims
that probably would not stand up under the kind of
scrutiny given to cancer markers (Chap. 7). These
claimed distinctions, nevertheless, persist in the pri-
mary literature and the reviews of that literature. It
is a major challenge keeping track of the exponen-
tially  proliferating  neoplastic  kinds  and  critically
evaluating  whether  or  not  they  should  figure  in
patient care decisions. Molecular kinds will soon be
making their contribution to this burden. It is at this
point  that  the  principles  set  out  below  become
important as a way of effectively focusing on one’s
clinically relevant diagnostic efforts.

Guiding Principles in Clinical
Decision-Making: What Are We Trying
to Accomplish for a Particular Patient?

The  guiding  principle  is  to  doggedly  pursue  the
question:  What’s  the  clinical  context?  What  does
the clinician need to know about the specimen sub-
mitted to get on with the next stage of clinical deci-
sion-making?  Let’s  look  at  this  more  carefully.
Clinically  relevant  differential  diagnostic  sets  are
generated during the course of diagnosis. For exam-
ple, we might initially sort the relevant possibilities
into a differential diagnosis organized around phe-
notype. We then might sort the members of these
phenotype sets into three groups: those associated
with a benign clinical course (“good actors”), those
with  a  clinically  malignant  course  (“bad  actors”),
and  those  with  an  intermediate  clinical  behavior.
Further  diagnostic  testing  should  have  as  a  goal
establishing  to  which  broad  category  the  case
belongs. For example, when confronted with a high-
grade  malignancy  in  the  soft  tissues  and  having

ruled out mimics (e.g., metastasis, local extension
from  another  site,  melanoma,  hematolymphoid
malignancy)  and  established  that  the  tumor  is
descriptively  a  pleomorphic,  high-grade  primary
soft tissue sarcoma, it can be argued that one’s clini-
cally  relevant  work  is  done.  It  remains  an  open
question  whether  fine-tuning  this  diagnosis  (Is  it
leiomyosarcoma,
liposarcoma,
dedifferentiated
poorly  differentiated  synovial  sarcoma,  etc.?)  is
clinically  useful.  Once  a  case  can  be  assigned
unequivocally to one or the other category, from the
point of view of clinical action, nothing more need
be done. If all of the “benign” KNeop’s in a particular
location will be treated the same way and likewise
for  all  the  malignant  KNeop’s,  further  diagnostic
efforts may be in service of nonmanagerial goals,
but have nothing to do with clinical decision-making.

The Value of Information: Do I Need
More Information to Make this Clinically
Relevant Decision?

lavish

The question that should probably drive the elabo-
rateness of the histopathological workup in a cost-
effective  environment  should  be:  What  does  the
clinician  need  to  get  on  with  clinical  workup,
 treatment? The analysis may be different for dif-
ferent stages of the clinical workup. The answers
to this question will differ for a needle biopsy of a
mammographically suspicious lesion or a needle
biopsy  of  a  retroperitoneal  soft  tissue  mass  than
for  the  respective  resection  specimens.  The  con-
ventional
immunohistochemical  (IHC)
workup of a soft tissue neoplasm doesn’t need to
be performed on the limited sample provided by a
needle biopsy; it can await the resection specimen.
For a resection specimen, a more elaborate workup
is conventional. When should we stop? Do dimin-
ishing returns set in when working up, for exam-
ple,  what  is  noncontroversially  a  high-grade
pleomorphic soft tissue sarcoma? EBP has a role
here  in  scrutinizing  the  claimed  distinctions
between the dozen denizens in this particular zoo.
We  are  painfully  aware  that  standard  of  care
consideration often does not reflect this practical
approach to diagnosis. We may, for various com-
pelling local reasons, respond to these pressures,

10  Decision Analysis and Decision Support Systems in Anatomic Pathology

179

but, even so, it is valuable to maintain a clear-eyed
view of the evidentiary warrant (or lack) for these
decisions.  There  are  some  requirements:  (1)  We
need  to  start  off  with  a  focused  question.  For
example, distinguishing “A” or “B” and what are
the  most  discriminating  tests  to  order?;  (2)  We
need to know what we will do with possible results.
For an IHC study, we need to know what we would
do with a positive or negative or an inconclusive
result.  In  short,  don’t  order  the  test  unless  you
know what you would do with the possible answers
you  might  obtain.  If  you  would  make  the  same
diagnosis  given  any  possible  test  result,  the  test
has a “value of information” of zero; you should
not have ordered the test. When multiple tests are
ordered,  the  “curse  of  dimensionality”  rears  its
ugly head. It is not at all uncommon to receive a
consult case with twenty or more IHC studies. It
reminds us that “more information is not necessar-
ily better.” See the  discussion in Chap. 7.

Often, of more value than additional IHC test-
ing  is  clinical  or  radiological  information.  It
makes no sense to try to leverage the location of
a uterine  cancer (from cervix or fundus) using an
IHC panel when a call to the clinician might  settle
the issue. That’s not to say that there are not cases
of  cancer centered on the uterine isthmus which
are   problematic;  it’s  to  argue  against  the  reflex
ordering of a panel to sort out what might well be
a  perfectly  obvious  clinical  situation.  It’s  like
using  the  degree  of  actinic  elastosis  in  a  skin
biopsy to ascertain the age of a patient as an alter-
native to looking at the age box in the pathology
requisition. Again, whether a well-differentiated
cartilaginous neoplasm of bone is “enchondroma”
or  “chondrosarcoma”  is  a  distinction  that  the
radiologist  makes,  not  the  pathologist  on  her
own; the radiologic findings are constitutive ele-
ments of the final “pathologic” diagnosis.

What Have I Learned from My New
Information? Bayesian Updating

thinking lies at the heart of DA. It tells us how to
combine what we knew with what we found out
to discover what we should now believe.

The odds formulation of Bayes Theorem is a
particularly  transparent  way  of  visualizing  this
principle (Fig. 10.1). Diagnostic IHC studies can
be understood only within this framework. IHC
yields,  in  general,  a  set  of  likelihood  ratios  on
diagnostic  possibilities.  Learning  from  some
combination  of  test  results  requires  a  set  of
“input” prior odds on those possibilities [19].

Independence of Informational
Evaluations

There are two attitudes assessing a patient’s his-
tology.  The  first,  the  integrative  view,  insists  on
having  all  the  relevant  background  information
(clinical  history,  imaging  findings,  etc.)  before
evaluating the histology of a case. Opposed to this
is the attitude, the independent assessment view,
that  the  independence  of  the  histologic  input
should be preserved by examining the slides with-
out any of that background information. There is
truth in both approaches. “Independent assessment”
is  most  consistent  with  Bayesian  principles.  To
incorporate the radiological information in one’s
assessment  of  histology  and  in  updating  using
both histology and radiology, treated as indepen-
dently  evaluated  inputs,  may  lead  to  “double
counting”  of  the  radiologic  evidence.  It  is  also
true  that  no  histological  assessment  should  be
reported without a careful integration of the infor-
mational  inputs  provided  by  clinical  and  radio-
logical features. Ideally, independent assessment
should  be  followed  by  clinicopathological  inte-
gration using Bayesian updating.

Would I Make a Different (Clinically
Relevant) Decision if My Probabilities
were Slightly Adjusted? Sensitivity
Analysis

Bayes Theorem is a trivial algebraic consequence
of  the  axioms  of  mathematical  probability.  It
becomes  interesting  when  it  is  interpreted  as  a
formula for learning from experience [2]. Bayesian

The question how robust is my diagnosis to “wig-
gling”  my  input  probabilities?  leads  to  the  con-
cept of sensitivity analysis: How sensitive is my

180

M. Hendrickson and B. Balzer

Fig. 10.1  The diagnostic test matrix and Bayes’ odds. The
unit area probability square provides a particularly intui-
tive representation of Bayes’ theorem. The logical possi-
bilities are color coded in the ‘Logical Space’ on the right.
Red for true positives, light blue for false positives etc. The
logical possibility squares are distorted to reflect the prob-
abilities of each of these logical possibilities to produce the
large probability square on the right. The ‘Priors’ rectangle
depicts the total probability of true positives/negatives and
has  unit  area;  the  other  rectangles  (‘Test  Characteristics’
and ‘Posteriors’) represent conditional probabilities; each,
of  course,  have  unit  area.  This  is  a  particularly  intuitive
formulation; it tells how we pass from what we knew prior
to the test (the prior odds) and what we found out by doing
the test (the likelihood ratio) to what we should now believe
in light of that new evidence (the posterior odds). The rule

is simple: multiply the prior odds by the likelihood ratio.
The odds form of Bayes theorem can be evaluated visually
by forming the ratios of the appropriate rectangles depicted
in the probability space. For example, it is clear that with
the situation depicted the posterior odds on disease given a
positive test is roughly 0.5 (the ratio of the red patch over
the light blue patch in the large probability square. Forming
each of the ratios (visually) in the product makes this geo-
metrically  plausible.  Bayesian  updating  of  beliefs  is  an
iterative process: the posterior odds can become the prior
odds  of  another  round  of  diagnostic  tests.  Applying  this
rule  sequentially  by  multiplying  the  likelihood  ratios
makes the simplifying assumption that the tests are condi-
tionally independent; this is usually an unrealistic assump-
tion  in  most  practical  situations  [39].  Bayesian  networks
can accommodate a feature dependency structure [27]

diagnosis under slight changes in my prior odds
or my likelihood ratios? Would a change in diag-
nosis lead to a different action? Does sensitivity
to  one  particular  input  suggest  that  I  need  more

information to narrow that uncertainty? Do I need
to perform more tests? These are the basic ques-
tions involved in the decision analyst’s sensitivity
analysis.

10  Decision Analysis and Decision Support Systems in Anatomic Pathology

181

What’s at Stake for this Particular
Patient? Cost Functions

Decision analytic methodology insists on a sepa-
ration of patient utilities from the action and infor-
mation  inputs  into  the  process.  Pathologists
sometime engage in scope-side group DA. “How
can I make a diagnosis of Grade I endometrial car-
cinoma in this 30 year old woman infertility patient
when I know that the risk to life with such a lesion
is minimal and that there are treatments short of
hysterectomy that are sometimes curative?” One’s
natural impulse is to downshift one’s diagnosis to
“complex  atypical  hyperplasia.”  Given  the  same
pattern in a postmenopausal woman with dysfunc-
tional  bleeding,  one  might  have  no  hesitation  in
making  a  diagnosis  of  Grade  I  adenocarcinoma.
Locating  a  process  along  a  graded  morphologic
continuum  is  one  thing;  deciding  what  clinical
action to take for a particular patient (or class of
patients)  with  that  histology  is  another.  For
example, what clinical action is warranted given a
particular  morphological  patterns  depends  upon
whether one is dealing with a “high penalty hys-
terectomy”  situation  (reproductive  conservation
important or the patient is a high-risk surgical can-
didate) or a “low penalty hysterectomy” situation
(reproductive conservation not an issue and patient
is  a  good  surgical  candidate).  There  is  nothing
paradoxical  or  mysterious  about  this;  it  is  a
straightforward  issue  of  there  being  different
“action threshold” along a morphological contin-
uum for different classes of patients.

The  basic  idea  behind  managerial  threshold
setting is the cost function. Consider two overlap-
ping  bell-shaped  curves;  we  want  to  determine
the optimal threshold in the overlap area for shift-
ing from calling cases “A” to calling cases “B.”
The optimal threshold is one that minimizes the
total cost of misclassification. The inputs for this
calculation are (1) the prior probabilities encoun-
tering an “A” or a “B” and (2) the costs that attach
to errors of the two types: miscalling an “A” for a
“B” and miscalling a “B” for an “A.” Debate in
histopathologic  diagnosis  is  often  erroneously
focused  on  the  details  of  morphology  when  it
really is about utilities – anticipating the impact
of a diagnosis.

This  discussion  raises  a  number  of  lessons:
First,  diagnoses  are  often  not  simply  a  report  of
objective  findings,  they  may  be  value-laden.
Second,  there  is  sometimes  confusion  about  who
should  be  integrating  clinicopathologic  informa-
tion to come up with treatment recommendations
for  the  patient?  A  pathologist’s  diagnosis  may
amount to a recommendation for treatment rather
than being a report of an objective finding, an infor-
mational  input.  Third,  it  is  sometimes  not  clear
whose  utilities  are  being  reflected  in  decision-
making:  the  patient’s,  the  pathologist’s,  the  clini-
cian’s, or the insurance carrier’s. These are difficult
issues beyond the scope of this discussion.

What Do I Do in the Face of Irresolvable
Uncertainty in Diagnosis when it Makes
a Clinical Difference?

In Chap. 6, in discussing problem cases, one of us
unhelpfully pointed out that it is in the nature of
INeop’s that problem cases never disappear. I urged
a philosophical attitude that these cases pointed to
the inevitable failure of static, discretizing classifi-
cation systems to do justice to evolving processes
distinguished by their continuous spatio-temporal
variation. I also pointed out that as one moves from
“core” cases to cases occupying the “PeTI” region
(or  encounters  taxonomically  embarrassing  het-
erogeneity), several things happen with great regu-
larity: (1) for experts, diagnosis and classification
collapse into a single activity; (2) experts’ appeal
to  published  criteria  gives  way  to  arbitrary  (but
often principled) stipulation using noncriterial fea-
tures and (3) interexpert agreement degrades. It is
further argued in Chap. 6 that, in the absence of
expert  consensus
in  possession  of
(when
 “complete”  information),  there  is  no  fact  of  the
matter about the correct assignment.

That is all well and good, the reader might say,
but we are still left with the practical issue of diag-
nosing such cases. Here we are only concerned with
assignments that make a managerial difference. The
relevant experts will assign nonmanagerial problem
cases  by  appeal  to  one  or  another  oncogenetic  or
histogenetic theory. What about managerially rele-
vant diagnostic decisions? Here the basic strategy is

182

M. Hendrickson and B. Balzer

to “look up” from the microscope and ask if the cli-
nician should care about the distinction you are try-
ing to make. First, consider the KNeop’s on either side
of the in-between case. What is the claimed differ-
ence in behavior? The next question to ask is: “Are
the claimed differences supported by credible evi-
dence?” For example, low-grade serous carcinomas,
psammocarcinomas, and serous borderline tumors
of  peritoneal  origin  are  all  part  of  a  morphologic
continuum. They are rare and there is limited infor-
mation  about  their  long-term  behavior,  although
they all are, relative to the usual serous carcinoma of
the ovary, clinically low grade. In this case, it is not
at all clear from the literature that there are substan-
tial differences in the long-term behavior of the three
(nor, in fact, that mere mortals can follow the experts
in distinguishing among them) [20]. What is one to
do with an in-between case in this spectrum?

The  next  question  to  ask:  “What’s  at  stake?”
Even  if  there  are  differences  in  behavior,  are  they
sufficient to warrant different therapeutic approaches?
Debulking is recommended for all three; there is no
good evidence that chemotherapy is effective with
these  low-grade  neoplasms.  The  issue,  then,  is
whether  to  undertake  radical  debulking  of  disease
(with removal of the internal genitalia) or conserva-
tive debulking, involving preservation of ovarian tis-
sue and the uterus. Next question: “How old is the
patient?”  If  reproductive  conservation  is  relevant,
then debulking with preservation of the uninvolved
ovary and the uterus is indicated; if not, radical deb-
ulking is the appropriate choice. This would be the
choice for all three. Thus, in the face of assignment
uncertainty, attention turns to the pros and cons of
various  clinical  options  given  that  all  three  have
more or less the same behavior, despite the fact that
two are labeled “cancer” and the other is not.

The Rubber Band Paradox

The  in-between  case  raises  another  curious
 diagnostic practice. A case lies between “A” and
“B” along a multivariate morphologic continuum.
After  a  good  bit  of  extensive  testing  and  hand-
wringing, we decide that the balance of the evi-
dence  is  for  “A.”  A  standard  argument  for  this
practice  is  that  the  behavior  of  “A”  is  substan-
tially different from “B.”

There are a number of ways of making sense of
this. Here is a pragmatic argument: it may be the
case that the chemotherapy for A is different from
the chemotherapy for B or A is treated with surgery
and B is not. This is all well and good and makes
decision analytic sense particularly when the diag-
nostic dilemma is expressed probabilistically. That
is: “I put 0.80 probability on A and the 0.20 on B.”
A cost function can be developed and an expected
utility calculation done that issues in a decision.

What  doesn’t  make  sense  is  to  conclude  that,
because  you  have  –  after  long  agonizing  and  in
possession  of  “complete”  information  –  decided
that it’s an “A” it will behave like the average “A.”
The rubber band image come to mind because the
in-between cases “snap” to one or the other mea-
sure of central tendency in the neighborhood, A or
B in this case. In fact, it is probably more impor-
tant that the case was difficult to classify than that
it  was  finally  assigned  to  the  “A”  category.  This
rubber band move is a covert form of essentialism;
cases either have an “A” essence or a “B” essence
and that essence is captured by the measure of central
tendency (the mean or the median); the variation
(that  the  case  has  strayed  into  the  in-between
region)  is  confusing  random  “noise”  that  the
pathologist has now, with his testing, seen through
to the “signal.” This view conflates random mea-
surement variation with potentially important bio-
logical  variation.  As  the  late  S.J.  Gould  put  it:
“The median is not the message!” [21]. It’s impor-
tant  to  look  at  the  entire  distribution,  the  Full
House, in making clinical predictions.

The Novel Case and the “Closest Fit”
Strategy

Just as the “hybrid case” can be thought of as the
embarrassingly  heterogeneous  case,  the  “novel
case”  can  be  thought  of  as  the  embarrassingly
unique case. These, as might be expected, are rela-
tively common in a consultation practice. The basic
strategy here is to roam the relevant phenospace in
search of the “closest fit” and invoke the heuristic
“Looks like therefore most likely will behave as.”
Examples include: “Histologically low-grade mes-
enchymal  proliferation  with  potential  for  local
recurrence (see learned comment wherein all of the

10  Decision Analysis and Decision Support Systems in Anatomic Pathology

183

relevant  differential  diagnostic  possibilities  are
considered and serially discarded).”

The Fallible Reasoner: Judgmental
Psychology

There are pitfalls in intuitive probabilistic reason-
ing; these errors in judgment have been an active
area of research since the 1970s and a Nobel prize
in economics was awarded to one of the founders
of  the  field,  Daniel  Kahneman.  Space  does  not
permit a discussion of this topic, but an important
element of the EBP program should be a study of
the  relevance  of  judgmental  psychology  to
 oncopathological  decision-making.  There  are
several accessible introductions to the important
topic [3, 22–24].

Decision Support Systems

Space  does  not  permit  a  review  of  DSS  and  we
will only examine these efforts from the perspec-
tive set out in Chap. 6. DSS are basically classifi-
ers that evaluate evidence and classify a situation
either  in  service  of  morphological  diagnosis  or
prediction. Diagnostic systems aim to simulate the
diagnostic performance of experts in the domain;
they are imitative and attempt to solve the “transla-
tion-transmission” problem set out in Chap. 6. The
goal of predictive systems is to generate a clinical
prediction using, what’s been termed in the (DSS)
literature,  “case-based  reasoning  (CBR).”  CBR
amounts to matching the phenotype of the current
patient  (clinical   features,  histological  features,
etc.)  with  those  contained  in  a  database  of  thou-
sands of patients about whom both phenotype and
clinical outcome are known. The patient is assigned
the prognosis of those database patients with the
closest phenotype match.

Diagnostic Systems

One of the earliest applications of expert systems
was to the task of medical diagnosis. In the 1980s,
a very active area of research was the construc-

tion of expert systems – computer-based systems
that  replace  or  assist  an  expert  in  performing  a
complex  task.  The  construction  of  a  diagnostic
program  typically  involves  “downloading”  the
classificatory vision of an expert in the particular
domain.  This  is  exemplified  by  the  Pathfinder
expert  system  which  was  designed  by  David
Heckerman, then a Stanford Medical student, and
colleagues  to  simulate  the  diagnostic  perfor-
mance of expert hematopathologists in diagnos-
ing lymph node pathology [25, 26]. I participated
in some stages of this work. In its last versions,
the  model  contained  more  than  sixty  different
diseases and around a hundred different features.
An extensive library of images accompanies this
program. The basic idea was to capture the exper-
tise  of  a  group  of  academic  hematopathologists
as a Bayesian network. A Bayesian network, or
belief  network,  is  a  graphical  model  that  repre-
sents a set of random variables (nodes) and their
conditional dependences (by lines connecting the
nodes). Roughly, this can be thought of as a high-
dimensional joint probability distribution relating
observed features (both morphological and clini-
cal) to diagnostic categories. Its diagnostic capa-
bilities  were  evaluated  using  actual  cases  and
comparing Pathfinder’s performance with that of
the experts who originally provided the expertise
for the system. Knowledge-based expert systems
of this sort have not caught on; there is very little
about them in the literature after 2000. There are
a number of reasons for this including legal lia-
bility issues for misdiagnoses and compatibility
with  the  physicians’  workflow  [27].  Very  few
pathologists are willing to spend an hour entering
data on a problem case; it is a lot easier to send
the case off to an expert.

Attempts of this sort are, however, of theoreti-
cal interest. In the language of Chap. 6, Pathfinder
and the like diagnostic systems are an attempt to
solve  the  “translation-transmission”  problem;
the translation of the expert’s classificatory vision
into language and images and its transmission to
a nonexpert user. We can make a number of pre-
liminary observations, again, using Pathfinder as
an example: (1) the system reflects a composite
of  several  experts’  expertise;  realistically,  we
would  anticipate  that  these  experts  would  not
always  be  in  agreement  on  a  particular  case

184

M. Hendrickson and B. Balzer

assignment;  (2)  the  system  is  limited  by  the
experts’  knowledge;  in  particular,  the  experts’
current  location  along  the  macrorevisionary
cycle. This is a real problem for hematopathology;
classifications  change  with  some  regularity;  (3)
the system can be anticipated to have difficulties
with problem cases. The discussion of Chap. 6 of
the  expert’s  private  microrevisionary  cycles
emphasized  that  when  confronting  problem
cases, classification and diagnosis collapse into a
single  act.  In  diagnosing  problem  cases,  the
expert may depart from a prior rules to make his
assignment; it appears, on the face of it, unlikely
that the expert’s diagnostic-classificatory moves
would be anticipated by a set of conditional prob-
abilities  downloaded  in  a  few  interview  during
the  construction  of  the  program.  In  short,  the
expert doesn’t know what his criteria will be until
he encounters the problem case. Again, using the
language of Chap. 6, we would expect excellent
performance on “core” cases, but, for peripheral
terra incognita (PeTI) cases, performance would
degrade.

Advantages and Disadvantages
of Predictive Systems
and Case-Based Reasoning

There is a fundamental problem with population-
based  studies  –  such  studies  tell  us  about  the
characteristics of groups, not individuals. Thus,
while clinical judgment and CBR self- consciously
attend to the particularity and uniqueness of the
individual under consideration, population-based
studies scrupulously strip away all of that detail
replacing it with a handful of observed features.
This reductionist move is in service of gener-
ating  stable,  statistically  credible  population
averages.  Thus,  evidence-based  medicine  has
ideologically (and rhetorically) positioned itself
against anecdotal CBR. The pendulum, however,
swings!

CBR has become a popular approach to real-
ize the goal “personal prognosis.” CBR involves,
in  the  language  of  Bartels  et  al.,  identification
rather than classification [28]. Classification, the
partitioning of a domain into classes, involves the

selection  of  a  relatively  small,  manageable
 number of features and representing each case in
a  feature  space  spanned  by  those  dimensions.
Limiting the number of features is forced by the
“curse of dimensionality.” (See Chap. 7 for dis-
cussion)  They  characterize  this  as  a  closed  fea-
ture  space.  Clinical  and  morphologic  prediction
rules  have  this  character.  Identification,  in  con-
trast, uses as many dimensions as are available to
establish the identity (or closest fit) to other cases
in  the  database.  The  prediction  for  that  case  is
that of the group of nearest neighbors. This is an
open feature space.

Montironi  et  al.  describe  the  role  of  CBR  in

prognostic support systems:

CBR  establishes  a  prognosis  for  a  particular
patient, and thus differs significantly from statisti-
cal classification, in which the patient is assigned
to a group, all of whose members were given the
same  diagnosis.  Statistical  classification  allows  a
prognosis  on  the  basis  of  what  is  known  for  the
group,  for  example,  a  probability  to  progress  or
survive for a  certain period of time. However, sta-
tistical  procedures are neither usually intended nor
designed to characterize individuals. For example,
for a given patient it is not possible to say whether
the prognostic outlook is poor or better within the
bounds  given  for  the  group.  CBR,  conversely,  is
designed  to  provide  individual  patient  prognosis.
Case based reasoning compares the new case with
cases from a large database of cases for which the
clinical outcome is known. From such a database,
only the most similar cases are retrieved and used
to  predict  the  outcome.  The  data  may  include
 qualitative  and  quantitative  histopathological
 feature values, patient anamnestic data, treatment,
and observed response to treatment, thus providing
a  very  detailed  characterization  of  the  patient’s
situation [29].

Except in the most general sense of searching
for similar cases, the use of CBR is problematic.
For Montironi et al., it is simply an application
of  the  heuristic:  “Looks  like,  therefore  will  act
like.”  It  is  certainly  a  long  way  off  from  Osler
sitting  at  the  bedside  puzzling  over  a  singular
patient,  Sigmund  Freud  probing  the  psyche  of
Anna  O.,  Sherlock  Holmes  in  his  Baker  Suite
rooms, or the other Holmes, Oliver Wendell Jr.,
mulling over issue of precedence in the Supreme
Court [30].

In the DSS literature, CBR has two features:
the  patient’s  phenotype  can  be  characterized

10  Decision Analysis and Decision Support Systems in Anatomic Pathology

185

using an open-ended number of features and the
classification rule is: associate this patient’s point
in  the  feature  space  with  the  nearest  (i.e.,  most
similar)  case(s)  in  the  high-dimensional  neigh-
borhood. In mathematical-statistical terms, this is
a k-nearest-neighbor (kNN) search.

It’s worth recording the differences between
the  folksy  examples  I  provided  and  a  kNN
search. A few example suffice: (1) the patient’s
particularity  has  to  be  reduced  to  a  set  of
observed features and some subset of those fea-
tures recorded. There are problems in including
too much (what turns out, in the fullness of time,
to  be  “noise”)  and  too  little  (missed  “signal”).
Recall  the  years  of  our  examining  peptic  ulcer
surgery specimens and ignoring the Helicobacter
organisms;  (2)  the  observations  have  to  be
 preprocessed  into  a  computer  digestible  form,
 usually nonfuzzy; (3) a similarity measure must
be  selected  from  a  large  number  of  workable
metrics  (e.g.,  Euclidean,  Mahalanobis).  We
 discovered  in  Chap.  6  that  high-dimensional
biology is plagued by the “curse of dimensional-
ity”; kNN searches are no exception. The “near-
est  neighbor”  looses  meaning  with  a  modest
increase in the dimensionality of the data. That
is,  as  the  dimensionality  of  the  phenospace
increases the ratio of the distance to the nearest
neighbor  and  the  distance  to  the  most  distant
neighbor  asymptotically  approaches  unity.  See
Chap. 7 for discussion.

Who Should Be Making
the Decisions in Oncopathology?

Unguided  statistical  intuitions  are  notoriously
flawed and keeping track, without assistance, of
the  large  number  of  conditional  probabilities
involved in a practical decision-making problem
is impossible. The evaluation of the tsunami of
evidence from clinical trials, from genomic stud-
ies,  requires,  as  we  have  seen,  highly  special-
ized knowledge from a variety of disciplines for
which  pathologists  have  little  training.  The
futuristic  vision  of  the  unaided  community  (or
academic) pathologist as integrator of informa-
tion from multiple levels of organization – from

cancer  gene  to  histopathologic  findings  to
FClin(t)  –  is  a  fantasy.  We’ll  need  some  sort  of
help. There is much debate about who should be
integrating  this  growing,  complex  quantity  of
patient  information.  Not  surprisingly,  some
pathologists argue that it should be the patholo-
gist [31, 32].

However, this issue may be settled, there is no
question  that  light  microscopy  is  an  essential
organizing  level  in  cancer  management,  and
expertise  in  histopathology  will  be  required  no
matter  what  the  future  holds  for  the  molecular-
genetic  dimensions  of  neoplasia.  It  is  not  only
reasonable but necessary for pathologists to resist
the  methodological  imperialism  de  jour.  There
will  be  a  continuing  role  for  surgical  pathology
oncopathological decision-making in the postge-
nomic age [33–38].

References

  1.  Hacking  I.  The  taming  of  chance.  Cambridge:

Cambridge University Press; 1990.

  2.  Hacking I. An introduction to probability and induc-
tive logic. Cambridge: Cambridge University Press;
2001.

  3. Sox Jr HC, Blatt MA, Higgins MC, Marton KI. Medical

decision making. Boston: Butterworths; 1988.

  4.  Howard  RA.  The  foundations  of  decision  analysis
revisited. In: Edwards W, Miles JRF, Winterfeldt DV,
(eds.) Advances in Decision Analysis. New York, NY:
Cambridge University Press; 2007. p. 32–56

  5.  Russell S, Norvig P. Artificial intelligence: a modern
approach.  3rd  ed.  Englewood  Cliffs:  Prentice-Hall;
2009.

  6.  Howard  R.  Decision  analysis:  practice  and  promise.

Manage Sci. 1988;34(6):679–93.

  7.  Zadeh LA. Fuzzy sets. Inf Control. 1965;8:338–53.
  8.  Vineis P. Methodological insights: fuzzy sets in medi-
cine.  J  Epidemiol  Community  Health.  2008;62(3):
273–8.

  9.  Kosko  B,  Isaka  S.  Fuzzy  logic.  Sci  Am.  1993

(July):76–81.

 10. Seising R. From vagueness in medical thought to the
foundations of fuzzy reasoning in medical diagnosis.
Artif Intell Med. 2006;38(3):237–56.

 11. Sadegh-Zadeh K. The prototype resemblance theory

of disease. J Med Philos. 2008;33(2):106–39.

 12. Bartels PH, Thompson D, Weber JE. Expert systems
in histopathology. IV. The management of uncertainty.
Anal Quant Cytol Histol. 1992;14(1):1–13.

 13. Spiegelhalter  DJ.  Understanding  uncertainty.  Ann

Fam Med. 2008;6(3):196–7.

186

M. Hendrickson and B. Balzer

 14. Hájek A. Interpretations of probability. The Stanford
Encyclopedia  of  Philosophy,  In:  Edward  N.  Zalta
 editor  2010.  Springer;  http://plata.stanford.edu/
archives/spr2010/entries/probability-interpret/.
Accessed 5 April 2011.

 15. Goodman SN. Probability at the bedside: the knowing
of  chances  or  the  chances  of  knowing?  Ann  Intern
Med. 1999;130(7):604–6.

 16. Hendrickson  M,  Longacre  T.  Pathology  of  uterine
cancers.  In:  Gershenson  D,  McGuire  WP,  Gore  M,
Quinn MA, Thomas G, editors. Gynecologic cancer.
controversies  in  management.  1st  ed.  Edinburgh:
Elsevier Churchill Livingstone; 2004. p. 209–28.
 17. Kempson RL, Fletcher CD, Evans HL, Hendrickson
MR, Sibley RK. Tumors of the soft tissues (atlas of
tumor  pathology  (AFIP)).  1st  ed.  Washington:
American Registry of Pathology; 2001.

 18. Wick  MR,  Bourne  TD,  Patterson  JW,  Mills  SE.
Evidence-based principles and practices in pathology:
selected  problem  areas.  Semin  Diagn  Pathol.  2005;
22(2):116–25.

 19. Vollmer  RT.  Differential  diagnosis  in  immunohis-
tochemistry  with  Bayes  theorem.  Am  J  Clin  Pathol.
2009;131(5):723–30.

 20. Weir  MM,  Bell  DA,  Young  RH.  Grade  1  peritoneal
serous carcinomas: a report of 14 cases and compari-
son with 7 peritoneal serous psammocarcinomas and
19  peritoneal  serous  borderline  tumors.  Am  J  Surg
Pathol. 1998;22(7):849–62.

 21. Gould SJ. Full house. The spread of excellence from
Plato to Darwin. New York: Three Rivers Press; 1996.
 22. Tversky  A,  Kahneman  D.  Judgment  under  uncer-
tainty: heuristics and biases. Science. 1974;185(4157):
1124–31.

 23. Hastie R, Dawes RM. Rational choice in an uncertain
world.  The  psychology  of  judgment  and  decision
making. Thousand Oaks: Sage Publications; 2001.
 24. Gilovich  T,  Griffin  D,  Kahneman  D.  Heuristics  and
judgment.

biases.
Cambridge: Cambridge University Press; 2002.

the  psychology  of

intuitive

 25. Nathwani BN, Clarke K, Lincoln T, et al. Evaluation
of an expert system on lymph node pathology. Hum
Pathol. 1997;28(9):1097–110.

 26. Heckerman  DE,  Horvitz  EJ,  Nathwani  BN.  Toward
normative expert systems: Part I. The Pathfinder proj-
ect. Methods Inf Med. 1992;31(2):90–105.

 27. Koller  D,  Friedman  N.  Probabilistic  graphical  mod-
els: principles and techniques. Cambridge: MIT Press;
2009.

 28. Bartels  PH.  Future  directions  in  quantitative  pathol-
ogy: digital knowledge in diagnostic pathology. J Clin
Pathol. 2000;53(1):31–7.

 29. Montironi R, Cheng L, Lopez-Beltran A, Mazzucchelli
R, Scarpelli M, Bartels PH. Decision support systems
for  morphology-based  diagnosis  and  prognosis  of
prostate  neoplasms:  a  methodological  approach.
Cancer. 2009;115(13 Suppl):3068–77.

 30. Forrester  J.  If  p,  then  what?  Thinking  in  cases.  Hist

Hum Sci. 1996;9(3):1–25.

 31. Costa  J.  Is  clinical  systems  pathology  the  future  of
pathology? Arch Pathol Lab Med. 2008;132(5):774–6.
 32. Donovan  MJ,  Costa  J,  Cordon-Cardo  C.  Systems
pathology: a paradigm shift in the practice of diagnos-
tic and predictive pathology. Cancer. 2009;115 Suppl
13:3078–84.

 33. Rosai J. Why microscopy will remain a cornerstone of
surgical pathology. Lab Invest. 2007;87(5):403–8.
 34. Natkunam  Y,  Mason  DY.  Prognostic  immunohisto-
logic markers in human tumors: why are so few used
in clinical practice? Lab Invest. 2006;86(8):742–7.
 35. Ladanyi  M,  Chan  WC,  Triche  TJ,  Gerald  WL.
Expression profiling of human tumors: the end of sur-
gical pathology? J Mol Diagn. 2001;3(3):92–7.

 36. Beckman M. Tumor complexity prompts caution about
sequencing. J Natl Cancer Inst. 2006;98(24): 1758–9.

 37. Crawford  JM.  Original  research  in  pathology:  judg-
ment,  or  evidence-based  medicine?  Lab  Invest.
2007;87(2):104–14.

 38. Fischer AH. The evolution of tumor biology: seeking
a balance between gene expression profiling and mor-
phology studies. J Mol Diagn. 2002;4(1):65.

 39.  Moons  KG,  van  Es  GA,  Deckers  JW,  Habbema  JD,
Grobbee  DE.  Limitations  of  sensitivity,  specificity,
likelihood ratio, and bayes’ theorem in assessing diag-
nostic probabilities: a clinical example. Epidemiology.
1997;8(1):12–7.

Part II

Solutions Offered by Evidence-Based
Pathology and Laboratory Medicine

Evidence-Based Approach
to Evaluate Information Published
in the Pathology Literature
and Integrate It with Personal
Experience

Alberto M. Marchevsky and Mark R.  Wick

11

Keywords
Evidence-based pathology • Pathology literature evaluation • Evaluating
information  in  pathology  •  Pathology  interpretation  of  data  •  Personal
experience in pathology

This  chapter  explores  how  to  interpret  and
 evaluate information published in the pathology
literature  and  integrate  it  with  personal  experi-
ence using a systematic approach based on gen-
eral Evidence-Based Pathology (EBP) principles
[1, 2]. Previous chapters have described some of
the methodological problems encountered in the
current  pathology  literature  and  have  explained
basic concepts of EBP as a derivative of Evidence-
Based Medicine. Perusal through these materials
can certainly raise legitimate questions regarding
whether  the  “EBP  approach”  has  really  intro-
duced new concepts, a topic that is discussed in
more  detail  in  Chap.  2  [3–8].  As  reviewed  by
Drs. Costa and Whitaker, pathologists generally
believe  that  most  information  used  in  our  daily
practice  is  based  on  sound  observations,  the
results of evaluating tissue and other body sam-
ples  with  the  latest  analytical  methods,  and  the

A.M. Marchevsky (*)
Pulmonary and Mediastinal Pathology,
Department of Pathology and Laboratory Medicine,
Cedars-Sinai Medical Center, Los Angeles, CA, USA
and
David Geffen School of Medicine, University
of California, Los Angeles, CA, USA
e-mail: Alberto.Marchevsky@cshs.org

use of statistically significant data. It is probably
not  too  adventurous  to  estimate  that  many
 pathologists probably view EBP as the “repack-
aging” of information under the catchy “evidence-
based” logo. Yet, if we review the current literature
from an epistemological point of view and using
the systematic approach described in this chapter,
one can argue that the quality of future pathology
publications  could  be  enhanced  by  the  use  of
more precise methodology that explicitly lists the
objectives of each study, considers the limitations
resulting from the characteristics of the materials
being investigated in the development of conclu-
sions, and analyzes the results with an eye toward
developing information that is useful for the eval-
uation,  diagnosis,  and  treatment  of  individual
patients.  Greater  awareness  of  this  EBP-based
process  will  also  hopefully  assist  pathologists
planning  to  perform  future  studies  that  would
yield information that could be applicable for the
diagnosis of pathological specimens.

Epistemology  is  the  branch  of  philosophy
interested in the theory of knowledge [9–14]. It
promotes  the  analysis  of  the  nature  and  limita-
tions of various conceptual paradigms and obser-
vational  methods  used  for  the  acquisition  and
interpretation  of  new  information.  The  first

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_11, © Springer Science+Business Media, LLC 2011

189

190

A.M. Marchevsky and M.R. Wick

 sections  of  the  chapter  will  review  from  an
 epistemological standpoint the study designs that
are  generally  being  used  in  pathology  research
and  teaching,  as  exemplified  by  recent  publica-
tions in peer-reviewed journals, using a system-
atic process to evaluate the validity and clinical
applicability  of  the  results  and  conclusions  of
these  studies.  The  various  comments  are  not
intended  to  judge  on  the  quality  of  the  articles
selected for review, but to explore methodologi-
cal characteristics and details in an effort to eval-
uate the validity of the results of each study and
their applicability in current pathology practice.
The last section of the chapter will introduce the
problem  of  how  to  use  EBP  principles  to  inte-
grate  the  information  published  in  the  literature
with personal data and experience, a topic that is
described in more detail in Chap. 13 and 15.

EBP Guide to Readers: Is the
Information Valid and Applicable
to My Cases?

The  scientific  method  is  a  body  of  techniques
designed for knowledge acquisition, based on the
collection  of  data  through  observation,  experi-
mentation,  and  the  formulation  and  testing  of
hypotheses  [15–21].  The  goal  of  the  scientific
method is to seek the truth. However, knowledge
about  elusive  “truths”  frequently  evolves  as  a
result of an iterative process where new informa-
tion poses new questions, leading to the genera-
tion of new hypotheses that stimulate the collection
of additional data that update previous knowledge.
In  addition,  knowledge  is  influenced  by  beliefs
held as a result of tradition, education, and various
cultural,  psychological,  and  sociological  factors.
Beliefs  can  alter  the  perception  of  observations
and influence the interpretation of data resulting
in a variety of biases that can distort the validity of
presumably  scientific
information.  The  peer
review system has been designed to evaluate the
information  and  conclusions  being  presented  in
scientific  studies  in  an  effort  to  prevent  the
dissemination  of  erroneous  information  and
minimize the publication of biased and scientifi-
cally unsound information [20, 22–32]. However,

Table 11.1  Systematic evidence-based process to  evaluate
published information

Does the study include comprehensive and unbiased
background information?
Does the study list one or more clear hypothesis/es?
What study design is used?
Are the conditions of the study sufficient to test the
hypothesis?
Are the results of the study internally valid?
Does the study test for the external validity of the
results?
What is the evidence level of the study results?
What is the applicability of the study results for the
evaluation and diagnosis of my individual patients?

as  expert  reviewers  often  have
their  own
 preconceptions and biases, the fact that new infor-
mation has been reported in the peer review litera-
ture offers no absolute guarantee about its scientific
value, leaving the reader with the personal respon-
sibility to evaluate the validity of published data
[30]. In addition, practicing pathologists are likely
to read the literature with these two general ques-
tions in mind: do these conclusions apply to my
patients? and what can I learn from this study that
could  be  applied  to  the  evaluation  of  my  tissue
specimens or other laboratory samples?

The  systematic  EBP-based  process  listed  in
Table 11.1 can be useful to evaluate the quality
and clinical validity of the information published
in the peer review literature and other sources of
medical data and integrate it with personal expe-
rience  for  the  evaluation  of  tissue  samples  and
laboratory  specimens  from  individual  patients.
The  questions  listed  in  the  table  can  then  be
expanded to formulate the more specific queries
listed in other tables.

Does the Study Include Comprehensive
and Unbiased Background Information:
Narrative Reviews Versus Systematic
Literature Reviews

An initial step during the evaluation of the validity
of the content of pathology publications is to iden-
tify the methodology used for the selection of per-
tinent background information. Such   information
from previous literature is frequently used to  justify

11  Evidence-Based Approach to Evaluate Information Published in the Pathology Literature

191

the hypothesis being tested in a study, evaluate the
results,  formulate  conclusions,  and/or  integrate
them with previous knowledge. The methodology
used to select background information is particu-
larly  pertinent  when  evaluating  the  content  of
review  articles.  Unfortunately,  perusal  through
multiple articles in the recent pathology literature
shows that it is generally customary to use a highly
personalized  approach  to  the  selection  of  refer-
ences  and  background
information  [33–36].
Authors,  presumably  based  on  their  own  experi-
ence and professional judgment, pick and choose
selected  information  from  a  variable  number  of
references usually found in the Pubmed database
of  the  National  Library  of  Medicine  and  do  not
explain  why  certain  publications  were  included,
while others may have been excluded by design or
neglect.  As  discussed  in  previous  chapters,  this
unstructured  process  for  the  selection  of  back-
ground  information  does  not  necessarily  provide
an objective and comprehensive review process or
assure readers that additional studies that contra-
dict the conclusions of the present study and/or the
current beliefs of its authors were considered.

Systematic  reviews  are  research  summaries
that  use  explicit,  objective,  and  well-defined
search  criteria  to  perform  a  thorough  literature
search followed by critical appraisal of individ-
ual  studies  to  identify  valid  and  applicable  evi-
dence  in  multiple  databases  [32,  37–39].  The
Centre for Evidence-Based Medicine of Oxford
University  and  the  Cochrane  collaboration  sug-
gest that  systematic reviews include five sections:
background,  objectives,  methods  of  the  review,
results  and  conclusions.  They  also  recommend
seven steps for the preparation and maintenance
of   systematic  reviews,  as  shown  in  Table  11.2
[22,  40–46].  Somewhat  ironically,  not  even
 systematic  reviews  are  necessarily  uniform,  as
there are no widely agreed-upon sets of standards
for  the  production  of  systematic  reviews.  For
example, a recent review of 300 studies by Moher
et al. [47] found that different strategies are being
used  for  the  preparation  of  systematic  reviews
and not all are equally reliable.

Data  from  systematic  reviews  can  be  inte-
grated and analyzed with the statistical method of
meta-analysis, as discussed in Chap. 9.

Table  11.2  Seven  steps  suggested  by  the  Cochrane
Collaboration  for  the  preparation  and  maintenance  of
 systematic literature reviews

Formulating a problem
Locating and selecting studies
Critical appraisal of studies
  Evidence levels
Collecting data
Analyzing and presenting results
  Meta-analysis
Interpreting results
Improving and updating reviews

Does the Study List One or More Explicit
Hypotheses? What Is the Purpose
of the Study?

The  majority  of  original  contributions  in  the
pathology literature include one or more hypoth-
esis,  but  it  can  be  difficult  at  times  to  find  the
description of hypothesis or specific purposes of
the study in the text of a publication and to fully
understand  why  the  investigators  have  selected
particular  approaches  to  the  evaluation  of  their
pathologic  materials.  For  example,  the  recent
paper  by  Mahajan  et  al.  [48]  is  an  interesting
study describing gastric foveolar-type and other
types  of  dysplasia  in  patients  with  Barrett’s
esophagus. Reading the article using the system-
atic  approach  shown  in  Table  11.1  suggests  the
following  questions  and  answers:  (1)  Does  the
study include comprehensive and unbiased back-
ground information? It is difficult to answer this
question as a systematic literature review was not
performed.  (2)  Does  the  study  list  one  or  more
clearly formulated hypotheses? The article does
not list specific questions to be investigated. The
Abstract  describes  the  purpose  of  the  paper  as
“The prevalence, diagnostic criteria, and natural
history  of  gastric-type  Barrett’s  dysplasia  were
systematically  evaluated  in  1854  endoscopic
biopsies from a cohort of 200 consecutive Barrett’s
dysplasia  patients.”  The  Materials  and  Methods
section of the paper describes the process of find-
ing cases and how they were studied, but does not
include  explicit  questions  or  hypothesis  to  be
investigated.  The  lack  of  explicit  questions  or
hypotheses  to  be  investigated  may  appear  to

192

A.M. Marchevsky and M.R. Wick

pathology readers as an unnecessary or redundant
process, but often leads to questions that cannot
be answered by the readers and ambiguity in the
interpretation of data by pathologists other than
the authors of the study. For example, reading the
Materials  and  Methods  section  of  the  Mahajan
et al. paper, we learn that the study included all
cases of Barrett’s esophagus and dysplasia diag-
nosed at their institution during a particular time
span  using  clearly  spelled  out  criteria  for  the
intestinal variant of dysplasia in Barrett’s esopha-
gus. In contrast, it is less clear how the diagnosis
of gastric-type dysplasia was rendered by “simul-
taneous consensus agreement of the two gastro-
intestinal  pathologists  and  pathology  resident
authors.”  The  paper  does  include  a  table  with
diagnostic  criteria,  but  it  is  not  referred  to  in
Methods and is not referenced, so is difficult to
understand  whether  these  criteria  were  derived
before or after evaluation of the various diagnos-
tic criteria that were analyzed with statistics. This
ambiguity  can  lead  to  respectful  questions  such
as how did they diagnose their cases, or what is
their  “gold  standard”  other  than  themselves?
Indeed,  in  our  opinion,  this  diagnostic  process
raises the suspicion of a circular reasoning pro-
cess that is not all that unusual in pathology pub-
lications: diagnoses are rendered because lesions
look in a certain way to authors who then evalu-
ate the prevalence of specific features in lesions
that  they  classified  in  a  certain  manner.  In  con-
trast, the formulation of explicit tasks, hypothe-
sis, or patient-centered questions, as proposed by
EBM and EBP advocates, could have precluded
some of these problems and perhaps even improve
on the readability and comprehensiveness of the
study. For example, the methodology could have
been  structured  with  definitions  and  hypothesis
that could have organized the information as fol-
lows:  (1)  define  foveolar-type  dysplasia  in  the
presence of explicit histopathologic criteria from
the literature and not as “unequivocal neoplastic
epithelium  confined  to  the  luminal  side  of  the
basement membrane,” a definition subject to vari-
able interpretation by readers. (2) Define the cri-
teria for grading dysplasias, with references. (3)
Discuss  whether  foveolar-type  dysplasia  and
mixed foveolar-type dysplasias should be graded

in  a  similar  or  distinct  manner  from  the  more
common  form  of  intestinal-type  dysplasia.  (4)
Test with kappa statistics whether the classifica-
tion and/or grading based on the explicit criteria
is reproducible among all authors, including the
residents who are probably not experts in gastro-
intestinal  pathologists.  (5)  Divide  the  data  into
three groups: pure foveolar-type dysplasia, gastric-
type dysplasia, and both. (6) Evaluate for each of
these  three  groups  how  many  cases  evolved  to
higher  grades  of  dysplasia  and  carcinoma.  (7)
Evaluate by dysplasia type and grade how many
cases evolved to higher grades of  dysplasia and
carcinoma. (8) Evaluate the time that it took for
the development of carcinoma, by dysplasia type
and  grade.  Evaluation  of  each  of  these  specific
tasks could have precluded some of the questions
suggested in the following section of this chapter
or suggest specific questions for future research,
should the currently available clinico-pathologic
materials be insufficient to provide answers to all
six topics.

Recent  studies  have  explored  the  use  of  this
proposed approach where specific questions are
formulated and the study organized to systemati-
cally answer them [49–51]. They are discussed in
more detail in Chap. 10. It remains to be explored
whether  a  more  structured  study  design  that
includes  a  list  of  specific  tasks  or  questions
improves on the quality and readability of pathol-
ogy publications.

What Study Design Is Used?

It can be helpful to understand the type of study
design used by the authors of a particular publi-
cation in order to estimate the potential validity
of its results and conclusions [52–55]. In general,
there are three categories of biomedical research:
observational,  experimental,  and  evaluation  of
treatment  effects  [55].  In  addition,  investigators
can use meta-analysis to aggregate the results of
different studies and reconcile differences.

Experimental pathology studies are generally
designed  using  adequate  control  groups  and
tightly  controlling  the  various  experimental
 conditions  in  order  to  decrease  the  influence  of

11  Evidence-Based Approach to Evaluate Information Published in the Pathology Literature

193

covariates  in  the  statistical  analysis  of  the  data
[53,  54,  56].  However,  most  publications  in  the
pathology  literature  are  observational  and  are
designed  to  correlate  the  relationship  between
laboratory  findings  or  specific  pathologic  find-
ings  detected  with  histopathology,  immunohis-
tochemistry,  molecular,  and  other  methods  and
some  aspect  of  disease,  such  as  progression,
recurrence,  or  death  [57].  Observational  studies
can suggest significant associations between vari-
ables,  but  cannot  generally  used  to  determine
cause and effect [10, 12]. The results of observa-
tional studies are often influenced by covariates
that are not independent of each other [58, 59].

Different study designs can be used in obser-
vational studies (Table 11.3) [57, 60, 61]. Cohort
studies  are  usually  used  in  epidemiology,  but
have been used in pathology to describe the find-
ings that develop prospectively or retrospectively
over a period of time in a population exposed to
putative carcinogens or other environmental vari-
ables [55, 62, 63]. Time-series studies and case-
control  studies  are  more  commonly  used  in
anatomic  pathology  [64].  Time-series  studies
investigate the correlation of certain findings with
the  development  of  others  at  various  points  in
time.  The  data  are  best  collected  prospectively,
but can be retrospective. In the more commonly
used case-control study design format, the cases

Table 11.3  Types of study designs

Observational studies
  Cohort study
  Prospective
  Retrospective
  Time series

  Case-control study

  Nested case-control
  Cross-sectional study
Experimental studies
  Case-control
Treatment studies
Randomized controlled trials
  Double-blind randomized trial
  Single-blind randomized trial
  Nonblind trial
Nonrandomized trial

are divided into case-subjects and controls. The
latter  study  design  format  can  be  structured  as
nested  case-control  studies  where  the  data  are
stratified into various subgroups.

It  is  our  impression  that  the  type  of  study
design  is  not  customarily  spelled  out  in  most
pathology publications. For example, if we query
as to the study design of the Mahajan et al. [48]
paper,  we  are  not  told  specifically  but  surmise
that  is  probably  a  time-series  type  study  where
certain  features  were  described  in  a  population
that was investigated using “longitudinal follow-up
information”  [48].  Yet,  the  “statistical  analysis”
section of the paper does not evaluate whether the
cohort  had  enough  subjects  in  each  category  to
derive  forecasts  or  explain  to  nonstatisticians
whether the statistical tests used to evaluate the
results  were  appropriate  for  time-series  type  of
data. The lack of this information leads nonstatis-
ticians to become skeptical whether the study had
enough subjects in the various dysplasia classes
to  reach  clinically  significant  conclusions.  For
example,  the  study  included  “200  consecutive
Barrett’s  dysplasia  patients”  followed  for  vari-
able periods of time and up to 8 years. Data were
collected  retrospectively  and  prospectively.  The
cohort included only 11 cases of pure foveolar-
type dysplasia and 19 with mixed dysplasia types,
while the majority (n = 170) of cases showed find-
ings of the more common intestinal-type dyspla-
sia. Twelve of the 30 patients with pure or mixed
gastric-type dysplasia progressed to higher grade
of  dysplasia,  while  13  did  not  progress  and  5
were lost to follow up. Five of the eleven of the
patients that progressed had mixed type of dys-
plasia. What have we learnt about the prognostic
significance  of  pure  gastric-type  dysplasia  in
patients  with  Barrett’s  esophagus  that  we  could
use in our practice? The study probably supports
that  patients  with
our  previous  knowledge
Barrett’s  esophagus  and  dysplasia  are  at  a  high
risk of developing malignancy. Is the risk higher
for patients with foveolar-type dysplasia than in
patients  with  mixed  dysplasia  or  intestinal-type
dysplasia? The evidence in the paper is probably
inconclusive  to  answer  this  question.  Does  the
dysplasia  grade  predict  the  likelihood  of  the
development of malignancy or when a patient is

194

A.M. Marchevsky and M.R. Wick

most  likely  to  be  diagnosed  with  malignancy?
The  study  does  not  answer  this  question.
Although  the  study  describes  a  carefully  evalu-
ated group of patients with dysplasia and Barrett’s
esophagus,  the  results  do  not  provide,  in  our
opinion, best evidence supporting the conclusion
that  “the  recognition  of  Barrett’s  gastric-type
dysplasia and use of the proposed grading criteria
should promote better diagnostic classification of
the Barrett’s neoplasm spectrum.”

As  suggested  in  the  previous  section  of  this
chapter, the use of a more systematic study design
that  included  a  more  explicit  description  of  the
goals  of  the  study  and  explored  which  study
design was most appropriate to evaluate various
problems  may  have  precluded  some  of  these
questions and perhaps temper the conclusions of
this study.

Are the Conditions of the Study
Sufficient to Test the Hypothesis?

Clinico-pathologic studies in anatomic pathology
are frequently difficult to conduct as they are fre-
quently designed to evaluate tissue samples with
unusual  conditions  that  are  hard  to  collect  and
involve the use of expensive and time-consuming
tests. It is somewhat surprising that, in contrast to
the painstaking attention to detail that is routinely
devoted to the description of the technical analy-
sis of the samples, little publication space is often
devoted  to  discussing  whether  the  pathologic
materials  are  sufficient  in  sample  size  and  the
conditions of a study are adequate to test specific
hypotheses.  In  addition,  there  is  a  tendency  to
assume that because additional findings are found
with new methods in various lesions, these find-
ings are of clinical value. These problems are par-
ticularly  evident  in  studies  that  evaluate  the
diagnostic validity of new and sophisticated diag-
nostic methods. For example, the recent study by
Brunelli et al. [65] evaluated and beautifully illus-
trated  the  “Diagnostic  usefulness  of  fluorescent
cytogenetics in differentiating chromophobe renal
cell  carcinoma  from  renal  oncocytoma.”  The
study  evaluated  11  chromophobe  renal  carcino-
mas and 12 renal oncocytomas “showing  different

clinical  outcomes.”  The  investigators  concluded
that  “the  study  demonstrates  that  indeed  FISH
performed on formalin-fixed, paraffin-embedded
tissue  can  provide  clinically  useful  information
more reliably than karyotyping of most of these
tumors.” Analyzing the study using the epistemo-
logical approach by using the questions suggested
in  Table  11.1,  we  learn  that  the  study  does  not
include a systematic review of the literature, list-
ing of specific hypothesis, or a specific descrip-
tion  of  the  study  format.  As  in  the  study  by
Mahajan, the lack of a description of the explicit
purposes of the study can lead to ambiguities. For
example,  readers  could  argue  that  100%  of  the
cases  were  diagnosed  as  either  chromophobe
renal  cell  carcinoma  or  renal  oncocytoma  using
histopathology, so what is the diagnostic advan-
tage  of  using  FISH  or  karyotyping  in  the  cases
used to derive conclusions? Where is the evidence
that FISH and karyotyping improved on the spec-
ificity  and/or  sensitivity  of  the  differential  diag-
nosis between chromophobe renal cell carcinoma
and renal oncocytoma? In addition, as the study
does  not  provide  correlation  between  the  FISH,
karyotype, or other findings such as clinical stage,
prognosis, or other clinical data, what is the evi-
dence that the findings “provide clinically useful
information”? The paper does not provide data to
answer  these  questions  and  perhaps,  more  trou-
blesome, does not elaborate on these issues that
are probably of interest to practicing pathologists
in its discussion. Indeed, although FISH is appar-
ently better than karyotyping for the evaluation of
these  neoplasms,  “uncertainty  remains  as  to
whether  variations  in  tumor  karyotype  can  pro-
duce confounding results that bring into question
the usefulness of FISH analysis in distinguishing
between these 2 tumor types.”

The EBP process being described in this chap-
ter could have obviated critiques to a paper that
was conducted with exquisite attention to labora-
tory  details.  For  example,  the  study  could  have
been  structured  around  four  explicit  questions:
(1) What are the abnormalities that can be found
with  FISH  in  well-characterized  cases  of  chro-
mophobe renal cell carcinoma and renal oncocy-
toma? (2) What are the abnormalities that can be
found  with  karyotyping  in  well-characterized

11  Evidence-Based Approach to Evaluate Information Published in the Pathology Literature

195

cases of chromophobe renal cell carcinoma and
renal oncocytoma? (3) Can FISH or karyotyping
improve on the specificity of a differential diag-
nosis  between  renal  oncocytoma  and  chromo-
phobe renal cell carcinoma in difficult cases? (4)
Which  are  the  FISH  and/or  karyotyping  abnor-
malities  that  are  most  helpful  to  improve  the
specificity  of  this  differential  diagnosis  in  diffi-
cult  cases?  Formulations  of  specific  questions
such as these may have suggested to the authors
the need to include in the study cases that were
particularly  difficult  to  diagnose  and  the  use  of
some external diagnostic “gold standard” for the
diagnosis  of  chromophobe  renal  cell  carcinoma
such as disease progression or metastasis or the
opinion of an external panel of experts.

Are the Results of the Study Internally
Valid?

It is beyond the scope of this chapter to discuss
in detail the various methodological details that
can  influence  the  results  of  observational  stud-
ies. Table 11.4 suggests seven questions that can
help  readers  evaluate  whether  the  results  of  a
study  are  supported  by  the  data.  Most  of  the
answers to these questions are discussed in the
review of the indications and limitations of vari-
ous statistical tests in Chap. 4. An issue that is
usually not explored in clinico-pathologic stud-
ies  in  the  pathology  literature  is  whether  the
number  of  samples  of  various  conditions  pro-
vides sufficient sample sizes to investigate vari-
ous hypotheses. As explained in Chap. 8, power
analysis  can  be  used  to  estimate  from  prelimi-
nary data the  optimal number of cases to exclude

Table  11.4  Specific  queries  to  evaluate  the  internal
validity of a study

What study design was used?
Is it a prospective or retrospective study?
Were control cases selected appropriately?
Are the sample sizes adequate?
Was power analysis performed?
Were the findings evaluated with the appropriate
statistical tests?
Do the findings support the conclusions of the study?

the  possibility  that  negative  results  are  not
 significant  to  a  power  of  0.80.  Application  of
this  methodology  to  the  study  of  conditions,
such as thymomas, that are associated with the
potential to recur or metastasize many years after
initial diagnosis can yield surprising results. For
example,  a  recent  study  with  meta-analysis  of
almost  1,000  thymomas  estimated  that  over
7,000 cases would be needed to conduct a study
valid to a power of 0.80 [66].

Does the Study Test for the External
Validity of the Results?

The  conclusions  of  observational  studies  in
pathology are validated by analyzing the results
collected  from  relatively  small  samples  with
descriptive statistical methods. The samples are
usually samples of convenience which are gener-
ally not selected randomly from the entire popu-
lation  of  subjects  with  a  particular  entity  of
interest  [1].  The  adequacy  of  sample  sizes  are
usually not estimated. Observational studies per-
formed under these conditions cannot adequately
estimate  whether  their  results  are  applicable  to
subjects in other population groups. These prob-
lems can be minimized by collecting large sam-
ples from multiple institutions located in different
states  or  countries.  Another  approach  is  to  test
the results of a study with another “test sample”
composed  of  specimens  that  were  not  used  to
derive  the  conclusions  of  the  study  [1,  2,  24].
These  test  samples  can  be  collected  retrospec-
tively by dividing cases into two groups, training
and testing or validation sets prior to the perfor-
mance  of  the  study.  The  results  obtained  from
the training set are tested with “unknown” cases
in the validation group.

The question as to whether external validation
of  classification  results  is  really  needed  in  well-
conducted  observational  studies  was  systemati-
cally explored in an analysis of the classification
of individual lung cancer cell lines based on DNA
methylation markers analyzed with two multivari-
ate  statistical  tests,  linear  discriminant  analysis
and artificial neural networks [67]. The conditions
of this study were better controlled than the usual

196

A.M. Marchevsky and M.R. Wick

clinico-pathologic study as the cell lines had been
previously well characterized by other studies and
therefore there was no question about the correct
diagnoses. In addition, classification was rendered
using  the  “objective”  process  of  collecting  data
with molecular methods and evaluated with multi-
variate statistical methods. Initially, the data from
all  cell  lines  were  included  in  one  data  set  with
similar number of cell lines in the two diagnostic
categories. All cell lines were classified correctly
in this data set by using selected DNA methyla-
tion  markers  and  artificial  neural  networks,  sug-
gesting that this technology allows for an objective
diagnosis of these cell lines in all cases. However,
this conclusion would have been fraught with the
circular  reasoning  problem  described  above  of
classifying cases according to the findings in cer-
tain  cases  and  then  concluding  that  certain  vari-
ables  contributed  to  classification  of  the  same
samples. Indeed, when the data were divided into
training and test cases and organized into ten dif-
ferent combinations of randomly selected paired
training and test sets, the results varied consider-
ably  from  the  initial  conclusion.  The  number  of
correctly  classified  test  cell  lines  dropped  from
100%  to  62–87%,  according  to  which  combina-
tions of training and test sets were analyzed. The
latter results suggested that although the technol-
ogy was promising as a method to classify these
cell lines on the basis of DNA methylation mark-
ers and multivariate statistical tests, larger popula-
tions of cell lines and/or perhaps other molecular
data were probably needed to derive more robust
classifications models that would apply more con-
sistently  to  test  cases.  If  this  study  would  have
been performed according to the study format that
is currently being used in most pathology studies,
evaluating a particular test or tests using all cases
in one data set, it would have concluded that cell
lines can be diagnosed with 100% accuracy using
DNA  methylation  markers  and  artificial  neural
network technology. The contrast in results under-
scores  the  need  to  validate  the  conclusions  of
studies  proposing  new  diagnostic  criteria  using
validation or test data that were not used to derive
the classification features.

External  validation  of  results  is  currently
 seldom  used  in  the  pathology  literature  and  is

increasingly being used in the oncology and other
literature  [68–70].  New  diagnostic  criteria  are
usually proposed on the basis of the analysis of a
particular  group  of  cases  and  it  is  assumed  that
other  pathologists  evaluating  other  specimens
could reach similar conclusions, without testing
this  assumption.  In  our  view,  this  practice  can
result  in  considerable  interobserver  variability
diagnostic problems that increase variability and
confusion in the literature. For example, a recent
study  reviewing  the  prognosis  of  patients  with
idiopathic  interstitial  lung  disease  showed  that
the survival of patients with a diagnosis of usual
interstitial pneumonia (UIP) in the seven studies
where  the  survival  of  these  patients  was  com-
pared with the prognosis of nonspecific intersti-
tial  pneumonia  (NSIP)  cases  ranged  from  11%
(4.4–24.9  95%  confidence  interval)  to  58%
(44.6–70.3)  [71].  The  survival  proportions  of
NSIP  patients  ranged  from  39%  (23.3–57.3)  to
100%  (85.1–100).  The  marked  variability  in
prognosis  may  be  related  to  differences  in  the
clinical  characteristics  of  patients  and  variable
therapeutic  modalities  in  various  international
hospitals, but are so considerable that they sug-
gest  that  patients  with  UIP  and  NSIP  are  not
being consistently diagnosed as such by different
expert pulmonary pathologists. Indeed, a study of
interobserver  variability  in  the  diagnosis  of
chronic  diffuse  lung  diseases  with  kappa  statis-
tics has shown only moderate agreement between
different investigators diagnosing UIP and NSIP,
with  kappa = 0.590  and  kappa = 0.420,  respec-
tively [72]. There is a need for better diagnostic
criteria  for  the  differential  diagnosis  between
these two conditions which is applicable to dif-
ferent  populations  of  patients  diagnosed  by  dif-
ferent pathologists.

Chapter  7  discusses  the  topic  of  the  external

validity of study results in more detail.

What Is the Evidence Level of the Study
Results?

As discussed in previous chapters, the standard-
ization of various medical procedures, evaluation
of  “quality”  of  care,  and  the  “evidence-based”

11  Evidence-Based Approach to Evaluate Information Published in the Pathology Literature

197

rubric  are  increasingly  important  processes  in
modern  Medicine  [73,  74].  Many  medical  spe-
cialties  currently  sponsor  the  development  of
“evidence-based”  practice  guidelines,  but  as  a
group  pathologists  have  been  slow  to  adopt  a
similar approach.

EBM advocates have also promoted the use of
various “evidence levels” (ELs) schemes, gener-
ally  aimed  at  an  assessment  of  the  validity  and
clinical  applicability  of  therapeutic  procedures
[74,  75].  For  example,  a  recent  book  by  Straus
et al. [74] codifies several ELs with level I being
the  best.  Level  1a  is  the  label  for  systematic
reviews with homogeneity of randomized clinical
trials (RCTs); level 1b refers to individual RCTs
with narrow confidence intervals; level 2a denotes
systematic homogeneous reviews of cohort stud-
ies;  level  2b  includes  individual  cohort  studies
including  “low-quality”  RCTs  (e.g.,  with  <80%
follow-up); level 3a refers to systematic homoge-
neous reviews of case-control studies; level 3b is
principally represented by individual case-control
studies;  level  4  denotes  case  series  with  poor-
quality  cohort-based  and  case-control  studies;
and level 5 is the EL represented by “expert opin-
ion”  without  explicit  critical  appraisal  or  first-
hand generation of data (“first principles”). Other
comparable EL systems have been published by
the  Cochrane  collaboration  and  similar  groups
[22, 40–45]. Generally, only information obtained
by RCTs, or systematic review with meta-analy-
sis  of  homogeneous  case-control  studies,  has
been  considered  as  evidence  in  levels  1  and  2.
Data derived from individual case-control assess-
ments are usually considered to be in level 3 or
higher,  denoting  lower  quality,  because  it  has
been  shown  that  such  observational  studies  are
affected negatively by sources of bias that result
from  patient  selection,  sample  size,  distribution
of data, lack of independent validation, and oth-
ers. Ironically, RCT often use pathologic diagno-
ses as rigid classifiers in their statistical analysis.
However, such lesions are diagnosed pathologi-
cally  by  different  pathologists  or  by  “central
pathology review” on the basis of criteria previ-
ously published in EL 3 or “worse” literature.

The ELs that are used for evaluation of clinical
treatment  protocols  generally  pose  a  somewhat

unfair proposition for pathologists [76]. Because
scientific  studies  in  our  specialty  do  not  lend
themselves to the use of RCT designs, clinical EL
systems  essentially  consign  most  pathology  lit-
erature  to  EL  3  or  worse.  However,  the  notion
that  pathologist-generated  literature  is,  at  best,
mediocre undervalues the many contributions of
our specialty to the body of medical knowledge.
Indeed, well-designed case series and even some
seminal case reports published in the pathology
literature have described new diseases and clini-
co-pathological entities.

The  classification  of  most  information  pub-
lished in the pathology literature as EL 3 or higher
using clinical EL scales seems to have little rele-
vance to the particulars of our professional disci-
pline and may act as a disincentive for pathologists
to improve the quality of the design and interpre-
tation of future studies. The notion that our litera-
ture  provides  at  best  mediocre  information
markedly undervalues the many contributions of
pathology  to  medical  knowledge.  Indeed,  many
case  reports  and  case  series  in  pathology  have
provided the initial descriptions of new diseases
and  clinico-pathologic  entities.  In  addition,  if
one  accepts  the  proposition  that  studies  by
pathologists are only likely to produce level 3 or
worse  evidence,  there  is  little  incentive  to
improve on the use of sound EBP principles to
improve on the design quality of future studies.
We  have  proposed  a  scale  of  ELs  for  publica-
tions  in  pathology  and  laboratory  medicine
which takes into account the various issues dis-
cussed in this book and is shown in Table 11.5
[76]. This scale classified as level I the evidence
derived with well-designed case-control studies
with external validation of results using prospec-
tive validation sets collected at other institutions,
meta-analysis  of  level  2  studies,  and  expert
 recommendations  based  on  the  latter.  Other
types  of  observational  studies  are  classified  as
 providing ELs 2–5. There is a need for profes-
sional societies such as the College of American
Pathologists (CAP), Association of Directors of
Anatomic  and  Surgical  Pathology  (ADASP),
and others to develop more comprehensive and
authoritative EL scales to evaluate the quality of
evidence in the pathology literature.

198

A.M. Marchevsky and M.R. Wick

Table 11.5  Proposed scale of evidence levels for publi-
cations in pathology and laboratory medicine

Level 1   Case-control studies with external validation
of results, using prospective validation data
sets from other institutions
Meta-analyses of level 2 studies
 “Expert” recommendations based on
meta-analyses of level 2 or 3 studies
Level 2   Case-control studies with validation of results,

using prospective validation data-sets from
the same institution
Meta-analyses of level 3 studies
 “Expert” recommendations based on a
systematic review of literature without formal
meta-analyses

Level 3   Case-control studies with validation of results

using retrospective validation data sets from
the same institution

Level 4  Case-control studies without validation
Level 5   Case series without controls, or individual

case reports

What Is the Applicability of the Study
Results for the Evaluation
and Diagnosis of Individual Patients?
Guide to the Integration of Best
Available Evidence from the Literature
with Personal Experience

There is little current pathology literature explor-
ing the topic of how to integrate the best available
evidence  with  personal  experience  using  EBP
principles. As discussed before, it is well known
that  diagnostic  criteria  and  laboratory  details
developed at other institutions may not be auto-
matically  applicable  to  others.  Indeed,  it  is  cur-
rently required by the CAP accreditation process
that each institution issues its own technical man-
ual  for  the  performance  and  interpretation  of
laboratory tests, rather than using external docu-
ments  without  review  and  adaptation  to  local
practices [77–81].

Table 11.6 suggests some specific queries that
can  be  used  to  guide  pathologists  integrate  best
available  evidence  from  the  literature  with  per-
sonal experience. The process involves  performing
a systematic review of pertinent literature, identi-
fication of best available evidence and estimation
of ELs, accrual of cases of personal experience,
and  test  whether  the  recommendations  in  the

Table  11.6  Specific  queries  to  evaluate  what  is  the
 applicability of the results of a study for the diagnosis and
prognostication of individual patients

What prior knowledge and beliefs do I have regarding
the topic being investigated in this study?
How can I use the results of the study for the patho-
logic evaluation of my patients?
Can our laboratory perform the tests reported in the
study?
What is the sensitivity and specificity of the results?
What are the positive and negative predictive values of
the results?
What is the incremental diagnostic value of the
proposed new tests?
How accurate is the prognostic information being
offered by the results of the study?
How useful is the prognostic information offered by the
results of the study for the treatment of our patients?

literature  apply  to  local  cases  and  under  what
conditions.  This  EBP-based  methodology  was
recently  applied  in  a  study  “Evidence-based
evaluation  of  the  risks  of  malignancy  predicted
by thyroid fine needle aspiration biopsies” [82].
A  National  Cancer  Institute  (NCI)  “Thyroid
Fine-Needle  Aspiration  (FNA)”  State  of  the
Science Conference proposed in 2008 standard-
ized  nomenclature  and  “risks  of  malignancy”
associated  with  various  diagnostic  categories.
Six categories were proposed for the diagnosis of
thyroid FNAs: benign, follicular lesion of unde-
termined  significance  (FLUS),  follicular  neo-
plasm, suspicious for malignancy, malignant, and
nondiagnostic [7]. With the exception of nondi-
agnostic, each category in the proposed thyroid
FNA classification scheme was associated with a
“risk of malignancy” derived from data collected
from  the  literature  [8–12].  In  the  NCI  publica-
tions,  the  risks  of  malignancy  reported  to  be
associated  with  the  benign,  FLUS,  neoplasm
(follicular  neoplasm  or  Hurthle  cell  neoplasm),
suspicious  for  malignancy,  and  malignant  cate-
gories were <1, 5–10, 20–30, 50–75, and 100%,
respectively [7]. We performed a systematic lit-
erature review and evaluated our experience with
879  thyroid  FNA.  Interestingly,  the  manuscript
was initially written using several specific ques-
tions as explained above, but this did not conform
to  editorial  guidelines.  Systematic  literature

11  Evidence-Based Approach to Evaluate Information Published in the Pathology Literature

199

review  yielded  mostly  EL  3  information  with
malignancy risks calculated on the basis of surgi-
cal  follow-up.  As  clinical  findings  other  than
FNA results are considered during the selection
of  patients  with  a  thyroid  nodule  that  should
undergo thyroidectomy, we calculated our malig-
nancy  risks  using  other  denominators,  such  as
total number of cases, patients with FNA follow-
up, and others. Analysis of our data yielded vari-
ous  relative  risk  estimates  and  showed  that,  as
suspected, the risk estimates proposed by the NCI
group  of  experts  probably  overestimated  the
probability  of  thyroid  malignancy  for  patients
with FNA diagnoses of “benign” and “follicular
lesions of undetermined significance.” In contrast
for patients with FNA diagnosed as malignant or
suspicious  for  malignancy,  the  malignancy  risks
in  our  population  were  similar  to  those  in  the
literature. Our data also showed that in our patient
population, the FNA diagnoses could be grouped
from five categories other than nondiagnostic to
three  diagnostic  categories,  “benign,”  “FLUS +
neoplasm,” and “suspicious + malignant,” which
provided nonoverlapping risks of malignancy. A
more recent study showed that the three-category
diagnostic scheme for thyroid FNA also decreases
interobserver diagnostic variability among differ-
ent cytopathologists.

Meta-analysis can also be used to integrate the
results  from  the  literature  and  personal  experi-
ence, as exemplified by recent studies of thymo-
mas, discussed in more detail in Chap. 15.

References

  1.  Marchevsky AM. Evidence-based medicine in pathol-
ogy:  an  introduction.  Semin  Diagn  Pathol.  2005;22:
105–15.

  2.  Marchevsky  AM,  Wick  MR.  Evidence-based  medi-
cine, medical decision analysis, and pathology. Hum
Pathol. 2004;35:1179–88.

  3.  Guerette PH. Managed care: cookbook medicine, or
quality, cost-effective care? Can Nurse. 1995;91:16.
  4.  Holm  RP.  Cookbook  medicine.  S  D  Med.  2009;

62:371.

  7. Steinberg KE. Cookbook medicine: recipe for disaster?

J Am Med Dir Assoc. 2006;7:470–2.

  8.  Sackett DL, Rosenberg WM, Gray JA, et al. Evidence
based  medicine:  what  it  is  and  what  it  isn’t.  BMJ.
1996;312:71–2.

  9.  Bibace R, Watzlawik M. Epistemology and values are

interrelated. Fam Med. 2009;41:690–1.

 10. Dellavalle  RP,  Freeman  SR,  Williams  HC.  Clinical
evidence epistemology. J Invest Dermatol. 2007;127:
2668–9.

 11. Dunn  M,  Ives  J.  Methodology,  epistemology,  and
empirical  bioethics  research:  a  constructive/ist  com-
mentary. Am J Bioeth. 2009;9:93–5.

 12. Michel  LA.  The  epistemology  of  evidence-based

medicine. Surg Endosc. 2007;21:145–51.

 13. Pena A. Personal epistemology and uncertainty. Fam

Med. 2009;41:691–3.

 14. Rodgers  JL.  The  epistemology  of  mathematical  and
statistical  modeling:  a  quiet  methodological  revolu-
tion. Am Psychol. 2010;65:1–12.

 15. Defining the scientific method. Editorial. Nat Methods.

2009;6:237.

 16. Haig  BD.  How  to  enrich  scientific  method.  Am

Psychol. 2008;63:565–6.

 17. Haig  BD.  Scientific  method,  abduction,  and  clinical

reasoning. J Clin Psychol. 2008;64:1013–8.

 18. Kenkel JM. Revisiting the scientific method. Aesthet

Surg J. 2009;29:167–8.

 19. Levins R. Whose scientific method? Scientific methods
for a complex world. New Solut. 2003;13:261–74.
 20. Noseda  M,  McLean  GR.  Where  did  the  scientific

method go? Nat Biotechnol. 2008;26:28–9.

 21. Satava RM. The scientific method is dead – long live
the  (new)  scientific  method.  Surg  Innov.  2005;12:
173–6.

 22. Cundiff  DK.  Evidence-based  medicine  and  the
Cochrane Collaboration on trial. MedGenMed. 2007;
9:56.

 23. DeAngelis CD, Thornton JP. Preserving confidential-
ity in the peer review process. JAMA. 2008;299:1956.
 24. Marchevsky AM. The application of special technolo-
gies in diagnostic anatomic pathology: is it consistent
with  the  principles  of  evidence-based  medicine?
Semin Diagn Pathol. 2005;22:156–66.

 25. Miracle  VA.  The  peer  review  process.  Dimens  Crit

Care Nurs. 2008;27:67–9.

 26. Molassiotis  A,  Richardson  A.  The  peer  review  pro-
cess  in  an  academic  journal.  Eur  J  Oncol  Nurs.
2004;8:359–62.

 27. Moore  KN.  Keeping  up  journal  integrity:  the  peer-
review  process.  J  Wound  Ostomy  Continence  Nurs.
2005;32:3–5.

 28. Mullins  CD.  The  peer-review  process:  gifts  of  time.

Clin Ther. 2005;27:1962.

 29. Scanes  CG.  The  peer-review  process.  Poult  Sci.

  5.  Leape L. Are practice guidelines cookbook medicine?

2008;87:1–2.

J Ark Med Soc. 1989;86:73–5.

  6.  Parmley  WW.  Practice  guidelines  and  cookbook
medicine  –  who  are  the  cooks?  J  Am  Coll  Cardiol.
1994;24:567–8.

 30. Smith R. Peer review: a flawed process at the heart of
science and journals. J R Soc Med. 2006;99:178–82.
 31. Vettore MV. The peer review process in health jour-

nals. Cad Saúde Pública. 2009;25:2306–7.

200

A.M. Marchevsky and M.R. Wick

 32.  Wright RW, Brand RA, Dunn W, et al. How to write a
systematic  review.  Clin  Orthop  Relat  Res.  2007;
455:23–9.

 33. Adeniran  AJ,  Tamboli  P.  Clear  cell  adenocarcinoma
of  the  urinary  bladder:  a  short  review.  Arch  Pathol
Lab Med. 2009;133:987–91.

 34. Laurini JA, Carter JE. Gastrointestinal stromal tumors:
a  review  of  the  literature.  Arch  Pathol  Lab  Med.
2010;134:134–41.

 35. Popescu OE, Landas SK, Haas GP. The spectrum of
eosinophilic  cystitis  in  males:  case  series  and  litera-
ture review. Arch Pathol Lab Med. 2009;133:289–94.
 36. Ueng  SH,  Mezzetti  T,  Tavassoli  FA.  Papillary  neo-
plasms of the breast: a review. Arch Pathol Lab Med.
2009;133:893–907.

 37. Chen  TH,  Li  L,  Kochen  MM.  A  systematic  review:
how  to  choose  appropriate  health-related  quality  of
life (HRQOL) measures in routine general practice?
J Zhejiang Univ Sci B. 2005;6:936–40.

 38. Deenadayalan Y, Grimmer-Somers K, Prior M, et al.
How  to  run  an  effective  journal  club:  a  systematic
review. J Eval Clin Pract. 2008;14:898–911.

 39. Hunt  DL,  Haynes  RB.  How  to  read  a  systematic

review. Indian J Pediatr. 2000;67:63–6.

 40. What  does  the  Cochrane  Collaboration  say  about
adherence  to  evidence-based  practice  recommenda-
tions? Physiother Can. 2009;61:116.

 41. Clarke  M.  The  Cochrane  Collaboration  and  the
Cochrane  Library.  Otolaryngol  Head  Neck  Surg.
2007;137:S52–4.

 42. Overman VP. The Cochrane collaboration. Int J Dent

Hyg. 2007;5:62.

 43. Scherer RW. 2.2 Evidence-based health care and the
Cochrane  Collaboration.  Hum  Exp  Toxicol.  2009;
28:109–11.

 44. Summerskill W. Cochrane Collaboration and the evo-

lution of evidence. Lancet. 2005;366:1760.

 45. Tanjong-Ghogomu E, Tugwell P, Welch V. Evidence-
based medicine and the Cochrane Collaboration. Bull
NYU Hosp Jt Dis. 2009;67:198–205.

 46. Winkelstein Jr W. The remarkable Archie: origins of the
Cochrane Collaboration. Epidemiology. 2009;20:779.

 47. Moher D, Tsertsvadze A, Tricco AC, et al. A system-
atic  review  identified  few  methods  and  strategies
describing  when  and  how  to  update  systematic
reviews. J Clin Epidemiol. 2007;60:1095–104.

 48. Mahajan D, Bennett AE, Liu X, et al. Grading of gas-
tric  foveolar-type  dysplasia  in  Barrett’s  esophagus.
Mod Pathol. 2010;23:1–11.

 49. Gupta R, Dastane A, McKenna Jr RJ, et al. What can
we learn from the errors in the frozen section diagno-
sis  of  pulmonary  carcinoid  tumors?  An  evidence-
based approach. Hum Pathol. 2009;40:1–9.

 50. Gupta  R,  McKenna  Jr  R,  Marchevsky  AM.  Lessons
learned from mistakes and deferrals in the frozen sec-
tion  diagnosis  of  bronchioloalveolar  carcinoma  and
well-differentiated  pulmonary  adenocarcinoma:  an
evidence-based  pathology  approach.  Am  J  Clin
Pathol. 2008;130:11–20.

cancer from primary lung adenocarcinoma on thoracic
frozen section. Am J Clin Pathol. 2009;131:122–8.
 52. Glasziou  P,  Heneghan  C.  A  spotter’s  guide  to  study

designs. Evid Based Nurs. 2009;12:71–2.

 53. Lu  CY.  Observational  studies:  a  review  of  study
designs, challenges and strategies to reduce confound-
ing. Int J Clin Pract. 2009;63:691–7.

 54. Noordzij  M,  Dekker  FW,  Zoccali  C,  et  al.  Study
designs  in  clinical  research.  Nephron  Clin  Pract.
2009;113:c218–21.

 55. Ridgway PF, Guller U. Interpreting study designs in
surgical research: a practical guide for surgeons and
surgical  residents.  J  Am  Coll  Surg.  2009;208:
635–45.

 56. Li S, Dickson DW, Iacobuzio-Donahue CA, et al. The
launch of international journal of clinical and experi-
mental pathology. Int J Clin Exp Pathol. 2008;1:i.
 57. Foucar E, Wick MR. An observational examination of
the literature in diagnostic anatomic pathology. Semin
Diagn Pathol. 2005;22:126–38.

 58. Vollmer RT. Multivariate statistical analysis for ana-
tomic  pathology.  Part  II:  failure  time  analysis.  Am
J Clin Pathol. 1996;106:522–34.

 59. Vollmer  RT.  Multivariate  statistical  analysis  for
pathologist.  Part  I,  The  logistic  model.  Am  J  Clin
Pathol. 1996;105:115–26.

 60. Foucar E. Diagnostic precision and accuracy in inter-
pretation  of  specimens  from  cancer  screening  pro-
grams. Semin Diagn Pathol. 2005;22:147–55.

 61. Foucar E. Classification of error in anatomic pathol-
ogy: a proposal for an evidence-based standard. Semin
Diagn Pathol. 2005;22:139–46.

 62. Li Q, Kuriyama S, Kakizaki M, et al. History of chole-
lithiasis  and  the  risk  of  prostate  cancer:  The  Ohsaki
cohort study. Int J Cancer. 2011;128(1):185–91.
 63. Silva DR, Menegotto DM, Schulz LF, et al. Mortality
among patients with tuberculosis requiring intensive
care:  a  retrospective  cohort  study.  BMC  Infect  Dis.
2010;10:54.

 64. Meier  DS,  Weiner  HL,  Guttmann  CR.  Time-series
modeling  of  multiple  sclerosis  disease  activity:  a
promising window on disease progression and repair
potential? Neurotherapeutics. 2007;4:485–98.

 65. Brunelli M, Delahunt B, Gobbo S, et al. Diagnostic use-
fulness  of  fluorescent  cytogenetics  in  differentiating
chromophobe renal cell carcinoma from renal oncocy-
toma:  a  validation  study  combining  metaphase  and
interphase  analyses.  Am  J  Clin  Pathol.  2010;133:
116–26.

 66. Marchevsky A, Gupta R, Casadio C, et al. World Health
Organization classification of thymomas provides sig-
nificant  prognostic  information  for  selected  stage  III
patients:  evidence  from  an  international  thymoma
study group. Hum Pathol. 2010;41(10):1413–21.
 67. Marchevsky  AM,  Tsou  JA,  Laird-Offringa  IA.
Classification  of  individual  lung  cancer  cell  lines
based on DNA methylation markers: use of linear dis-
criminant  analysis  and  artificial  neural  networks.
J Mol Diagn. 2004;6:28–36.

 51.  Herbst  J,  Jenders  R,  McKenna  R,  et  al.  Evidence-
based  criteria  to  help  distinguish  metastatic  breast

 68. Blute ML. Editorial comment on: external validation
of  the  Mayo  Clinic  stage,  size,  grade,  and  necrosis

11  Evidence-Based Approach to Evaluate Information Published in the Pathology Literature

201

(SSIGN) score for clear-cell renal cell carcinoma in a
single  European  centre  applying  routine  pathology.
Eur Urol. 2010;57:110–1.

 69. Shariat SF, Karakiewicz PI, Godoy G, et al. Survivin
as a prognostic marker for urothelial carcinoma of the
bladder: a multicenter external validation study. Clin
Cancer Res. 2009;15:7012–9.

 70. Utsumi  T,  Kawamura  K,  Suzuki  H,  et  al.  External
validation and head-to-head comparison of Japanese
and  Western  prostate  biopsy  nomograms  using
Japanese data sets. Int J Urol. 2009;16:416–9.

 71. Marchevsky  A,  Gupta  R.  Evidence-based  pathol-
ogy: interobserver diagnostic variability at “moder-
ate”
levels  could
significantly  change  the  prognostic  estimates  of
clinico-pathologic  studies.  Ann  Diagn  Pathol.
2010;14(2):88–93.

to  “substantial”  agreement

 72. Park JH, Kim DS, Park IN, et al. Prognosis of fibrotic
interstitial pneumonia: idiopathic versus collagen vas-
cular disease-related subtypes. Am J Respir Crit Care
Med. 2007;175:705–11.

 73. Straus  SE,  Sackett  DL.  Bringing  evidence  to  the

clinic. Arch Dermatol. 1998;134:1519–20.

 74. Straus SE, Richardson WS, Glasziou P, et al. Evidence-
based  medicine.  How  to  practice  and  teach  EBM.
New York: Elsevier; 2005.

 75. Gross R. Decisions and evidence in medical practice.

St. Louis: Mosby; 2001.

 76. Marchevsky  AM,  Wick  MR.  Evidence  levels  for
publications  in  pathology  and  laboratory  medicine.
Am J Clin Pathol. 2010;133:366–7.

 77. American  College  of  Physicians.  Clinical  Efficacy
Assessment  Project;  2010.  Ref  Type:  Internet
Communication.

 78. Novis DA, Gephardt GN, Zarbo RJ. Interinstitutional
comparison  of  frozen  section  consultation  in  small
hospitals:  a  College  of  American  Pathologists
Q-Probes study of 18, 532 frozen section consultation
diagnoses  in  233  small  hospitals.  Arch  Pathol  Lab
Med. 1996;120:1087–93.

 79. Steindel  SJ,  Howanitz  PJ,  Renner  SW.  Reasons  for
proficiency  testing  failures  in  clinical  chemistry  and
blood gas analysis: a College of American Pathologists
Q-Probes study in 665 laboratories. Arch Pathol Lab
Med. 1996;120:1094–101.

 80. Tholen D, Lawson NS, Cohen T, et al. Proficiency test
performance and experience with College of American
Pathologists’  programs.  Arch  Pathol  Lab  Med.
1995;119:307–11.

 81. Wagner  LR,  Carson  JG.  The  College  of  American
Pathologists, 1946-1996: membership and its benefits.
Arch Pathol Lab Med. 1997;121:427–37.

 82. Marchevsky AM, Walts AE, Bose S, et al. Evidence-
based evaluation of the risks of malignancy predicted
by  thyroid  fine-needle  aspiration  biopsies.  Diagn
Cytopathol. 2010;38(4):252–9.

Evidence-Based Cell Pathology
Revisited: A Personal View

12

Kenneth A. Fleming

Keywords
Evidence-based pathology •  Cell pathology •  Personal experience in pathology
• Morphological diagnoses • Report communication • Sampling

In  1996,  I  argued  that  cell  pathology  for  the
twenty-first century needed a more rigorous evi-
dence  base  for  its  three  key  components:  sam-
pling,  morphological  diagnosis,  and  report
communication [1]. I suggested that the criteria
used in each component should be based on evi-
dence  which  showed  the  component  to  be  both
reproducible (for example, as in kappa value) and
relevant, as expressed by an appropriate measure
of  accuracy  in  predicting  clinical  status  (for
example, sensitivity and specificity).

At that time, the evidence base for these crite-
ria  in  the  great  majority  of  diseases  was  often
rudimentary. Since then, there has been a major
expansion  of  knowledge  in  many  of  the  areas,
although  there  is  still  much  to  be  done.  More
importantly,  understanding  what  is  needed  in
terms  of  methodology  to  establish  the  evidence
base was only beginning to be defined 15 years
ago.  Much  work  has  been  done  on  establishing
standards  for  the  reporting  of  diagnostic  tests  –
for example, STARD (Standards for Reporting of
Diagnostic Accuracy) [2] – and on how to assess

K.A. Fleming ()
Director, Oxford University Clinical Academic
Graduate School, Associate Dean, Oxford Post Graduate
Medicine and Dental Deanery, Oxford, UK
e-mail: kenneth.fleming@medsci.ox.ac.uk

the  quality  of  the  literature  on  diagnostic  tests
[3], although wider dissemination is needed.

Despite these advances, there is still a consid-
erable  gap  between  the  depth  of  the  evidence
base used in, for example drug therapy, and that
used in cell pathology. Thus, for example, clas-
sification of types of evidence into various levels,
to  rank  the  quality  of  research  used  to  analyze
a  particular  problem,  has  been  a  fundamental
aspect of evidence-based medicine. These levels
are well accepted and increasingly widely used in
evidence-based  therapeutics,  but  there  are  cur-
rently no uniformly accepted definitions of levels
of  evidence  for  evidence-based  cell  pathology.
There have been several proposals and, for exam-
ple, the Royal College of Pathologists (RCPath)
datasets for cancer (http://www.rcpath.org/index.
asp?PageID=154, see below) use a modification
of the Scottish Intercollegiate Guidelines Network
(SIGN) (http://www.sign.ac.uk/guidelines/fulltext/
50/index.html) guidelines. These guidelines have
much  in  common  with  those  of  the  GRADE
group (Grading of Recommendations Assessment
Development and Evaluation) [4], particularly in
their  emphasis  on  patient-related  outcomes,  in
addition to study design. In addition, by analogy
with drug research, Gludd and Gludd [5] have pro-
posed  that  studies  in  evidence-based  diagnostics

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_12, © Springer Science+Business Media, LLC 2011

203

204

K.A. Fleming

should have four phases, with phases III and IV
determining the impact on patient outcome.

Unfortunately, however, the great majority of
research in cell pathology, aimed at establishing
how relevant a particular feature is in diagnosis,
prognosis, or management, consists of retrospec-
tive, cohort studies, with no controls, frequently
with small numbers and often of relatively short
duration. Randomization and external validation
are very rare, as are power calculations to assess
whether the size of the study is sufficient to deter-
mine  the  statistical  significance  of  a  particular
result. In the therapeutic field, such types of stud-
ies would be judged as among the lowest levels of
evidence and carry proportionately little weight.
This relative lack of rigor is a result partly of the
still-developing  nature  of  the  methodology,  and
also of the inherent difficulty of designing, in cell
pathology,  the  equivalent  of  a  therapeutic  ran-
domized trial. Despite this, cell pathology needs
to establish evidence levels of equivalent rigor to
those used in evidence-based medicine and apply
them with the same degree of universality.

In  this  chapter,  I  shall  concentrate  mainly  on
research from UK and Europe, which over the last
15 years has explored the development and appli-
cation  of  the  evidence  base  to  each  of  the  three
key  areas  of  cell  pathology  mentioned  above.  I
shall largely, but not exclusively, use carcinoma,
especially  colorectal  carcinoma,  and  hepatic
pathology as exemplars from a much wider range
of examples to illustrate the developments.

Sampling

As we all know, you can be using the most sophis-
ticated diagnostic test available, but if the tissue
you are provided with is not from the appropriate
region, then the test is useless. So what is the evi-
dence base for optimum sampling of an organ to
establish  accurate  diagnosis  and  to  guide  prog-
nostication and therapy?

One of the most systematically analyzed areas
in this field is sampling in relation to malignancy.
Over the last 15 years or so, there have been con-
centrated efforts to establish a better evidence base
for  appropriate  sampling  of  a  large  variety  of
tumors.  In  the  UK,  one  of  the  most  complete  of

these evidence bases is a series of publications by
the RCPath on data sets for tumors (http://www.
rcpath.org/index.asp?PageID=154). Currently, the
RCPath has produced over 30 such data sets which
cover  most  of  the  common  tumors.  There  are
strictly defined rules on how the dataset should be
organized,  what  issues  should  be  addressed,  and
what types of evidence are acceptable. The aim of
these data sets is wider than sampling, but signifi-
cant parts of the protocols are focused on defining
which parts of a tumor, including resection mar-
gins,  should  be  sampled  for  accurate  diagnosis,
staging, prognostication, and guidance of therapy.
As  an  example  of  the  approach,  one  of  the
most intensively studied areas is the adequacy of
sampling in colorectal cancer (CRC). One com-
ponent of this is the evidence that involvement of
tumor  in  the  nonperitoneal  surgical  margin  –
 previously  called  the  circumferential  or  radial
margin – is a strong predictor of local recurrence.
This  has  resulted  in  clear  acceptance  that  this
margin  must  be  identified  and  sampled  and,  if
positive,  additional  therapy  such  as  radiation  or
chemotherapy, instigated.

However, despite the fact that there have been
concerns about this issue for many years, the evi-
dence  of  how  best  to  identify  and  sample  these
margins is relatively recent. Thus in 1986, Quirke
et  al.  [6],  in  a  prospective  analysis  of  52  cases
with a median follow-up of 23 months, showed
clearly that those tumors with spread to the lat-
eral  margin,  as  identified  by  whole-specimen
mounting and 5–10 mm serial tissue-sectioning,
were very strongly associated with local  recurrence
– 92% specificity, 95% sensitivity, and 85% posi-
tive predictive value. In this paper, the proportion
of  local  recurrences  in  a  retrospective  control
group  of  52  stage-  and  grade-matched  patients,
followed up for a median of 90 months, who had
been staged as negative for lateral margin involve-
ment by routine sampling, was the same as in the
patients who had been staged as positive by serial
sectioning.  This  clearly  indicated  that  routine
sampling was not detecting most cases of lateral
margin involvement.

Subsequently,  Ng  et  al.  [7]  published  a
 prospective  cohort  study  of  80  cases  of  rectal
 carcinoma with median follow-up of 26.6 months.
They used whole-mounting and serial sectioning

12  Evidence-Based Cell Pathology Revisited

205

at 5–8 mm intervals. After uni- and multivariate
analysis, clearance as determined by this type of
examination  was  one  of  three  pathological  fea-
tures  which  independently  related  to  prognosis.
Interestingly, a proportion of tumors which were
staged  as  fully  excised  by  this  method  suffered
recurrence.  While  there  are  several  possible
explanations, this may suggest that sectioning at
5–8 mm is too great an interval, but to my knowl-
edge, this has not been assessed.

Although these papers, and many others pub-
lished  on  this  topic,  clearly  indicate  the  impor-
tance  of  sampling  the  nonperitoneal  margin
properly,  in  general,  they  are  of  short  duration
and  are  cohort  studies,  with  small  numbers  and
no  randomizing,  and  as  such  provide  relatively
low-level  evidence  to  support  their  hypothesis.
Furthermore,  although  the  original  papers  were
published over 20 years ago, it is clear that inad-
equate  sampling  is  still  widespread.  Thus,  for
example, while it has been estimated that, using
the  technique  outlined  above,  on  average,  one
would expect to find evidence of extramural vas-
cular invasion in 30% of cases of CRC, in a paper
from 2007, Quirke and Morris [8] stated that, in
practice, pathologists only report this finding in
around 10% of cases.

Another  important  area  of  research  into  opti-
mum sampling has been the attempts of recent years
to identify what sampling is necessary to establish,
with acceptable degrees of certainty, the presence or
absence of secondary deposits of tumor in regional
lymph nodes (LN). This work has largely dealt with
the  number  of  nodes  to  be  sampled,  what  micro-
scopic sampling should be performed, and the ana-
tomical location of these nodes.

Until relatively recently, there was no evidence-
based advice on the number of nodes which need
to  be  sampled  to  provide  reasonable  certainty
about the presence or absence of metastases. The
expectation was that as many as could be found
were  examined.  Interestingly,  in  the  1990s,  the
average number found in CRC was around 6 per
case [8]. Now for many tumors, specific numbers
of lymph nodes are recommended. Thus, for CRC,
at least 12 lymph nodes are recommended (http://
www.rcpath.org/index.asp?PageID=154).

However, what is the evidence for this? Since
1989, when Scott and Grace [9] investigated the

use  of  fat  clearance  as  a  method  of  improving
lymph node recovery in CRC, several papers have
examined the relationship between the total num-
ber of LN recovered and the likelihood of identi-
fying metastases in these nodes. In 2003, Swanson
et  al.  [10]  undertook  a  retrospective  analysis  of
the  National  Cancer  Data  Base  (which  is  a  pro-
spective database of more than 260,000 cases of
colon cancer in the USA) to correlate clinical out-
come with number of LN examined by the pathol-
ogist.  They  showed  that  less  than  8  nodes  were
inadequate to assign node-negative status to a T3
tumor, while conversely identification of 13 nodes
was sufficient to stage a tumor as node-negative.
Subsequently,
there  has  been  considerable
research around this topic supporting this view –
see systematic review of this area [11].

However, there are dissenting views. A recent
publication [12] suggested explanations other than
more accurate staging for the association between
higher lymph node numbers and better outcomes.
Indeed, a paper from 2002 [13], using mathemati-
cal  modeling,  suggested  that,  for  early-stage
colonic  tumors,  more  than  30  LNs  need  to  be
examined to ensure 85% probability of true nega-
tivity,  whereas  examining  12  nodes  gives  only  a
25% probability in T3/4 tumors. In addition, given
that between 1998 and 2001, fewer than 50% insti-
tutions  in  the  US  (involving  only  44%  patients)
adhered to the current guideline of 12 LN [14], this
makes the achievement of a larger LN harvest both
a high priority and highly problematic.

A related and arguably more important factor
in lymph node analysis is the detection (or not) of
metastases. Indeed, what is the evidence of how
best to detect metastatic tumor in a LN?

Despite the crucial importance of this aspect
of  tumor  pathology,  extraordinarily,  there  is  no
agreed  evidence-based  protocol.  There  is  much
variation  in  the  methods  used  by  pathologists,
ranging  from  simple  bisection  and  embedding
of each half, face down, followed by a solitary
H. and E. section, to the examination of multiple
and even serial levels with immuno-histology or
molecular analysis for carcinoma cells. These lat-
ter approaches have shown that the occurrence of
metastases  is  often  missed  by  less  intensive
examination and thereby that they can convert a
tumor to a higher stage [15].

206

K.A. Fleming

Putting aside the controversial issue of whether
detection of micrometastases (as in breast carci-
noma) is associated with poorer prognosis – for
which there is an extensive and somewhat contra-
dictory literature – most of these techniques are
too  labor-intensive  for  routine  examination  of  a
large  numbers  of  nodes  and  so  several  recent
papers  have  proposed  focusing  on  the  sentinel
nodes and subjecting them to intensive sampling.
Protocols  involving  elaborate  modeling  of  dis-
section  and  sectioning  of  the  sentinel  node
(involving,  for  example,  many  sections  and
immunochemistry)  have  been  proposed,  but  to
do  this  in  a  truly  evidence-based  manner  –
 prospectively, at appropriate power, with external
validation  –  even  for  breast  carcinoma,  would
take tens of thousands of patients and at least 10
years  [15].  This  latter  paper  recommended  that
an  achievable  aspiration  would  be  to  use  a  less
comprehensive, but a statistically valid sampling
method, with standardized protocols for evalua-
tion and classification of metastases and correlat-
ing these to clinical outcomes in a population-based
registry or national cancer database.

Outside  the  arena  of  malignancy,  there  has
been  some  investigation  of  the  more  general
problem of sampling. In the liver, where a needle
biopsy  represents  between  a  30,000th  and
50,000th of the organ, the question is, how repre-
sentative is such a small proportion?

One way of addressing this question has been
by performing two or more biopsies (either from
the left and right lobes or by performing multiple
passes from the same biopsy site) in a variety of
conditions and comparing the appearances. This
has shown variation in the appearances of the dif-
ferent  biopsies.  Thus,  in  noncaseating  granulo-
mas, over 50% of the multiple biopsies failed to
show  the  abnormality,  and  in  cirrhosis,  25%  of
biopsies did not show the lesion [16]. For such an
important diagnosis, this is extremely worrying.
Similarly,  in  focal  diseases  such  as  Primary
Sclerosing  Cholangitis,  there  was  considerable
discordance between the two biopsies – advanced
disease  was  missed  in  40%  of  biopsies  and  cir-
rhosis in 37% [17].

A related aspect is the adequacy of the amount
of  material  obtained,  whether  by  one  or  more

passes. Again, the question is what is an adequate
amount of tissue to provide a reasonable sample?
Several  papers  [18,  19]  have  examined  the
effect of variation of length of a liver biopsy on
the  grading  and  staging  of  inflammation  and
fibrosis in chronic viral hepatitis. Thus, Colloredo
et al. [19], by masking increasing fractions of a
biopsy, showed that as biopsy length decreased,
mild  grading  increased  –  mild  grade  increased
from 49.7% in tissue equal/greater than 3 cm, to
86.6% in a 1 cm long portion of the same biopsy.
Similarly,  the  proportion  of  biopsies  showing
mild fibrosis increased from 59% in 3 cm biop-
sies, to 80% in 1 cm biopsies.

Bedossa et al. [18], using the METAVIR scor-
ing system for fibrosis, image analysis, and vir-
tual  biopsies  of  increasing  length,  showed  that
increasing  length  from  15  to  25  mm  decreased
the coefficient of variation of fibrosis from 55 to
45%. To reach a CV of 25% required a biopsy of
over 80 mm and increasing beyond this did not
produce further reduction. A 15 mm biopsy cor-
rectly assigned the METAVIR score in only 65%
of the biopsies, which increased to 75% in 25 mm
biopsies. Further increase in length did not result
in  further  improvement.  In  view  of  this,  the
authors recommended a biopsy of 25 mm length
as the minimum length for fibrosis assessment.

As a result of analyses such as the above, there
has been a consensus for some time that a liver
biopsy of at least 20–25 mm length and contain-
ing at least 11 complete portal tracts is the mini-
mal  size  needed  for  adequate  assessment  –  this
despite the fact that such biopsies are, at best, still
nowhere  near  100%  accurate.  Amazingly,  the
evidence is that even these sizes are not routinely
achieved [20] and that this situation still persists.
Thus, a recent paper [21] retrospectively reviewed
163 biopsies in a tertiary referral hospital in the
UK  and  found  that  the  median  length  was
13.3 mm (range 5.6–50 mm) with a median of 4
complete portal tracts (range 0–18).

Almost certainly part of this failure to obtain
reflects  concerns  about
adequate  biopsies
increased  adverse  complications  resulting  from
increased sample size and passes. For this reason,
trans-jugular  liver  biopsy,  with  four  passes,  has
been  suggested  as  a  safer  and  more  effective

12  Evidence-Based Cell Pathology Revisited

207

method  of  liver  biopsy,  producing  material  of
adequate size and volume [22]. Curiously again,
despite the evidence supporting this approach, it
is not the norm, suggesting therefore that inade-
quate  diagnoses  are  widespread.  Why  is  this?
Could it be that ignorance of the problem is the
main reason, rather than safety concerns.

Summary

As can be seen from the above, the evidence base
for valid sampling of many organs has still to be
fully  established.  However,  irrespective  of  the
complexities  and  divergent  views,  the  key  mes-
sage  is  that  evidence-based  methodologies  for
determination of the best methods of sampling –
whatever the organ or disease – are available and
can be used to establish the necessary guidance
for the twenty-first century.

Morphological Diagnoses

Examining the microscopic features of tissue to
obtain  a  diagnosis  is  the  cornerstone  of  cell
pathology, and in many instances, is regarded as
the gold standard. To be acceptable in the twenty-
first  century,  where  molecular  signatures  are
being developed, the microscopic features which
rule in or rule out a particular diagnosis should be
reproducible and relevant. Yet, when one exam-
ines the evidence base for these two components
of cell pathology diagnosis 150 years or so after
Virchow, it is still surprisingly limited.

Reproducibility

Reproducible  means  that  when  several  patholo-
gists look for a particular morphological feature,
they should all reach the same conclusion about
its presence or absence (interobserver reproduc-
ibility) and when the same pathologist looks for
the feature on different occasions (intraobserver
reproducibility),  again  she/he  agrees  about  its
presence or absence on those different occasions.
Reproducibility  is  usually  measured  by  kappa

value, although this method of statistical analysis
has some weaknesses [23].

Reassuringly, measurement of reproducibility
of  pathological  features  is  being  used  with
increasing frequency in a wide range of diseases
and organs. Depressingly, when applied to many
of the time-honored and traditional morphologi-
cal  features  used  in  tissue  diagnosis,  these  fea-
tures  are  often  so  poorly  reproducible  between
pathologists  as  to  undermine  the  basis  of  their
continued use [1].

In  liver  for  example,  several  papers  [24–26]
have examined the inter-/intraobserver reproduc-
ibility  of  the  inflammatory  and  fibrosis  compo-
nents  of  the  various  scoring  systems.  Almost
universally,  the  inflammatory  components  have
kappa  values  which,  at  best,  are  fair,  while  in
comparison,  fibrosis  scoring  is  regularly  more
reproducible.  This  in  [24]  the  kappa  value  for
periportal necrosis was 0.36, for lobular necrosis
was 0.38, and for portal inflammation was 0.25,
while the kappa value for fibrosis was 0.78.

Here,  inflammatory  features  thought  to  be
important for diagnosis, prognosis, and manage-
ment  are  so  relatively  poorly  reproducible  that
they probably cannot be trusted. Despite this, and
although scoring systems are recommended only
for clinical trials, on occasions they are used in
clinical  service  and  therapeutic  decisions  influ-
enced by them. It would be interesting to know
what  patients  think  of  this  rather  unsatisfactory
situation.

How  do  we  improve  this?  One  approach  is
clearer and more precise definitions of the abnor-
mality  concerned  (as  in  IgA  nephropathy  –  see
below).  Another  method  is  the  use  of  “good
example”  images  which  can  be  visualized  on  a
computer  screen  and  compared  by  the  patholo-
gist  to  the  image  she/he  is  seeing  down  their
microscope.  The  pathologist  picks  the  “good
example” which best fits his/her own microscopic
image  to  identify  the  presence  or  absence  of  a
particular microscopic pathological feature. Such
technology can also be combined with a comput-
erized decision support system (DSS) (based on
Bayesian  belief  networks)  not  only  to  measure
and improve reproducibility, but also to provide a
teaching tool.

208

K.A. Fleming

To illustrate this, a recent paper [27] described
this process in cervical pathology. The authors
selected eight morphological features (evidence
nodes) which were linked to five final diagnoses
(decision  nodes)  via  a  conditional  probability
matrix. The latter gives a numerical probability
of the likelihood of finding a particular feature
in  a  particular  diagnosis  (for  example,  severe
basal  cell  nuclear  pleomorphism  in  normal
equals 0.01).

How does this work? In practice, the observer
views  a  biopsy  and  classifies  each  of  the  eight
morphological features by comparing the micro-
scopic  image  with  the  on-screen  image.  To  do
this,  she/he  positions  a  sliding  pointer  on  the
spectrum of images  which  most closely resem-
bles  the  image  seen  down  the  microscope.  The
software automatically calculates a likelihood of
finding  the  particular  feature  in  each  possible
diagnosis. After all the features have been scored,
the diagnosis with the highest probability is the
final  diagnosis.  A  cumulative  probability  graph
is generated which shows the changes in likeli-
hood of diagnosis as each morphological feature
is assessed.

The  system  was  tested  on  50  colposcopic
biopsies selected to have the full range of diag-
noses  and  tested  on  two  experienced  patholo-
gists, two trainee pathologists and two medical
 students.  Intra-  and  interobserver  reproducibil-
ity were measured using a weighted kappa value
(weighted  such  that  more  serious  differences
are given greater weight), with and without use
of  the  DSS.  This  showed  that  while  intraob-
server  reproducibility  was  the  same  in  both
approaches, interobserver reproducibility for the
consultants improved from a 0.46 to 0.54 using
the DSS.

Perhaps  more  importantly  than  modestly
improving interobserver reproducibility, the sys-
tem  allows  comparison  between  individuals  in
their  analysis  of  each  feature  and  is  thus  an
invaluable  teaching  tool.  It  also  potentially
allows,  with  relative  ease,  assessment  of  a  par-
ticular feature by large numbers of pathologists
of variable experience and competence. This will
allow more informed selection of which features
should be used in a diagnosis and abandonment
of those that are insufficiently reproducible.

Relevance

In contrast to reproducibility, by relevant, I mean
that the presence of the feature in question indi-
cates  likelihood  of  presence  of  a  particular  dis-
ease or clinical outcome, the degree of likelihood
being expressed numerically. The latter is usually
described as accuracy of diagnosis and there are
several ways (all of which have particular merits
and  demerits)  in  which  this  can  be  described  –
sensitivity, specificity, positive and negative pre-
dictive  values  (PPV,  NPV),  odds,  and  hazard
ratios – for the disease in question (see elsewhere
in this book for definitions). The methodologies
for  determining  the  sensitivity,  specificity,  PPV,
NPV, odds, and hazard ratios for any feature are
well established. Of these, while sensitivity and
specificity  are  being  used  in  the  cell  pathology
literature  with  increasing  frequency,  the  other
methods  of  measuring  accuracy  are  used  much
less frequently.

However, there has been one recent example of
an  evidence-based  approach  to  morphological
diagnosis which combined assessment of both the
reproducibility  of  the  pathological  features  and
their accuracy in predicting outcome. This is the
Oxford classification of IgA nephropathy [28].

The  authors  of  the  paper  wished  to  establish
which of the pathological features of IgA neph-
ropathy  best  correlated  with  clinical  outcome,
independent of treatment or other factors. To do
this, they initially agreed a list of what morpho-
logical features can be present (divided between
glomerular,  tubulo-interstitial,  etc.),  agreed  spe-
cific  definitions  –  for  example,  extra  capillary
proliferation or cellular crescent is “extra capil-
lary  lesion  comprising  cells  and  extra-cellular
matrix,  with  less  than  50%  cells  and  less  than
90%  matrix”  –  and  tested  these  morphological
features for reproducibility among themselves.

On the basis of these results, they refined their
definitions  and  divided  the  features  into  several
groups according to kappa value – high reproduc-
ibility,  kappa  value  greater  than  0.6,  moderate
reproducibility, kappa 0.4–0.6, and poor reproduc-
ibility, less than 0.4. The latter features were then
excluded from further consideration. Twenty-four
features were scored, and of these, fourteen were
either high or moderately  reproducible. The authors

12  Evidence-Based Cell Pathology Revisited

209

then  established  what  correlations  there  were
between each of these features (correlation coeffi-
cients) and selected one feature as representative of
each  correlation  group,  for  subsequent  analysis.
The selection was based on reproducibility, ease of
identification, and susceptibility to sampling error.
This resulted in six features being identified as the
lesions  of  IgA
evidenced-based  pathological
nephropathy.

In  an  accompanying  paper  [29],  the  authors
then performed analyses of the correlation of the
selected  pathological  features  with  a  variety  of
clinical  features  (for  example,  proteinuria,  GFR,
mean arterial pressure) and outcomes (for exam-
ple, rate of decline of renal function, survival with-
out dialysis). They then determined a measurement
of the accuracy which the pathological feature had
for a particular clinical feature/outcome.

Thus, the pathological features which had been
shown  to  be  reproducible  were  analyzed  by  uni-
and multivariate analysis for correlation with clini-
cal  features  and  outcomes.  For  some  of  the
pathological variables (continuous variables with
skewed  distribution),  Receiver  Operating  Curves
(ROC) were constructed to determine optimal cut-
offs between positive or negative results. Hazard
ratios were calculated, as were odds ratios.

The  final  result  was  a  recommendation  that
four microscopic features should be assessed and
given  a  score,  each  providing  an  independent,
evidence-based,  measure  of  likelihood  of  pro-
gression of disease.

While  this  investigation  was  a  retrospective
observational  study  with  variable  sourcing  of
data, it is a relatively rare example of a rigorously
evidence-based analysis of the reproducibility of
the  microscopic  features  of  a  disease  and  how
they predict clinical outcome. The authors recog-
nized that validation in an independent prospec-
tive  study,  with  data  collected  in  a  uniform
manner,  is  necessary  for  confirmation  of  their
findings, but the basic approach is a model for all
histopathology in the twenty-first century.

Summary

Morphological  diagnosis  is  the  beating  heart  of
cell pathology, but when examined systematically,

many  of  the  features  we  assess  have  relatively
poor  reproducibility  and  inadequate  assessment
of their relevance to clinical outcome. However,
as  in  sampling,  we  know  how  to  tackle  these
issues  and  improving  this  situation  can  be
addressed  by  a  long-term,  systematic  commit-
ment to generating quantified data on the repro-
ducibility  and  accuracy  of  the  pathological
features of each disease, along the lines of the IgA
nephropathy papers.

Report Communication

As I said some 14 years ago, there has been very
little  research  into  the  evidence-base  of  the  best
format for the composition of the cell pathology
report. Since then, there has been a considerable
move  towards  data  sets,  especially  in  cancer
reporting  (see  above).  Such  data  sets  are  struc-
tured  as  a  proforma  to  list  all  of  the  features
thought to be relevant to diagnosis, prognosis, and
management, the role of the pathologist being to
fill  in  the  appropriate  measurement  and/or  com-
ment. The rationale behind such forms is twofold:
first,  by  listing  all  relevant  factors,  the  report
should  therefore  contain  all  the  information
thought to be important. Second, by minimizing
free text, it reduces the possibility of misinterpre-
tation of the report by the clinician or patient.

Two questions arise from this transformation of
the  report  format.  First,  has  the  completeness
of the reporting increased? Second, have the new
formats
communication
between the pathologist and clinician?

increased/decreased

In  answer  to  the  first  question,  a  number  of
papers have examined this question and generally
the answer is yes. Thus in CRC, in one department,
the completeness of the reported data set improved
from 0 to 96% [30], while a randomized prospec-
tive  study  of  the  use  of  computerized  proforma
reports  in  16  hospitals  in  Wales,  for  breast  and
CRC,  involving  over  2,000  reports,  showed  a
28.4% increase in completeness of reports, in com-
parison with nonproforma reports [31]. However in
this  latter  study,  31.2%  of  the  CRC  reports  still
were  incomplete  for  core  data.  At  one  level,  this
seems  extraordinary.  Why  would  information
which is thought to be important not be reported?

210

K.A. Fleming

Of course, some of this may simply be forgetting to
fill in relevant section, perhaps because of pressure
of work. Alternatively, it may be that the data are
not  easy  to  elucidate  or  that  the  pathologist  does
not (at least subconsciously) believe that the infor-
mation is truly relevant. Presumably such causes of
incompleteness can be addressed, at least in com-
puterized reporting, by ensuring that the report can-
not  be  signed  off  if  data  points  are  still  missing.
However,  there  has  been  little  research  into  the
causes  of  the  missing  information.  Until  this  has
been properly researched, it cannot be said that the
evidence-base  for  the  most  effective  report  com-
munication has been established.

What  about  the  second  question  –  has  the
adoption  of  data  set  reports  improved  the  inter-
pretation of pathology findings by the clinician?

There  is  essentially  no  quantified  research  on
this matter. In the study from Wales [31], surgeons
greatly welcomed the reconfiguring of reports into
this type of format. Anecdotally and from personal
experience, similar views have been expressed, but
hard evidence that this has improved communica-
tion and decreased mistakes does not exist at pres-
ent.  It  could  be  argued  either  that  this  is  so
self-evident that formal confirmation is not needed,
or  that,  as  the  great  majority  of  cell  pathologists
participate in meetings with the clinicians, at which
cases  are  discussed,  these  provide  a  satisfactory
channel for accurate communication. While this is
undoubtedly true, it is also true that usually only a
subset of cases is discussed at such meetings, leav-
ing  the  majority  of  reports  not  considered.
Furthermore,  in  referral  cases  this  often  does  not
apply. Also, increasingly in the UK at least, there
are  moves  towards  more  distant  provision  of  cell
pathology  in  off-site  labs,  which  will  make  the
holding of such clinical meetings less straightfor-
ward. While potentially telepathology can provide
a substitute, this underlines the need for reports to
contain  only  relevant,  accurate  information,  pre-
sented in as unambiguous a form as possible.

Conclusion

Evidence-based  cell  pathology  as  an  approach
for  the  twenty-first  century  has  made  consider-
able  advances  in  the  last  14  years  or  so.  Most,

if  not  all,  of  the  appropriate  methodology  now
exists, but the challenge is in the application of
the  various  methodologies
to  particular
problems.

Part of this lack of implementation reflects the
inherent difficulty of designing the equivalent of
a therapeutic randomized trial in cell pathology,
but  I  suspect  the  greatest  barrier  to  widespread
application  is  not  technical  or  methodological,
but a failure by pathologists to recognize the full
extent of the problems such as those outlined in
this  book.
this  chapter  and  elsewhere
Addressing this issue will require greater profile
for the evidence-based cell pathology movement,
through the usual channels of publications, con-
ferences, and so on, but probably the most impor-
tant  factor  will  be  the  incorporation  of  the
principles into curricula for pathology training.

in

References

  1.  Fleming  KA.  Evidence-based  pathology.  J  Pathol.

1996;179:127–8.

  2.  STARD  Group.  Towards  complete  and  accurate
reporting  of  studies  of  diagnostic  accuracy:  the
STARD initiative. Clin Chem. 2003;49:1–6.

  3.  Mariska  MG,  et  al.,  on  behalf  of  the  Cochrane
Diagnostic Test Accuracy Working Group. Systematic
reviews of diagnostic test accuracy. Ann Intern Med.
2008;149:889–97.

  4.  Schunemann  HJ,  et  al.,  for  the  GRADE  Working
Group.  Grading  quality  of  evidence  and  strength  of
recommendations  for  diagnostic  tests  and  strategies.
BMJ. 2008;336:1106–10.

  5.  Gludd  C,  Gludd  LL.  Evidence  based  diagnostics.

BMJ. 2005;330:724–6.

  6.  Quirke P, Dixon MF, Durdey P, Williams NS, Local
recurrence  of  rectal  adenocarcinoma  due  to  inade-
quate  surgical  resection:  histopathological  study  of
lateral  tumour  spread  and  surgical  excision.  Lancet.
1986;328:996–9.

  7.  Ng  IOL  et  al.  Surgical  lateral  clearance  in  resected
rectal carcinomas. A multivariate analysis of clinico-
pathological features. Cancer. 1993;71:1972–6.

  8.  Quirke  P,  Morris  E.  Reporting  colorectal  cancer.

Histopathology. 2007;50:103–12.

  9.  Scott  KWM,  Grace  RH.  Detection  of  lymph  node
metastases  in  colorectal  carcinoma  before  and  after
fat clearance. Br J Surg. 1989;76:1165–7.

 10. Swanson  RS,  Compton  CC,  Stewart  AK,  Bland  KI.
The prognosis of T3N0 colon cancer is dependent on
the  number  of  lymph  nodes  examined.  Ann  Surg
Oncol. 2003;10:65–71.

 11. Chang GJ, Rodriguez-Bigas MA, Skibber JM, Moyer
VA. Lymph node evaluation and survival after  curative

12  Evidence-Based Cell Pathology Revisited

211

resection  of  colon  cancer:  systematic  review.  J  Natl
Cancer Inst. 2007;99:433–41.

 12. Kenelly R, Winter DC. Quality assurance measures in
rectal cancer: caveat utilitor. Gut. 2010;59:139–40.
 13. Goldstein NS. Lymph node recoveries from 2427 pT3
colorectal  resection  specimens  spanning  45  years:
recommendations  for  a  minimum  number  of  recov-
ered  lymph  nodes  based  on  predictive  probabilities.
Am J Surg Pathol. 2002;26:179–89.

 14. Baxter NN et al. Lymph node evaluation in colorectal
cancer  patients:  a  population-based  study.  J  Natl
Cancer Inst. 2005;97:219–25.

 15. Weaver  DL.  Pathology  evaluation  of  sentinel  lymph
nodes  in  breast  cancer:  protocol  recommendations
and rationale. Mod Pathol. 2010;23:S26–32.

 16. Maharaj B et al. Sampling variability and its influence
on the diagnostic yield of percutaneous needle biopsy
of the liver. Lancet. 1986;1:523–5.

 17. Olsson R et al. Sampling variability of percutaneous
liver biopsy in primary sclerosing cholangitis. J Clin
Pathol. 1995;48:933–5.

 18. Bedossa P, Dargere D, Paradis V. Sampling variability
of  liver  fibrosis  in  chronic  hepatitis  C.  Hepatology.
2003;38:1449–57.

 19. Colloredo  G,  Guido  M,  Sonzogni  A,  Leandro  G.
Impact of liver biopsy size on histological evaluation
of chronic viral hepatitis: the smaller the sample, the
milder the disease. J Hepatol. 2003;39:239–44.

 20. Cholongitas E et al. A systematic review of the quality
of  liver  biopsy  specimens.  Am  J  Clin  Pathol.
2006;125:710–21.

 21. Chan  J,  Alwahab  Y,  Tilley  C,  Carr  N.  Percutaneous
medical liver core biopsies: correlation between tissue
length and the number of portal tracts. J Clin Pathol.
2010;63:655–66.

 22. Cholongitas E, Burroughs AK. Is it difficult to obtain
an  optimal  liver  biopsy  specimen?  Hepatology.
2009;51:355–6.

 23. Altman DG. Practical statistics for medical research.
London: Chapman and Hall/CRC Texts in Statistical
Science; 1990.

 24. Bedossa P, on behalf of the French METAVIR Study
Group.  Intraobserver  and  interobserver  variations  in
liver  biopsy  interpretation  in  patients  with  chronic
hepatitis C. Hepatology. 2005;20:15–20.

 25. Westin  J,  Lagging  LM,  Wejstål  R,  Norkrans  G,
Dhillon AP. Interobserver study of liver histopathol-
ogy  using  the  Ishak  score  in  patients  with  chronic
hepatitis C virus infection. Liver. 1999;19:183–7.
 26. Grønbaek K et al. Interobserver variation in interpre-
tation  of  serial  liver  biopsies  from  patients  with
chronic hepatitis C. J Viral Hepat. 2002;9:443–9.
 27. Price GJ et al. Computerised diagnostic decision sup-
port system for the classification of preinvasive cervical
squamous lesions. Hum Pathol. 2003;34:1193–203.
 28. Roberts  ISD  et  al.  The  Oxford  classification  of  IgA
nephropathy: pathology definitions, correlations, and
reproducibility. Kidney Int. 2009;76:546–56.

 29.  Cattran  DC  et  al.  The  Oxford  classification  of  IgA
nephropathy:  rationale,  clinicopathological  correla-
tions, and classification. Kidney Int. 2009;76:534–45.
 30. Cross SS, Feeley KM, Angel CA. The effect of four
interventions on the informational content of histopa-
thology  reports  of  resected  colorectal  carcinomas.
J Clin Pathol. 1998;51:481–2.

 31. Branston IK et al. The implementation of guidelines
and  computerized  forms  improves  the  completeness
of cancer pathology reporting. The CROPS project: a
randomized controlled trial in pathology. Eur J Cancer.
2002;38:764–72.

Development of Evidence-Based
Diagnostic Criteria and Prognostic/
Predictive Models: Experience
at Cedars Sinai Medical Center

Alberto M. Marchevsky and Ruta Gupta

13

Keywords
Evidence-based  diagnostic  criteria  •  Prognostic  models  in  pathology
• Evidence-based pathology • Cedar Sinai Medical Center experience in
evidence-based diagnostic criteria

Evidence-based  pathology
(EBP)  offers  a
 conceptual framework and analytical tools to eval-
uate  the  scientific  quality  and  potential  clinical
validity of information published in the literature
[1–3].  EBP  general  concepts  and  principles  also
suggest the specific question-data-method-Bayesian
inference-appraisal  (QDMBA)  paradigm  shown
in Table 13.1. This paradigm can help guide the
review of information in the pathology literature
and  help  formulate  the  experimental  design  of
clinico-pathologic studies. The paradigm is based
on six general assumptions: (a) clinico-pathologic
problems are best approached by explicitly formu-
lating  answerable  patient-based  questions  that
need  to  be  investigated  using  the  literature  and
personal experience, (b) data trumps authority and

A.M. Marchevsky ()
Pulmonary and Mediastinal Pathology,
Department of Pathology and Laboratory Medicine,
Cedars-Sinai Medical Center, Los Angeles, CA, USA
and
David Geffen School of Medicine,
University of California, Los Angeles, CA, USA
e-mail: Alberto.Marchevsky@cshs.org

tradition, (c) the experimental design of studies is
important to optimize the information that can be
obtained  from  available  data  and  generate  the
highest possible evidence level, (d) it is often more
valuable  to  analyze  data  using  a  Bayesian  infer-
ence approach that considers the pre-test and post-
test probabilities of findings rather than analyzing
it with descriptive statistics, (e) the limitations of
the data and experimental design of a study need
to  be  considered  in  the  discussion  and  explicitly
disclosed, and (f) the conclusions of a study need
to be appraised over time with additional prospec-
tive data in a process of continuous improvement.
In  this  chapter,  we  briefly  describe  several
studies  performed  in  our  laboratory  at  Cedars-
Sinai  Medical  Center  exploring  different  ele-
ments of the QDMBA paradigm for the evaluation
of clinico-pathologic problems related mostly to
our area of interest, thoracic pathology. We will
review  these  articles  and  others  from  an  episte-
mological  viewpoint  in  an  effort  to  provide
examples  about  how
the  proposed  “EBP”
approach can potentially improve on the quality
of  the  evidence  generated  by  clinico-pathologic
studies in anatomic pathology.

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_13, © Springer Science+Business Media, LLC 2011

213

214

A.M. Marchevsky and R. Gupta

Table  13.1  Question-data-method-Bayesian  inference-
appraisal  (QDMBA)  paradigm  for  the  evaluation  of
 clinico-pathologic problems

Table 13.2  Specific patient-centered questions: the first
step  to  evaluate  information  using  an  evidence-based
approach

A.  Frame specific patient-based questions regarding
particular diagnoses or other problems of interest
1.  What are we trying to study and why?

B.  Collect data from literature and own experience

1.

 Data (“evidence”) trumps eminence and tradition

Questions are formulated using a root (who, what, where,
how, why) followed by a verb
Background questions

Query for general knowledge regarding disease,
treatment, or other topic

C.  Methodological details of studies are important to

Foreground questions

assess evidence levels

1.

D.  Bayesian inference approach to evaluation data
 Estimate quantitatively or qualitatively the
pre-test probabilities of the pathologic findings or
test results of interest
 Estimate quantitatively or qualitatively the
post-test probabilities of the pathologic findings
or test results of interest

2.

E.  The results and conclusions of a study need to be
appraised over time and updated as more data
becomes available

Questions That Address Specific
Patient-Centered Problems: How
to Ask Practical and Answerable
Questions of Clinical Relevance

Medical  knowledge  is  constantly  evolving  at  an
ever  more  rapid  course.  Pathologists  striving  to
diagnose their cases using the latest classification
schema and latest information regarding the  latest
immunostains,  molecular  tests,  and  other  infor-
mation need to hone their skills at asking relevant
answerable  clinico-pathologic  questions  and  at
developing strategies designed to find this infor-
mation in the literature and integrate it with their
personal  experience  [2,  3].  Unfortunately,  these
are not skills that are generally emphasized or for-
mally taught during pathology residency training.
Various teaching tactics for the formulation of
answerable  clinical  questions  are  described  in
detail  in  the  excellent  book  on  “Evidence-Based
Medicine  How  to  Practice  and  Teach  EBM”  by
Straus et al. Queries are categorized as “background”
and  “foreground”  questions  (Table  13.2)  [4].
“Background”  questions  are  designed  to  ask  for
general knowledge regarding a disease,  treatment,
pathologic condition, or other topic. They are for-
mulated using a question root such as who, what,
where,  when,  how,  why,  followed  by  a  verb.

Query for specific knowledge to inform clinical
decisions or actions

Background questions generally address a specific
disease, pathologic entity, test, or other aspect of
health care. Examples of background type question
are  as  follows:  What  is  the  etiology  of  diffuse
alveolar  damage?  How  do  carcinomas  of  the
prostate usually disseminate? Why is there necrosis
in cases of invasive aspergillosis?

“Foreground”  questions  according  to  Straus
et al. [4] query for specific knowledge to inform
clinical  decisions  or  actions.  They  have  four
essential  components:  (1)  patient-specific  prob-
lem, (2) intervention or exposure, (3) comparison
if  relevant,  and  (4)  clinical  outcomes,  including
time frame if relevant. In anatomic pathology, the
four  components  of  foreground  questions  can
probably be simplified into three: patient-specific
problem,  pathologic  examination  or  laboratory
test, and relevance for patient care (prognosis or
prediction  of  response  to  specific  therapy).
Examples of background type question are as fol-
lows: Which immunostains should be used  during
the evaluation of transbronchial biopsy to differ-
entiate adenocarcinoma from squamous cell car-
cinoma?  How  many  immunostains  should  be
used to distinguish malignant mesothelioma from
adenocarcinoma? What is the prognosis of a non-
smoking woman with a stage I lung adenocarci-
noma that shows the EGFR gene mutation?

Why Bother Formulating Clear
Questions?

Straus  et  al.  [4]  suggest  that  formulating  well-
designed  patient-centered  questions  can  help
practitioners in seven ways: (1) help focus scarce
learning  time  into  gathering  knowledge  that  is

13  Development of Evidence-Based Diagnostic Criteria and Prognostic/Predictive Models

215

relevant  to  our  patients  needs,  (2)  help  focus
learning  on  evidence  that  addresses  specific
aspects of practice, (3) suggest high-yield search
strategies to find relevant information in the lit-
erature, (4) suggest how the answers can be for-
matted  to  provide  clinically  useful  information,
(5) help in communication with other physicians,
(6) provide a teaching tool to help train students,
residents,  and  others,  and  (7)  help  grow  our
knowledge  base  as  the  questions  are  answered.
Most  of  these  concepts  probably  apply  to  the
practice and learning of anatomic pathology and
laboratory medicine.

Data (“Evidence”) Trumps Eminence
and Tradition
The  evidence-based  medicine  (EBM)  literature
reveals some ongoing tension between the data-
based  approach  favored  by  EBM  advocates  and
the  more  traditional  teaching  and  practice  of
medicine  that  reveres  personal  experience  and
clinical expertise [5–10]. This debate has resulted
in the use of some colorful acronyms. For exam-
ple,  EBM  advocates  have  proposed  the  term
“Eminence-based  Medicine”  to  deride  the  prac-
tice of medicine based on the opinion and advice
of  recognized  experts,  while  some  of  the  latter
physicians  grumble  about  “Evidence-Slaved
Medicine,”  “Economy-Based  Medicine,”  and
“Cookbook  Medicine”  [11–19].  A  detailed  dis-
cussion of the arguments for each of these com-
peting views of the current practice of medicine is
beyond the scope of this chapter. Briefly, one can
conclude  that:  (1)  the  “best  evidence”  collected
by randomized clinical trials and revered by EBM
aficionados  as  the  best  type  of  available  knowl-
edge has some limitations and/or is often nonex-
istent,  (2)  there  are  many  medical  interventions
that  have  never  been  validated  in  randomized
clinical trials but are yet very valuable for patient
care, and (3) there are various widely used medi-
cal practices that are either wasteful, ineffectual
and/or not supported by current knowledge.

Pathologists, a particularly conservative group
of physicians, have generally ignored this debate
and  continued  pursuing  the  testing  of  various
specimens  with  the  latest  available  technology
and using in their daily practice various disease
classification  schemas  developed  years  ago  by

groups  of  experts  and  updated  over  time.
Diagnostic classes tend to be split over into mul-
tiple  subclasses  with  limited  debate  regarding
their  diagnostic  reproducibility  and  clinical
applicability.  In  addition,  schemas  such  as  the
current  World  Health  Organization  (WHO)
 classifications of lung neoplasms, sarcomas, and
other  neoplasms  do  not  generally  incorporate
current  information  regarding  the  results  of
immunostains and/or molecular studies as defini-
tional  criteria,  although  these  tests  are  being
widely used and probably variably interpreted by
different pathologists [20, 21].

EBP  advocates  the  critical  evaluation  of  the
purpose  of  classification  models,  and  the  evi-
dence levels of the data supporting various clas-
sification  models  and  other  practices.  These
efforts will hopefully advance anatomic pathology
and  laboratory  medicine  into  more  scientific
endeavors,  although  it  is  fully  recognized  that
there  is  a  considerable  “art”  component  in  the
practice of pathology related to the nature of the
field and the variable ability and clinical experi-
ence of different practitioners.

Widely Accepted and/or Long-Held
Practices and “Traditions” Need
to Be Changed When Not Supported
by Current Best Evidence

In  instances  where  the  best  available  evidence
does not support widely accepted and/or long-held
practices,  EBP  advocates  for  a  change.  Recent
studies of thymomas performed in our laboratory
using various elements of the QDMBA paradigm
can  be  used  to  illustrate  this  problem  [22–24].
Generations of pathologists have been trained by
eminent experts to evaluate thymomas very care-
fully for the presence of microscopic transcapsular
invasion  [25].  Indeed,  a  previous  classification
schema of thymomas advocated the classification of
the  tumors  into  benign  or  malignant  thymomas
based on the absence or presence of local invasion
[25].  In  addition,  thymomas  that  exhibit  micro-
scopic transcapsular invasion have been classified
by Masaoka et al. [26] and others since the early
1980s  as  stage  II  disease.  These  concepts  were
accepted for many years and advocated by one of

216

A.M. Marchevsky and R. Gupta

us (AM) in the 1980s in two subsequent editions a
book  devoted  to  the  surgical  pathology  of  medi-
astinal lesions and in other publications [27–29].
However,  after  becoming  interested  in  EBP,  we
decided to evaluate, following some of the meth-
ods described in this book, whether these widely
accepted concepts are actually supported by best
evidence  [24].  We  formulated  two  simple  back-
ground type questions: Is there a significant differ-
ence  in  prognosis  between  patients  with  stages
I and II thymoma? What level of evidence is avail-
able to answer the previous question? A system-
atic  review  of  the  literature  was  performed  and
only  level  III  data  were  found.  The  systematic
review did not find any randomized clinical trials
or level II studies in the English literature evaluat-
ing  the  prognostic  significance  of  transcapsular
invasion  in  thymoma  patients.  The  level  III
data from 2,451 thymomas reported in 21 studies
were  analyzed  with  meta-analysis,  showing  no

significant   survival  differences  between  patients
with Masaoka stages I and II thymomas (Fig. 13.1a,
b). The lack of  significant differences in the prog-
nosis  of  patients  with  stages  I  and  II  thymomas
supports  the  notions  that  (1)  evaluation  of  tran-
scapsular  invasion  is  of  limited  clinical  value  in
tumors that lack invasion of neighboring organs or
the pleura and (2) the staging schema for thymo-
mas needs to be updated. Interestingly, review of
the seminal study of 27 patients with stage I thy-
momas  and  7  patients  with  stage  II  disease  by
Masaoka  et  al.  [26]  showed  that  while  patients
with clinical stages I and II thymoma had slightly
different  92.6  and  85.7%  5-year  survival  rates,
respectively,  these  apparent  survival  differences
were not statistically significant. In summary, this
is a simple example of how certain concepts that
had been taught by eminent physicians for many
years  to  the  point  of  becoming  a  “tradition”  are
found to lack best evidence to support them.

Fig. 13.1 ( a, b) The level III data from 2,451 thymomas
reported in 21 studies were analyzed with meta-analysis,
showing  no  significant  survival  differences  between

patients  with  Masaoka  stages  I  and  II  thymomas  (From
Gupta et al. [24]; with permission)

13  Development of Evidence-Based Diagnostic Criteria and Prognostic/Predictive Models

217

Fig. 13.1  (continued)

The Experimental Design of Studies
Is Important: Evidence Levels

Table 13.3  Proposed scale of evidence levels for publi-
cations in pathology and laboratory medicine

Level 1    Case-control studies with external validation of

The  importance  of  selecting  the  correct  method-
ological approach to particular clinico-pathologic
problems and the evaluation of the quality of the
information  published  in  the  literature  using
“evidence levels” is discussed in detail in Chap. 11
[30]. The discussion included the pathology-specific
scale  of  evidence  levels  shown  in  Table 13.3.  In
summary,  well-designed  prospective  studies  that
validate  their  results  using  prospective  data  that
was not used to develop the proposed diagnostic
criteria or other results are given the best marks as
level I or II evidence [30]. By contrast, clinicians
generally  classify  the  information  published  in
well-designed  randomized  prospective  clinical
trials as level I evidence [4, 10, 31–34].

Level 2

results, using prospective validation data-sets
from other institutions
Meta-analyses of level 2 studies
“Expert” recommendations based on
 meta-analyses of level 2 or 3 studies

 Case-control studies with validation of results,
using prospective validation data-sets from the
same institution
Meta-analyses of level 3 studies
“Expert” recommendations based on a
systematic review of literature without formal
meta-analyses

Level 3

 Case-control studies with validation of results
using retrospective validation datasets from the
same institution

Level 4

Level 5

 Case-control studies without validation
 Case series without controls, or individual case
reports

218

A.M. Marchevsky and R. Gupta

The Importance of Disclosing the
Potential Flaws of the Interpretations
of Results

The formulation of patient-centered questions can
help evaluate the validity of the conclusions of a
study  and  suggest  future  investigations.  For
example,  the  meta-analysis  described  above
showing  no  significant  prognostic  differences
between  patients  with  stages  I  and  II  thymoma
yielded conclusions that were limited by the fact
that some patients with stage II disease had been
treated  with  postoperative  radiation  therapy  in
some of the studies included in the analysis [24].
This selective treatment of some patients suggests
the following two patient-centered questions: Is it
possible that the prognosis between patients with
stages  I  and  II  thymomas  was  not  significantly
different  because  some  individuals  with  stage  II
disease  had  received  radiation   therapy  while
patients with stage I disease have not? What best
evidence is available to evaluate the effect of radi-
ation therapy in patients with stage II thymoma?
This was recently investigated using another sys-
tematic review of best evidence with meta-analysis
[35].  The  study  showed  that  radiation  therapy
does  not  significantly  change  the  prognosis
of  patients  with  stage  II  thymomas,  supporting
the concept that the lack in significant prognostic
differences between patients with stages I and II
thymomas does not result from treatment effect.

The Importance of Evaluating
Whether a Study Analyzed
a Sufficient Sample Size

As explained in Chap. 8, sample size estimations
and  power  analysis  are  currently  routinely  per-
formed in clinical trials and other clinical studies
but are seldom performed in studies published in
the anatomic pathology literature. This can lead
to overly optimistic or pessimistic evaluations of
negative results. For example, in a recent meta-
analysis  of  905  thymomas  classified  by  WHO
and  staged  by  Masaoka  staging,  collected  from
multiple hospitals in Asia, Europe, and California,

we  concluded  that  the  only  WHO  histologic
type  of  thymomas  that  provided  prognostic
 information  independent  of  stage  was  A  in
stage  III  disease  [36].  However,  power  analysis
showed  that  7,077  cases  were  really  needed  to
exclude the possibility that other WHO histologic
types of thymoma may provide stage-independent
prognostic  information  to  a  power  of  80%.
A  similar  problem  was  encountered  in  a  recent
study  evaluating  the  prognostic  significance  of
isolated tumor cells and micrometastases in the
intrathoracic  lymph  nodes  of  patients  with
adenocarcinoma  and  other  nonsmall  cell  carci-
nomas of the lung [37]. The study was the largest
to date and included review of 4,148 lymph nodes
from 266 of our own patients and meta-analysis
of all cases reported in the English literature. It
concluded  that  there  was  no  evidence  that  the
presence of these small metastatic deposits was
of  prognostic  significance.  However,  power
analysis  showed  that  even  this  seemingly  com-
prehensive study was considerably underpowered,
as  3,060  patients  followed  for  60  months  were
needed to achieve 80% power [37].

Bayesian Inference Can Be More
Useful for Clinical Purposes than
Analysis of Data with Descriptive
Statistics: The Importance
of Distinguishing Prior Probabilities
from Posterior Probabilities

Bayesian inference is an analytical method based
on  the  principles  of  Bayesian  statistics  that
involves evaluating how the degree of belief in a
hypothesis changes as additional data or evidence
is  collected  [38–44].  Bayes’  theorem  adjusts
probabilities given new data using the formula

P(A\B)

=

P(B\A) P(A)
P(B)

where,  P (A\B):  posterior  probability  given
prior  probability  A  probability  from  event  B.
P(A) = prior probability of A that does not take into
account event B. P(B) probability after collecting

13  Development of Evidence-Based Diagnostic Criteria and Prognostic/Predictive Models

219

information from event B. Pathologists certainly
do  not  need  to  estimate  Bayesian  statistics  in
daily practice but would probably benefit from an
understanding  of  the  differences  between  prior
probabilities, the likelihood of certain diagnoses
or other events prior to the evaluation of patho-
logic  specimens  or  laboratory  samples,  from
posterior probabilities, the likelihood of certain
conclusions  after  the  pathologic  or  laboratory
samples  are  examined  [45–49].  Surprisingly,
this simple distinction is often lost in the ana-
tomic pathology literature when studies reporting
the diagnostic value of a new test are designed
following the format: “cases classified as either
A or B by histopathology were tested with new
test  C.  New  test  C  was  positive  in  95%  of  A
cases,  therefore  test  C  is  very  useful  for  the
differential  diagnosis  between  A  and  B.”  One
could argue that the prior probability of correct
diagnoses of either A or B by histopathology in
this group of patients was 1.0. It is not possible
for a new test C to provide posterior probabili-
ties higher than 1.0 in the same population.

Ironically,  many  anatomic  pathologists  have
probably  been  trained  not  to  use  a  qualitative
Bayesian  inference  process.  For  example,  it  is
often recommended that histologic slides should
be looked at without prior clinical information,
not to “bias” the observations, although to our
knowledge  there  are  no  studies  showing  that
learning  about  the  clinical  history  of  a  patient
prior  to  pathologic  examination  decreases  the
quality  of  diagnostic  interpretations.  The  pro-
cess  of  Bayesian  inference  that  analyzes  how
the  results  improve  sequentially  as  additional
evidence is available is also not usually recom-
mended during the utilization process of immu-
nostains  or  other  tests.  By  contrast,  use  of  the
concept of prior and posterior probabilities can
be  helpful  in  daily  practice.  For  example,  in  a
recent  review  of  the  pathology  of  metastatic
lesions,  we  explained  how  estimating  the  prior
probabilities of the most probable diagnoses in a
particular  patient,  based  on  gender,  age  and
location of the lesions, and the development of a
short  list  of  the  most  likely  diagnoses  prior  to
the  evaluation  of  histologic  slides  and/or  the

performance of immunostains or others tests can
improve on the diagnostic process and guide the
selection  of  appropriate  immunohistochemical
tests [50].

Utilization  of  the  Bayesian  inference  process
could also probably improve on the quality of future
clinico-pathologic studies in anatomic pathology.
Indeed,  most  studies  in  anatomic  pathology  have
evaluated their results by comparing the data in two
or more populations with univariate and multivari-
ate statistics and/or survival statistics. The results
of these studies often provide important insights
regarding  the  clinical  significance  of  particular
findings in different populations of interest, but
it is often difficult to apply their conclusions to
the  evaluation  of  tissue  specimens  or  clinical
laboratory  samples  from  individual  patients,  as
there is often some overlap in values, as explained
in the next section.

Use of Probabilities, Odds,
and Various Ratios to Sort Out
Overlapping Diagnostic Criteria

Most  pathologic  entities  exhibit  a  spectrum  of
pathologic  findings  that  overlap  to  some  extent
with those present in other entities that need to be
considered in a differential diagnosis. A similar
problem is present during evaluation of the results
of  immunostains,  molecular  and  other  tests,  as
there  are  few  ancillary  tests  that  provide  100%
specificity  for  a  particular  diagnosis.  Seasoned
pathologists  usually  interpret  the  presence  of
overlapping  diagnostic  criteria  and  test  results
using a qualitative approach that places available
information “in context” based on prior clinical
experience, and decide whether a particular diag-
nosis  is  more  likely  than  others  based  on  prior
experience.  Somewhat  surprisingly,  there  have
been  relatively  few  studies  where  a  similar
approach  has  been  applied  in  a  more  formal,
quantitative manner using the mathematical con-
cepts  of  probabilities,  odds,  probability  ratios,
odds  ratios,  and  likelihood  ratios.  By  contrast,
these  metrics  have  been  widely  used  in  labora-
tory medicine.

220

A.M. Marchevsky and R. Gupta

Recent studies from our laboratory evaluating
the  diagnosis  of  bronchioloalveolar  carcinoma
(BAC), well-differentiated pulmonary adenocar-
cinoma, and carcinoid tumor on frozen sections
can  be  used  to  illustrate  the  potential  value  of
using the Bayesian inference process in anatomic
pathology  [51,  52].  Table  13.4,  taken  from  the
study  comparing  BAC  and  well-differentiated
adenocarcinoma  with  reactive  epithelial  atypia,

Table  13.4  Incidence  of  11  statistically  significant
parameters in cases of reactive atypical epithelial hyper-
plasia and BAC or well-differentiated adenocarcinomas of
the lung

Parameter
Grossly evident nodule or lesion
Abrupt transition
Multiple patterns of growth
Granuloma
Anisocytosis
Proportion of atypia
Nuclear pseudoinclusion
Macronucleoli
N/C >80
Irregular nuclear membrane
Atypical mitoses
From Gupta et al. [52]. © 2003–2010 American Society
for Clinical Pathology. © 2003–2010 American Journal of
Clinical Pathology

RA (%)
50
62.5
18.5
30.90
16.98
33.9
44
 0
44.6
53.7
 0

AC (%)
93.1
88.46
61.5
  3.8
57.69
80.76
73.91
  8.5
84.61
84.61
25.7

shows the incidence of 11 histopathologic features
that in our clinical experience can be helpful to
distinguish  well-differentiated  adenocarcinomas
of the lung and BAC from reactive atypia on fro-
zen  section  [52].  They  include  histopathologic
features  such  as  the  abrupt  transition,  nuclear
cytoplasmic  ratio  and  others  that  are  more  fre-
quent in BAC and adenocarcinomas than in reac-
tive  type  II  pneumocyte  atypia  and  others,  and
other criteria that are more frequent in the latter
condition,  such  as  the  presence  of  granulomas
(Fig.  13.2).  However,  most  of  these  histopatho-
logical features are present in significantly differ-
ent proportions in the two populations of interest
(malignant  vs.  benign)  as  shown  in  Table  13.5.
How can a pathologist use this variable informa-
tion to diagnose a single lung biopsy? One pos-
sible  approach  is  to  rely  on  the  sensitivity  and
specificity of each pathological feature, by diag-
nosis, shown in Table 13.5. This type of informa-
tion  poses  interpretation  conundrums  as  it  is
difficult
reconcile  variable  sensitivities
and  specificities.  Looking  at  Table  13.5,  taken
from  the  same  study,  how  can  a  pathologist
decide  whether  “grossly  evident  nodule/lesion”
with sensitivity of 0.95 and specificity of 0.52 is
better or worse for the diagnosis of malignancy
than “anisocytosis” with a sensitivity of 0.56 and

to

Fig. 13.2  They include
histopathologic features
such as the abrupt
transition, nuclear
cytoplasmic ratio and
others that are more
frequent in bronchioloal-
veolar carcinoma (BAC)
and adenocarcinomas than
in reactive type II
pneumocyte atypia and
others, and other criteria
that are more frequent in
the latter condition, such as
the presence of granulomas
(From Gupta et al. [24];
with permission)

13  Development of Evidence-Based Diagnostic Criteria and Prognostic/Predictive Models

221

Table  13.5  Analysis  of  “statistically  significant”  diagnostic  features  for  the  diagnosis  of  pulmonary  BAC  or
well-differentiated adenocarcinoma using Bayesian statistics

Chi square
test (p value)
0.00
0.016
0.00
0.006
0.00
0.00
0.017
0.011
0.001
0.007
0.00

Feature
Grossly evident nodule/lesion
Abrupt transition
Multiple growth patterns
Granuloma
Anisocytosis**
Proportion of atypia
Nuclear pseudoinclusions
Macronucleoli
N/C ratio >80%
Irregular nuclear membrane
Atypical mitoses
a Bayesian statistics
** Anisocytosis was noted when the size of atypical epithelial cells varied by 3 times or more the size of the neighboring
epithelial cells
From Gupta et al. [52]. © 2003–2010 American Society for Clinical Pathology. © 2003–2010 American Journal of
Clinical Pathology

Sensitivitya Specificitya Odds ratioa Relative riska
0.95
0.88
0.64
0.004
0.56
0.81
0.74
0.12
0.85
0.85
0.38

Likelihood
ratioa
1.99
1.40
3.77
0.13
3.21
2.30
1.68
9,999
1.83
1.58
9,999

7.75
3.172
0.301
0.140
3.125
4.2
2.470
3.348
3.862
3.125
3.528

0.52
0.37
0.83
0.70
0.83
0.65
0.56
1.00
0.54
0.46
1.00

19.25
0.217
7.04
0.089
0.150
0.129
0.277

0.157
4.74
0.00

specificity  of  0.83?  Another  approach  to  the
interpretation of overlapping data is to use simple
statistics favored in the EBM literature, such as
probability  ratios,  odds  ratios,  and  likelihood
ratios. These metrics, also shown in Table 13.5,
allow sorting out the diagnostic value of each his-
topathological  criteria  or  combinations  of  fea-
tures by diagnosis and for use of the information
for  the  diagnosis  of  single  patients.  Probability
ratio or relative risk (RR) is the ratio of the prob-
ability of an event occurring in a population ver-
sus the probability of taking place in another. For
example, if a particular diagnostic feature is pres-
ent in 80% of A cases and 20% of B cases, the
probabilities of such as features being present in
populations A and B are 0.8 and 0.2 respectively.
The RR is 4 for population A as compared to B,
indicating that the feature is 4× more probable to
be present in the first of the two populations. As
explained in Chap. 4, odds are estimated by the
simple formula: odds = probability/(1−probability).
For  example  of  a  probability  of  0.8,  the  odds
would be 0.8/(1−0.8) = 4. OR offer a measure of
effect size that describes the strength of associa-
tion  between  two  binary  data  values.  RRs  are
easier to interpret and offer more intuitive data.
OR is usually used with logistic regression and in
situations where RR cannot be readily estimated.
RR and OR do not take into account the  prevalence

of different populations. LR is based on  sensitivity
and  specificity  and  therefore  takes  into  account
the prevalence of different conditions. LR+ = sen-
sitivity/(1−specificity)  and  provides  of  the  pres-
ence of a particular finding in a population that
combines  both  the  sensitivity  and  specificity  of
such  feature.  LR− = 1−sensitivity/specificity  and
provides an estimate of the potential validity of a
negative test.

Table 13.5 and Fig. 13.2 show how the infor-
mation provided by these ratios can be used in a
more intuitive manner for the diagnosis of indi-
vidual frozen sections as more closely resemble
the  reasoning  process  that  is  usually  used  by
pathologists  for  a  differential  diagnoses:  “diag-
nosis A is more likely than diagnosis B because
of  the  presence  of  a  combination  of  particular
histologic  features.”  If  we  look  at  the  various
LR+ listed in Table 13.5, the presence of macro-
nucleoli  and  atypical  mitoses  strongly  supports
the possibility of malignancy, while others such
as N/C ratio > 80% and irregular nuclear mem-
brane  are  less  valuable.  Figure  13.2  shows  this
information  in  a  simple  graphical  manner.  In
this  figure,  the  vertical  line  separates  the  two
diagnoses, reactive atypia to the left and adeno-
carcinoma  to  the  right.  Features  with  high
LR+ such as macronucleoli and atypical mitosis
show  almost  no  overlap  in  the  two  diagnoses,

222

A.M. Marchevsky and R. Gupta

while others such as the irregular nuclear mem-
branes  and  abrupt  transition  show  considerable
overlap.  Although  the  information  in  Fig.  13.2
does  not  provide  a  pathologist  with  absolute
diagnostic  criteria  for  the  diagnosis  of  reactive
atypia  or  adenocarcinoma,  it  can  be  helpful  to
evaluate the likelihood of any of the two diagno-
ses  based  on  the  presence  of  features  with  the
highest LR+.

Use of Probability, Odds,
and Likelihood Ratios for the Selection
of Cost-Effective Immunohistochemistry
and Other Ancillary Tests

There are no widely used methodologies to help
develop  evidence-based  guidelines  for  the  cost-
effective  utilization  of  immunostains  and  other
diagnostic and prognostic tests in anatomic pathol-
ogy. Pathologists have to rely on the information,
usually presented in tables, included in books and
other publications. However, as there are few anti-
bodies or other tests that are 100% specific for any
one diagnosis, these tables frequently show results
using variable number of +/− or listing the sensi-
tivity and specificity data of each test. Tables 13.6
and 13.7 show an example of the variable sensi-
tivity and specificity of different immunostains in
cases of malignant mesothelioma and adenocarci-
noma, collected in a recent systematic review [53].
In  the  presence  of  such  variability,  pathologists

often rely on the advice of recognized experts in
the field to select which markers are more helpful
and how many markers should be used, although
experts often do not completely agree with each
other. Surprisingly, there has been little interest in
the application of simple statistics such as proba-
bility  ratios  (risk  ratios),  odds  ratios,  and  likeli-
hood ratios that can be very helpful to sort out the
information that is more likely to provide a par-
ticular answer of interest. For example, a system-
atic review of 88 publications provided information
about the use of 15 antibodies for the differential
diagnosis  between  pulmonary  adenocarcinoma
and  malignant  mesothelioma  and  listed  the
 opinions  of  various  experts  [53].  The  review
showed that while most studies identified that cer-
tain antibodies such as calretinin, Wilm’s tumor-1
(WT-1), Ber-EP4, and others were most helpful in
this differential diagnosis, various experts did not
agree  about  how  many  immunohistochemical
tests were necessary and which antibodies needed
to  be  included  in  a  panel.  Analysis  of  OR
(Table 13.8) clearly showed that the use of a large
number  of  antibodies  was  considerably  worse
than the use of even 1 marker and that 7 antibod-
ies provided optimal sensitivity and specificity for
this differential diagnosis: MOC-31, BG8, CEA,
TTF-1,  CK5/6,  WT-1,  and  HBME-1  [53].
Table 13.9 shows that the OR provided by selected
panels of immunostains for the diagnosis of epi-
thelioid  malignant  mesothelioma.  For  example
use  of  all  15  markers  provides  OR = 9.46  while

Table 13.6  Summary of data from the literature averag-
ing the results from multiple studies: sensitivity and speci-
ficity  of  carcinoma  markers  for  identifying  pulmonary
adenocarcinomas  during  the  differential  diagnosis  with
malignant mesothelioma

Table 13.7  Summary of data from the literature averag-
ing the results from multiple studies: sensitivity and speci-
ficity  of  mesothelial  markers  for  identifying  epithelioid
malignant mesothelioma during the differential diagnosis
with adenocarcinoma

Sensitivity (%) Specificity (%)
Marker
83
CEA (n = 1,524)
80
Ber-EP4 (n = 702)
80
B72.3 (n = 769)
72
LEU-M1 (p = 1,473)
93
MOC-31 (n = 213)
86
E-Cadherin (n = 183)
72
TTF-1 (n = 366)
Lewis-BG8 (n = 213)
93
From Westfall et al. [54], with permission of Wiley

95
90
93
93
93
82
100
93

Sensitivity (%)
83
62
82
85
61

Marker
CK5/6 (n = 402)
Vimentin (n = 773)
Calretinin (n = 885)
HBME-1 (n = 769)
Thrombomodulin
(n = 831)
78
N-Cadherin (n = 151)
WT-1 (n = 264)
77
From Westfall et al. [54], with permission of Wiley

Specificity (%)
85
75
85
43
80

84
84

13  Development of Evidence-Based Diagnostic Criteria and Prognostic/Predictive Models

223

Table 13.8  Odds ratios of negative immunoreactivity in malignant mesothelioma

Proportion of
negative MM*
0.95
0.9
0.93
0.93
0.93
0.82
0.82
1.00
0.17
0.38
0.18
0.15
0.39
0.22
0.23

Epitope
CEA
Ber-EP4
B72.3
LEU-M1
MOC-31
E-Cadherin
TTF-1
Lewis-BG8
CK5/6
Vimentin
Calretinin
HBME-1
Thrombomodulin
N-Cadherin
WT-1
* Malignant mesothelioma
**Adenocarcinoma

MM # cases
1,818
899
700
1,204
276
218
240
197
402
773
885
769
831
151
264

Proportion of
negative AC**
0.17
0.20
0.20
0.28
0.07
0.14
0.28
0.07
0.85
0.75
0.85
0.43
0.8
0.84
0.96

AC # cases
1,524
702
769
1,473
213
183
366
231
402
815
912
676
964
121
213

Odds ratio
92.76
36.00
53.143
34.16
176.51
27.98
1,233.19
176.51

0.036
0.204
0.04
0.23
0.16
0.54
0.01

Table 13.9  Odds ratios of selected panels of immunostains for the diagnosis of epithelioid malignant mesothelioma

Panel
A All 15 markers reviewed in the study by King and associates
B 7 Markers selected for their superior specificity and specificity (CEA, MOC-31, TTF-1, BG8,

Odds ratio
9.46
27.01

CK5/6, WT-1, and HBME-1)

C 2 Mesothelial markers with the best individual OR (CK5/6 and WT-1)
D 2 Epithelial markers with the best individual OR (MOC-31 and TTF-1)
E Combination of the “best” two mesothelial and epithelial markers (MOC-31, TTF-1, CK5/6, and WT-1)
F Combination of the “best” mesothelial and epithelial markers (TTF-1 and WT-1)

34.44
198.18
48.63
96.34

use of the best mesothelial and epithelial markers
yields OR = 96.34.

Similar  methodology  helped  optimize  the
selection of antibody panels for the evaluation of
pleural effusions with malignant epithelioid cells
[54].  Table  13.10  shows  that  while  presentation
of  data  using  the  sensitivity  and  specificity  for
each antibody used for the diagnosis of mesothe-
lioma and carcinoma by site of origin shows con-
siderable overlap, analysis of the data. Analysis
of this data using post-test odds helps stratify the
results  by  differential  diagnosis.  As  a  result  of
this information, we were able to select antibody
panels  for  male  (calretinin,  TTF-1,  PSA,  and
CDX2)  and  female  patients  (calretinin,  TTF-1,
ER, and CA125) that provided the most optimal
information to evaluate the site of origin of a met-
astatic carcinoma in a pleural effusion.

Back to the Future: Is Molecular
Pathology Going to Replace
Pathologic Diagnoses? Classification
and Prognostic/Predictive Models
Based on Multivariate Data Analysis

Rapid  advances  in  molecular  pathology  suggest
the possibility that molecular tests will be able to
identify in the near future various conditions that
are  currently  being  diagnosed  by  pathologists
using microscopy. From an epistemological stand-
point, these claims are somewhat reminiscent of
the interest 2 or 3 decades ago at developing image
analysis  systems  that  could  diagnose  pathologic
and  cytologic  samples  objectively  and  reliable
[49]. These investigations led to the development
of image analysis systems for the semi-automatic

224

A.M. Marchevsky and R. Gupta

Table 13.10  Post-test odds of positive immunoreactivity by antibody and diagnosis in pleural effusions with malignant
mesothelioma or metastastic carcinomas

Diagnosis (post-test odds)
Lung
Mesothelioma
0.6
0.0
0.6
0.0
1.7
0.0
0.0
4.0
0.3
3.0
0.0
0.2
1.4
0.0
0.2
N/A
0.0
∞
0.1
N/A
0.0
N/A
0.3
N/A
0.0
N/A
0.0
N/A

Antibody
Ber-EP4
MOC-31
CEA
Calretinin
CK5/6
WT-1
CK7
CK20
TTF-1
ER
PR
CA125
CDX2
PSA
N/A Not applicable

Breast
0.5
0.6
0.1
0.0
N/A
0.7
0.4
0.0
0.0
4.0
∞
N/A
N/A
N/A

Müllerian
0.2
0.2
0.0
0.2
0.0
0.7
0.1
0.0
0.0
0.2
0.0
3.5
0.0
N/A

Stomach
0.1
0.1
0.1
0.0
N/A
N/A
0.0
0.2
0.0
N/A
N/A
N/A
0.5
0.0

Colon
0.0
0.0
0.1
0.0
0.0
N/A
0.0
1.5
0.0
N/A
N/A
N/A
2.0
N/A

Prostate
0.0
0.0
0.0
N/A
N/A
N/A
0.0
0.0
0.0
N/A
N/A
N/A
N/A
∞

screening  of  pap  tests  that  evaluate  multiple
features  with  multivariate  statistics,  neural
 networks, or other mathematical tools for reason-
ing  with  uncertainty  [55–57].  Some  of  these
instruments  are  currently  approved  by  the  Food
and Drug Administration (FDA) for clinical use.
However,  the  road  to  the  development  of  auto-
mated image analysis systems for diagnosis was
difficult, not because of the lack of resolution of
the image analysis systems or of computer power,
but partly because of the difficulties at validating
the results of studies so that they would be appli-
cable to the population at large of pap tests. The
analysis of data in these systems is based on the
analysis  of  multivariate  data  using  methods  that
apply  probability  theory.  Small  errors  due  to
chance that are acceptable within the limits of the
statistical test tend to become magnified as a large
number  of  variables  are  analyzed,  resulting  in
occasional  spurious  outputs  derived  from  data
over fit or shrinkage. For example, if a feature is
statistically significant to p = 0.001 in two differ-
ent entities, there is 1 in 100 chance that it will be
encountered  in  the  wrong  end  of  a  differential
diagnosis.  When  this  1%  is  propagated  through
hundreds  of  features,  it  can  lead  to  spurious
results. Validation of these systems requires large
numbers  of  test  cases  that  are  often  difficult
and  very  costly  to  gather  and  analyze  [56,  57].

In general, a ratio of ten test cases per variable is
recommended  to  minimize  the  probability  of
errors due to data overfit [47].

Molecular Classifications Based
on Multivariate Data

Molecular  classifications  and  prognostic/predic-
tive models using information from multiple genes
analyzed with high-throughput methods will likely
face similar problems when the data is analyzed
with bioinformatics techniques [58–60]. Our pre-
vious study exploring the development of classifi-
cation models for lung cancer cell lines based on
DNA methylation markers can help illustrate the
problem  of  attempting  to  classify  pathologic
lesions  using  molecular  data  [61].  We  evaluated
well-characterized  cells  lines  of  small  cell  lung
cancer and nonsmall cell lung cancer for the pres-
ence of DNA methylation levels at 20 loci, using
the  real  time  PCR  assay  MethyLight.  Cell  lines
were divided into various training set and test sets.
Cases were rotated to be included in some of the
training and test sets, using jackknife techniques.
The  data  were  analyzed  with  linear  discriminant
analysis  and  neural  networks.  The  initial  results
were excellent, and neural network models could
apparently  classify  all  the  cell  lines  with  100%

13  Development of Evidence-Based Diagnostic Criteria and Prognostic/Predictive Models

225

Table 13.11  Classification of test cases (n = 16) by linear discriminant models and artificial neural networks

Linear discriminant
analysis
Number of
correctly classified
cell lines

Linear discriminant analysis
after logarithmic
transformation of the data
Number of
correctly classified
cell lines

Kappa
coefficient

12
10
12
10
10

Kappa
coefficient

Model (training
cell lines n = 71)
Models trained with all variables
1
2
3
4
5
Models trained with five variables (ESR1, MTHFR, PTGS2, CDKN2A, CALCA)
6
7
8
9
10

0.5
0.5
0.75
0.35
0.62

0.50
0.25
0.50
0.25
0.25

0.62
0.25
0.62
0.62
0.62

0.62
0.25
0.75
0.62
0.62

12
12
14
11
13

13
10
13
13
13

13
10
14
13
13

Artificial neural network
Number of
correctly classified
cell lines

Kappa
coefficient

16
16
16
16
16

16
14
14
14
15

1
1
1
1
1

1
0.75
0.75
0.75
0.88

specificity.  However,  when  the  same  data  were
split into different training and test sets, the results
varied, as shown in Table 13.11 [61]. Interestingly,
some  of  the  same  cell  lines  were  classified  as
either small cell or nonsmall cell by different neu-
ral networks or linear discriminant analysis when
the  models  were  trained  using  slightly  different
data  subsets.  These  results  suggest  that  future
studies attempting to classify tumors using multi-
variate  molecular  or  other  data  will  need  to  be
validated with large sets of test data before their
clinical  validity  is  established,  a  process  that  is
likely to be expensive and time consuming, unless
better  data  analysis  methods  are  developed  as  a
result of advances in bioinformatics.

Forecasting Models Based
on Multivariate Data: Beyond Cell Type
and Stage as Predictors of Prognosis
and Response to Therapy

Linear discriminant analysis, multivariate logistic
regression, neural networks, and Bayesian belief
networks  can  also  be  used  to  model  prognostic
systems
that  estimate  prognosis  or  other
 clinical variable using data collected with histo-
pathology,
immunohistochemistry,  and  other
methods [61–64]. These methods have been used
 experimentally  in  our  laboratory  to  estimate  the

 likelihood  of  positive  regional  lymph  nodes  in
patients with breast cancer and colon cancer, the
prognosis of lung cancer patients and other condi-
tions [65–67]. It is beyond the scope of this chap-
ter to discuss this topic in detail, but if pathologists
are willing to explore beyond the standard consid-
erations of using cell type and survival statistics
to predict prognosis and predict therapy response,
there is a wealth of bioinformatics techniques for
the  analysis  of  multivariate  data  that  could  be
used to combine clinico-pathologic data with that
obtained with new IHC and molecular tests. These
models could be specifically designed to estimate
the  prognosis  of  various  diseases  and  likely
response  to  selected  therapies  and  could  help
reestablish  the  traditional  role  of  pathologists
guiding  the  hands  of  surgeons  and  other  physi-
cians in the bioinformatics era.

Appraisal and Integration
of Published Evidence
with Personal Experience

Previous  chapters  of  this  book,  particularly
Chap.  11,  have  described  various  methods  that
can be used by pathologist to evaluate the proba-
ble quality and validity of information published
in the literature. However, it is well known that
the  results  of  a  study  performed  on  a  particular

226

A.M. Marchevsky and R. Gupta

patient  cohort  may  not  apply  to  other  patients,
because of diagnostic variability, demographics,
and other factors. While clinical laboratories have
developed various methods of proficiency testing
to  ensure  that  different  laboratories  will  yield
similar results on the same blood or other sam-
ples,  there  is  relatively  scanty  literature  in  ana-
tomic pathology dealing with the problem of how
best  to  standardize  diagnosis  and  integrate  best
evidence from the literature with personal experi-
ence [68, 69].

Appraisal of Classification Schema
Proposed by Groups of Experts
and Integration into Personal Practice

Classification  schema  proposed  by  groups  of
experts could and probably should be evaluated
by practicing pathologists before they are imple-
mented in routine practice by reviewing the best
evidence that supports various recommendations.
As  there  is  variability  among  different  patient
populations,  it  is  probably  advisable  that  some
preliminary testing be performed to evaluate how
well the new schema can be applied to the diag-
nosis  and  management  of  local  patients.  To
explore  this  subject,  we  recently  evaluated  the
risks  of  malignancy  predicted  by  thyroid  fine-
needle aspiration (FNA) biopsies published by a
group  of  experts  sponsored  by  the  National
Cancer  Institute  [70].  A  review  of  the  publica-
tions listed in the NCI document revealed that the
experts  had  relied  mostly  on  level  III  evidence
based on surgical follow-up. Such information is
probably biased toward higher risk of malignancy
estimates,  as  patients  who  undergo  thyroidec-
tomy  do  so  because  of  the  FNA  results  and/or
other clinical findings. To test this hypothesis, we
analyzed  our  own  data  from  879  patients  who
underwent thyroid FNA at our hospital during a
2-year  period,  using  different  denominators  to
estimate malignancy risks: surgery, repeat FNA,
both surgery or repeat FNA, and all patients as a
surrogate for clinical follow-up [70]. As expected,
the risk estimates for patients with malignant or
suspicious  for  malignant   categories  by  thyroid
FNA  were  similar  for  calculations  performed

using all four denominators. By  contrast, for the
benign  category,  the  risk  estimates  calculated
using  surgical  follow-up  were  considerably
higher  than  for  those  using  surrogate  clinical
 follow-up as the denominator. The study showed
that NCI recommendations were generally valid
for  our  patients  with  a  diagnosis  of  “suspicious
for  malignancy”  and  “malignant”  categories,
while  they  probably  variably  overestimated  the
risk  of  malignancy  for  our  patients  with  other
diagnoses.  It  also  demonstrated  that  stratifying
the diagnostic categories into three groups other
than  nondiagnostic:  “benign,”   “follicular  lesion
of undetermined significance or neoplasm,” and
“suspicious or malignant” resulted in better, non-
overlapping risk predictions.

What Is the Purpose of Classifications
in Anatomic Pathology: Should Lesions
Be Grouped by Histogenesis,
Morphology, or Their Forecasting
Value?

Various  classes  of  classifiers  have  been  used  to
organize  classification  schema  of  tumors  and
other lesions in pathology. Schemas are generally
based on the presumed histogenesis of the  neo-
plasms,  their  morphologic  features  and/or  their
ability to forecast the prognosis of patients and/or
the  efficacy  of  various  therapeutic  measures.
There  is  no  general  agreement  regarding  which
classifiers are preferable. For example, the WHO
classifies tumors of soft tissue and bone using a
histogenetic  or  cell  type  approach,  lung  neo-
plasms using a mixed histogenetic and histomor-
phological  approach,  and  pleural  neoplasms
using a histomorphological approach [20, 21]. In
addition,  some  classification  schemas  of  neo-
plasms attempt to correlate “cell type” or “histo-
logic type” with prognosis while others use tumor
grade for this purpose. Reviewing the classifiers
used in various schemas from an epistemological
point of view, one could propose that classifica-
tions  based  on  morphology  should  strive  to  be
very descriptive with clear and explicit  diagnostic
features so that they can be reproducibly applied
by different pathologists with excellent agreement

13  Development of Evidence-Based Diagnostic Criteria and Prognostic/Predictive Models

227

levels as measured by kappa statistics. By contrast,
schema designed to forecast prognosis or predict
response to therapy should provide precise esti-
mates of the future. In addition, often both types
of classifiers are considered in the organization of
lesions  in  pathologic  classifications,  although  it
is often very difficult to optimize the schema to
achieve both diagnostic reproducibility and excel-
lent forecasting ability. Classifications optimized
for the latter frequently need to incorporate fea-
tures beyond morphology, such as the results of
tumor markers and other tests, disease stage, and
other  clinical  considerations  and  effects  of
therapy.

Recent studies of thymomas can also be used
to illustrate the concept that, as the specific pur-
poses of various classification schemas are often
not explicitly listed by their authors, there is some
confusion in the way pathologists currently tend
to organize and use classification schema. Thymic
epithelial  neoplasms  are  currently  classified  by
WHO based on their histomorphology into thy-
momas types A, B1, B2, B3, AB, and thymic car-
cinomas [22, 23, 71]. We explored the forecasting
ability of this classification schema by perform-
ing  a  systematic  review  with  meta-analysis  of
available best evidence for patients with thymo-
mas classified by WHO criteria. As in the previ-
ous study, only level III data from 2,192 thymomas
reported in 15 studies were identified [36]. Such
best  available  evidence  showed  considerable
variability in the proportions of WHO thymoma
cell types in different studies, suggesting interob-
server  variability  problems.  For  example,  the
proportion of type A thymomas varied from 5 to
24% while the proportion of B3 thymomas varied
from 6 to 34%. This variability suggested that the
subclassification of thymomas according to cur-
rent WHO criteria may not be entirely reproduc-
ible among different pathologists. This conclusion
is  supported  by  the  study  by  Rieker  et  al.  [72]
showing in a large multicenter study that interob-
server  agreement  for  the  subclassification  of
WHO  type  B  thymomas  into  B1,  B2,  and  B3
lesions was only at the low moderate level with
kappa = 0.49. Analyzing the classification scheme
from  the  view  point  of  how  well  it  forecasts
 survival, our meta-analysis showed no significant

survival   differences  for  patients  with  thymomas
A, AB, and B1 [36]. By contrast, there were sig-
nificant  survival  differences  between  patients
with A/AB/B1 thymomas and those with B2 and
B3  thymomas  (Fig.  13.3a,  b),  suggesting  that
only three categories of thymic epithelial lesions
other  than  carcinomas  are  of  prognostic  value.
The results of the meta-analysis raise interesting
questions about how to modify the WHO classifi-
cation  of  thymomas  in  the  future.  Should  these
neoplasms continue to be classified into five his-
tologic types, because of the way they look to at
least some observers under the microscope, and
in spite of interobserver variability problems? If
WHO  continues  to  recommend  a  classification
scheme  including  five  histologic  types  should
they be organized into three grades that appear to
predict survival? Should the classification schema
be  collapsed  into  only  three  histologic  types
based on prognosis? This may reduce the possi-
bility of interobserver variability as pathologists
will  have  fewer  diagnostic  choices  but  would
involve  aggregating  thymomas  A,  AB,  and  B1
that  in  typical  cases  look  different  from  each
other  under  the  microscope.  To  our  knowledge,
there is no consensus among the intellectual lead-
ers  in  pathology  about  how  to  approach  these
types of questions in a consistent manner. In our
view, it would be sensible to develop two types of
classifications for lesions such as thymomas and
other  neoplastic  and  non-neoplastic  conditions:
(1) diagnostic classification schema and (2) mul-
tivariate forecasting models. The diagnostic clas-
sification schema would serve to stratify various
conditions  in  a  manner  that  continues  to  take
advantage from the extensive clinico-pathologic
knowledge  collected  by  physicians  over  many
years.  These  classifications  would  use  very
explicit diagnostic criteria identifiable with gross
pathology,  histopathology,  immunohistochemis-
try  and  molecular  techniques,  and  would  be
designed  to  provide  the  best  possible  interob-
server  diagnostic  agreement  levels,  so  that
patients would be consistently classified with the
same  disease  or  entity  at  different  medical
 facilities.  The  gold  standard  for  this  type  of
 classifications would be very high kappa coeffi-
cients  of  interobserver  agreement.  Multivariate

Fig.  13.3 ( a)  Significant  survival  differences  exist
between patients with A/AB/B1 thymomas and (b) those
with  B2  and  B3  thymomas  (From  Gupta  et  al.  [52].

©  2003–2010  American  Society  for  Clinical  Pathology.
© 2003–2010 American Journal of Clinical Pathology)

13  Development of Evidence-Based Diagnostic Criteria and Prognostic/Predictive Models

229

forecasting  models  are  not  really  classification
schemas  and,  should  probably  be  based  on  multi-
variate statistical analysis or other tools for reason-
ing with uncertainty such as decision tree analysis,
neural  networks,  Bayesian  belief  networks,  and
others. Forecasting models would be optimized to
predict survival and/or response to selected treat-
ment options with the highest possible precision
and at the lowest possible cost. These forecasting
models  could  include  selected  information  pro-
vided by the diagnostic classifications, stage and
other clinical information, laboratory, molecular,
and other data optimized in a multivariate analy-
sis designed to best provide the information that
surgeons and oncologists need to treat patients in
a cost-effective manner.

How Valid Is the Prognostic
Information Provided by Pathologic
Diagnoses? The Inconvenient Problem
of Interobserver Variability

A discussion of the appraisal of information from
the literature and integration with personal experi-
ence  cannot  be  complete  without  a  brief  discus-
sion of the problem of interobserver variability and
its  influence  on  prognostic  estimates  and  on  the
definition of new entities [73]. Clinico-pathologic
entities are usually described when a significant
statistical association is found between a set of
diagnostic  features  and  survival  or  other  out-
come variables. The new entity is thereafter inte-
grated  into  the  appropriate  classification  schema.
Pathologic classifications are regularly published
without  evaluation  of  whether  pathologists  other
than the authors can reproducibly diagnose cases
of  the  new  entity.  Depending  on  how  distinct
particular  histopathologic  features  are  and  how
often they are present in different entities that need
to be included in a differential diagnosis, different
pathologists  can  arrive  at  different  conclusions,
resulting  in  interobserver  diagnostic  variability.
This  problem  has  been  documented  in  multiple
studies involving neoplasms of the lung, gyneco-
logic tract, and others [74, 75]. It is less understood
how  these  diagnostic  variability  could  influence
the statistical  significance of the data that is used to
define new clinico-pathologic entities. We recently

this  problem  using

explored
the  distinction
between  usual  interstitial  pneumonia  (UIP)  and
nonspecific  interstitial  pneumonia  (NSIP),  two
closely related forms of chronic diffuse lung dis-
ease that can be difficult to distinguish from each
other  on  wedge  lung  biopsies  [73].  Using  the
QDMBA process described in this chapter, we for-
mulated  specific  questions  and  performed  a  sys-
tematic review of the literature. Seven retrospective
level  III  studies  were  found  that  had  evaluated
patients with both UIP and NSIP and provided sur-
vival information. As shown in Table 13.12, there
is considerable interstudy variability in the prog-
nosis of patients diagnosed as either UIP or NSIP.
In addition, 95% confidence intervals of the data
showed  considerable  overlap  in  survival  propor-
tions among patients with UIP and NSIP in several
of the  studies reviewed. Although all studies con-
firmed the general concept that NSIP patients have
 significantly  better  survival  than  those  with  UIP,
the survival proportions reported for UIP and NSIP
patients ranged from 11–58% to 39–100% respec-
tively.  This  variability  showed  that  the   survival
proportions  of  patients  diagnosed  with  UIP  at
some  centers  was  5×  better  than  in  others.
Variability  for  NSIP  patients  was  in  the  order  of
3×.  As  all  these  studies  were  retrospective  cases
series, it is possible that the results were influenced
by demographics, the severity of disease at diag-
nosis, and treatment effect. Interestingly, a simula-
tion  performed  using  the  data  from  each  study,
keeping the number of patients surviving the dis-
ease as a constant and increasing or decreasing at
5–30% intervals the number of patients with either
UIP or NSIP, to simulate interobserver diagnostic
variability,  showed  that  changing  approximately
10% of the diagnoses would have changed the sta-
tistical significance of all the studies. Analysis of
the data generated by the various simulations with
kappa statistics showed that kappa values at mod-
erate agreement levels could significantly change
the  prognostic  estimates  of  studies  reporting
the prognosis of patients with UIP and NSIP. The
results of the study strongly underscore the need to
develop  pathologic  classifications  that  minimize
the  problem  of  interobserver  variability  and  the
importance  of  testing  for  possible  interobserver
variability  before  new  pathologic  classifications
are disseminated.

230

A.M. Marchevsky and R. Gupta

Table 13.12  Evidence summary from studies evaluated with the simulation tool

Number of usual
interstitial pneumonia
(UIP) patients

Number
of cases
109
70
362
104
109
101
78
697

Author
Parra
Riha
Park
Bjoraker
Flaherty
Travis
Nicholson
Total
a 95% confidence intervals (CI) were estimated from published data
From Marchevsky and Gupta [73], with permission of Elsevier

UIP survival %
and 95% CIa
36.3 (24.9–49.5)
58 (44.6–70.3)
49 (42.2–55.8)
28 (18.4–40.1)
30 (19.2–43.6)
43 (30.9–56.0)
11 (4.4–24.9)
40.9 (36.7–45.2)

55
53
203
63
51
56
37
518

Number of nonspecific
interstitial pneumonia
(NSIP) patients

22
7
66
14
30
22
28
189

NSIP survival %
and 95% CIa
  77.3 (56.6–89.9)
  80 (43.3–95.4)
  73 (61.3–82.2)
  80 (53.9–93.2)
  90 (74.4–96.5)
100 (85.1–100)
  39 (23.3–57.3)
  75.1 (68.5–80.7)

Field Testing New Pathologic
Classifications Before
They Are Published

A logical approach to minimize the influence of
interobserver diagnostic variability would be for
the authors of new classifications that are likely
to be used by many pathologists worldwide, such
as  those  published  by  WHO,  to  field-test  the
diagnostic reproducibility of the proposed schema
with an adequate sample of pathologists to evalu-
ate whether they could apply them consistently in
their practices. This process could lead to modifi-
cations in the proposed classification schema or
definitions of various diagnostic criteria prior to
publication,  in  an  effort  to  decrease  possible
interobserver diagnostic variability. Currently, it
is sometimes disturbing that even the authors of
diagnostic  classifications  disagree  among  them-
selves, a problem that is sometimes highlighted
when  various  experts  render  variable  diagnoses
during  slide  symposia  at  national  and  interna-
tional teaching conferences. We recently explored
the  concept  of  testing  the  validity  of  proposed
diagnostic  before  publication  in  a  recent  study
suggesting  several  evidence  based  criteria  to
help  distinguish  metastastic  breast  cancer  from
primary  lung  adenocarcinoma  on  thoracic  fro-
zen  sections  [76].  The  study  of  129  frozen
 sections was conducted using the QDMBA para-
digm  and  initially  showed,   somewhat  to  our
 surprise, that in most patient populations, includ-
ing  ours,  primary  lung  adenocarcinomas  were

approximately  twice  more  frequent  than  meta-
static breast cancer, a somewhat counterintuitive
finding  in  patients  with  a  previous  history  of
breast  cancer.  Using  these  pre-test  probabilities
and the incidence of various  pathologic features
in the two populations we identified, using post-
test OR several significant pathologic criteria that
favored the diagnosis of primary lung adenocar-
cinoma. They include the presence of acini, lepidic
growth,  nuclear  pseudoinclusions,  and  central
scar. By contrast, the presence of comedonecro-
sis, solid nests of tumor cells, trabecular architec-
ture, and cribriform growth favored the probability
of  metastastic  breast  cancer  (Fig.  13.4).  Once
these diagnostic criteria were obtained, they were
explained  to  a  group  of  attending  pathologists
and  residents,  and  their  validity  tested  using
exams  administered  before  and  after  the  train-
ing session. The exercise showed that most par-
ticipants  were  able  to  significantly  improve  the
accuracy of the diagnosis of either primary lung
adenocarcinoma or metastastic breast carcinoma
using  the  proposed  criteria.  Feedback  from  the
exercise was used to improve on the definition of
various  criteria  and  the  way  they  were  grouped
prior to publication.

Conclusion

It is apparent from the epistemological review of
current  practices  provided  in  this  book  that
pathologists have been much more interested in

13  Development of Evidence-Based Diagnostic Criteria and Prognostic/Predictive Models

231

Fig. 13.4 ( a–d)  The  presence  of  comedonecrosis,  solid
nests  of  tumor  cells,  trabecular  architecture,  and  cribri-
form  growth  favors  the  probability  of  metastastic  breast

cancer (From Herbst et al. [76], © 2003–2010 American
Society  for  Clinical  Pathology.  ©  2003–2010  American
Journal of Clinical Pathology)

collecting  new  information  that  to  consider  its
validity and/or clinical applicability. This chapter
suggests a systematic approach to the evaluation
of  data  that  could  advance  the  specialty  to  the
next  level.  The  proposed  systematic  approach
does  not  offer  any  new  analytical  concepts  but
merely  organizes  the  process  of  collecting  and
evaluating  data  in  a  manner  that  reflects  basic
 elements  of  the  scientific  method.  The  chapter
also discusses the fact that, unfortunately, pathol-
ogists have been reluctant to develop novel para-
digms  that  integrate  new  data  with  preexistent
knowledge  taking  advantage  of  statistical  and
other analytical methods that are currently being
use  in  clinical  medicine,  business,  engineering,
and other fields of interest. In an era where evi-
dence  levels,  quality  of  care,  cost-effectiveness,
and  other  quantitative  yardsticks  are  being
increasingly  used  to  evaluate  the  added  value

being  provided  by  different  physicians  to  the
continuum  of  patient  care,  the  application  of
some  of  the  concepts  being  illustrated  in  this
chapter will hopefully stimulate some interest in
the application of EBP concepts to their research
and practice.

References

  1.  Fleming  KA.  Evidence-based  pathology.  J  Pathol.

1996;179:127–8.

  2.  Marchevsky  AM.  Evidence-based  medicine

in
 pathology:  an  introduction.  Semin  Diagn  Pathol.
2005;22:105–15.

  3.  Marchevsky  AM,  Wick  MR.  Evidence-based  medi-
cine, medical decision analysis, and pathology. Hum
Pathol. 2004;35:1179–88.

  4.  Straus SE, Richardson WS, Glasziou P, et al. Evidence-
based  medicine.  How  to  practice  and  teach  EBM.
New York: Elsevier; 2005.

232

A.M. Marchevsky and R. Gupta

  5.  Kenkel JM. Revisiting the scientific method. Aesthet

Surg J. 2009;29:167–8.

  6.  Michel  LA.  The  epistemology  of  evidence-based

medicine. Surg Endosc. 2007;21:145–51.

 26. Masaoka A, Monden Y, Nakahara K, et al. Follow-up
study  of  thymomas  with  special  reference  to  their
clinical stages. Cancer. 1981;48:2485–92.

 27. Marchevsky A. The mediastinum. Pathology (Phila).

  7.  Sackett  D.  Evidence-based  medicine.  Lancet.  1995;

1996;3:339–48.

346:1171.

 28. Marchevsky A, Kaneko M. Surgical pathology of the

  8.  Sackett DL, Rosenberg WM. The need for evidence-

mediastinum. New York: Raven Press; 1991.

based medicine. J R Soc Med. 1995;88:620–4.

  9.  Sackett DL, Rosenberg WM, Gray JA, et al. Evidence
based  medicine:  what  it  is  and  what  it  isn’t.  BMJ.
1996;312:71–2.

 10. Treadwell JR, Tregear SJ, Reston JT, et al. A system
for  rating  the  stability  and  strength  of  medical  evi-
dence. BMC Med Res Methodol. 2006;6:52.

 11. Guerette PH. Managed care: cookbook medicine, or
quality, cost-effective care? Can Nurse. 1995;91:16.
 12. Holm  RP.  Cookbook  medicine.  S  D  Med.  2009;

62:371.

 13. Leape L. Are practice guidelines cookbook medicine?

J Ark Med Soc. 1989;86:73–5.

 14. Parmley WW. Practice guidelines and cookbook med-
icine – who are the cooks? J Am Coll Cardiol. 1994;
24:567–8.

 15. Steinberg  KE.  Cookbook  medicine:  recipe  for

 disaster? J Am Med Dir Assoc. 2006;7:470–2.

 16. Bhandari M, Zlowodzki M, Cole PA. From eminence-
based practice to evidence-based practice: a paradigm
shift. Minn Med. 2004;87:51–4.

 17. Leppaniemi A. From eminence-based to error-based
to  evidence-based  surgery.  Scand  J  Surg.  2008;97:
2–3.

 18. Petri  E,  Kolbl  H.  Eminence,  or  rather  eloquence,  or
rather  economy-based  medicine?  Int  Urogynecol  J
Pelvic Floor Dysfunct. 2004;15:147–8.

 19. Marchevsky D. Predicting violence. Br J Psychiatry.

1999;175:585.

 20. Brambilla  E,  Travis  WD,  Colby  TV,  et  al.  The  new
World  Health  Organization  classification  of  lung
tumours. Eur Respir J. 2001;18:1059–68.

 21. Fletcher  DM,  Unni  K,  Mertens  F.  World  Health
Organization classification of tumors. Tumors of soft
tissue and bone. Lyon, France: IARC Press; 2002.
 22. Marchevsky AM, McKenna Jr RJ, Gupta R. Thymic
epithelial  neoplasms:  a  review  of  current  concepts
using an evidence-based pathology approach. Hematol
Oncol Clin North Am. 2008;22:543–62.

 23. Marchevsky  AM,  Gupta  R,  McKenna  RJ,  et  al.
Evidence-based pathology and the pathologic evalua-
tion  of  thymomas:  the  World  Health  Organization
classification can be simplified into only 3 categories
other  than  thymic  carcinoma.  Cancer.  2008;112:
2780–8.

 24. Gupta  R,  Marchevsky  AM,  McKenna  RJ,  et  al.
Evidence-based pathology and the pathologic evalua-
tion of thymomas: transcapsular invasion is not a sig-
nificant  prognostic  feature.  Arch  Pathol  Lab  Med.
2008;132:926–30.

 25. Shimosato  Y,  Mukai  K.  Atlas  of  tumor  pathology.
Tumors of the mediastinum. Washington, DC: AFIP
Press; 1997.

 29. Marchevsky  AM,  Hammond  ME,  Moran  C,  et  al.
Protocol  for  the  examination  of  specimens  from
patients with thymic epithelial tumors located in any
area of the mediastinum. Arch Pathol Lab Med. 2003;
127:1298–303.

 30. Marchevsky AM, Wick MR. Evidence levels for pub-
lications in pathology and laboratory medicine. Am J
Clin Pathol. 2010;133:366–7.

 31. Straus  SE,  Sackett  DL.  Bringing  evidence  to  the

clinic. Arch Dermatol. 1998;134:1519–20.

 32. Dellavalle  RP,  Freeman  SR,  Williams  HC.  Clinical
evidence epistemology. J Invest Dermatol. 2007;127:
2668–9.

 33. Overman VP. The Cochrane collaboration. Int J Dent

Hyg. 2007;5:62.

 34. Summerskill W. Cochrane Collaboration and the evo-

lution of evidence. Lancet. 2005;366:1760.

 35. Marchevsky AM, Parakh RS, Hakimian B. Radiation
therapy  does  not  improve  the  prognosis  of  patients
with stage II thymoma, supporting previous evidence
suggesting that the presence of transcapsular invasion
is  not  a  significant  prognostic  feature.  Mod  Pathol.
2010;23(1S):409A. Ref Type: Abstract.

 36. Marchevsky  AM,  Gupta  R,  Casadio  C,  et  al.  The
World  Health  Organization  classification  of  thymo-
mas provides significant prognostic information  for
selected stage III patients: evidence from an interna-
tional  thymoma  study  group.  Hum  Pathol.  2010;
41:1413–21.

 37. Marchevsky  AM,  Gupta  R,  Kusuanco  D,  et  al.  The
presence of isolated tumor cells and micrometastases
in  the  intrathoracic  lymph  nodes  of  lung  cancer
patients  is  not  associated  with  decreased  survival.
Hum Pathol. 2010;41(11):1536–43.

 38. Evans  JS,  Handley  SJ,  Over  DE,  et  al.  Background
beliefs  in  Bayesian  inference.  Mem  Cognit.  2002;
30:179–90.

 39. Friston KJ, Penny W, Phillips C, et al. Classical and
theory.

in  neuroimaging:

inference

Bayesian
Neuroimage. 2002;16:465–83.

 40. Kersten D, Mamassian P, Yuille A. Object perception
as  Bayesian  inference.  Annu  Rev  Psychol.  2004;55:
271–304.

 41. Schmid  CH.  Using  Bayesian  inference  to  perform
meta-analysis. Eval Health Prof. 2001;24:165–89.
 42. Tenenbaum JB, Griffiths TL. Generalization, similar-
ity, and Bayesian inference. Behav Brain Sci. 2001;24:
629–40.

 43. Xu  F,  Tenenbaum  JB.  Word  learning  as  Bayesian

inference. Psychol Rev. 2007;114:245–72.

 44. Zelen  M,  Parker  RA.  Case-control  studies  and

Bayesian inference. Stat Med. 1986;5:261–9.

13  Development of Evidence-Based Diagnostic Criteria and Prognostic/Predictive Models

233

 45.  Marchevsky  AM,  Hauptman  E,  Shepard  C,  et  al.
Computerized  interactive  morphometry  of  brushing
cytology specimens. Acta Cytol. 1988;32:341–6.
 46. Marchevsky  AM,  Klapper  E,  Gil  J.  Computerized
classification  of  nuclear  profiles  in  non-Hodgkin’s
lymphomas. Am J Clin Pathol. 1987;87:561–8.
 47. Marchevsky AM, Gil J, Jeanty H. Computerized inter-
active morphometry in pathology: current instrumen-
tation and methods. Hum Pathol. 1987;18:320–31.
 48. Marchevsky  AM,  Hauptman  E,  Gil  J,  et  al.
Computerized  interactive  morphometry  as  an  aid  in
the diagnosis of pleural effusions. Acta Cytol. 1987;
31:131–6.

 49. Marchevsky AM, Gil J. Applications of computerized
interactive morphometry in pathology. II. A model for
computer  generated  diagnosis.  Lab  Invest.  1986;54:
708–16.

 50. Marchevsky  AM,  Gupta  R,  Balzer  B.  Diagnosis  of
metastatic  neoplasms:  a  clinicopathologic  and  mor-
phologic approach. Arch Pathol Lab Med. 2010;134:
194–206.

 51. Gupta R, Dastane A, McKenna Jr RJ, et al. What can
we learn from the errors in the frozen section diagno-
sis  of  pulmonary  carcinoid  tumors?  An  evidence-
based approach. Hum Pathol. 2009;40:1–9.

 52. Gupta  R,  McKenna  Jr  R,  Marchevsky  AM.  Lessons
learned from mistakes and deferrals in the frozen sec-
tion  diagnosis  of  bronchioloalveolar  carcinoma  and
well-differentiated  pulmonary  adenocarcinoma:  an
evidence-based  pathology  approach.  Am  J  Clin
Pathol. 2008;130:11–20.

 53. Marchevsky  AM,  Wick  MR.  Evidence-based  guide-
lines for the utilization of immunostains in diagnostic
pathology: pulmonary adenocarcinoma versus meso-
thelioma.  Appl  Immunohistochem  Mol  Morphol.
2007;15:140–4.

 54. Westfall  DE,  Fan  X,  Marchevsky  AM.  Evidence-
based guidelines to optimize the selection of antibody
panels in cytopathology: pleural effusions with malig-
nant  epithelioid  cells.  Diagn  Cytopathol.  2010;38:
9–14.

 55. Cengel KA, Day SJ, vis-Devine S, et al. Effectiveness
of  the  SurePath  liquid-based  Pap  test  in  automated
screening and in detection of HSIL. Diagn Cytopathol.
2003;29:250–5.

 56. Godfrey SE. The Pap smear, automated rescreening,
and  negligent  nondisclosure.  Am  J  Clin  Pathol.
1999;111:14–7.

 57. Keyhani-Rofagha S, Palma T, O’Toole RV. Automated
screening for quality control using PAPNET: a study
of  638  negative  Pap  smears.  Diagn  Cytopathol.
1996;14:316–20.

 58. Garcia JJ, Folpe AL. The impact of advances in molec-
ular genetic pathology on the classification, diagnosis
and treatment of selected soft tissue tumors of the head
and neck. Head Neck Pathol. 2010;4:70–6.

 59. Greco FA, Erlander MG. Molecular classification of
cancers  of  unknown  primary  site.  Mol  Diagn  Ther.
2009;13:367–73.

 60.  Tang P, Skinner KA, Hicks DG. Molecular classification
of  breast  carcinomas  by  immunohistochemical
analysis: are we ready? Diagn Mol Pathol. 2009;18:
125–32.

 61. Marchevsky  AM,  Tsou  JA,  Laird-Offringa  IA.
Classification  of  individual  lung  cancer  cell  lines
based on DNA methylation markers: use of linear dis-
criminant  analysis  and  artificial  neural  networks.
J Mol Diagn. 2004;6:28–36.

 62. Marchevsky A, Gil J, Silage D. Computerized interac-
tive morphometry as a potentially useful tool for the
classification  of  non-Hodgkin’s  lymphomas.  Cancer.
1986;57:1544–9.

 63. Marchevsky  AM,  Shah  S,  Patel  S.  Reasoning  with
uncertainty  in  pathology:  artificial  neural  networks
and logistic regression as tools for prediction of lymph
node  status  in  breast  cancer  patients.  Mod  Pathol.
1999;12:505–13.

 64. Marchevsky AM, Patel S, Wiley KJ, et  al. Artificial
neural  networks  and  logistic  regression  as  tools  for
prediction of survival in patients with Stages I and II
non-small  cell  lung  cancer.  Mod  Pathol.  1998;11:
618–25.

 65. Bellotti M, Elsner B, De Paez LA, et al. Neural net-
works as a prognostic tool for patients with non-small
cell  carcinoma  of  the  lung.  Mod  Pathol.  1997;10:
1221–7.

 66. Esteva  H,  Marchevsky  A,  Nunez  T,  et  al.  Neural
 networks as a prognostic tool of surgical risk in lung
resections. Ann Thorac Surg. 2002;73:1576–81.
 67. Singson RP, Alsabeh R, Geller SA, et al. Estimation
of tumor stage and lymph node status in patients with
colorectal adenocarcinoma using probabilistic neural
networks  and
logistic  regression.  Mod  Pathol.
1999;12:479–84.

 68. Steindel  SJ,  Howanitz  PJ,  Renner  SW.  Reasons  for
proficiency  testing  failures  in  clinical  chemistry  and
blood gas analysis: a College of American Pathologists
Q-Probes study in 665 laboratories. Arch Pathol Lab
Med. 1996;120:1094–101.

 69. Tholen D, Lawson NS, Cohen T, et al. Proficiency test
performance and experience with College of American
Pathologists’  programs.  Arch  Pathol  Lab  Med.
1995;119:307–11.

 70. Marchevsky AM, Walts AE, Bose S, et al. Evidence-
based evaluation of the risks of malignancy predicted
by  thyroid  fine-needle  aspiration  biopsies.  Diagn
Cytopathol. 2010;38:252–9.

 71. Marchevsky  A,  Gupta  R,  Casadio  C,  et  al.  World
Health Organization classification of thymomas pro-
vides  significant  prognostic  information  for  selected
stage  III  patients:  evidence  from  an  international
 thymoma  study  group.  Hum  Pathol.  2010;41(10):
1413–21.

 72. Rieker RJ, Hoegel J, Morresi-Hauf A, et al. Histologic
classification of thymic epithelial tumors: comparison
of  established  classification  schemes.  Int  J  Cancer.
2002;98:900–6.

 73. Marchevsky AM, Gupta R. Interobserver diagnostic
variability  at  “moderate”  agreement  levels  could

234

A.M. Marchevsky and R. Gupta

significantly  change  the  prognostic  estimates  of
clinicopathologic studies: evaluation of the problem
using evidence from patients with diffuse lung dis-
ease. Ann Diagn Pathol. 2010;14:88–93.

 74. Baak JP. The role of computerized morphometric and
cytometric  feature  analysis  in  endometrial  hyperpla-
sia  and  cancer  prognosis.  J  Cell  Biochem  Suppl.
1995;23:137–46.

 75. Travis WD, Gal AA, Colby TV, et al. Reproducibility
of  neuroendocrine  lung  tumor  classification.  Hum
Pathol. 1998;29:272–9.

 76. Herbst  J,  Jenders  R,  McKenna  R,  et  al.  Evidence-
based  criteria  to  help  distinguish  metastatic  breast
cancer  from  primary  lung  adenocarcinoma  on  tho-
racic  frozen  section.  Am  J  Clin  Pathol.  2009;131:
122–8.

Evaluation and Reduction
of Diagnostic Errors in Pathology
Using an Evidence-Based Approach

14

Raouf E. Nakhleh

Keywords
Diagnostic errors in pathology • Evidence-based pathology • Evaluation
of diagnostic errors in pathology • Human error in diagnostic pathology

A substantial proportion of patients’ diagnoses and
treatments are dependent on reliable tissue diagno-
ses in surgical pathology and cytopathology. This
can  easily  be  demonstrated  in  cases  of  cancer  as
well  as  many  inflammatory  conditions  such  as
organ rejection [1–4]. In cancer management, tis-
sue diagnosis and staging are the most important
determinants  of  prognosis  and  therapy.  Likewise,
determining the level of rejection in allograft biop-
sies is the main determinant of immunosuppressive
therapy. The importance of a correct diagnosis in
these situations cannot be overemphasized.

In attempting to reduce errors many advocate
a  systems  approach  [5].  At  the  heart  of  this
approach is the admission that humans are falli-
ble and will make mistakes and therefore the sys-
tems around them should be designed to minimize
errors while at the same time continuously check-
ing to identify errors and correcting them at the
earliest point in the process.

In this scheme of error reduction a handful of
reasons are cited as the primary causes of errors.
They include; lack of communication, variable input,
complexity,  inconsistency,  human   intervention,

R.E. Nakhleh ()
Department of Pathology, Mayo Clinic Florida,
4500 San Publo Road, Jacksonville, FL 32224, USA
e-mail: Nakhleh.raouf@mayo.edu

tight time constraints, and a hierarchical culture.
The  literature  on  pathology  errors  is  far  from
comprehensive and has not for the most part taken
this approach but does offer clues of how errors
occur  and  how  they  could  be  addressed.  In  this
chapter, I will discuss how errors occur in surgical
pathology and then attempt to adapt to pathology
existing  proven  knowledge  used  in  many  indus-
tries to reduce errors.

Errors in Surgical Pathology

Part  of  the  problem  in  addressing  errors  is  the
various ways that errors can occur and the vari-
ous ways that they may be reported (Table 14.1).
While the literature is variable in measuring the
level of errors that exist in anatomic pathology, it
is  safe  to  say  that  errors  exist  and  have  been
reported in a range up to 40% of cases, hence the
need  to  evaluate  and  determine  ways  to  reduce
errors [6]. A study by Meier et al. focuses on the
development  and  validation  of  a  taxonomy  of
defects  [7].  This  report  derived  its  information
from review of amended reports from seven insti-
tutions.  Errors  are  categorized  into  four  broad
categories; misinterpretations, misidentifications,
specimen defects, and report defects. Using these
categories, Meier et al. were able to estimate the

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_14, © Springer Science+Business Media, LLC 2011

235

236

R.E. Nakhleh

Table 14.1  Evidence-based approach to error reduction:
where do errors occur?

1.  Where in the test cycle

do errors occur?

2.  Where do the most

significant errors occur?

3.  What are the most
significant errors?

Quality assurance data
Preanalytic – up to 40%
Analytic – 25%
Postanalytic – 29–44%
Legal claims
Preanalytic – 8–9%
Analytic – 90%
Postanalytic – 1%
Analytic error
Specimen identification
Report defects

Table 14.2  Classification of errors

Error types
Misinterpretation

Misidentification

Specimen defects

Report defects

Error subtypes
False-negative
False-positive
Misclassification
Patient
Tissue
Laterality
Lost
Inadequate
Absent or discrepant measurements
Nonrepresentative sampling
Absent or inappropriate ancillary
studies
Typographical errors
Missing or wrong demographic or
procedural information
Electronic transmission or format
defects

occurrence of errors within the framework of the
surgical pathology test cycle (Table 14.2). About
a fourth of errors occur within the analytic phase
(misinterpretation and some specimen defects) of
the test cycle. The remaining errors occur about
equally within the preanalytic (misidentification
and  some  specimen  defects)  and  postanalytic
(report defects) phases of the test cycle.

There are other means of evaluating the exis-
tence  of  error  which  can  focus  on  significant
errors or errors that have the potential for patient
harm. Evaluation of errors from a legal perspec-
tive  yields  a  completely  different  picture  [8,  9].
Reports of legal judgments and settlements against
pathologists  demonstrate  that  the  vast  majority
(>90%)  of  these  cases  are  analytic  errors,  and
60–70% of these errors are false-negative results.

Since  legal  judgments  and  settlements  usually
result because of patient harm, it may be safe to
say  that  these  represent  significant  diagnostic
errors.  Error  reduction  efforts,  therefore,  should
be focused on the analytic phase of the test cycle
and the factors in the pre- and postanalytic phase
that  have  a  strong  influence  on  determining  an
accurate diagnosis.

Errors Within the Different Phases
of the Test Cycle

In  this  section,  errors  are  discussed  in  relation-
ship to where they occur within the test cycle. In
the next section, most of these errors will be dis-
cussed as to the reason they occur and possible
remedies to help reduce errors.

Preanalytic Errors

While all errors in the preanalytic phase of the test
cycle are potentiality significant, by virtue of its
potential for catastrophe, specimen misidentifica-
tion  stands  out  as  the  most  important  potential
error [10]. Misidentified specimens have resulted
in  surgical  procedures  being  performed  on  the
wrong  site  and  even  on  the  wrong  patient.
Specimen  identification  errors  not  only  occur
principally within the preanalytic phase of the test
cycle  but  are  also  well  documented  within  the
analytic and postanalytic phases of the test cycle.
The responsibility of initial specimen identifi-
cation is shared between the laboratory and every
other  department  where  specimens  are  genera-
tion.  This  includes  operating  rooms,  endoscopy
suits, physicians’ offices, outpatient surgical cen-
ters, and interventional radiology among others.
Problems occur because the vast majority of indi-
viduals  that  label  specimens  are  usually  not
trained by pathology and are not accountable to
pathology.  The  system  is  extremely  complex
when you consider the number of locations and
individuals  involved.  To  get  a  handle  on  this
problem, an  institution has to bring focus on the
problem. The Joint Commission has focused on
patient identification as a patient safety goal and

14  Evaluation and Reduction of Diagnostic Errors in Pathology Using an Evidence-Based Approach

237

identification

specimen identification is part of this goal [11]. It
is recommended that specimen identification be
made  an  institutional  goal  and  not  simply  a
 laboratory  goal  [10,  12,  13].  In  this  light,  the
responsibility  of  specimen
is
shared equally between clinical departments and
 pathology.  Factors  that  have  been  shown  to
improve specimen identification are twofold [14].
First is the introduction of redundant checks such
as  remote  order  entry  for  inclusion  of  patients
into  the  laboratory  system,  checks  of  patient
identity  at  every  hand-off  such  as  at  specimen
pick-up  and  at  accessioning  and  checking  the
patient identity before release of reports. Second,
over time continuous monitoring has been shown
to improve specimen identification. The reason is
not clear, but it is thought the continuous moni-
toring keeps the focus on the problem that results
in long-term improvement.

Analytic Errors

Analytic  errors  or  diagnostic  errors  occur  for  a
variety  of  reasons,  some  of  which  are  addressed
blow. While analytic errors are not insignificant, if
one considers the complexity of systems needed to
arrive  at  a  correct  diagnosis,  it  is  a  wonder  that
more  errors  do  not  occur.  To  arrive  at  a  correct
diagnosis  three  systems  must  operate  adequately
to achieve the desired result. (1) A lesion must be
clinically  identified  and  adequately  sampled.  (2)
The laboratory must be able to appropriately pro-
cess the tissue and have the ability to provide all
necessary  ancillary  tests.  (3)  A  pathologist  must
have sufficient knowledge, experience, and judg-
ment  to  arrive  at  the  appropriate  diagnosis.  The
first system resides in the preanalytic realm and is
mostly  beyond  the  control  of  the  laboratory,  and
therefore will not be addressed here. The second
system speaks to the optimal operation of the labo-
ratory.  Error  reduction  in  laboratory  systems  is
discussed below by using lean design, automation,
reducing complexity, and by incorporating multi-
ple  checks  into  the  system  [15, 16].  One  further
step that should be addressed regarding ancillary
tests is the establishment of appropriate validation
procedures  and  the  prudent  use  of  proficiency

 testing  material  where  available  [17].  The  third
system includes the pathologists’ individual train-
ing,  specialization,  organization,  and  individual
traits. Error reduction in this area is addressed col-
lectively with the use of consensus diagnostic cri-
teria, prudent use of redundant sign-out including
the use of specialists, the use of ancillary testing
when appropriate, and the use of checklists. Many
of these topics are also addressed below.

Postanalytic Errors

The two most often cited postanalytic errors are
incomplete  reports  and  lack  of  communication
for significant and unexpected (critical) findings
[18, 19]. Effectively addressing incomplete reports
has been demonstrated with the use of computer
based  checklist  reports  [20].  Occasionally,  how-
ever, reports are incomplete because the wrong or
incomplete  history  is  given.  This  can  be  estab-
lished with simple examples; colonic biopsies are
performed for multiple reasons including colonic
polyps  or  to  rule  out  inflammatory  processes.
If  the  biopsy  is  accompanied  by  the  history  of
polyp, the pathologist will address the differential
diagnosis of a mass and if no findings of a polyp
are  identified,  the  diagnosis  will  most  likely  be
“benign colonic mucosa.” However, if the history
is “diarrhea” then the pathologist will examine the
biopsy  tissue  more  carefully  for  inflammatory
conditions and if none are found the diagnosis is
likely to be something akin to “no inflammatory
changes identified.” Sometimes a clinician has a
specific diagnosis to rule out such as amyloidosis.
If  this  is  not  conveyed,  it  may  be  easily  missed
and  ancillary  studies  may  not  be  performed  to
identify or exclude the specific finding.

Reasons for Diagnostic Errors
and Potential Remedies

Variable Input: Lack of Communication

Many  studies  have  demonstrated  that  communi-
cation failure is a key element in many errors in
medicine [21, 22] (Tables 14.3–14.5). One of the

238

R.E. Nakhleh

Table 14.3  What are the causes of errors?

What are the factors that contribute
to errors in medicine?
Variable input – communication
Complexity
Inconsistency
Human intervention
Hand-offs
Tight time constraints
Hierarchical culture

Can an example be found in pathology that demonstrates each factor?
Absent or incomplete clinical history
There are potentially over 100 steps in reaching a diagnosis
Use of diagnostic criteria, report formatting and content, training, and experience
The entire process is dependent on human handling of specimens
Tissue is transferred multiple times with the need to maintain ID
Batch mode is pervasive in surgical pathology
Lack of questioning of authority

Table 14.4  Potential remedies or solution to errors

What are the factors that contribute
to errors in medicine?
Variable input – communication

Complexity
Inconsistency

Human intervention
Hand-offs

Tight time constraints

Hierarchical culture

What are the potential solutions for pathology?
Electronic medical record, remote order entry with forced functions, automate
clinical history retrieval, multidisciplinary clinical teams
Lean production redesign, automate were possible, standardize
Standardization of diagnostic criteria, Standardize procedures, Standardize
report content and layout, continuous education
Use checklists to assure compliance, automate were possible, remove distractions
Use of tools such as ink, barcodes, RFID, remote order entry, redundant checks
to assure correct ID
Continuous processing as much as possible, emphasize doing the job well vs.
doing the job fast, remind all of the pitfalls
Change the culture, take away fear of reporting problems

Table 14.5  Additional error prevention strategies

Continuous
monitoring

Report
formatting

Continuous monitoring has been shown to
improve a measure over time. Two areas
relevant to surgical pathology include
specimen identification and frozen section
– permanent section correlation
1.  Use of diagnostic headlines to

 emphasize key points

2.  Maintenance of layout continuity with

other reports and over time

3.  Optimization of information density
4.  Reduction of extraneous information

most  important  communications  that  has  been
shown  to  affect  diagnostic  accuracy  and  com-
pleteness in the clinical information provided with
the specimen. Variability in the content and accu-
racy of clinical information provided to surgical
pathology with the biopsy tissue has been shown
in  multiple  studies  to  affect  diagnostic  accuracy
[23–26]. In a study of amended reports, 10% of
cases were amended because additional informa-

tion was obtained beyond the requisition slip [25].
An additional 20% of cases were amended because
the clinician asked for review of the case, presum-
ably  because  of  an  apparent  clinical-pathologic
discrepancy. In a study focused on clinical history
provided  by  clinicians,  6.0%  of  cases  in  which
additional history was obtained lead to a change
of diagnosis [24]. And in a study of malpractice
claims  against  pathologists,  up  to  20%  of  cases
were due to the pathologist’s ability to obtain all
the pertinent information [26]. A recent study of
atypical melanocytic lesions showed a significant
increase in diagnostic agreement with the  inclu-
sion of pertinent clinical information [23].
Unfortunately, there are no good studies that have
attempted  to  improve  on  the  clinical  input  to
pathologists.  The  increasing  availability  of  elec-
tronic medical records, although not proven, seems
to have alleviated some problems. The pathologist
still  has  to  take  the  initiative  to  find  the  desired
information.  Adoption  of  electronic  medical

14  Evaluation and Reduction of Diagnostic Errors in Pathology Using an Evidence-Based Approach

239

records  appears  to  be  underway  in  medium  and
large hospitals and laboratory  systems. Significant
gaps  remain  particularly  for  specimens  that  are
obtained at doctors’ offices and outpatient centers
beyond a defined healthcare system or institution.
Over time, developments of secure internet based
technology  solutions  are  likely  to  facilitate  the
electronic  medical  record.  One  method  that  has
been shown to improve patient identification and
could  improved  clinical  information  is  remote
order  entry  [14].  Functionality  that  would  force
the inclusion of the clinical history before a speci-
men  can  be  entered  into  the  laboratory  system
could be adopted. Another potential solution could
be the automatic inclusion of the clinical note of
the  physician  that  obtains  the  tissue.  Of  course
these solutions are not possible without the pres-
ence of robust computer systems.

tion  of  automation  appears  to  offer  the  best
opportunity  for  improvement  in  the  histology
laboratory [15, 27, 28]. Lean redesign addresses
three potential error prone processes. First, lean
aims  to  either  eliminate  steps  when  possible  or
better alien steps so that processes are smoother
and less disruptive (reduce complexity). Second,
lean  redesigns  of  surgical  pathology  introduces
the judicial use of technology with the use of bar-
codes  or  other  technologies  to  eliminate  redun-
dant  steps  such  as  reentry  of  identification  data
on slides and blocks. The introduction of technol-
ogy  addresses  issues  of  inconsistency  in  hand
writing  or  data  reentry  and  in  other  processes
such  as  staining  with  the  introduction  of  auto-
matic  stainers.  Third,  lean  redesign  results  in
standardization of processes and the elimination
of conflicting procedures and the need to train in
multiple procedures.

Complexity

There is a greater chance of mishap with greater
complexity.  Intuitively,  it  seems  obvious  that  a
process with many steps has a greater chance of
error than a similar process with only one or two
steps. This can actually be demonstrated mathe-
matically in hypothetical and real situations. If a
process has one step in it and has a 1% chance of
error,  a  similar  process  with  25  steps  and  a  1%
error at each step bring that total error risk to 22%
[5].  This  can  be  demonstrated  in  real  life  with
measured errors as well.

Surgical pathology errors have not been mea-
sured  at  every  step,  but  surgical  pathology  is  a
complex process requiring numerous steps within
the laboratory to complete tissue processing and
diagnosis with endless variations that may lead to
error. This is the reason why many have used lean
production techniques to improve histologic pro-
cesses, gain efficiency, and reduce errors. Using
lean methodology, Zarbo et al. reduced the over-
all  misidentification  case  rate  and  histological
slide  misidentification  rate  by  62  and  95%,
respectively [15].

Although variable results have been achieved,
at this time, lean redesign with selective introduc-

Inconsistency

Inconsistency can be demonstrated as a source
of error in at least two ways in surgical pathol-
ogy. The first is in the effect of diagnostic crite-
ria on diagnostic reproducibility and the second
is  in  the  pathologists’  ability  to  provide  more
complete  reports  with  the  use  of  synoptic
reports. During the past couple of decades dra-
matic  improvements  have  been  made  in  the
adoption of standardization in diagnostic crite-
ria  and  in  the  adoption  of  standardized  cancer
reports.

The  following  example  demonstrates  the
effect of the use of standardized diagnostic crite-
ria on the level of diagnostic agreement. In 1991,
Dr. Rosai conducted a study which strongly sug-
gested  that  inter-observer  concordance  in  the
classification  of  breast  ductal  proliferative  dis-
ease  was  unacceptably  low  [29].  In  this  study,
Dr. Rosai asked a panel of experts to review the
same  set  of  cases  and  render  their  diagnoses.
Soon after publication of this study, Schnitt et al.
published a similar study that demonstrated high
concordance among a panel of the same experts in
the diagnosis of proliferative ductal lesions [30].

240

R.E. Nakhleh

In  the  study  by  Schnitt  et  al.,  the  experts  were
instructed  to  use  standardized  criteria  for  the
diagnosis  of  lesions.  The  difference  is  a  stark
demonstration  of  the  power  of  standardization.
This  has  been  shown  in  other  areas  of  surgical
pathology such as urothelial neoplasia, Barrett’s
dysplasia and organ rejection [1, 2, 31, 32].

The  other  aspect  of  standardization  is  that  of
report  content.  This  is  particularly  important  in
oncology  where  different  treatment  options  are
available and are dependent on pathologic grading,
staging,  and  tumor  marker  expression  [33,  34].
This is easily demonstrated using the example of
breast cancer where a variety of treatment options
are  considered  based  on  tumor  grade,  stage,  and
the expression of ER, PR and HER2. The adoption
of national standards in the form of standard grad-
ing  and  staging  of  tumors  has  greatly  facilitated
and  accelerated  national  treatment  trials  in  the
evaluation  of  potential  therapies.  This  has  been
further  accentuated  with  the  use  of  standardized
computer  based  forms  that  have  been  shown  in
multiple studies but none as eloquently as in a ran-
domized  prospective  examination  of  pathology
reports in a study by Branston et al. [20]. The con-
trol arm of the study included eight hospitals that
did not use computer based cancer reports (check-
lists)  and  the  study  arm  included  eight  hospitals
that  used  computer  based  cancer  reports  (check-
lists). This study concluded that reports in the hos-
pitals  with  the  computer  checklists  were  more
complete 28% of the time. The study also found
that clinicians found these reports preferable while
pathologists found them acceptable.

One  aspect  of  reports  that  should  be  consid-
ered is the ability of clinicians to derive the infor-
mation that they need to treat the patient from the
report. Powsner demonstrated that clinicians rou-
tinely  misinterpreted  pathologists’  reports  30%
of  the  time  [35].  Factors  that  were  cited  to  be
associated with improvement of this gap included
familiarity with report format and clinical experi-
ence.  Dr.  Valenstine  in  a  review  of  pathology
report  formatting  suggests  that  four  evidence-
based and time-tested principles may be helpful
in formatting reports for more effective commu-
nication. These include: (1) the use of diagnostic
headlines  to  emphasize  key  points,  (2)  mainte-

nance of layout continuity with other reports and
over  time,  (3)  optimization  of  information  den-
sity, and (4) reduction of extraneous information
[36].  Dr.  Valenstine  based  his  conclusion  by
extension  of  research  performed  in  other  fields
outside of medicine including cockpit design in
aviation and newspaper print effectiveness.

Human Intervention

Surgical  pathology  remains  a  process  that  is
heavily dependent on human physical and intel-
lectual activity. With the exception of very short
segments of the test cycle, surgical pathology is
most assuredly dependent on humans doing their
jobs.  As  such,  surgical  pathology  is  subject  to
human error. As in other areas of health care, a
systems approach to quality management in sur-
gical pathology has been recommended to reduce
errors [37, 38]. At its core, this management style
advocates design of processes with two features
in  mind;  prevention  of  errors  and  detection  of
errors.

In design of systems that prevent errors, two
methods  have  prevailed.  First,  introduction  of
automation whenever possible works well where
information must be re-inputted into the system
[15]. The use of slide and block labelers as well
as the use of barcode technology are good exam-
ples  where  human  intervention  in  the  form  of
reentering information may be avoided with the
use  of  automated  equipment  thus  reducing  the
potential  for  error.  Automation  may  be  used  to
simplify  a  process  in  the  sense  that  a  machine
will do multiple steps, whereas from the human
perspective the process is reduced to one or two
steps. Machines also have the added advantage of
reducing procedural variations because machines
operate  at  a  tight  range  of  specification  and  are
not subject to distraction. Automatic stainers and
coverslipers demonstrate this utility well.

Reducing cognitive errors at the point of diag-
nosis  has  been  challenging,  but  methods  have
emerged that reduce or detect error. The principle
method of error prevention has been redundancy
in  the  form  of  review  of  cases  before  or  after
cases are verified or signed out.

14  Evaluation and Reduction of Diagnostic Errors in Pathology Using an Evidence-Based Approach

241

In  three  publications  it  has  been  shown  that
review  of  cases  by  more  than  one  pathologists
helps lower the number of amended reports and
possibly  the  error  rate  [25,  39,  40].  In  a  multi-
institutional study of amended reports, cases that
were reviewed before a case was signed out had
an amended report rate of 1.2/1,000, vs. 1.6/1,000
for  cases  that  were  reviewed  after  they  were
signed  out.  Renshaw  and  Gould  demonstrated
that cases reviewed by greater than one patholo-
gist  resulted  in  a  lower  disagreement  rate  and
amended report rate. Dr. Novis also presented evi-
dence that review of cases by two pathologists vs.
one resulted in lower error rates. The best strategy
for case review has not been formulated, but may
be dependent on the type of material seen at any
one  institution  and  the  number  of  pathologists
participating  in  case  sign-out.  Review  of  cases
after they have been verified is an extension of the
same  principle,  but  falls  into  the  realm  of  error
detection, since this process would occur beyond
the point of error prevention.

The use of checklists has been advocated as a
tool to control the extent of human intervention.
This can easily be demonstrated with the use of
cancer  checklists  for  reporting  all  necessary
parameters  in  cancer  reports  [20,  33,  34,  41].
With  the  use  of  checklists,  a  pathologist  is
reminded  of  all  the  items  that  should  be  in  that
report. Indeed, a computer system can be built to
force  individuals  to  complete  a  report  before  a
report  can  be  verified  or  signed  out.  Checklists
can also be used for a whole host of tasks in the
laboratory to assure that things get done [42]. An
example of this includes a list of tasks that a tech-
nologist  or  clerk  must  perform  to  prepare  an
accessioning station at the beginning of the day
and a list of tasks that must be done at the end of
the day to make sure nothing is forgotten.

Updated Knowledge on Diagnostic
Criteria and Staging
Various  subspecialty  groups  have  expended  a
great deal of effort to establish diagnostic crite-
ria for various diseases and conditions with the
intent  of  standardization.  But  pathologists  still
have to update themselves and their systems in
the  use  of  these  diagnostic  criteria.  Part  of  the

problem is the great diversity of specimen types
that  pathologists  have  to  address.  Some  larger
pathology  practice  groups  have  adopted  com-
plete subspecialization for their case sign outs.
In this situation, a GI pathologist takes care of
the GI cases, a hematopathologist signs out the
hematologic cases and so on. Individuals in each
subspecialty are responsible for updating them-
selves on the current literature in that field and
often are reasonably knowledgeable on the treat-
ment options and other clinical scenarios. In this
type  of  practice  the  pathologists  communicate
frequently  with  their  clinician  counter  parts  in
conferences  and  on  specific  cases.  Also,  in
larger groups there tends to be multiple special-
ists in the same field and so there is ample depth
and  opportunity  to  discuss  and  work  though
complex cases. In intermediate sized pathology
groups,  a  similar  strategy  has  taken  form,
although not to the same extent. In these groups,
most pathologists are generalists, but have sub-
specialty  interests.  Each  pathologist  with  sub-
specialty interest takes on the responsibility of
keeping up with a particular field and is respon-
sible  for  updating  their  pathology  colleagues
while at the same time serves as the point person
with their clinical colleagues in that field. For a
small practice, it is much more difficult to be up
to date in all subspecialty fields. For this reason,
smaller  practices  tend  to  liberally  use  expert
consultation in areas outside their comfort zone.
Therefore,  we  have  at  least  three  practice  sys-
tems  that  attempt  to  address  the  knowledge
needs of pathologists. It is not clear which sys-
tem  is  best  or  produces  the  least  amount  of
errors.  I  am  unaware  of  any  studies  that  have
attempted  to  directly  measure  the  efficacy  of
these  practice  settings.  Reports
that  have
attempted to study differences between general-
ists  and  specialists  in  clinical  practice  offer
some generality that may apply to pathology as
well. These studies suggest that specialists were
generally  more  knowledgeable  in  their  area  of
interest  and  were  quicker  to  adopt  new  treat-
ments,  but  also  used  more  resources  [43,  44].
There is a  suggestion that the quality of care by
specialists  exceeded  care  by  generalist  for
selected conditions.

242

Tight Time Constraints

A number of external pressures focus the need to
have  time  constraints  in  surgical  pathology.
Regulatory mandates, while not strict, are often
cited as a main reason to have good turnaround
time. However, pressure from clinical colleagues
and  an  inherent  need  to  please  our  customers
(patients and clinicians) have greater influence on
our  desire  to  produce  a  diagnostic  result  in  the
least amount of time possible.

The  total  turnaround  time  is  usually  not  the
real  issue  leading  to  errors,  the  problem  is  in
batch work and time constraints. In an ideal envi-
ronment work would be evenly spaced and main-
tained at regular intervals with sufficient time to
accomplish each task. Surgical pathology is prone
to  batch  work  and  time  constraints;  specimens
are typically delivered in batches and are acces-
sioned and processed in batches. After dissection
and  placement  in  cassettes,  the  tissue  typically
must  be  placed  onto  processors  that  begin  at  a
certain  point  in  time  thus  providing  time  con-
straints. This has a greater impact when the work-
load  is  heavier  than  usual  or  when  fewer
employees  are  available.  This  problem  also
applies to the pathologist at sign-out where cases
are usually brought in batches. While the pathol-
ogist rarely has a definitive deadline to complete
the work, there is pressure to get those cases done
that  day  and  maintain  an  adequate  turn-around
time.  This  pressure  may  be  intensified  if  the
pathologist has other commitments that occupy a
portion of the day and the work should be com-
pleted before a long weekend or a vacation.

The  net  effect  of  batch  work  and  time  con-
straints is that people may skip over critical steps
that  assures  proper  handling,  processing,  and
interpretation. This could include quality checks
that were instituted to prevent errors such as dou-
ble checking two patient identifiers.

Hierarchical Culture

A hierarchical culture is one in which authority is
not  questioned  for  fear  of  retribution  or  more
commonly  to  avoid  the  unpleasant  consequence

R.E. Nakhleh

of  such  episodes.  While  this  type  of  behavior
 frequently  is  generational  and  cultural,  it  is
 frequently  encouraged  or  accentuated  by  the
behavior  of  leadership.  While  this  is  often
 unintentional, the pathologist’s mood and response
to a simple event such as technologist bringing in
additional tray of slides on a busy day may be suf-
ficient to initiate a technologist’s avoidance behav-
ior.  It  takes  only  a  few  similar  episodes  for  a
technologist  to  decide  that  avoidance  is  the  best
strategy  for  a  harmonious  work  day.  So  when
problems occur that may be addressed quickly as
they occur, the choice is made to avoid communi-
cation  and  to  let  things  stand.  To  alleviate  this
situation the pathologist or others in a position of
authority have to remove that element of fear and
discomfort that comes with bringing forth prob-
lems.  This  way,  problems  are  managed  in  real
time and are not allowed to fester.

References

  1.  Racusen LC, Solez K, Colvin RB, et al. The Banff 97
working  classification  of  renal  allograft  pathology.
Kidney Int. 1999;55:713–23.

  2.  Demetris  AJ,  Batts  KP,  Dhillon  AP,  et  al.  Banff
schema for grading liver allograft rejection: an inter-
national  consensus  document.  Hepatology.  1997;25:
658–63.

  3.  Edge SB, Byrd DR, Compton CC, Fritz AG, Greene
FL,  Trotti  A,  editors.  AJCC  cancer  staging  manual.
7th ed. New York: Springer; 2009.

  4.  Raab  SS,  Grzybicki  DM,  Janosky  JE,  et  al.  Clinical
impact and frequency of anatomic pathology errors in
cancer diagnoses. Cancer. 2005;104:2205–13.

  5.  Spath  PL.  Reducing  errors  through  work  systems
improvement.  In:  Spath  PL,  editor.  Error  reduction
in  health  care.  Chicago:  AHA  Press;  1999.
p. 199–234.

  6.  Weiss  MA.  Analytic  variables:  diagnostic  accuracy.
In:  Nakhleh  RE,  Fitzgibbons  PL,  editors.  Quality
management
in  anatomic  pathology:  promoting
patient safety through systems improvement and error
reduction.  Northfield:  The  College  of  American
Pathologists; 2005. p. 55–61.

  7.  Meier  FA,  Zarbo  RJ,  Varney  RC,  et  al.  Amended
reports: development and validation of a taxonomy of
defects. Am J Clin Pathol. 2008;130:238–46.

  8.  Kornstein  MJ,  Byme  SP.  The  medicolegal  aspect  of
error in pathology; A search of jury verdicts and set-
tlements. Arch Pathol Lab Med. 2007;131:615–8.
  9.  Troxel DB. Medicolegal aspects of error in pathology.

Arch Pathol Lab Med. 2007;130:617–9.

14  Evaluation and Reduction of Diagnostic Errors in Pathology Using an Evidence-Based Approach

243

 10.  Nakhleh RE. Lost, mislabeled and unsuitable surgical
pathology  specimens.  Pathol  Case  Rev.  2003;8:
98–102.

 11. The  Joint  Commission.  Accreditation  Program;
Laboratory National Patient Safety goals. http://www.
jointcommission.org/GeneralPublic/NPSG/gp_npsg.
htm. Accessed 1 May 2010.

 12. Simpson JB. A unique approach for reducing speci-
men labeling errors: combining marketing techniques
with  performance  improvement.  Clin  Leadership
Manag Rev. 2001;15:401–5.

 13. Makary  MA,  Epstein  J,  Pronovost  PJ,  Millman  EA,
Hartmann EC, Freischlag JA. Surgical specimen iden-
tification errors: a new measure of quality in surgical
care. Surgery. 2007;141(4):450–5.

 14. Valenstine  PN,  Raab  SS,  Walsh  MK.  Identification
errors  involving  clinical  laboratories:  a  College  of
American Pathologists Q-Probes study of patient and
specimen  identification  errors  at  120  institutions.
Arch Pathol Lab Med. 2006;130:1106–13.

 15. Zarbo  RJ,  Tuthill  M,  D’Angelo  R,  et  al.  The  Henry
Ford Production System; reduction of surgical pathol-
ogy in-process misidentification defects by bar code-
specific  work  process  standardization.  Am  J  Clin
Pathol. 2009;131:468–77.

 16. D’Angelo  R,  Zarbo  RJ.  The  Henry  Ford  Production
System; Measures of process defects and waste in sur-
gical  pathology  as  a  basis  for  quality  improvement
initiatives. Am J Clin Pathol. 2007;128:423–9.

 17. Wolf  AC,  Hammond  EH,  Schwartz  JN,  et  al.
American  Society  of  Clinical  Oncology/College  of
American  Pathologists  guideline  recommendations
for human epidermal grown factor receptor 2 testing
in  breast  cancer.  Arch  Pathol  Lab  Med.  2007;
131(1):18–43.

 18. Nakhleh  RE.  Patient  safety  and  error  reduction  in
 surgical pathology. Arch Pathol Lab Med. 2008;132:
181–5.

 19. Nakhleh  RE,  Souers  R,  Brown  RW.  Significant  and
unexpected, and critical diagnosis in surgical pathol-
ogy:  a  college  of  American  Pathologists’  survey  of
1130  laboratories.  Arch  Pathol  Lab  Med.  2009;133:
1375–8.

 20. Branston LK, Greening S, Newcombe RG, et al. The
implementation of guidelines and computerized forms
improves  the  completeness  of  cancer  pathology
reporting.  The  CROPS  project:  a  randomized  con-
trolled  trial  in  pathology.  Eur  J  Cancer.  2002;38:
764–72.

 21. Lingard L, Espin S, Whyte S, et al. Communication
failures in the operating room: an observational clas-
sification  of  recurrent  types  and  effects.  Qual  Saf
Health Care. 2004;13(5):330–4.

 22. Krautscheild  LC.  Improving  communication  among
healthcare  providers:  preparing  student  nurses  for
practice. Int J Nurs Educ Scholarsh. 2008;5:1–13.
 23. Ferrara G, Argenyi Z, Argenziano G, et al. The influ-
ence  of  clinical  information  in  the  histopathologic
diagnosis of melanocytic skin neoplasms. PLoS One.
2009;4:e5375.

 24. Nakhleh  RE,  Gephardt  G,  Zarbo  RJ.  Necessity  of
clinical information in surgical pathology: a College
of American Pathologists Q-Probes Study of 771,475
surgical pathology cases from 341 institutions. Arch
Pathol Lab Med. 1999;123:615–9.

 25. Nakhleh RE, Zarbo RJ. Amended reports in surgical
pathology  and  implications  for  diagnostic  error
 detection  and  avoidance:  a  College  of  American
Pathologists’  Q-Probes  Study  of  1,  667,  547  acces-
sioned  cases  in  359  laboratories.  Arch  Pathol  Lab
Med. 1998;22:303–9.

 26. Troxell DB, Sabella JD. Problem areas in pathology
practice: uncovered by review of malpractice claims.
Am J Surg Pathol. 1994;18:821–31.

 27. Raab SS, Grzybicki DM, Condel JL, et al. Effect of
lean  method  implementation  in  the  histopathology
section of an anatomical pathology laboratory. J Clin
Pathol. 2008;61:1193–9.

 28. Condel JL, Sharbaugh DT, Raab SS, et al. Error free
pathology: applying lean production methods to ana-
tomic pathology. Clin Lab Med. 2004;24:865–99.
 29. Rosai  J.  Borderline  epithelial  lesions  of  the  breast.

Am J Surg Pathol. 1991;15:209–21.

 30. Schnitt  SJ,  Connolly  JL,  Tavassoli  FA,  et  al.
Interobserver reproducibility in the diagnosis of  ductal
proliferative breast lesions using standardized criteria.
Am J Surg Pathol. 1992;16(12):1133–43.

 31. Montgomery  E,  Bronner  MP,  Goldblum  JR,  et  al.
Reproducibility  of  the  diagnosis  of  dysplasia  in
Barrett  esophagus:  a  reaffirmation.  Hum  Pathol.
2001;32(4):368–78.

 32. Yin H, Leong AS. Histologic grading of noninvasive
papillary  urothelial  tumors:  validation  of  the  1998
WHO/ISUP  system  by  immunophenotyping  and
 follow-up. Am J Clin Pathol. 2004;121(5):679–87.
 33. Ruby SG, Henson DE. Practice protocols for surgical
pathology:  a  communication  from
the  Cancer
Committee of the College of American Pathologists.
Arch Pathol Lab Med. 1994;118:120–1.

 34. Fielding LP, Henson DE. Multiple prognostic factors
and  outcome  analysis  in  patients  with  cancer:  com-
munication  from  the  American  Joint  Committee  on
Cancer. Cancer. 1993;17:2426–9.

 35. Powsner SM, Costa J, Homer RJ. Clinicians are from
Mars and pathologists are from Venus: clinician inter-
pretation of pathology reports. Arch Pathol Lab Med.
2000;124:1040–6.

 36. Valenstein PN. Formatting pathology reports: apply-
ing four design principles to improve communication
and patient safety. Arch Pathol Lab Med. 2008;132:
84–94.

 37. Norris B. Human factors and safe patient care. J Nurs

Manag. 2009;17(2):203–11.

 38. D’Addessi  A,  Bongiovanni  L,  Volpe  A,  Pinto  F,
Bassi P. Human factor in surgery: from Three Mile
Island  to  the  operation  room.  Urol  Int.  2009;83(3):
249–57.

 39. Novis D. Routine review of surgical pathology cases
as a method by which to reduce diagnostic errors in a
community hospital. Pathol Case Rev. 2005;10:63–7.

244

R.E. Nakhleh

 40. Renshaw  AA,  Gould  EW.  Measuring  the  value  of
review of pathology material by a second pathologist.
Am J Clin Pathol. 2006;125:737–9.

ing patient safety through systems improvement and
error reduction. Northfield: The College of American
Pathologists; 2005. p. 77–92.

 41. Association  of  Directors  of  Anatomic  and  Surgical
Pathology. Standardization of the surgical pathology
report. Am J Surg Pathol. 1992;16(1):84–6.

 43. Harrold  LR,  Field  TS,  Gurwitz  JH.  Knowledge,
 pattern  of  care  and  outcomes  of  care  for  generalists
and specialists. J Gen Intern Med. 1999;14:499–511.

 42. Brown  RW.  Quality  management  in  the  histology
laboratory. In: Nakhleh RE, Fitzgibbons PL, editors.
Quality management in anatomic pathology: promot-

 44. Donohoe  MT.  Comparing  generalist  and  specialist
care;  discrepancies,  deficiencies  and  excesses.  Arch
Intern Med. 1998;158:1596–608.

Meta-Analysis 101 for Pathologists

Ruta Gupta and Alberto M. Marchevsky

15

Keywords
Meta-analysis  in  pathology,  evidence-based  pathology  •  Data  collection
in  pathology  •  Odds  ratios  •  Effect  sizes  •  Forest  plots  •  Funnel  plots
• Publication Bias

Meta-analysis  is  a  statistical  procedure  that
 integrates the results of independent studies with
a similar research hypothesis, explores data het-
erogeneity  and  synthesizes  summaries  if  appro-
priate  [1].  Well  conducted  meta-analysis  allows
for objective integration and comparison of mul-
tiple study results and can also be used to explain
the heterogeneity between study results [1]. The
basic  principles,  applications,  construction,  and
statistical methods of meta-analysis are discussed
in more detail in Chap. 9 by Dr. Vamvakas. This
statistical method has been applied to the integra-
tion of randomized controlled clinical trials in an
attempt to estimate overall effects. Meta-analysis
has been used in epidemiology to investigate the
reasons for differences in risk estimates between
observational studies and to discover patterns of
differences among study results. In this chapter,
we  attempt  to  provide  pathologists  potentially
interested  in  applying  meta-analysis  in  their
research with a simplified “how-to” guide to the

R. Gupta ()
Department of Anatomic Pathology,
The Canberra Hospital, ACT Pathology, Garran,
Australian Capital Territory 2606, Australia
e-mail: rutagupta@gmail.com

performance  of  meta-analysis,  using  examples
from our previous experience.

Evidence-based  medicine  (EBM)  has  advo-
cated the use of meta-analysis for systematic and
quantitative  analysis  of  randomized  control  tri-
als for over a decade [2]. Three general applica-
tions of meta-analysis include: (1) integration of
the findings of studies with varying sample size
but demonstrating treatment effect operating in
the same direction, (2) investigation of the rea-
sons for disagreements among studies reporting
contradictory treatment effects, and (3) integra-
tion of the findings of different studies with sim-
ilar  research  hypothesis  but  not  attaining
statistical significance due to small sample sizes
or  other  factors  that  influence  statistical  power
[3–5]. The results of the first type of studies are
usually used for the design of definitive random-
ized  controlled  clinical  trials  (RCT)  with  an
optimal cohort size that can provide level I evi-
dence. The results of the second type of studies
can  be  very  helpful  to  explain  the  reasons  for
contradictory  results  in  studies  using  similar
hypotheses.  The  use  of  meta-analysis  to  inte-
grate the results of underpowered, nonsignificant
studies  as  an  alternative  to  RCT  in  clinical
research is controversial [5].

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_15, © Springer Science+Business Media, LLC 2011

245

246

R. Gupta and A.M. Marchevsky

Applications of Meta-Analysis
in Anatomic Pathology

Anatomic pathologists have been slow to accept
the basic tenets of EBM, although there is a recent
increasing interest in their application to the spe-
cialty, so-called evidence-based pathology (EBP)
[3–6]. The traditional apprenticeship based teach-
ing  in  anatomical  pathology  emphasizes  the
learning from the “experience of one’s teachers.”
This learning model has great strengths as it helps
transmit information in an interactive manner and
provides students with the role models offered by
various teachers, but it also has some of the dis-
advantages  that  EBM  advocates  have  attributed
to so-called Eminence-Based Medicine [6–9]. In
particular, anatomic pathologists place great con-
fidence  in  the  opinion  and  publications  of  their
teachers and tend to disregard findings or recom-
mendations  published  by  others  that  contradict
them.  In  addition,  the  majority  of  literature  in
anatomical pathology comprises of case control
studies,  case  series,  case  reports,  and  opinion
based  narratives  where  the  importance  of  vari-
ables such as study design, sample size,  patient
selection  bias,  length  of  follow-up,  treatment
effect,  and  others  is  not  emphasized  [10].
Systematic  reviews  and  meta-analysis  provide
the  methodology  to  integrate  information  from
the literature in a manner that is potentially more
comprehensive  and  less  subjective  than  ad-hoc
literature reviews prepared by experts.

Only a few studies have attempted to use meta-
analysis  in  Anatomic  Pathology  [11–16].  For
example,  Faraji  et  al.  evaluated  renal  epithelioid
angiomyolipomas with meta-analysis in an attempt
to identify various prognostic factors for this newly
defined  relatively  unusual  entity  using  the  data
available in 69 studies in the literature and demon-
strated that male gender, large tumor size, marked
cytologic  atypia,  and  extensive  tumor  necrosis
portend  an  unfavorable  outcome  [14].  Anderson
et al. used meta-analysis to evaluate the utility of
immunohistochemical  panels  in  determining  the
site  of  origin  of  metastatic  malignancies.  The
results of their meta-analysis showed that studies
evaluating  the  utility  of  immunohistochemistry

using  both  primary  as  well  as  metastatic  tumors
provided correct identification in 82.3% cases as
against 65.6% in the studies using only metastatic
tumors.  The  authors  thus  confirmed  that  there
exists  an  unmet  need  for  additional  definitive
immunomarkers and also emphasized the impor-
tance  of  minimum  performance  measures  while
 diagnostic  modalities  [15].
evaluating  newer
Several novel prognostic markers are being evalu-
ated  for  melanoma,  however  none  of  these  are
incorporated  into  clinically  relevant  guidelines,
staging systems, or standard of care for melanoma
patients. Gould Rothberg et al. evaluated the rea-
sons  for  this  disconnects  using  meta-analysis.
Their conclusions reflect the current state of litera-
ture  in  anatomic  pathology  and  emphasize  the
need  for  stringent  adherence  to  reporting  guide-
lines, test validation, and cohort selection [16]. We
recently used meta-analysis to evaluate a variety of
problems related to anatomic pathology and some
of  these  materials  are  used  to  illustrate  various
aspects of the techniques in this chapter [11–13].

Meta-Analysis Methodology: A Step
by Step Guide to the Analysis

The application of meta-analysis to the evaluation
of data from the literature and/or own experience
is relatively simple with the use of modern soft-
ware.  We  have  performed  our  studies  using
Comprehensive  Meta-analysis  2.0  (Biostat,  Inc.
Englewood, New Jersey). Several other commer-
cial  softwares  are  available,  including  Clin  Tool
software (http://www.clintools.com/contact.html),
Meta-analyst  (http://tuftscaes.org/meta_analyst/)
providing free online calculators for meta-analysis
of binary, continuous and diagnostic data, Metastat
(http://echo.edres.org:8080/meta/metastat.htm)
and others.

Data Collection: Systematic Literature
Review and Evidence Summaries

Meta-analysis is usually well suited for compar-
ing the effects of a particular variable of interest

15  Meta-Analysis 101 for Pathologists

247

in  a  test  group  using  a  well-matched  control
group  to  estimate  odds  ratios  (OR).  The  first
and usually most difficult and time consuming
step in the application of meta-analysis to pub-
lished  data  is  the  performance  of  a  systematic
literature review to collect comprehensive data
from  the  literature.  As  explained  in  previous
chapters, systematic  literature reviews include a
specific  time  period  and  explicit  listings  of
database/s  searched  and  search  terms,  in  con-
trast to ad-hoc reviews that allow investigators
to select references from the literature based on
subjective criteria not specified in a manuscript.
The  data  elements  of  interest  for  study  with
meta-analysis could be, for example, the effect
of a particular treatment, the prognostic value of
a  test  (e.g.,  survival,  recurrence  rate,  other)  or
others. The data is organized in evidence sum-
maries in a manner that is suitable for analysis
and as explained below. It is often practical to
insert  it  into  spreadsheets  such  as  Excel
(Microsoft,  Redmond  WA),  in  a  manner  that
can  be  pasted  directly  into  Comprehensive
Meta-analysis  2.0  (Biostat,  Inc.  Englewood,
New Jersey) or other software.

Unfortunately,  it  is  often  quite  difficult  to
extract information that is suitable for meta-anal-
ysis  from  the  medical  literature  because  of  the
lack  of  reporting  standards.  Indeed,  attempts  at
performing meta-analysis on published data can
provide investigators with an eye-opening experi-
ence  regarding  the  extensive  variability  in  the
manner that results are often reported in the ana-
tomic pathology and other medical literature [16].
For example, the data collected in various studies
is  frequently  embedded  in  different  areas  of  a
manuscript,  such  as  “Methods,”  “Results”,  and
“Discussion.” Presence of protein expression by
immunohistochemistry  is  often  variably  defined
in different studies with lower limits for positivity
ranging from 5 to 20% [17–20]. Outcomes such
as survival and response to treatment are reported
using  variable  definitions  (e.g.,  overall  survival,
or  disease  specific  survival),  and  using  variable
lengths of follow-up [21, 22]. Use of ambiguous
terminology in medical publications is one of the
most  difficult  to  overcome  barriers  during  the
collection of data for meta-analysis [16].

Another  common  problem  during
the
 collection of data for meta-analysis is that medi-
cal publications frequently describe only second-
ary data such as p values, sensitivity, specificity,
or  other  selected  results  rather  than  listing  the
data  collected  during  the  study  and  used  by  the
investigators in their statistical analysis. This type
of secondary data does not permit reviewers or a
reader  to  double  check  on  the  accuracy  of  the
results reported in publications and does not make
available to other investigators the data originally
collected in a study that could be combined with
the  results  of  other  studies  reporting  similar
effects and analyzed with meta-analysis. With the
advent of relatively inexpensive storage capabil-
ity in computer networks, the widespread use of
the Web and the progressive migration of publica-
tions into electronic formats, it may be possible in
the near future to develop new publication stan-
dards  that  encourage  the  storage  of  the  primary
data used by the authors of a publication in their
calculations  in  “electronic  appendices”  that  are
made available to reviewers and readers and that
could be used by investigators in future  studies.
This  data  would  probably  need  to  be  copyright
protected, as currently text and tables are.

We  will  illustrate  the  performance  of  a  sys-
tematic  review,  collection  of  data,  and  prepara-
tion  of  evidence  summaries  for  meta-analysis
using examples from our recent study evaluating
the  prognostic  value  of  the  2004  World  Health
Organization (WHO) histologic classification of
thymomas  [13,  23].  In  our  study,  we  elected  to
query the English literature for the period 1999–
2007, as a previous classification scheme for thy-
momas  proposed  by  WHO  in  1999  was  quite
similar  to  the  2004  version  being  investigated,
using  the  PubMed  database  of  the  National
Library  of  Medicine  and  the  following  search
terms:  thymomas,  pathology,  prognosis,  and/or
stage. We arbitrarily elected as an inclusion crite-
ria  in  the  meta-analysis,  availability  of  5  years
minimum of clinical follow-up. The search iden-
tified  15  studies  with  2,192  thymoma  patients
classified according to WHO histologic type and
followed postoperatively for longer than 5 years
[13]. Survival and recurrence, by WHO histologic
type,  were  selected  as  the  variables  of  interest.

248

R. Gupta and A.M. Marchevsky

As the study was not designed to investigate the
effect of particular treatment on a test group, the
usual  application  of  Comprehensive  Meta-
analysis  2.0  (Biostat,  Inc.  Englewood,  New
Jersey)  software,  we  compared  the  number  of
patients being alive or dead at follow-up, by two
histologic types at a time, one representing a “test
group”  and  the  other  a  “control  group.”  As  the
meta-analysis  was   performed  comparing  the
prognosis  of  patients  with  two  different  WHO
histologic  types  of  type  at  a  time,  for  example
those of thymomas A and AB, and the software
needs the number of patients in each category to
estimate  odds  and  OR,  we  collected  four  data
points from each study: total number of thymoma
A patients, number of thymoma A patients alive
at  follow-up,  number  of  thymoma  AB  patients,
and  number  of  thymoma  AB  patients  alive  at
follow-up. The evidence summary of these data
is shown in Table 15.1.

Data Analysis – Calculation of Odds
Ratios, Preparation of Forest Plots,
and Selection of Model to Be Used
for Meta-analysis

Selected columns from Table 15.1 were readily
imported into Comprehensive Meta-analysis 2.0
(Biostat, Inc. Englewood, New Jersey) software
and OR, log OR, and standard errors were auto-
matically estimated for each study, as shown in
Fig. 15.1. Please note that the software estimates
OR  and  other  statistics  only  for  studies  that
include an “event.” In this example, an “event”
requires  that  a  study  report  some  patients  that
did not survive, resulting in smaller numbers of
“survived  patients”
than  “total  number  of
patients” in either the thymoma A or AB groups.
Studies such as those by Kim, Bedini, and others
that  do  not  report  “events”  for  these  particular
subsets of thymoma patients, resulting in OR = 1,
are  not  included  in  the  meta-analysis  [24,  25].
The investigator can also elect to exclude from
meta-analysis the data from selected studies that
are considered inadequate because of their design
characteristics,  small  sample  sizes,  or  other
technical flaws. In our example, we included all

15 studies identified by the systematic review, to
avoid  introducing  another  source  of  potential
bias.

Once the information is entered in the  software,
the  software  eliminates  from  analysis  the  data
from studies that report events, weighs the data of
the remaining studies according to the cohort size
evaluated  in  each  publication,  and  performs  the
statistical analysis, as shown in Fig. 15.2. The fig-
ure shows, from left to right, the model used for
the analysis, as explained below, the studies eval-
uated,  statistics  for  each  study  including  OR,
lower limit, upper limit, Z-value and p-value and
a  forest  plot  showing  in  a  graphical  manner  the
OR, and 95% confidence intervals for all studies.
It is beyond the scope of this chapter to explain
the  rationale  behind  the  use  of  fixed  or  random
models to evaluate data in meta-analysis. In gen-
eral, the fixed model assumes that the effect size
of all the studies is within a range that does not
need to be normalized, so the effect size of each
study is left as constant. In the random model it is
assumed that there is some variability in the effect
sizes of different studies due to variables such as
different sample sizes and others. The analysis of
effect sizes using random models applies a math-
ematical formula that normalizes the effects sizes
of all studies toward an overall mean effect size,
in an attempt to minimize the effect of design dif-
ferences between studies. In our study of thymo-
mas and others we have analyzed the data using
both fixed models and random models and have
generally obtained similar results [11–13].

Figure 15.2 shows that there are no significant
survival differences between patients with A and
AB thymomas, in any of the studies with p values
>0.05.  The  bottom  row  shows  the  statistics  for
the  entire  population  of  patients  from  all  the
selected studies, calculated using a fixed model;
it also shows a nonsignificant result with p = 0.544.
The same data can be viewed using high resolu-
tion graphics that exhibit in more detail the char-
acteristics of the forest plot, as shown in Fig. 15.3.
Each horizontal line represents a study. The verti-
cal lines show different OR, 0.01, 0.1, 1, 10, and
100. The OR from each study is summarized by a
square. The size of each square is proportional to
the weight being assigned to the results of each

15  Meta-Analysis 101 for Pathologists

249

3
B
a
m
o
m
y
h
T

2
B
a
m
o
m
y
h
T

1
B
a
m
o
m
y
h
T

B
A
a
m
o
m
y
h
T

A
a
m
o
m
y
h
T

v
r
u
S
%

0
5
.
4
5

0
4

A
N

3
7

0
5

2
9

1
8

0
3
.
1
7

5
3

0
8

8
3

0
7

6
3

8
7

0
8

C
N
T

3
3

3
1

0
2

1
5

6
1

2
1

3
2

6
2

9
2

5

2
1

7
2

6
2

0
1

5
1

2
9
–
8
3

8
1
3

5

8
1

A
N

P
S

8

7
3

1
1

9
1

8
1

0
1

4

5

9
1

9

8

v
r
u
S
%

0
7
.
6
6

1
7

A
N

2
9

4
1
.
8
6

4
9

1
9

0
4
.
2
8

3
3

5
9

5
8

5
7

9
5

2
8

2
1

3
8
1

0
9
.
0
9

2
9
–
9
5

C
N
T

1
2

0
5

2
3

4
2

9
2

8

8
5

5
4

8
2

5
1

9
2

9
3

7
9

2
8

2
2

4
1

5
3

A
N

P
S

2
2

0
2

7

3
5

7
3

9

4
1

5
2

0
1

7
5

7
6

0
2

0
0
1

5
8

0
0
1

2
9

8
1
.
8
6

4
9

0
0
1

0
9
.
8
8

8
7

0
0
1

6
8

0
1
.
4
9

1
9

4
8

0
9
.
0
9

8
1

5
4

2
1

7
2

6
1

7
2

3
1

3
1

0
2

4
2

5
1

7
1

5
5

4
2

1
1

8
1

8
3

2
1

5
2

1
1

5
2

3
1

2
1

6
1

4
2

3
1

6
1

0
5

0
2

0
1

0
0
1

0
9

2
9

0
0
1

6
.
4
8

0
0
1

0
0
1

2
.
3
9

0
9

0
0
1

0
0
1

0
0
1

7
8

0
0
1

8
.
3
9

5
1

9
4

5
2

2
5

0
4

7
1

8
4

6
2

1
3

7

6
5

8
6

7
7

0
2

6
1

5
1

4
4

2

2
5

4
3

7
1

8
4

4
2

8
2

7

6
5

8
6

7
6

0
2

5
1

5
9

0
0
1

0
0
1

0
0
1

0
0
1

0
0
1

0
0
1

0
0
1

0
0
1

0
0
1

0
0
1

0
0
1

0
0
1

6
7

8
8
.
8
8

0
1

1
2

7

1
2

5

8

7

1
2

4
1

0
1

8
1

8

8
1

3
4

9

0
1

0
2

7

1
2

5

8

7

1
2

4
1

0
1

8
1

8

8
1

3
3

8

v
r
u
S
%

C
N
T

P
S

v
r
u
S
%

C
N
T

P
S

v
r
u
S
%

C
N
T

P
S

]
6
2
[

.
l
a

t
e

e
b
o
n
o
S

]
7
2
[

.
l
a

t
e

a
n
e
R

]
4
2
[

.
l
a

t
e
m
K

i

]
8
2
[

.
l
a

t
e

t
h
g
i
r

W

]
5
2
[

.
l
a

t
e

i
n
i
d
e
B

]
9
2
[

.
l
a

t
e

o
d
n
o
K

s
e
c
n
e
r
e
f
e
R

]
1
3
[

.
l
a

t
e

k
r
a
P

]
2
3
[

.
l
a

t
e

a
e
R

]
0
3
[

l
e
b
o
r
t
S

]
4
3
[

.
l
a

t
e

a
w
a
g
a
k
a
N

]
3
3
[

.
l
a

t
e

l
a
h
g
n
i
S

]
6
3
[

.
l
a

t
e

a
r
u
m
u
k
O

]
7
3
[

.
l
a

t
e

r
e
k
e
i
R

]
5
3
[

.
l
a

t
e

n
e
h
C

]
8
3
[

.
l
a

t
e

e
s
s
y
e
r
b
a
l
a
h
C

9
7
5

0
9
3

0
0
1
–
8
7

7
3
3

3
0
3

0
0
1
–
2
9

7
4
5

7
9
4

0
0
1
–
6
7

0
2
2

8
0
2

e
g
n
a
r

r
o

r
e
b
m
u
n

l
a
t
o
T

e
r
u
t
a
r
e
t
i
l

n
i

e
l
b
a
l
i
a
v
a

s
t
n
e
i
t
a
p

a
m
o
m
y
h
t
o
t

g
n
i
n
i
a
t
r
e
p

a
t
a
d
f
o

y
r
a
m
m
u
s

e
c
n
e
d
i
v
E

.

1
5
1
e
l
b
a
T

250

R. Gupta and A.M. Marchevsky

Fig. 15.1  Tabulation of survival data of thymoma patients
stratified  by  WHO  histologic  type  in  the  meta-analysis
software (Comprehensive Meta-analysis 2.0 [Biostat, Inc.
Englewood,  NJ]).  The  left  hand  side  columns  list  the
names of various studies, the total number of patients, and
the number of patients surviving in each histologic type.
The right hand side columns  provide the odds ratio, the

log odds ratio with standard error for survival differences
in  patients  with  thymoma  type  A  vs.  patients  with  thy-
moma type AB. The software computes statistics only for
those studies in which an event (death or recurrence) has
occurred (from Marchevsky et al. [13], with permission of
John Wiley and Sons)

Fig.  15.2  Statistical  analysis  computed  by  the  software.
The figure shows, from left to right, the model used for the
analysis, as explained below, the studies evaluated, statistics

for  each  study  including  OR,  lower  limit,  upper  limit,
Z-value and p-value, and a forest plot showing in a graphical
manner the OR and 95% confidence intervals for all studies

study,  usually  as  a  result  of  the  cohort  size.
The  bottom  of  the  forest  plots  show  a  diamond
 summarizing  the  integrated  odds  ratio  and  95%
confidence intervals for all studies in the analysis.

The width of the diamond is proportional to the
overall 95% CI. Please note that all squares and
the diamond are close to the vertical line repre-
senting an OR of 1.

15  Meta-Analysis 101 for Pathologists

251

Study name

Statistics for each study

Odds ratio and 95% CI

Odds ratio

Lower limit

Upper limit

Z-Value

p-Value

Rena

Kim

Bedini

Park

Rea

Okumura

Reiker

Chalabreysse

2.273

1.596

2.072

1.531

3.561

5.756

0.078

0.533

1.366

0.249

0.069

0.102

0.066

0.172

0.322

0.004

0.029

0.499

20.741

37.077

42.210

35.526

73.703

102.868

1.400

9.708

3.742

0.728

0.291

0.474

0.265

0.822

1.190

–1.732

–0.425

0.606

0.467

0.771

0.636

0.791

0.411

0.234

0.083

0.671

0.544

0.01

0.1
Thymoma A

1

10

100

Thymoma AB

Meta Analysis

Fig. 15.3   Forest plot as computed by the software. The
right hand side of the figure shows the Forest plot com-
prising  of  vertical  lines  which  represent  odds  ratios
(OR) of 0.01, 0.1, 1, 10, and 100. Each horizontal line
represents  the  95%  confidence  interval  (CI)  for  each
study. The square size represents the cohort size of each
study.  The  diamond  at  the  bottom  of  the  graph  repre-
sents the overall odds ratio. The width of the diamond is

proportional  to  the  overall  95%  CI.  All  the  horizontal
black lines corresponding to the CI of each individual
study cross the vertical black line corresponding to the
OR  of  1  indicating  lack  of  significant  survival  differ-
ence  between  patients  with  thymoma  type  A  and  type
AB. Also the diamond giving the integrated odds ratio
intersects  the  vertical  line  corresponding  with  odds
ratio of 1

Fig. 15.4  Range of statistics computed by meta-analysis softwares

Data Analysis: Evaluation of the
Statistical Significance of the Results
Using Various Statistical Tests

cance of meta-analysis beyond a simple under-
standing of p  values probably need to enlist help
from  professional statisticians.

In  addition  to  the  p  values  shown  in  Fig.  15.3
and the forest plot, the software provides more
detailed  statistics  in  various  tables.  For  exam-
ple, Fig. 15.4 shows the effect sizes of the data
using  fixed  and  random  models,  with  95%  CI,
results  of  2-tailed  t-test  and  other  statistics.
Pathologists  attempting  to  evaluate  the  signifi-

Evaluation of Potential Data
Heterogeneity and Publication Bias

Meta-analysis  integrates  the  results  of  data
 collected by other investigators under somewhat
variable  conditions,  raising  questions  as  to

252

R. Gupta and A.M. Marchevsky

whether the results obtained with this methodol-
ogy are reliable. This is a particular problem in
anatomic  pathology  as  diagnoses  are  frequently
used as classifiers or dependent variables in vari-
ous  studies  and  there  is  a  certain  degree  in
 interobserver diagnostic variability, as discussed
in  other  chapters.  Heterogeneity  is  defined  as
 differences  in  results  between  studies  due  to
 variations  in  the  characteristics  of  the  popula-
tions  being  investigated,  methodology  used  for
data  collection,  various  forms  of  bias,  and
how  the  outcome  is  measured  and  interpreted.
Heterogeneity  becomes  significant  when  data
variability  between  studies  is  greater  than  it
would be expected from sampling variation alone.
Publication bias occurs when the publication of
research results depend on their nature and direc-
tion  [39].  It  often  results  from  the  tendency  for
researchers to report the results of studies that are
“positive,”  and  show  a  statistically  significant
finding and for reviewers to reject results that do

not conform with what has been previously been
reported. Publication bias results in the so-called
file  drawer  problem, that  many  studies  are  con-
ducted  but  not  published  because  they  did  not
produce  statistically  significant  results,  poten-
tially resulting in information that is uknown in
the literature and skewed toward positive results.
Various statistical methods have been designed
to evaluate for data heterogeneity and publication
bias. Heterogeneity can be explored with graphi-
cal methods such as forest plots and radial plots
and  various  statistical  tests  such  as  the  Q-test,
meta-regression,  and  others.  Forest  plots  allow
readers to compare the results of all studies at a
glance, as shown in Fig. 15.3.

Publication bias is explored with funnel plots
and various tests designed to test for funnel test
of  asymmetry,  such  as  the  rank  correlation
method,  Egger’s  linear  regression,  trim  and  fill
and  others. An example of funnel plot is shown in
Fig.  15.5.  The  plot  shows  a  vertical  line,  two

Funnel Plot of Precision by Log odds ratio

1.0

0.8

0.6

0.4

0.2

)
r
r
E
d
t
S
/
1
(
n
o
i
s
i
c
e
r
P

0

−3

−2

−1

0

1

23

Log odds ratio

Fig.  15.5  Evaluation  of  heterogeneity  between  study
results by Funnel plot. Comparison of Thymomas A and
AB  amongst  various  studies  shows  the  data  to  be  het-
erogenous.  The  plot  shows  a  vertical  line,  two  lateral
curves, and multiple small circles. Each circle represents
a  single  study.  The  height  of  each  circle  represents  the
weight  being  assigned  to  the  results  of  the  study.  In

instances when the data from various studies is homoge-
neous,  the  curves  are  close  to  the  vertical  line  and  all
circles are clustered near the vertical line in a symmetri-
cal distribution. In this analysis, the plot shows six stud-
ies  to  the  left  of  the  vertical  line  and  only  two  in  the
opposite  direction,  indicating  marked  heterogeneity  of
the data

15  Meta-Analysis 101 for Pathologists

253

 lateral lines and multiple small circles. Each circle
represents the results of a single study. The height
of each circle represents the weight being assigned
to the results of the study. Funnel plots of homo-
geneous data usually show the lateral lines close
to the vertical line and all circles clustered near
the central vertical line in a symmetrical distribu-
tion balanced in height and number of circles on
both  sides  of  the  vertical  line.  In  contrast,
Fig. 15.5, resulting from the meta-analysis com-
paring  thymomas  A  with  thymomas  AB  shows
considerable data heterogeneity: six circles are to
the right of the vertical line and one other to the
left of the vertical line. Figure 15.6 shows a com-
posite of the various statistical tests provided by
Comprehensive  Meta-analysis  2.0  (Biostat,  Inc.
Englewood, New Jersey) software to evaluate for
funnel test asymmetry.

It  is  beyond  the  scope  of  this  chapter  to
review  in  detail  the  theory  and  applications  of
various statistical tests for the evaluation of data
 heterogeneity and publication bias during meta-
 analysis. In our previous research, we have used
funnel plots and the Egger’s regression intercept
test to evaluate our data.

Brief Review of Our Experience
with the Use of Meta-analysis
for the Evaluation of Selected
Problems in Anatomic Pathology

We  have  used  meta-analysis  in  our  laboratory
for the evaluation of the prognostic role of micro-
metastases  and  isolated  tumor  cells  in  patients

Fig. 15.6  Examples of statistics computed by meta-analysis software

254

R. Gupta and A.M. Marchevsky

with  lung  cancer,  the  clinical  applicability  of
various  tests  for  the  evaluation  of  epidermal
growth  factor  receptor  (EGFR)  in  lung  cancer
patients,  and  the  study  of  the  prognosis  of
patients  with  thymomas,  relatively  infrequent
neoplasms  that  are   associated  with  indolent
 clinical behavior [11–13].

Use of Meta-analysis for the Evaluation
of Prognostic and Predictive Features
and for the Integration of Personal
Experience with Published Data

Our  recent  study  of  the  prognostic  role  of  iso-
lated  tumor  cells  and  micrometastases  in  the
intrathoracic lymph nodes of lung cancer patients
provides an example of how to use this statistical
method for the evaluation of prognostic features
in  anatomic  pathology,  integrating  data  from
own  experience  with  that  previously  published
in the literature [40]. A few studies of the prog-
nostic  role  of  these  small  nodal  deposits,
described  under  various  names  such  as  occult
metastases,  micrometastases,  and  others  have

suggested  that  they  are  associated  with  poor
prognosis  and  decreased  survival  rates.  In  con-
trast,  several  other  studies,  including  one  from
our laboratory, have not been able to demonstrate
a significant  association between the presence of
isolated tumor cells or micrometastases and sur-
vival [41] (Table 15.2). To evaluate this topic in
a more formal way we reviewed our experience,
performed  a  systematic  literature  review  and
analyzed all available results with meta-analysis.
Our  recent  experience  consisted  of  4,148
intrathoracic lymph nodes from 266 consecutive
clinical  stage  I  non-small  lung  cancer  patients
evaluated  with  hematoxylin  and  eosin  stained
slides and keratin immunostains for the presence
of isolated tumor cells and micrometastases. The
systematic literature review identified 13 studies
providing data on the prognostic role of micro-
metastases  in  835  patients  detected  with  either
immunohistochemistry  or  molecular  methods,
including  our  current  data,  with  non-small  cell
carcinomas. Table 15.2 shows the evidence sum-
mary of the data.  Meta-analysis of data from the
835 non-small cell carcinoma of the lung patients
showed that there was no significant correlation

Table  15.2  Evidence  summary:  immunohistochemical  detection  of  micrometastases  in  the  regional  lymph  nodes
of NSCLC patients

Author and
number of
cases (n)
Melphi (16)

Study
design
CS

Evidence
level
IV

CS
Rena (87)
Izbicki (93)
CS
Marchevsky (60) CS
CS
Ishiwa (54)
CS
Osaki (115)
CS
Passlick (54)
CS
Gu (49)
CS
Wu (103)
CS
Goldstein (80)
CS
Hashimoto (31)
CS
Nicholson (49)
CS
Maruyama (44)

IV
IV
IV
IV
IV
IV
IV
IV
IV
IV
IV
IV

pN0 to
PN0 (I+)
NA

IHC
CK, CK7,
CK 19
11
AE1/AE3
NA
Ber-Ep4
 7
CK
NA
CK
NA
AE1/AE3
NA
Ber-Ep4
NA
CK, p53
AE1/AE3
NA
CK, Ber-Ep4 NA
NA
Cam 5.2
NA
CK
NA
Cam 5.2

pN0 to
pN1mi

2

3
16
3
11
19
5
9
13
2
8
3
19

Statistically significant
difference in survival
pN1 vs.
pN0 vs.
pN2mi
pN1mi
NA
NA

pN0 to
pN2mi
NA

pN1 to
pN2mi
NA

 1
NA
 0
 7
13
 8
13
 8
 1
 9
NA
12

NA
6
1
1
NA
2
NA
NA
NA
5
NA
NA

No
No
No
No
Yes
No
Yes
Yes
No
No
No
Yes

NA
No
No
No
NA
NA
NA
NA
NA
No
No
NA

15  Meta-Analysis 101 for Pathologists

255

between the presence of micrometastases detected
with either immunohistochemistry or molecular
methods  and  prognosis  and  that  there  was  no
sufficient data to evaluate for the  prognostic role
of isolated tumor cells in patients with non-small
cell   carcinoma  of  the  lung.  The  results  of  the
meta-analysis performed with the micrometastases

data   collected  using  immunohistochemistry,
with the corresponding forest plot are shown in
Fig.  15.7.  Figure  15.8  shows  the  forest  plot  of
results from the literature using molecular meth-
ods  for  the  detection  of  micrometastases,  also
nonsignificant. The results of both meta-analysis
indicate that detection of micrometastases does

Study name

Statistics for each study

Odds ratio and 95% CI

Odds ratio

Lower limit

Upper limit

Z-Value

p-Value

0.151
0.009
0.056
0.124
0.431
0.014
0.189
0.235
0.272
0.267
0.570
0.608

5.801
3.300
4.473
5.171
1.921
6.275
4.499
1.644
2.594
7.349
2.184
1.254

–0.071
–1.174
–0.620
–0.234
–0.247
–0.782
–0.099
–0.958
–0.303
0.398
0.319
–0.735

0.943
0.240
0.535
0.815
0.805
0.434
0.921
0.338
0.762
0.691
0.750
0.462

Rena
Mineo
Hashimoto
Marchevsky
Osaki
Goldstein
Ishiwa
Wu
Gu
Nicholson
CSMC

0.936
0.168
0.500
0.800
0.910
0.295
0.923
0.622
0.840
1.400
1.115
0.873

Meta Analysis

0.01

0.1
pN0

1

10

100

pN1(mi)

Fig. 15.7  Results of meta-analysis evaluating the prog-
nostic  value  of  micrometastases  detected  with  immuno-
histochemistry in patients with non-small cell carcinoma
of the lung. The meta-analysis allowed for the quantitative

integration of our own results with those identified in the
literature  by  a  systematic  review.  Please  note  that  the
results are not significant, as shown by the forest plot and
summary statistics

Study name

Statistics for each study

Odds ratio and 95% CI

Odds ratio

Lower limit

Upper limit

Z-Value

p-Value

Hashimoto

Nosotti

Wang

0.429

0.741

0.593

0.591

0.096

0.209

0.218

0.295

1.910

2.621

1.615

1.185

–1.111

–0.466

–1.023

–1.482

0.266

0.642

0.306

0.138

Meta Analysis

Fig. 15.8  Results of meta-analysis evaluating the prog-
nostic value of micrometastases detected with molecular
 methods in patients with non-small cell carcinoma of the

lung.  Please  note  that  the  results  are  not  significant,  as
shown by the forest plot and summary statistics

0.01

0.1
pN0

1

10

100

pN1(mi)

256

R. Gupta and A.M. Marchevsky

not  portend  prognostic  significance.  However,
evaluation  of  the  results  of  our  meta-analysis
with  power  analysis  demonstrated  that  3,060
patients  followed  for  60  months  would  be
needed to achieve 80% power in a study designed
to  detect  survival  differences  between  patients
with negative nodes and micrometastases.

Use of Meta-analysis for the
Study of Infrequent Diseases
that are Associated with Indolent
Clinical Course: Opportunities
for National and International
Collaborations

Our  recent  study  with  meta-analysis  showing
that  the  WHO  classification  of  thymomas  pro-
vides  significant  prognostic  information  for
selected stage III thymoma patients can be used
to   illustrate  the  value  of  this  methodology  for
the  integration  of  data  collected  at  different
international hospitals [42]. Thymomas and thy-
mic carcinomas are relatively uncommon medi-
astinal lesions that are difficult to study because
no  institution  can  collect  the  large  number  of
patients  required  to  achieve  significant  statisti-
cal  power  and  the  survival  effect  size  is  small
because  the  tumors  usually  follow  an  indolent
clinical  course,  with  only  some  tumors  recur-
ring  and/or  metastasizing  10  years  or  longer
after initial treatment. It is very difficult to orga-
nize a randomized clinical trial to study thymo-
mas.  Indeed,  our  systematic  literature  review
showed that no such studies have been reported
[12, 13]. Previous studies of thymoma patients,
including  two  studies  with  meta-analysis  per-
formed  in  our  laboratory,  showed  that  WHO
histologic  type  and  Masaoka  stage  provide
 significant prognostic information for thymoma
patients.  However,  there  is  only  one  study
where prognosis was evaluated by WHO histo-
logic  type  of  thymoma  previously  stratified

by  Masaoka  stage  [30].  This  information  is
important  as  therapy  is  usually  selected  on  the
basis of stage rather than histology. As informa-
tion  about  thymoma  patients  stratified  by  both
WHO histologic type and stage are not available
in  the  literature,  we  contacted  by  email  the
authors of recent studies reporting the prognosis
of thymoma patients categorized using the WHO
scheme and were able to collect data from 905
patients  treated  at  hospitals  in  Japan,  Korea,
Italy, Germany, and the United States, formatted
for  meta-analysis.
in  a  manner  suitable
Table  15.3  shows  the  evidence  summary  listed
in these data. Meta-analysis showed than when
stratified  by  stage,  significant  survival  differ-
ences could be estimated in patients with stage
III disease, between thymomas A and B2 and A
and  B3.  Figure  15.9  shows  the  meta-analysis
comparing the survival of stage III patients with
thymomas  A  and  B2,  with  the  corresponding
forest  plot.  The  latter  shows  that  most  studies
show OR < 1 and that evaluation of the data with
the fixed model yields p = 0.003.

Does Meta Analysis have a Future as
a Useful Statistical Tool in Anatomic
Pathology?

As illustrated with the previous examples, meta-
analysis could be used more widely in anatomic
pathology  to  integrate  the  results  of  multiple
studies in a more precise manner than with cur-
rently  used  ad-hoc  summary  tables.  Experience
with  this  methodology  will  hopefully  persuade
pathologists  about  the  need  to  report  data  in  a
more  consistent  and  explicit  manner  so  that  it
could be readily extracted in the future by other
investigators.  Meta-analysis  could  also  be  used
more often to estimate effect sizes across multi-
ple  studies  in  efforts  at  integrating  the  results
from current studies with those previously pub-
lished in the literature.

15  Meta-Analysis 101 for Pathologists

257

3
B
e
p
y
t

O
H
W

2
B
e
p
y
t

O
H
W

1
B
e
p
y
t

O
H
W

B
A
e
p
y
t

O
H
W

A
e
p
y
t

O
H
W

V

I

0
5

3
3

7
8

0
0
1

0
0
1

A
N

5
7

0
7

0
8

0
0
1

0
0
1

0
5

0
9

0
0
1

I
I
I

0
7

2
9

0
0
1

0
0
1

0
0
1

A
N

6
8

I
I

0
5

7
6

A
N

0
0
1

0
0
1

A
N

A
N

I

V

I

0

3
3

2
8

0
5

0
0
1

A
N

3
8

3
7

0
5

0
0
1

0
0
1

0
5

A
N

1
9

I
I
I

4
8

5
8

0
0
1

0
0
1

7
6

A
N

0
0
1

I
I

6
7

5
9

0
0
1

0
0
1

0
0
1

A
N

0
0
1

I

V

I

0
5

A
N

0
0
1

A
N

7
6

A
N

A
N

0
7

0

0
0
1

A
N

0
5

A
N

A
N

I
I
I

3
7

0
0
1

0
0
1

0
0
1

5
7

0
0
1

0
0
1

I
I

5
6

0
0
1

0
0
1

0
0
1

0
5

0
0
1

0
0
1

I

V

I

0

0
0
1

0

A
N

A
N

0

A
N

3
8

A
N

0
0
1

0
0
1

I
I
I

0

A
N

0
0
1

0
6

7
5

4
9

0
0
1

0
0
1

0
5

0
0
1

I
I

8
6

3
8

0
0
1

0
0
1

0
0
1

0
0
1

3
9

I

V

I

A
N

A
N

A
N

A
N

A
N

A
N

0
5

0
0
1

A
N

0
0
1

A
N

A
N

A
N

0
5

I
I
I

0
0
1

0
0
1

0
0
1

0
0
1

0
0
1

0
0
1

0
0
1

I
I

1
7

0
0
1

0
0
1

7
6

A
N

0
0
1

0
0
1

I

)
1
7
2
=
n
(

a
r
u
m
u
k
O

s
e
c
n
e
r
e
f
e
R

)
7
1
1
=
n
(

m
K

i

)
0
8
1
=
n
(

a
n
e
R

)
4
9
=
n
(

a
d
i
h
s
o
Y

)
6
5
=
n
(

C
M
S
C

)
0
2
=
n
(

H
M
T

)
8
6
1
=
n
(

l
e
b
o
r
t
S

e
g
a
t
s

a
k
o
a
s
a

M
d
n
a

e
p
y
t

c
i
g
o
l
o
t
s
i
h
O
H
W
y
b

d
e
fi
i
t
a
r
t
s

,
s
r
a
e
y

5

f
o
m
u
m
i
n
i
m
a

r
o
f

a
m
o
m
y
h
t

r
i
e
h
t

d
e
v
i
v
r
u
s

o
h
w
s
t
n
e
i
t
a
p

f
o
r
e
b
m
u
n

:
y
r
a
m
m
u
s

e
c
n
e
d
i
v
E

.

3
5
1
e
l
b
a
T

258

R. Gupta and A.M. Marchevsky

Study name

Statistics for each study

Odds ratio and 95% CI

Odds ratio

Lower limit

Upper limit

Z-Value

p-Value

Okumura

Rena

Strobel

1.145

0.012

0.033

0.070

0.043

0.000

0.002

0.012

30.400

0.347

0.491

0.415

0.081

–2.571

–2.478

–2.932

0.935

0.010

0.013

0.003

0.01

0.1
Thymoma A

1

10

100

Thymoma B2

Meta Analysis

types  B2.  The  software  estimates

Fig. 15.9  The forest plots of the meta-analysis of patients
with stage III thymomas with histologic type A vs. histo-
logic
the  odds
(odds = probability/1 − probability)  for  each  outcome,
weighs  the  data  from  various  institutions  according  to
their  cohort  size,  estimates  the  overall  OR  for  all  cases,

and  calculates  p  values.  The  vertical  lines  represent  the
levels of OR. The small squares represent the results of
each study. The size of each square is proportional to the
weight  assigned  to  the  results  of  each  study.  The  rhom-
boid represents the results of the overall data. The hori-
zontal lines represent the 95% CI of each study

References

  1.  Egger  M,  Smith  GD,  Phillips  AN.  Meta-analysis:
principles  and  procedures.  BMJ.  1997;315(7121):
1533–7.

  2.  Antman  EM,  Lau  J,  Kupelnick  B,  Mosteller  F,
Chalmers TC. A comparison of results of meta-analyses
of randomized control trials and recommendations of
clinical experts. Treatments for myocardial infarction.
JAMA. 1992;268(2):240–8.

  3.  L’Abbe KA, Detsky AS, O’Rourke K. Meta-analysis
in  clinical  research.  Ann  Intern  Med.  1987;107(2):
224–33.

  4.  Jenicek M. Meta-analysis in medicine. Where we are
and  where  we  want  to  go.  J  Clin  Epidemiol.
1989;42(1):35–44.

  5.  Goodman SN. Have you ever meta-analysis you didn’t

like? Ann Intern Med. 1991;114(3):244–6.

  6.  Steinberg KE. Cookbook medicine: recipe for disas-

ter? J Am Med Dir Assoc. 2006;7(7):470–2.

  7.  Guerette PH. Managed care: cookbook medicine, or
quality,  cost-effective  care?  Can  Nurse.  1995;
91(7):16.

  8.  Holm  RP.  Cookbook  medicine.  S  D  Med.  2009;

62(9):371.

  9. Leppaniemi A. From eminence-based to error-based to
evidence-based surgery. Scand J Surg. 2008;97(1):2–3.
 10. Crawford  JM.  Original  research  in  pathology:  judg-
ment,  or  evidence-based  medicine?  Lab  Invest.
2007;87(2):104–14.

 11. Gupta  R,  Dastane  AM,  McKenna  Jr  R,  Marchevsky
AM.  The  predictive  value  of  epidermal  growth
 factor  receptor  tests  in  patients  with  pulmonary
 adenocarcinoma:  review  of  current  “best  evidence”
with meta-analysis. Hum Pathol. 2009;40(3):356–65.

 12. Gupta  R,  Marchevsky  AM,  McKenna  RJ,  et  al.
Evidence-based pathology and the pathologic evalua-
tion of thymomas: transcapsular invasion is not a sig-
nificant  prognostic  feature.  Arch  Pathol  Lab  Med.
2008;132(6):926–30.

 13. Marchevsky  AM,  Gupta  R,  McKenna  RJ,  et  al.
Evidence-based  pathology  and  the  pathologic  evalua-
tion of thymomas: the World Health Organization clas-
sification can be simplified into only 3 categories other
than thymic carcinoma. Cancer. 2008;112(12):2780–8.

 14. Faraji  H,  Nguyen  BN,  Mai  KT.  Renal  epithelioid
angiomyolipoma:  a  study  of  six  cases  and  a  meta-
analytic study. Development of criteria for screening
the entity with prognostic significance. Histopathology.
2009;55(5):525–34.

 15.  Anderson GG, Weiss LM. Determining tissue of ori-
gin for metastatic cancers: meta-analysis and literature
review  of  immunohistochemistry  performance.  Appl
Immunohistochem Mol Morphol. 2010;18(1):3–8.
 16. Gould Rothberg BE, Bracken MB, Rimm DL. Tissue
biomarkers  for  prognosis  in  cutaneous  melanoma:  a
systematic  review  and  meta-analysis.  J  Natl  Cancer
Inst. 2009;101(7):452–74.

 17. Alonso  SR,  Ortiz  P,  Pollan  M,  et  al.  Progression  in
cutaneous  malignant  melanoma  is  associated  with
distinct expression profiles: a tissue microarray-based
study. Am J Pathol. 2004;164(1):193–203.

 18. Niezabitowski A, Czajecki K, Rys J, et al. Prognostic
evaluation of cutaneous malignant melanoma: a clini-
copathologic and immunohistochemical study. J Surg
Oncol. 1999;70(3):150–60.

 19. Straume  O,  Sviland  L,  Akslen  LA.  Loss  of  nuclear
p16  protein  expression  correlates  with  increased
tumor cell proliferation (Ki-67) and poor prognosis in
patients  with  vertical  growth  phase  melanoma.  Clin
Cancer Res. 2000;6(5):1845–53.

15  Meta-Analysis 101 for Pathologists

259

 20.  Florenes VA, Maelandsmo GM, Faye R, Nesland JM,
Holm R. Cyclin A expression in superficial spreading
malignant  melanomas  correlates  with  clinical  out-
come. J Pathol. 2001;195(5):530–6.

 21. Dziadziuszko  R,  Witta  SE,  Cappuzzo  F,  et  al.
Epidermal  growth  factor  receptor  messenger  RNA
expression,  gene  dosage,  and  gefitinib  sensitivity  in
non-small  cell  lung  cancer.  Clin  Cancer  Res.  2006;
12(10):3078–84.

 22. Pugh  TJ,  Bebb  G,  Barclay  L,  et  al.  Correlations  of
EGFR  mutations  and  increases  in  EGFR  and  HER2
copy  number  to  gefitinib  response  in  a  retrospective
analysis  of  lung  cancer  patients.  BMC  Cancer.
2007;7:128.

 23. Travis WD, Brambilla E, Muller-Hermelink HK, et al.
Tumors of the lung, pleura, thymus and heart. Lyon:
IARC Press; 2004.

 24. Kim  DJ,  Yang  WI,  Choi  SS,  Kim  KD,  Chung  KY.
Prognostic and clinical relevance of the World Health
Organization schema for the classification of thymic
epithelial  tumors:  a  clinicopathologic  study  of  108
patients  and  literature  review.  Chest.  2005;127(3):
755–61.

 25. Bedini  AV,  Andreani  SM,  Tavecchio  L,  et  al.
Proposal of a novel system for the staging of thymic
epithelial  tumors.  Ann  Thorac  Surg.  2005;80(6):
1994–2000.

 26. Sonobe S, Miyamoto H, Izumi H, et al. Clinical useful-
ness of the WHO histological classification of thymoma.
Ann Thorac Cardiovasc Surg. 2005;11:367–73.

 27. Rena  O,  Papalia  E,  Maggi  G,  et  al.  World  Health
Organization histologic classification: an independent
prognostic factor in resected thymomas. Lung Cancer.
2005;50:59–66.

 28. Wright CD, Wain JC, Wong DR, et al. Predictors of
recurrence in thymic tumors: importance of invasion,
World  Health  Organization  histology,  and  size.
J Thorac Cardiovasc Surg. 2005;130:1413–21.

 29. Kondo K, Yoshizawa K, Tsuyuguchi M, et al. WHO
histologic  classification  is  a  prognostic  indicator  in
thymoma. Ann Thorac Surg. 2004;77:1183–8.

 30. Strobel  P,  Marx  A,  Zettl  A,  Muller-Hermelink  HK.
Thymoma  and  thymic  carcinoma:  an  update  of  the
WHO  Classification  2004.  Surg  Today.  2005;35:
805–11.

 31. Park MS, Chung KY, Kim KD, et al. Prognosis of thy-
mic  epithelial  tumors  according  to  the  new  World

Health  Organization  histologic  classification.  Ann
Thorac Surg. 2004;78:992–97; discussion 997–8.
 32. Rea F, Marulli G, Girardi R, et al. Long-term survival
and  prognostic  factors  in  thymic  epithelial  tumours.
Eur J Cardiothorac Surg. 2004;26:412–8.

 33. Singhal S, Shrager JB, Rosenthal DI, et al. Comparison
of stages I-II thymoma treated by complete resection
with or without adjuvant radiation. Ann Thorac Surg.
2003;76:1635–41; discussion 1641–1632.

 34. Nakagawa K, Asamura H, Matsuno Y, et al. Thymoma:
a  clinicopathologic  study  based  on  the  new  World
Health Organization classification. J Thorac Cardiovasc
Surg. 2003;126:1134–40.

 35. Chen G, Marx A, Wen-Hu C, et al. New WHO histo-
logic classification predicts prognosis of thymic epithe-
lial tumors: a clinicopathologic study of 200 thymoma
cases from China. Cancer. 2002;95:420–9.

 36. Okumura M, Ohta M, Tateyama H, et al. The World
Health  Organization  histologic  classification  system
reflects the oncologic behavior of thymoma: a clinical
study of 273 patients. Cancer. 2002;94:624–32.

 37. Rieker RJ, Hoegel J, Morresi-Hauf A, et al. Histologic
classification of thymic epithelial tumors: comparison
of  established  classification  schemes.  Int  J  Cancer.
2002;98:900–6.

 38. Chalabreysse L, Roy P, Cordier JF, et al. Correlation of
the WHO schema for the classification of thymic epi-
thelial neoplasms with prognosis: a retrospective study
of 90 tumors. Am J Surg Pathol. 2002;26:1605–11.
 39. Dickersin  K.  The  existence  of  publication  bias  and
risk factors for its occurrence. JAMA. 1990;263(10):
1385–9.

 40.  Marchevsky AM, Gupta R, Kusuanco D, Mirocha J,
McKenna RJ. The presence of isolated tumor cells and
micrometastases  in  the  intrathoracic  lymph  nodes  of
patients with lung carcinomas is not associated with
decreased survival. Hum Pathol. 2010;41:1536–43.
 41. Marchevsky AM, Qiao JH, Krajisnik S, Mirocha JM,
McKenna  RJ.  The  prognostic  significance  of
 intranodal  isolated  tumor  cells  and  micrometastases
in patients with non-small cell carcinoma of the lung.
J Thorac Cardiovasc Surg. 2003;126(2):551–7.

 42.  Marchevsky  AM,  Gupta  R,  Casadio  C,  et  al.  World
Health Organization classification of thymomas provides
significant prognostic information for selected stage III
patients: evidence from an International Thymoma Study
Group. Hum Pathol. 2010;41: 1413–21.

Evidence-Based Practices in Applied
Immunohistochemistry: Dilemmas
Caused by Cross-Purposes

16

Mark R. Wick, Paul E. Swanson, and Alberto M. Marchevsky

Keywords
Evidence-based  pathology  •  Immunohistochemistry  •  Evidence-based
immunohistochemistry  •  Prognostic-predictive  immunohistochemistry
• Diagnostic immunohistochemistry

Immunohistochemistry  (IHC)  has  existed  as  an
area of scientific inquiry for 70 years, and it truly
has changed the way in which anatomic pathology
is  practiced  during  that  span  of  time  [1,  2].
Conventional  histochemistry  antedated  IHC  by
almost a century, and had itself been a huge tech-
nological breakthrough. However, until the avail-
ability  of  IHC,  scientists  and  physicians  were
limited in their ability to identify cellular products
in situ, as histochemical methods were limited in
their  capacity  to  identify  many  cellular  products
that may have diagnostic, prognostic, or predictive
value in the practice of Medicine [3–6]. The same
comment can be made for another adjunctive his-
tomorphological procedure, transmission electron
microscopy (TEM) [7–10], which had been intro-
duced by Ernst Ruska – a physicist – in 1931 [11].
Surprisingly, and rather inexplicably as there is
little  doubt  that  TEM  significantly  extended  the
analytic  potentials  of  light  microscopy  and  his-
tochemistry, both TEM and IHC were only slowly
integrated into the clinical (hospital-based) prac-
tice of pathology. Indeed, ultrastructural analysis

M.R. Wick (*)
Department of Pathology, University of Virginia Medical
School, Charlottesville, VA, USA
e-mail: mrw9c@virginia.edu

was still being “introduced” as a useful procedure
for patient care 50 years after its inception [8], and
IHC did not enjoy widespread interest or applica-
tion by practitioners until around 1980 [2].

Perhaps  because  of  this  protracted  evolution,
little attention was given, until relatively recently,
to  the  role  of  quality  assurance  (QA)  in  either
TEM  or  diagnostic  IHC  (DIHC).  In  particular,
pathologists and other physicians especially tended
to  have  a  naïve  expectation  that  immunostains
were  merely  formulaic  –  in  other  words,  if  one
used appropriate reagents and followed prescribed
procedural steps, an optimal result was expected
to obtain. That attitude likely derived from experi-
ence  with  histochemistry,  where  such  provisions
would typically produce the expected outcome. In
addition, there is limited consensus about how to
use IHC tests for the work-up of various clinico-
pathologic entities. Different investigators propose
the use of various IHC tests based on the results of
selected studies, and there are few expert- consen-
suses or evidence-based guidelines to guide prac-
titioners during the selection of the antibodies that
should  be  tested  during  the  work-up  of  specific
differential  diagnoses.  There  is  also  a  lack  of
guidelines suggesting how to interpret the results,
particularly when there is some overlap in  findings,
as IHC results are often not included as diagnostic

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_16, © Springer Science+Business Media, LLC 2011

261

262

M.R. Wick et al.

criteria  in  the  various  classification  schema
 proposed  by  widely  respected  groups  of  experts
selected by the World Health Organization (WHO)
and other professional groups.

As we shall consider shortly, DIHC is anything
but  mechanical.  Many  biological  and  chemical
factors  have  a  meaningful  impact  on  the  final
results of this method, and these must be addressed
individually wherever possible. The topic of how
to select the most effective antibodies that need to
be tested for various differential diagnoses, prob-
lems  related  to  the  over  utilization  of  IHC,  and
the fervent but misdirected hope that this method-
ology can be employed in a nondiagnostic setting
to provide prognostic and predictive data will be
discussed. Finally, technical alternatives to immu-
nohistologic evaluation will be summarized.

Diagnostic Immunohistochemistry:
An Historical Perspective

The concept of adding a detectable chemical “tag”
to target-specific reagent antibodies seems today to
be  a  straightforward,  if  not  simple,  idea.  Never-
theless,  such  a  conclusion  is  purely  contextual.
In 1940, the structure of antibodies was only rudi-
mentarily understood, and the notion of attaching a
visible chromophore to them was completely novel.
A 27-year-old medical resident from Boston, MA –
Dr. Albert Hewett Coons (Fig. 16.1) – developed
the idea while on vacation in Europe [12]. At least
one German colleague – Dr. Kurt Apitz of Charite’
Hospital  in  Berlin  –  thought  little  of  it,  for  good
reasons  [12].  The  necessary  process  of  joining
chemicals to antibodies had never been attempted,
and  synthesis  of  the  chemical  “tags”  that  Coons
had in mind – fluorescent molecules – also was a
fledgling  area.  Finally,  no  microscope  capable  of
visualizing fluorophores then existed.

Undeterred, Coons returned to Boston to work
out each of these problems. By 1941, he and his
colleagues had demonstrated not only the feasi-
bility  but  also  the  applicability  of  fluorescent
immunohistology for the localization of particu-
lar  protein  targets  in  human  tissues  [13,  14]
(Fig. 16.2). That development launched the entire
scientific discipline of IHC and earned Coons the
prestigious Albert Lasker Award in 1959 [15].

Fig. 16.1 ( a)  Albert  Hewett  Coons,  M.D.  (1912–1978),
the  originator  of  immunohistochemistry.  Dr.  Coons  was
given the Lasker Award in 1959 for that contribution. (b) In
direct immunofluorescence methods, as devised by Coons,
a  fluorophore  is  attached  to  a  reagent  antibody,  which  is
specific for a polypeptide epitope in substrate tissue

Fig. 16.2  Immunofluorescence  study  for  alpha-methyl-
acyl-CoA-racemase,  seen  as  a  red-orange  signal  in  this
section of prostatic adenocarcinoma

16  Evidence-Based Practices in Applied Immunohistochemistry

263

Probably  because  immunofluorescence  mic-
roscopy  does  not  allow  for  a  simultaneous
appreciation of morphological detail, the proce-
dure  did  not  enjoy  widespread  clinical  use  and
was primarily regarded as a research tool. There
were exceptions to that statement, however, prin-
cipally represented by the diagnostic use of fluo-
rophores in renal pathology and dermatopathology
[16–19].  Fundamentally,  and  especially  in  the
practice of hospital-based pathology, an expanded
application of DIHC depended on further devel-
opment of visible  chemical “partners” for reagent
antibodies.

The next step in this evolution was taken in the
area  of  TEM,  where  it  was  realized  that  certain
electron-dense  (and  therefore  visible)  chemical
moieties – such as ferritin, osmium, and gold salts
– could be bound directly to reagent antibodies, as
fluorescein  isocyanate  had  been  [20–23].  Hence,
those reagents provided another method for local-
izing protein targets in substrate tissues, but at an
ultrastructural  level.  An  additional  development
involved the use of a gold-protein-A adduct as an
indicator in TEM, preceded by incubation of target
tissues with unlabeled reagent antibody (n.b.: pro-
tein-A is a proteinaceous product of Staphylococcus
aureus, and is capable of binding to the Fc portion
of  all  immunoglobulins)  [24].  Once  again,  how-
ever, ultrastructural IHC was impractical for most
practicing anatomic pathologists, because they did
not  have  access  to  electron  microscopes  and  the
technique in question was quite tedious.

Finally,  in  the  late  1960s,  Ludwig  Sternberger
(Fig. 16.3) and colleagues developed an effective
immunohistological procedure that could be used
with formalin-fixed, paraffinized tissue sections and
the light microscope [25, 26]. Three molecules of
horseradish peroxidase were complexed with two
antiperoxidase  antibodies  by  precipitation  from  a
mixture of enzyme and crude serum. This pentag-
onal  structure,  dubbed  peroxidase-antiperoxidase

Fig.  16.3  Ludwig  Sternberger,  M.D.,  who,  along  with
colleagues,  devised  the  first  practical  light  microscopic
technique  in  immunohistochemistry  in  the  late  1960s  –
the peroxidase-antiperoxidase method

(PAP) complex by Sternberger, had the intriguing
attribute  of  being  thermodynamically  stable  inde-
pendent of antiperoxidase serum quality or affinity
(and thus easy and cheap to prepare). The utility of
PAP as a delivery vehicle for the reporter enzyme
was also independent of affinity, since linkage of
PAP  to  a  specific   tissue-bound  reagent  antibody
depended  solely  on  the  affinity  of  the  secondary
(bridge) antibody for the free Fc fragments of PAP
and  the  primary  reagent  (Fig.  16.4).  Antibodies
comprising  the  PAP  complex  were  raised  in  the
same animal hosts as those which produced the pri-
mary  reagent  antibodies,  whereas  the  secondary
“bridge”  antibody  derived  from  another  species.
The  most  common  early  example  of  such  a  con-
struct was a rabbit primary antibody-sheep antirab-
bit “bridge” antibody-rabbit PAP complex [26].

Peroxidases  are  redox  enzymes  that  catalyze
reactions between electron donors and recipients,
according to the following equations:

ROOR electron donor (2e ) 2H

¢ +

+

-

+

®

ROH R OH,
+ ¢

or

Acceptor H O

+

2

®

2

oxidized acceptor H O.

+

2

264

M.R. Wick et al.

Fig.  16.4  In  the  peroxidase-antiperoxidase  method,  an
unlabeled  reagent  primary  antibody  is  linked  to  a  tertiary
reporter complex comprising two antibodies and several per-
oxidase molecules. The first and third antibodies are raised

in  the  same  animal  species,  whereas  the  second  “bridge”
antibody is a generic reagent raised against  animal immuno-
globulins  representing  the  primary  reagent  and  PAP  com-
plex (e.g., rabbit primary – sheep antirabbit – rabbit PAP)

mark  the  sites  of  specific  primary  antibody
 binding, where target proteins reside in the tissue
(Fig. 16.5).  That  construct  is  responsible  for  the
slang term “brown stains,” in reference to DIHC.

Finally, pathologists and other scientists had a
practical, relatively rapid (24 h), and ecumenical
alternative  technique  to  immunofluorescence
microscopy that could be used in everyday prac-
tice. It seemingly remained only for an increasing
number of specific primary antibodies to be raised
and marketed, before the entire panoply of human
proteins could be localized in tissue sections.

Realities and limitations of the PAP technique
soon lessened that grand expectation. It became
evident that some proteinaceous targets existed in
only low densities in various tissues. Moreover,
the  standard  process  of  formalin-fixation  and
paraffin-embedding appeared to denature, mask,
or  cross-link  some  proteins  in  such  a  way  that
primary  antibodies  could  not  bind  to  them
[27, 28]. Depending upon the particulars of tissue
procurement and processing, PAP stains for any
given  target  in  any  given  specimen  might  be
strongly  reactive,  weakly  positive,  or  altogether
negative, in an unpredictable way.

Fig.  16.5  Dense  brown-black  precipitates  of  diamin-
obenzidine-HCl are seen at sites in formalin-fixed human
tissue where anti-human keratin primary antibodies have
bound  in  a  paraffin  section.  The  peroxidase-antiperoxi-
dase method was employed as the detection system

In the classical PAP technique, hydrogen per-
oxide  (H2O2)  is  used  as  the  electron  donor,  and
3-3¢-diaminobenzidine tetrahydrochloride – which
forms a colored precipitate when oxidized – is the
electron recipient. The final result is a light-micro-
scopic  preparation  in  which  brown-black  labels

16  Evidence-Based Practices in Applied Immunohistochemistry

265

This  chain  of  events  brings  us  to  a  crucial
watershed  in  the  development  of  DIHC  as  a
method,  and  philosophies  about  how  the  tech-
nique should be used. For practicing pathologists,
the  aim  of  immunohistology  was,  and  is,  to
 visualize  molecular  constituents  of  tissue  that
have  diagnostic  importance.  Inexplicable  vari-
ability  in  staining  intensity,  as  mentioned  in  the
previous  paragraph,  threatens  that  goal  and  was
quickly recognized as a serious potential source
of interpretative error. For example, if S100 pro-
tein were to be found in a metastatic undifferenti-
ated  large-cell  malignancy,  in  the  absence  of
keratin, the probable diagnosis would be one of
metastatic  melanoma.  However,  keratin  might
actually be present in the tumor cells but missed
because of technical problems in tissue process-
ing or immunohistochemical procedure. Keratin-
positive, S100-positive neoplasms are represented
by carcinomas that originate in selected sites [29].
Therefore, failure to detect keratin – or other sim-
ilarly dispositive markers – in such lesions would
produce a significant mistake in the generation of
categorical  data,  an  issue  related  ultimately  to
poor method sensitivity. On the other hand, given
that  small  quantities  of  a  given  marker  of  diag-
nostic importance might be expressed in diagnos-
tically problematic settings (keratins in melanoma,
to extend the argument), there also needs to be an
appreciation  for  the  relationship  between  high
method sensitivity and errors in categorical data.

With this in mind, it is important to realize that
quantification of results in DIHC is meaningful only
in a binary context – i.e., immunostains are ideally
either positive or negative. As a consequence of that
premise, a cardinal objective for diagnostic patholo-
gists became the maximization of specific immuno-
labeling, through a variety of methods, while at the
same time maintaining minimal background “noise”
in  IHC  preparations  [30].  To  a  large  extent,  that
intent and practice remain in place today.

In line with our earlier comment that masking,
degradation,  or  cross-linking  of  available  epitopes
may  occur  in  formalin-fixed  tissue,  compensatory
efforts  at  signal  amplification  were  two-pronged.
One mode of attack on the problem was to devise
ever more sensitive IHC techniques, in the hope of
recognizing low levels of an available protein target.
The best known of higher-sensitivity alternatives to

Fig.  16.6  Su-Ming  Hsu,  M.D.,  Ph.D.,  the  principal
developer of the avidin-biotin-peroxidase complex (ABC)
procedure in immunohistochemistry

Fig. 16.7  The avidin-biotin-peroxidase complex method
offers a high level of sensitivity because of the multimeric
nature of the peroxidase-bearing “reporter” molecule

the PAP procedure was developed in the late 1970s
by Hsu (Fig. 16.6) and colleagues [31–33]; namely,
the avidin-biotin-peroxidase complex (ABC) proce-
dure. That innovative  technique  capitalized on the
ability to attach biotin molecules to secondary anti-
bodies, and also the capacity to build large reporter
complexes which include avidin, biotin, and horse-
radish  peroxidase.  The  latter  composites  can  be
attached  to  the  biotinylated  secondary  antibody,
which is, in turn, bound to the Fc portion of a  specific
primary reagent antibody. The result compounds the
number of peroxidase molecules that are associated
with any one protein target, far beyond the biochem-
ical  capability  of  the  PAP  technique  (Fig.  16.7).

266

M.R. Wick et al.

[40,  41]  –  the  same  procedure  was  applied  to
paraffin sections in DIHC in the late 1970s. Pepsin,
trypsin,  proteinase-3,  ficin,  pronase,  papain,  and
bromelain were, and still are, employed in this set-
ting [42–47]. Predictably, the results demonstrated
that different enzymes affected various targets dif-
ferently.  In  other  words,  one  catalyst  might
enhance  immunoreactivity  for  protein  “A”  but
decrease  labeling  for  protein  “B.”  Another  off-
shoot of this work was the realization that certain
classes  of  tissue  constituents  were  routinely
masked by formalin fixation; a prime example is
represented by the intermediate-filament proteins,
including  keratin,  vimentin,  desmin,  neurofila-
ment, and glial fibrillary acidic protein [28]. Those

Therefore, an amplification of the immunostaining
signal is the predictable outcome.

Later variations on that theme included labeled
streptavidin-biotin-peroxidase  (LSAB),  ABPAP
(serially  combined  PAP  and  ABC  procedures),
and  alkaline  phosphatase-antialkaline  phos-
phatase  (APAAP)  methods  [34–38],  and,  more
recently, a paradigm in which approximately 20
secondary antibodies from more than one animal
source  are  attached  polymerically  to  a  dextran
backbone that also carries >100 peroxidase mol-
ecules [37, 39] (Fig. 16.8). That approach, called
dextran-polymer-based  (EnvisionR),  and  other
proprietary formulations, IHC obviates the need
for  separate  labeled  secondary  antibodies  from
differing  animals.  At  the  same  time,  it  greatly
increases final immunostaining intensity in most
applications.

All of those approaches for signal maximiza-
tion center on the notion of increasing the num-
bers of signal molecules that are bound to a target
protein in tissue. They are all effective in visual-
izing  low  densities  of  antigens  whose  epitopes
are still at least partially open to bind to primary
antibodies.  However,  what  could  be  done  about
desired  targets  with  completely  “masked”  or
cross-linked epitopes?

Trading, perhaps, on pathologists’ experiences
in immunohematology – where it has been known
for  decades  that  controlled  enzymatic  digestion
could  unmask  certain  antigens  on  erythrocytes

Fig. 16.8  In the dextran-polymer-based system of immu-
nohistochemistry,  several  secondary  “link”  antibodies,
from different animal hosts, are bound to a dextran carrier
that also carries multiple peroxidase molecules

Fig. 16.9 ( a) Immunostaining of a formalin-fixed, poorly
differentiated,  prostatic  adenocarcinoma  for  pankeratin,
with no epitope-retrieval techniques. There is no discern-
ible  reactivity.  (b)  Prior  treatment  of  the  sections  with
ficin “unmasks” the target antigen and allows the antikera-
tin antibody to bind

16  Evidence-Based Practices in Applied Immunohistochemistry

267

But, what, exactly, is being “undone” by prote-
olysis  or  HIER?  To  this  day,  the  answer  to  that
question  is  still  vague.  Several  hypotheses  have
been advanced to account for epitope unmasking.
These  include  the  breakage  of  fixation-induced
coupling  of  “irrelevant”  but  sterically  interfering
large proteins to peptide epitopes; the abrogation
of electrostatic, van der Waal-like charges between
epitopes and Fab fragments of reagent antibodies;
dissolution of cagelike calcium complexes around
epitope  sequences;  and  a  reversal  of  Mannich
reactions between proteins [56–59]. The latter are
organic  reactions  featuring  the  amino-alkylation
of acidic protons, placed next to carbonyl groups
during  formaldehyde  fixation  [58].  On  the  other
hand, there is an increasing understanding that the
tertiary  structure  of  the  target  epitope  (liner  vs.
conformational) may greatly influence the ability
of retrieval methods to improve immunoreactivity
in routinely processed materials.

Other tissue fixatives have been evaluated as
alternatives  to  formalin,  including  solutions
based on methyl alcohol, ethyl alcohol, acetone,
or combinations thereof [60–65]. These are either
expensive or unwieldy for use in routine hospital
pathology;  moreover,  they  produce  their  own
peculiar  alterations  in  epitope  preservation  and
are not necessarily any “kinder” to certain target
proteins than formalin.

All of these considerations may seem arcane
in regard to the standardization of immunohistol-
ogy. However, they are, in fact, key preanalytical
and intra-analytical elements that affect the latter
process. It is not possible to obtain perfect con-
trol of the concentrations and pH of fixatives and
buffers, the duration of fixation, the dimensions
of tissue blocks and histologic sections used for
IHC,  the  biological  activities  of  various  prote-
olytic  enzymes,  and  the  nature  of  heat  distribu-
tion during HIER procedures. We do not mean to
say that pathologists must not try to accomplish
that task, but a more realistic goal is to aim for an
end  result  of  consistent  and  functionally  binary
(positive or negative) results in DIHC. Undeniably,
an  element  of  artificiality  accompanies  that
approach,  because  one  externally  controls  the
ranges of reactivity in any immunostaining pro-
cedure through the subjective process of antibody

Fig. 16.10 ( a) Immunostaining of a paraffin section for
PAX2, a nuclear marker, in renal cell carcinoma with no
epitope retrieval. (b) Heat-induced antigen retrieval with
citrate buffer allows for antibody recognition of the target

markers  can  only  be  visualized  optimally  using
some  type  of  unmasking  procedure  (Fig.  16.9).
The same statement applies to virtually all intra-
nuclear proteins [48] (Fig. 16.10).

The next major advance in this area of DIHC
occurred in the early 1990s. Empirical experience
showed that the controlled heating of paraffin sec-
tions, when immersed in ionic solutions in a micro-
wave oven or a steamer, could accomplish the same
results as proteolytic unmasking methods [49–55].
Thus,  the  term  “heat-induced  epitope  retrieval”
(HIER)  was  coined.  Today,  this  process  is  a  de
rigueur element of practical immunohistology. In
similarity to enzymatic digestion, HIER augments
the intensity of immunolabeling for some markers
and  decreases  it  for  others,  vis-à-vis  IHC  proce-
dures that omit an unmasking step [55].

268

M.R. Wick et al.

titration against target tissues and, by extension,
specific  target  diagnoses.  Nevertheless,  as  long
as  technical  parameters  are  maintained  within
narrow confines, even an artificial system can be
effectively  used  diagnostically.  The  Canadian
Association  of  Pathologists  has  recently  pub-
lished  a  set  of  guidelines  that  are  useful  in  this
specific context [66]. They also include a discus-
sion of proper “positive” and “negative” controls
in DIHC, as well as a consideration of cross-val-
idating  techniques  (see  http://ajcp.ascpjournals.
org/content/133/3/354.full). Furthermore, a series
of  other  papers,  written  over  a  20-year  period,
has also outlined methods for QA in diagnostic
immunohistology [67–73].

Specific Methods for Quality Control
in DIHC

“Validation” and “verification” are terms that are
often used in reference to DIHC. In a pure sense,
the first of them – validation – refers to the pro-
cess  of  testing  putatively  reactive  and  nonreac-
tive tissues for the target antigen, to document the
absence  of  false-positive  and  false-negative
results. The second, verification, relates to proper
performance  of  an  immunohistochemical  assay
in a specific setting – e.g., in paraffin sections as
opposed to frozen tissue [66].

There are alternative meanings to one of these
two terms that are also appropriate. Chronological
validation of immunostains can and should be done
over  time  in  any  given  laboratory;  here,  known
positive and negative test cases are studied over and
the  consistency  of  results.
to  monitor
over
Procedural  validation
(or  “cross”-validation)
implies that a “positive” immunostain is confirmed
by data generated through another testing method.
A  representative  example  is  electron  microscopic
evidence  of  epithelial  differentiation  in  the  same
tissue  sample  that  showed  immunoreactivity  for
epithelial  markers.  Extramural  validation  applies
when tissue samples show the same immunoreac-
tivity patterns in at least two laboratories, with one
acting as a reference [67]. All three of these proce-
dures should be a part of QA measures in any DIHC
laboratory. If vendors or lots are changed at some
point for a particular antibody reagent, or there is

an alteration in procedural platforms (e.g., manual
vs.  automated  staining),  internal  QA  assessments
should reflect those facts. It is  particularly  important
to give an extramural reference laboratory a detailed
description of one’s own IHC methods, so that its
personnel can apply the same procedures to moni-
tor reproducibility of results.

Unfortunately,  because  these  techniques  take
time, effort, and money to do regularly, many hos-
pital  laboratories  have  abridged  or  ignored  them.
Nevertheless, that is a prescription for performance
problems  over  time.  The  Canadian  experience  is
relevant here. After well-publicized problems with
intra-  and  inter-laboratory  reproducibility  for
selected biomarkers, provincial efforts to standard-
ize  laboratory  practice  through  guidelines  and
through  centralized  external  QA  have  prompted
considerable  attention  to  test   validation/verifica-
tion and significant improvement in lab-to-lab con-
cordance. These processes, now being centralized
under a Canadian Association of Pathologists ini-
tiative called cIQc (www.ciqc.ca), have the poten-
tial  to  provide  realtime  feedback  to  participating
laboratories regarding best practices in tissue prep-
aration,  methodology  (and  method  platforms),
controls,  reagent  selection,  and  interpretation.
Similar programs exist in the UK as well, and per-
haps the most robust external QA program avail-
able  to  pathology  labs  worldwide  is  NordiQC
(www.nordiqc.org). By contrast, centralized exter-
nal QC in the United States has its only meaningful
expression  in  College  of  American  Pathologists
IHC  surveys,  and  despite  well  over  a  decade  of
experience, this program provides little meaningful
feedback to participating labs. More recent atten-
tion on test validation has prompted the develop-
ment of recommended guidelines for performance
of selected biomarkers (*Her2/neu, estrogen recep-
tor  protein  [ERP]  and  progesterone  receptor  pro-
tein  [PRP])  and  more  specific  guidelines  for  test
validation,  but  these  guidelines  lack  a  clear  evi-
dence-based argument for many of the core recom-
mendations. Perhaps the requirement of initial and
ongoing  test  validation  for  these  biomarkers  will
focus laboratory attention on the value of external
and internal QA, but at the time of this writing, it is
likely that fewer than half of American laboratories
have meaningful validation procedures in place for
most immunohistochemical tests.

16  Evidence-Based Practices in Applied Immunohistochemistry

269

Published Literature on DIHC: How
Should It Be Used?

Medical  publications  concerning  IHC  span  at
least 40 years, with many variations in technique
as  well  as  results.  Even  today,  some  readers  of
the  literature  miss  the  fact  that  differences  in
reagents and protocols will have potentially strik-
ing  influences  on  the  final  staining  “product.”
This is due, in part, to a lack of complete method-
ologic  information  in  some  reports,  making
selected  elements  of  the  published  literature
unreproducible. Recent consensus work on IHC,
fluorescent in situ hybridization, and other tech-
niques may help mitigate this shortcoming.

Even so, in our current working environment,
if  one  wishes  to  achieve  a  reliable,  workable
structure  of  diagnostic  immunohistology  in  any
given laboratory, one of two requirements must
be met. The first, and the most onerous, demands
that  extensive  “catalog”  testing  be  done  with
each  antibody  one  wishes  to  use,  probably  by
analysis  of  tissue  “microarrays”  (Fig.  16.11).
Parenthetically,  “microarray”  is  simply  a  new

Fig.  16.11  A  tissue  microarray,  comprising  many  sam-
ples of neoplastic tissue from a  variety  of  human  tumor
types,  stained  with  hematoxylin  and  eosin  (top)  and  an
antibody to pankeratin (bottom)

name for an old concept  introduced by Dr. Hector
Battifora, who described the use of  “multitumor
[‘sausage’] tissue blocks” 25 years ago [74]. In
this approach, one records the immunoreactivity
of  each  particular  antibody,  with  a  particular
staining  platform,  against  many  examples  of
many tumor types, generating a statistical matrix
to  be  used  in  interpretation  [75–77].  The  time
and resources necessary to accomplish this task
are  daunting.  Alternatively,  and  more  expedi-
tiously,  one  can  use  the  literature  to  see  which
reagents and procedures have been used by refer-
ence  IHC  laboratories  that  have  published  their
results  widely.  Those  reagents  and  procedures
can then be replicated – exactly – with the expec-
tation  that  the  results  of  the  external   laboratory
will be mirrored in one’s own experience.

The principal error that enters into this process
is focused on attempts to compare things that are
incomparable.  If  reagents  or  procedures  are  not
the same in laboratory A as in laboratory B, their
results will not be parallel over time [78].

Does DIHC Conform to the Principles
of Evidence-Based Medicine?

The  foregoing  discussion  begs  a  question  –  is
DIHC a truly evidence-based area of pathology?
The response is necessarily equivocal, depending
on  supporting  information.  If  one  accepts  the
premise that DIHC is best used as a binary, some-
what artificial but reproducible practical tool, the
reply  is  a  qualified  “yes.”  However,  even  that
response  has  a  major  caveat,  focused  on  the
unswerving need to copy and control the reagents
and  procedures  used  by  investigators  with  wide
and published experiences. And, in this context,
the  literature  (the  evidentiary  basis  for  DIHC)
may  unintentionally  mislead  those  who  consult
uncensored studies, data sets, or meta-analyses of
existing  information.  Indeed,  without  a  clear
understanding  of  the  shortcomings  of  selected
studies  (aggravated  by  the  lack  of  complete
 methodologic detail noted earlier), a casual reader
of  the  literature  might  conclude  that  few,  if
any,  markers  of  putative  diagnostic  interest  are
 reliable.  At  issue  is  the  value  of  the  extant

270

M.R. Wick et al.

 literature taken as a whole. It does not escape our
attention that a dichotomous (and diagnostically
useful) stain result, when based on reliable litera-
ture sources or external QA, will assume a more
continuous (and less useful) expression across a
targeted  differential  diagnosis  when  uncensored
literature is employed. But the pathologist is not
merely  responsible  for  being  able  to  access  the
most reliable evidence for a given test. As one of
us has stated in an earlier communication:

Surgical  pathologists  and  cytopathologists  must,
by  consensus  or  by  mandate,  use  only  validated
 standardized  methods  for  DIHC,  to  the  absolute
exclusion of others. This statement seems simple,
but  it  is  far  from  that.  Many  variables,  including
the  size  and  thickness  of  tissue  blocks  used  for
immunohistology,  the  nature  and  length  of  fixa-
tion, methods used for epitope ‘retrieval,’ antibody
binding-detection technique, choice of chromoge-
nic substrate, and the use of intensifiers of immu-
noprecipitation, are all included under the rubric of
immunohistochemical ‘method’ [71].

In referring to the flow diagram of evidence-
based  medicine  (EBM)  devised  by  Friedland

et al. [79] (Fig. 16.12), one finds additional points
of potential departure between the fields of DIHC
and pristine EBM. These come under the heading
of  “medical  decision-making  techniques,”  and
are represented by probability assessments, deci-
sion  analysis,  and  evaluation  of  cost-effective-
ness.  In  specific  reference  to  immunohistology,
pathologists (and other physicians) are commonly
oblivious to the concepts of prior diagnostic prob-
ability, posterior diagnostic probability, and like-
lihood  ratios.  They  often  obtain  immunostains
with little or no attention to how (or if) the results
will alter their morphological diagnostic impres-
sions. Are the tests likely to be dispositive, will
they substantially narrow the field of possibilities,
or  might  they  merely  cause  confusion?  The
answers  to  those  questions  must  come  from  a
combination  of  empirical  experience  and  data
obtained  from  the  pertinent  literature  [80].  An
immunostain should not be procured simply based
on its availability, because, with a probability of
>0,  results  may  not  conform  to  the  “expected”
product. An example follows.

Fig. 16.12  Organization diagram depicting the elements of evidence-based medicine, as conceptualized by Friedland
et al. [79]

16  Evidence-Based Practices in Applied Immunohistochemistry

271

Immunohistochemical Results:
Relationship to Prior and Posterior
Probability

An irregular mass was detected mammographically
in the left breast of a 57 year old woman (Fig. 16.13).
An excisional biopsy of the lesion demonstrated an
infiltrative carcinoma featuring the linear growth of
small polygonal cells with regular, round nuclei; only
small  nucleoli;  and  a  low  mitotic  rate  (Fig. 16.14).
Profiles of tumor cells tended to surround preexist-
ing interlobular and  intralobular ducts. The attending
pathologist obtained an  immunostain for E-cadherin,
which yielded positive results (Fig. 16.15). There was
also reactivity for estrogen and progesterone receptor
proteins, and a lack of HER-2 gene amplification, a
diagnosis of Nottingham grade II invasive ductal ad-
enocarcinoma was therefore made (erroneously).

The  scenario  just  described  is  not  rare,  in  our
experience, and it illustrates problems that accom-
pany  an  ignorance  of  probability  refinement.
Essentially,  all  pathologists  would  accept  the
microscopic image of the breast tumor in the exem-
plary  case  as  completely  diagnostic  for  invasive
lobular carcinoma (ILC). Thus, the prior diagnostic
probability approximates 100%, and is not lessened
appreciably  by  a  single  unexpected  immunohis-
tochemical result. Da Silva et al. [81] have shown
that approximately 20% of ILCs manifest aberrant
immunoreactivity for E-cadherin; hence, that test is
far from determinative, in and of itself. In short, an
expert  knowledge  of  morphology  still  provides
very high prior diagnostic probabilities. Procuring
unnecessary immunostains in that circumstance is

more  likely  to  produce  confusion  than  certainty.
This  reality,  in  turn,  feeds  directly  into  “medical
decision  analysis.”  Surgical  pathologic  diagnoses
never exist in a vacuum – the attending physician
will  use  such  information  to  structure  a  plan  of
treatment,  and  pathologists’  mistakes  may  well
become clinicians’ mistakes. Da Silva et al. reached
similar conclusions, stating:

Fig.  16.13  Mammographic  image  of  the  left  breast,
showing  an  irregular  mass  density  having  the  morpho-
logic features of a malignancy

Fig. 16.14  (a, b) Linear profiles are seen of an invasive breast carcinoma exhibiting cellular monomorphism and rela-
tively bland nuclear features. The images are diagnostic of infiltrating lobular mammary adenocarcinoma

272

M.R. Wick et al.

apply DIHC using a “shotgun approach,” perhaps
reasoning  that  “the  more  the  better”  and  without
full consideration of what they are planning to do
with  the  results.  This  practice  results  not  only  in
unnecessary costs but also in diagnostic dilemmas
that lead to further testing, potential confusion, and/
or diagnostic errors.

It  is  well  known  that  there  are  few  if  any
entirely specific IHC results for any one diagno-
sis. In addition, the definitions of various clinico-
pathologic  entities  provided  by  WHO,  Armed
Forces  Institute  of  Pathology  (AFIP),  and  other
standard textbooks and publications do not gen-
erally include various potential DHIC results as
diagnostic  criteria  [82–86].  Except  for  hemato-
logic  and  lymphoproliferative  disorders  and
selected other conditions, pathology publications
generally define a variety of entities on the basis
of clinico-pathologic criteria, describe their gross
pathology  and  histopathologic  features,  and
include in a subsequent section a description of
the characteristic immunophenotype of the lesion
and  sometimes  the  sensitivity  and  specificity  of
various  antibodies  [87,  88].  These  descriptions
do  not  generally  provide  specific  information
regarding when and how to use selected antibod-
ies  to  confirm  or  disprove  the  diagnosis  of  the
entity  being  described.  This  practice  raises  the
question  of  how  best  to  use  immunophenotypic
information  obtained  by  testing  antibodies  that
are usually less than 100% specific for the diag-
nosis of entities that have been previously defined
on the basis of clinical features, gross pathology,
and H&E histopathology. In particular, in which
cases  should  the  DHIC  test  results  override  the
diagnostic  impressions  collected  from  the  clini-
cal findings, gross pathology, and H&E histopa-
thology? In daily practice, it is often comforting
to observe that the immunophenotype of a lesion
conforms with the description provided in the lit-
erature, “confirming the diagnosis,” but it can be
equally discomforting to render a particular diag-
nosis in the presence of negative results after the
tests  have  been  performed.  In  these  instances,
practicing  pathologists  are  expected  to  interpret
the puzzling DIHC results based solely on their
“clinical  judgment,”  a  paradigm  that  has  been
somewhat  discredited  in  the  Evidence-Based
Medicine literature [89, 90].

Fig. 16.15  Aberrant immunoreactivity in lobular breast
carcinoma  for  E-cadherin.  The  latter  marker  is  by  no
means  determinative  of  ductal  differentiation  in  breast
cancers, as supposed by some observers

Positive  staining  for  E-cadherin  should  not
 preclude  a  diagnosis  of  lobular  in  favor  of  ductal
carcinoma. Molecular evidence suggests that even
when E-cadherin is expressed, the cadherin-catenin
complex maybe nonfunctional. Misclassification of
tumors may lead to mismanagement of patients in
clinical practice… [81]

Revisionistic  approaches  to  histopathologic
diagnosis,  based  exclusively  on  information  from
nonmorphological adjunctive techniques, are unsci-
entific  and  nonbiological.  In  fact,  they  typically
reflect a kind of circular reasoning.

Use and Abuse of DIHC for Diagnostic
Purposes in Routine Pathology Practice

When and How Should DIHC Be Used
During the Diagnostic Process?
Readers may consider that it is silly to discuss when
and how to use DIHC in daily practice in 2010, but
it  is  our  anecdotal  experience  from  consultations
that there is considerable variability and confusion
in the pathology community about which antibod-
ies should be used for certain differential diagno-
ses,  how  many  antibodies  should  be  tested,  and
how to interpret IHC results that do not conform to
diagnoses that would otherwise be rendered on the
basis  of  histopathologic  criteria.  Indeed,  many
pathologists  and  reference  laboratories  appear  to

16  Evidence-Based Practices in Applied Immunohistochemistry

273

For  example,  one  of  us  (AM)  recently  con-
sulted  two  separate  internationally  respected
experts regarding the diagnosis of a spindle cell
lung neoplasm that had histomorphologic features
of solitary fibrous tumor (SFT) but exhibited neg-
ative  tumor  cell  immunoreactivity  for  CD34,  in
the presence of positive immunoreactivity for that
antibody  in  tumor  blood  vessels.  One  of  the
experts opined that the tumor was indeed an SFT
while the other did not. Who rendered the correct
diagnosis?  Should  we  continue  using  the  CD34
test during the diagnostic process of a presumed
SFT if we are not certain about how to interpret
negative  DIHC  results?  In  our  view,  if  more
detailed  information  regarding  how  to  interpret
CD34  immunoreactivity  during  the  diagnostic
process  of  an  SFT  were  explicitly  addressed  in
pathology textbooks and other publications, either
as  formal  recommendations  or  as  a  definitional
criteria, the need to worry about the diagnosis of
this  case  and/or  generate  a  consultation  at  addi-
tional cost to the patient would be obviated.

Another  example  from  AM’s  consultation
practice  illustrates  some  of  the  problems  gener-
ated  by  the  use  of  unnecessary  DIHC  tests.
A  pneumonectomy  specimen  showed  two  nod-
ules of carcinoid tumor involving the lung and N2
mediastinal  lymph  nodes.  The  consult  materials
included  15  different  immunostains,  including
neuroendocrine markers, CK7, CK20 and TTF-1,
and others. DIHC for Ki-67, an immunostain that
is being routinely used for the evaluation of neu-
roendocrine neoplasms in our laboratory was not
performed.  The  tumor  cells  exhibited  cytoplas-
mic immunoreactivity for chromogranin, synap-
tophysin,  CK7,  and  CK20  but  only  weak  and
patchy nuclear immunoreactivity for TTF-1. Does
this  patient  have  a  metastatic  carcinoid  tumor
from the gastrointestinal (GI) tract or a primary
pulmonary  carcinoid  tumor?  Literature  review
showed a few studies of a small number of carci-
noid tumors arising in the lung and GI tract [91].
Primary  pulmonary  carcinoid  tumors  were  uni-
formly negative for CK20 and positive for CK7
while  most  GI  primaries  exhibited  cytoplasmic
immunoreactivity  for  CK20  and  variable  CK7
immunoreactivity.  Is  this  enough  evidence  to
diagnose the case as a metastatic carcinoid tumor
to the lung from a primary GI lesion? If neither

the  original  pathologist  nor  the   consultant  is
 certain  about  how  to  interpret  the  information
provided  by  the  CK7  and  CK20  tests,  what  is
their diagnostic value or clinical applicability?

Further  questions  can  be  asked  regarding  the
use of Ki-67 IHC for the work-up of pulmonary
neuroendocrine neoplasms. Should we really per-
form  this  test  routinely,  as  we  currently  do  at
Cedars-Sinai  Medical  Center  based  on  requests
from our oncologists? In reality, the WHO diag-
nostic criteria for pulmonary neuroendocrine neo-
plasms  do  not  include  the  use  of  the  Ki-67  test,
and there are no widely accepted guidelines in the
literature about what specific cut-off values should
be  used  to  distinguish  typical  from  atypical  pul-
monary  carcinoid  tumors  and  these  tumors  from
high-grade  neuroendocrine  carcinomas  [92,  93].
We  are  being  asked  to  perform  and  provide  an
interpretation of the relevance of the percentage of
nuclear Ki-67 immunoreactivity in the tumor cells
based on our overall pathologic impression about
a  pulmonary  neuroendocrine  tumor.  The  Ki-67
DIHC test is being used for the evaluation of GI
neuroendocrine tumors and cut-off ranges of <2,
2–20,  and  >20%  have  been  proposed  by  the
American  Joint  Commission  on  Cancer  for  the
distinction  between
intermediate-
grade, and high-grade lesions, based on very lim-
ited available data [94].

low-grade,

Review  of  the  various  principles  of  Evidence-
Based Pathology discussed in this volume and some
of the problems illustrated in this chapter strongly
suggests that there is a need for more specific guide-
lines for the use of DIHC in daily practice that will
explicitly describe which antibodies should be used
to render particular diagnoses and how to interpret
respective positive and negative results.

Should DIHC Be Used to Distinguish
Benign from Malignant Lesions?

Certain DIHC tests are useful to help distinguish
benign from malignant lesions, such as the expres-
sion of racemase in prostate biopsies that exhibit
atypical  epithelial  lesions  or  of  myoepithelial
markers in breast biopsies with sclerosing adenosis
[95, 96]. However, as certain immunophenotypes
are  more  frequently  expressed  than  others  in

274

M.R. Wick et al.

selected benign or malignant lesions, pathologists
can be tempted to use a variety of antibodies that
have  been  described  for  other  purposes  to  help
diagnose  malignant  lesions.  For  example,  AM
periodically receives lung biopsies that have been
tested for p53 to help distinguish benign reactive
atypical  pneumocytes  from  bronchioloalveolar
carcinoma of the lung (BAC) or reactive mesothe-
lial  hyperplasia  from  malignant  mesothelioma.
Although p53 immunoreactivity has been described
in BAC, malignant mesothelioma, and some pre-
sumably  premalignant  conditions,  these  descrip-
tive observations were probably not intended as a
diagnostic test of malignancy, and have not been
prospectively  validated  for  this  purpose  [97, 98].
Indeed,  when  we  receive  these  cases,  we  often
wonder why we were consulted if p53 was positive
after being ordered by a pathologist who presum-
ably believes in its diagnostic value. Internationally
renowned experts can also disagree on when and
how to use DHIC to help distinguish benign from
malignant lesions. For example, AM periodically
submits  in  consultation  difficult  thyroid  lesions
that could represent an encapsulated papillary car-
cinoma, follicular variant, or a follicular adenoma
with some cells exhibiting nuclear folds or equivo-
cal  chromatin  clearing.  Some  experts  appear  to
favor the use of HMBE1 and CK19 testing for this
differential  diagnosis  while  others  rely  solely  on
the  interpretation  of  the  cytologic  features  of  the
lesion  [99].  To  our  knowledge,  there  is  limited
information regarding the clinical validity of using
DHIC in this differential diagnosis and no accepted
gold standard to validate the results.

Which Antibodies Should Be Used
for a Particular Differential Diagnosis?
Use of Positive Likelihood Ratios
to Help Select the Most Cost-Effective
Components of an Antibody Panel

Evidence-Based  Medicine  and  EBP  principles
favor the use of a systematic probabilistic approach
for the interpretation of information and the selec-
tion of antibody panels and other tasks that aid in
diagnosis, as discussed in more detail in Chaps. 4
and  13.  In  anatomic  pathology,  the  general
approach  involves  identification  of  the  particular

group of diagnoses that need to be sorted out in a
particular specimen, evaluation of the relative pre-
test probabilities of various diagnoses based on the
incidence of the various entities being considered,
query  for  the  presence  of  selected  pathological
features that can identify the most likely diagno-
ses,  identification  of  the  “best”  antibodies  based
on  the  +likelihood  ratio  (+LR)  of  each  test,  and
selection of the smallest panel of DHIC or other
tests that can help sort out the “final” differential
diagnosis  using  probability  ratios  (PR)  or  odds
ratios (OR). Table 16.1 shows a series of questions
that can guide pathologists in their use of DHIC
using this systematic probabilistic approach.

Different  sets  of  statistics  are  available,  but
+LR probably provides the most useful statistical
tool to help identify the most effective antibody
for  a  particular  diagnosis,  as  they  incorporate
information  regarding  the  prevalence  of  various
diagnoses  and  information  regarding  the  effec-
tiveness of a test as illustrated by its true-positive,
true-negative,  false-positive,  and  false-negative
results. They can be calculated using the formula
+LR = sensitivity/1 − specificity.

Table  16.1  Questions  that  can  help  pathologists  navi-
gate the process of evaluating specimens with diagnostic
 immunohistochemistry  using  a  systematic  probabilistic
approach

What is my differential diagnosis?
Which are the most likely clinico-pathological entities in
the differential diagnoses, based on their prevalence in
my patient population (pretest probabilities of various
diagnoses)?
Which are the gross pathology features or imaging
findings, if available, that can help me narrow the
differential diagnosis?
What is my postgross pathology/radiology test differential
diagnosis?
Which are the histopathological features that can help me
narrow the previous differential diagnosis?
What is my posthistology test differential diagnosis?
Which are the most effective antibodies to help me work
out the previous differential diagnosis, by their +likeli-
hood ratios (+LR)?
Which panel of antibodies with best available +LR that
would give the best probability ratios (PR) or odds ratios
(OR) of rendering a final diagnosis?
What is my final diagnosis after consideration of all the
available information?

16  Evidence-Based Practices in Applied Immunohistochemistry

275

A recent study by Westfall et al. used this gen-
eral  methodology  to  develop  evidence-based
guidelines  to  optimize  the  selection  of  antibody
panels in pleural cytology specimens with malig-
nant epithelioid cells [100]. The study evaluated
retrospectively the use of DHIC in 153 consecu-
tive  pleural  effusions  diagnosed  at  Cedars-Sinai
Medical  Center.  Cases  were  randomly  divided
into training and test cases as explained in Chaps.
10 and 11. The prevalence of different malignan-
cies  in  the  training  set  was  identified;  the  most
frequent diagnoses were carcinomas of the lung,
breast,  Mullerian  tract,  stomach,  and  colon.
Thereafter, the percentage of cases that were posi-
tive  for  various  antibodies,  by  diagnosis,  was
identified. These data were used to calculate the
sensitivity  and  specificity  of  each  DHIC  result
and  their  +LR.  The  clinical  usefulness  of  each
antibody  was  then  stratified  according  to  each
+LR, as the most sensitive and specific test DHIC
result provides the highest ratios. On the basis of
these data, antibody panels for the study of pleural
effusions in male (calretinin, TTF-1, and CDX-2)
and  female  (TTF-1,  ER,  and  CA125)  patients
were selected. The diagnostic value of these pan-
els was then tested using the test set of cases and
showed that they provided 100% specificity, and
77%  and  50%  sensitivity  for  male  and  female
patients, respectively. The study also showed that
the  use  of  additional  antibodies  such  as  CK5/6,
CK7,  Ber-EP4,  CK20,  and  many  others  did  not
improve the results obtained with the panels.

How Many Antibodies Should Be Used
for a Particular Differential Diagnosis?
Use of Probability Ratios and Odds
Ratios to Help Select the Optimal
Number of Antibodies that Should
Be Incorporated in an Antibody Panel

0.85  and  the  probability  of  a  negative  finding  is
0.15.  The  PR  is  0.85/0.15 = 5.7.  These  data  can
also  be  transformed  into  odds  and  OR  using  the
formulas  odds = probability/1 − probability  and
OR = Odd1/Odd2 resulting in values of 5.7, 0.17,
and  33.5,  respectively.  Probabilities  of  multiple
tests can be combined by multiplication, and the
PR and OR of various panels can be calculated.

Marchevsky and Wick used this approach for
evaluation of the use of DHIC for the differential
diagnosis  between  pulmonary  adenocarcinoma
and malignant mesothelioma, using data from a
systematic  literature  review  [101].  The  results
clearly  showed  that  the  OR  of  a  mesothelioma
diagnosis  rendered  by  using  only  one  antibody
were superior to those obtained by using antibody
panels composed with as many as 15 antibodies,
disproving  in  this  situation  the  theory  that  “the
more the better.” Indeed, although a pathologist
may intuitively think that the larger the number
of IHC tested, the more comprehensive and pre-
cise the evaluation of a lesion, in reality each test
is associated with a certain number of false-positive
and  negative  results  that  in  aggregate  progres-
sively decrease the accuracy of the diagnosis as
more antibodies are tested. For example, the OR
provided  by  1  antibody,  2  antibodies  (MOC-31
and TTF-1), and 15 antibodies for the diagnosis
of  epithelioid  malignant  mesothelioma  were
80.35, 198.18, and 9.46, respectively. The study
did not advocate using only one or two antibodies
for the diagnosis of malignant mesothelioma, but
suggested that OR information should be consid-
ered  during  the  selection  of  sensible  and  cost-
effective antibody panels.

Prognostic-Predictive
Immunohistology and EBM

Probability theory can also be used to help identify
how many antibodies should be incorporated in an
antibody panel to provide the most cost-effective
results. Different statistical tools are available, but
PR  and  OR  probably  provide  the  simplest  and
most effective tools for this task. For example, if
TTF-1 is positive in 85% of pulmonary adenocar-
cinomas,  the  probability  of  a  positive  finding  is

At this point in our discussion, we now come to a
“parting of the ways” from diagnostic immunohis-
tology, and will, hereafter, consider the related but
quite dissimilar discipline of  prognostic- predictive
immunohistochemistry  (PPIHC).  Some,  but  not
all, of the information presented hereafter is com-
parable to material in the chapter of this book that
is  specifically  directed  at  prognostication  and
prediction.

276

M.R. Wick et al.

DIHC  and  PPIHC  differ  in  important  ways.
The sole purpose and chief clinical application of
DIHC  are  the  production  of  reproducible  cate-
gorical  data,  based  on  groups  of  binary  test
results. By contrast, PPIHC concerns an attempt
to  generate  semiquantitative  data  that  often  are
substituted for information from molecular stud-
ies, and which are used in an attempt to forecast
overall  case-outcomes  and  responses  to  specific
therapies [102].

“Windows” of Immunoreactivity

As mentioned earlier, an integral part of antibody
testing in DIHC is the setting of arbitrary but defin-
able ranges for the detectability of target antigens
in tissue. That goal is accomplished by studying a
group of related specimens, e.g., prostatic adeno-
carcinomas with low, medium, and high Gleason
scores,  labeled  with  an  antibody  to  prostate-
specific antigen, which approximate, as closely as
possible, samples with fixation characteristics like
those which will be studied in one’s own labora-
tory prospectively. An antibody titer is chosen that
will recognize antigen densities within a “window”
defined by the lowest grade tumor on one hand and
the highest grade tumor on the other. At the same
time, attention must be paid to unwanted, nonspe-
cific “background” labeling (as well as unexpected
“true”  expression  in  diagnostically  confounding
cell types or patterns), with the aim of minimizing
it.  The  process  just  described  reflects  a  kind  of
contrivance or artifice, but it is needed in order to
include the desired diagnosis and exclude others.
This is a relatively straightforward procedure, and
establishes a platform for binary interpretations of
“positive” and “negative.”

The situation pertaining to PPIHC is different,
because that technique aims to detect intracellular
protein concentrations over a complete continuum
starting  at  zero  and  ending  at  infinity.  In  other
words, it is not sufficient to determine categorically
whether a target protein is present or not; instead,
one must provide a semiquantitative or quantitative
estimation  of  its  density  in  PPIHC,  instead  of
working within a predetermined “window” [103].

Moreover, a scientific leap of faith is attached; it is
usually assumed in PPIHC that the antibody spe-
cifically  recognizes  the  target  antigen  (and  only
this target) and that cellular protein concentrations
are a direct, linear reflection of gene transcription
and translation (including gene amplification), or,
alternatively, a sign of crucial gene mutation [104–
107]. These paradigms are often incorrect, because
they tend to oversimplify the molecular pathways
in  which
interest  participate
(Figs. 16.16 and 16.17).

the  genes  of

The  notion  of  quantitative  IHC  has  been
extant  for  many  years,  as  a  holy  grail  that  is
intended to provide a substitute for actual molec-
ular assessments [103, 108–112]. This quest has
persisted because IHC is relatively “easy” to per-
form vis-à-vis the demands of nucleic acid blot-
ting techniques, polymerase chain reaction-based
assays,  and  gene  sequencing.  PPIHC  is  also
much less expensive and much more available to
hospital practitioners. Nonetheless, there are two
principal reasons that it fails in its ultimate mis-
sion,  the  prognostication  and  prediction  of  dis-
ease progress. The first reason is that variations
in tissue fixation and processing, immunohisto-
logical technique, and ultimate visual interpreta-
tion  may  easily  shift  the  result  from  one  place
to    nother  on  a  continuous  scale  [103].  That  is
not nearly so true in the “windowed”  environment
of DIHC.

Intralaboratory  efforts  at  controlling  preana-
lytic variables may enjoy a certain level of success
in PPIHC; nevertheless, when samples are traded
between laboratories (as in protocol studies of var-
ious therapies, or patient referrals from one hospi-
tal  to  another),  distinct  differences  in  results  are
often seen. Second, one must consider the concept
of  “dynamic  range”  (DR)  in  evaluating  PPIHC
preparations, as well summarized by Rimm [102].

Dynamic Range: A Physical Concept
with Relevance to PPIHC

DR is a concept from the discipline of physics. It
is defined as the ratio between the smallest and
largest possible values of a changeable quantity

16  Evidence-Based Practices in Applied Immunohistochemistry

277

Fig.  16.16 ( a)  Glioblastoma  multiforme  of  the  frontal
 cerebral cortex, immunostained (b) for putatively mutant p53
protein.  (c)  Gleason  score-6  prostatic  adenocarcinoma,  also

immunolabeled for p53 protein (d). The complexity of the p53
pathway is shown here, providing several other explanations
for p53 immunostaining besides actual gene mutation (e)

278

M.R. Wick et al.

Fig. 16.17 ( a)  Gleason  score  9  adenocarcinoma  of  the
prostate,  labeled  for  bcl-2  protein  (b).  Micropapillary
adenocarcinoma of the lung (c), immunostained for bcl-2

(d). The pertinent genetic pathway impacting bcl-2 is also
complicated (e)

16  Evidence-Based Practices in Applied Immunohistochemistry

279

such as sound or light [113]. DR is closely related
to  “signal-to-noise  ratios,”  and  both  are  usually
measured in a base-10 logarithmic context, using
the  term  “base-doublings”  (dB).  This  approach
yields  the  following  equation  for  signals  with  a
wide DR, where SNR = signal-to-noise ratio and
P = average signal power [30]:

SNR

=dB

10

log10

(

P
signal

/

P
noise

).

In  DIHC,  one  aims  to  maximize  the  SNRdB,
and  the  desired  staining  product  is  represented
by a dense, dark precipitate over the target anti-
gen. In other words, almost all of the transmis-
sible  light  in  a  microscopic  preparation  is
absorbed  by  the  chromogen,  leaving  approxi-
mately 1% for analysis by the eye or another sen-
sor [102]. That is a good system for binary data
generation, as in DIHC, but not for quantitative-
continuous  analysis  as  desired  in  PPIHC.  The
observer is forced to parse the remaining 1% sig-
nal into even-smaller units if a scaled result is the
goal.  If  attempts  are  made  to  lessen  the  target-
signal  power,  the  signal  of  the  noise  assumes
proportionately  greater  significance.  That  point

is illustrated by Fig. 16.18, taken from the field
of photography; the greater the signal power, the
shorter the exposure (F-stop setting) is on a cam-
era, and the lower the noise. However, as F-stops
increase  because  signal  intensity  drops,  noise
steadily increases as well.

The  latter  construct  explains  why  one  has
 serious problems in trying to subdivide the mid-
portion of a DR plot based on 1% residual signal
power  in  PPIHC.  Very  slight  alterations  in  the
system, such as increasing the antibody concen-
tration,  adding  a  chromogen-intensifier,  or  sub-
stituting one chromogen for another, predictably
change the DR results, sometimes markedly. As
Rimm indicated:

…highly-expressing  cancers  may  not  be  resolved
from  the  majority  of  moderately-expressing  can-
cers  when  using  a  high  antibody  concentration,
owing to saturation of the assay. Using these types
of observations, an investigator [A] might resolve
only the low expressors… The reverse could be the
case  for  an  investigator  [B]  who  uses  a  very  low
concentration of antibody [102].

The end result of those dichotomous outcomes
is that observer A would likely group mid-range

Fig. 16.18  This plot of photographic dynamic range (DR)
shows  that  as  exposure  (f-stop)  times  increase,  noise  also
increases and DR (low-medium-medium/high-high) decreases.

In immunohistochemistry, a parallel paradigm obtains – the
lower the quantity of light transmitted through an absorbing
moiety (an immunostained slide), the lower the DR

280

M.R. Wick et al.

cases  with  “high”  expressors,  and  observer  B
would  place  them  among  “low”  expressors.  In
turn, these mishaps could lead the two observers
to  conclude  contradictorily  that  both  low  and
high  immunoexpression  of  the  same  analyte  in
PPIHC  are  prognostically  associated  with  the
same (good or bad) outcome. As stressed earlier,
any other factor that affects final immunostaining
intensity could potentially change the DR of the
technique  as  well.  Some  examples  are  out-of-
range pH in a given lot of formalin fixative, inap-
propriate fixation, inadequate epitope retrieval, or
inconsistency of immunodetection methodology
over time. This is likely also relevant to the appar-
ent dichotomization of ERP and PRP by the use
of altered tissue preservation techniques, or max-
imally sensitivity methodology.

Examples of Failure in Attempts
at Quantitative PPIHC

A truism in biomedical technology is that when
one  substitutes  a  vicarious  technique  for  first-
hand evaluation of any particular analyte, errors
will  result.  Looking  at  the  “genuine  article”
directly  is  always  the  best  course  of  action.
Nonetheless,  that  statement  is  idealistic.  Proper
assessment  of  biochemical  moieties  is  often
tedious,  technically  demanding,  and  expensive,
and it may well require special processing of tis-
sue or fluid samples that will serve as substrates.
By contrast, even though they have all been edu-
cated in science, physicians often seek the quick-
est,  cheapest,  and  easiest  way  to  a  test  result.
Indeed, that is the most direct explanation for the
current state of affairs in regard to PPIHC.

Typically, soon after a mechanistic link is dis-
covered  between  a  particular  gene  and  a  salient
intracellular process – especially in reference to
malignant  diseases  –  attempts  are  made  to  inte-
grate  the  observation  into  clinical  practice.  No
matter whether the gene in question is amplified,
overexpressed,  mutated,  or  deleted,  methods
quickly evolve to evaluate its status in human tis-
sue. Obviously, based on the foregoing comments,
the best mode of analysis would be a direct one;
i.e., first-hand assessment of the integrity  of  the

gene  itself  with  procedures  such  as  Southern
 blotting, polymerase chain reaction-based assays,
in  situ  hybridization,  and  nucleotide  sequencing
[114–116].  Nonetheless,  because  those  methods
are demanding ones in comparison with PPIHC,
the “default” position often has been to utilize a
“quantitative”  immunohistochemical  substitute,
whenever possible, for the technical “real McCoy.”
This  problem,  of  course,  would  be  lessened  if
laboratories carefully validated the PPIHC method
against  the  clinically  validated  marker  analysis
method. A prescription for success in this regard
was proposed by McGuire in 1991 [117].

Moreover, a great deal of scientific naïvete has
tainted such undertakings. Simply because a poly-
peptide gene product is detectable immunohisto-
logically, many observers are ready to leap to the
conclusion that biological inhibitors of the protein
will have an inevitable effect on its role in the cell.
Principal examples of that flawed line of reason-
ing include PPIHC testing for epidermal growth
factor receptor (EGFR), HER-2, and c-kit (CD117)
in  human  neoplasms  [118–120].  Prospective
attention  to  the  principles  of  EBM  would  likely
have obviated such problems. Other, slightly less
troublesome analytes in PPIHC are the ERPs and
PRPs in breast carcinoma.

Difficulties  with  the  clinical  evaluation  of
HER-2  status  in  human  tumors  have  been  dis-
cussed in Chap. 5 and are not recounted here. We
will subsequently examine the other topics cited
previously in more detail.

EGFR
EGFR  is  a  member  (along  with  HER-2)  of  the
ErbB  gene  family,  a  group  of  transmembrane
proteins  that  function  as  tyrosine  kinase  recep-
tors  and  are  activated  by  several  extracellular
ligands [121]. In the late 1990s, biological agents
that  showed  the  ability  to  block  the  binding  of
EGFR to its ligands were introduced, and several
such humanized anti-EGFR antibodies now are
available.  These  include  cetuximab,  panitu-
mumab,  erlotinib,  and  gefitinib  [122,  123].
EGFR is immunodetectable on the cell surfaces
of  several  tumors,  but  those  of  clinical  interest
mainly include squamous cell carcinomas of the
head  and  neck;  adenocarcinomas  of  the  lung

16  Evidence-Based Practices in Applied Immunohistochemistry

281

Fig. 16.19 ( a)  Adenocarcinoma  of  the  lung,  immunos-
tained for epidermal growth factor receptor protein (b)

(Fig.  16.19);  and  colorectal  adenocarcinomas
[122] (Fig. 16.20).

Early treatment protocols with EGFR inhibi-
tors  required  that  PPIHC  procedures  show  the
presence of EGFR protein in neoplastic cells, in
order  for  patients  to  be  eligible  for  therapy.
Biomedical companies with laboratory-medicine
arms  were  quick  to  respond,  marketing  EGFR
immunostaining  “kits”  that  were  approved  for
use by the U.S. Food and Drug Administration.
However,  the  antibody  reagents  in  those  kits
were questionably specific. For example, essen-
tially all colorectal carcinomas were labeled with
one  commercial  kit,  making  IHC  testing  super-
fluous [124]. In addition, comparisons with other
(nonkit-based) anti-EGFR antibodies often pro-
duced  strikingly  dissimilar  immunohistochemi-
cal results in the same tumors [125, 126].

Fig. 16.20 ( a) Adenocarcinoma of the colon, immunos-
tained for epidermal growth factor receptor protein (b)

After years of data accrual and analysis, virtually
all  recent  publications  have  concluded  that  the
immunohistologic  EGFR  status  of  human  neo-
plasms  has  no  predictive  value  for  their  possible
therapeutic  response  to  biological  inhibitors  [120,
124, 127–129]. It now appears that the dispositive
piece of information in that regard is the presence or
absence  of  selected  EGFR  gene  mutations  [129],
particularly  in  lung  adenocarcinoma,  or  the  con-
comitant  activating  mutation  of  downstream  ele-
ments,  such  as  K-ras  mutations  in  colorectal
carcinoma. Indeed, in the latter setting, K-ras muta-
tion is an independent predictive marker of kinase
inhibitor  therapy  in  metastatic  colon  carcinoma.
Neither  EGFR/K-ras  mutation  nor
treatment
response appears to have any meaningful relation-
ship to immunoreactivity for EGFR protein.

282

M.R. Wick et al.

CD117
CD117 (c-kit protein; “Steele factor”; “stem cell
factor”) is another cell membrane-based tyrosine
kinase  with  a  similar  function  to  that  of  ErbB
proteins. In the late 1990s, studies on gastrointes-
tinal stromal tumors (GISTs) showed that the vast
majority  of  them  were  immunoreactive  for
CD117 [130–132] (Fig. 16.21), and it became a
virtual conditio sine qua non for that neoplasm.
In  keeping  with  the  theme  described  above,  the
assumption  was  made  that  all  CD117-positive
tumors should, and would, respond to inhibitors
of c-kit binding to its activating ligands. The prin-
cipal biological agent in this category, imatinib,
did prove to be spectacularly effective in treating
metastatic GIST, as well as chronic myelogenous
leukemia (CML) [133]. In the latter of those con-
ditions,  an  activation  of  the  abl  gene  (another
tyrosine kinase protein) occurs because of a bcr-
abl  gene  fusion  relating  to  the  t(9;22)  chromo-
somal translocation in CML [134].

Once again, misdirected hopes of therapeutic
success  with  imatinib  arose  in  reference  to  all
tumors  that  were  immunoreactive  for  CD117.
They comprised a considerable group, including
primitive  neuroectodermal  tumor,  extraskeletal
myxoid  chondrosarcoma,  melanotic  schwan-
noma, melanoma, angiosarcoma, uterine leiomy-

osarcoma,  seminoma-dysgerminoma,  mast  cell
proliferations,  adenoid  cystic  carcinoma,  some
nasopharyngeal carcinomas, chromophobe renal
cell carcinomas, high-grade neuroendocrine car-
cinomas,  epithelial-myoepithelial  salivary  gland
carcinoma, ovarian carcinomas, and some ductal
breast carcinomas [135] (Fig. 16.22). Nonetheless,
only GIST and CML demonstrated any meaning-
ful clinical response to imatinib-mediated inhibi-
tion  of  tyrosine  kinase.  Further  analysis  has
demonstrated  once  more  that  critical,  activating
mutations  in  the  CD117  gene  (Fig.  16.23)  are
required to realize a biological response to c-kit-
inhibiting agents [136].

Hormone Receptors in Breast Carcinoma
In  the  1970s,  McGuire  and  others  developed  a
chemical competitive binding assay (CCBA) for
ERP and PRP, which was principally used in the
evaluation of breast carcinomas [137–140]. The
goal  of  that  assessment  was  to  study  a  possible
relationship  between  quantitative  ERP/PRP  sta-
tus and a response to hormone-modulating drugs.
Many studies over several years did indeed show
such an association. Breast cancers with a quanti-
tative  ERP  content  over  a  level  of  10  fmol/mg
protein  were  classified  as  “positive,”  because
they showed a uniform response to the adminis-
tration of tamoxifen, an estrogen antagonist [138,
139]. Indeed, the statistical level of clinical ben-
efit from that agent was a linear one; the higher
the ERP content of the tumor cells, the greater the
effect was of tamoxifen [141].

the

tissue

tumor

to  perform

Problems with the use of CCBAs for ERP and
PRP centered on the need for a critical volume of
tests.
fresh
Accordingly, other investigators began to evalu-
ate  tissue  section-based  immunoassays  as  alter-
natives,  in  the  1980s.  These  involved  both
immunofluorescence  and  immunoenzyme  tech-
niques,  as  applied  to  frozen  sections  of  breast
cancers [142]. Good – but not perfect – correla-
tion was observed between results in CCBAs and
immunohistologic methods [143, 144]. The abil-
ity  to  study  very  small  biopsies  with  IHC  was
regarded as a substantial benefit, and the use of
chemical competitive assays began to disappear.
By  the  advent  of  the  twenty-first  century,  they

Fig. 16.21  Intense immunoreactivity is present in a gastro-
intestinal stromal tumor, labeled for CD117 (c-kit protein)

16  Evidence-Based Practices in Applied Immunohistochemistry

283

Fig.  16.22  CD117  immunoreactivity  is  present  in  (a)
testicular  seminoma;  (b)  mastocytosis  of  the  bone  mar-
row; (c) small-cell neuroendocrine carcinoma of the lung;

and (d) metastatic melanoma. Despite their immunolabel-
ing, none of these tumors responds to targeted anti-CD117
biological therapy

were all but extinct, and advances in HIER had
made  studies  of  paraffin  sections  for  ERP  and
PRP routine [145] (Fig. 16.24).

Nevertheless,  potentially  valuable  data  were
lost  in  this  transition.  Recent  evaluations  have
shown a bimodal distribution [145, 146] of ERP
in mammary carcinomas that was not present in
the  CCBA  era,  where  a  linear  model  obtained
[139, 141]. As suggested earlier, this bimodality
is  almost  certainly  an  artifact  of  technique  and
the  limitations  of  PPIHC  in  delineating  mid-
range  biochemical  results,  particularly  when

increasingly  sensitive  techniques  are  employed.
Accordingly,  there  may  be  no  way  of  knowing
how  close  an  immunohistologically  “positive”
ERP result is to the previous 10 fmol/mg thresh-
old in CCBAs in many laboratories today.

This need not necessarily be the case, however.
In  their  initial  validation  of  estrogen  receptor
PPHIC,  Harvey  and  associates  (147)  analyzed
several  anti-ER  antibodies,  including  6F11,  and
compared their immunohistochemical results with
both existing validated CCBA data and survival in
over 1,000 patients. Using a modified H-scoring

284

M.R. Wick et al.

Fig. 16.23  Activating mutations in the CD117 gene, cor-
responding to PCR products marked by arrows in this blot
preparation,  are  necessary  in  order  for  patients  with

CD117-immunoreactive  tumors  to  realize  beneficial
effects from anti-tyrosine kinase medications

Fig. 16.24  Immunostaining for estrogen receptor protein
(a) and progesterone receptor protein (b) in ductal breast
carcinomas. These analytes  show a  bimodal  distribution

in paraffin sections that is likely an artifact of immunohis-
tochemical detection

system we now know as the Allred score, a rela-
tionship approximating linearity between overall
stain score and outcome, as well as Allred score
and  CCBA  values,  was  demonstrated.  It  is  also

important  to  note  that  the  distribution  of  scores
was not dichotomous, but more evenly distributed
over all stain outcome possibilities, with a distinct
cut-point  (Allred  score  3)  between  clinical

16  Evidence-Based Practices in Applied Immunohistochemistry

285

responders and nonresponders. It might be argued
that the Allred score (by measuring tissue distri-
bution and stain intensity) is merely a best-fit solu-
tion to the problem of correlation with CCBA data
and survival, rather than a direct measure of true
biological variation. It might also be argued that
the apparent linearity of the assay with respect to
CCBA and outcome was due to the use of an infe-
rior  reagent  in  an  insensitive  system  (antibody
6F11  using  protease/DNAse  retrieval).  Also,  as
Nadji has sagely observed:

…in the case of ER, immunohistochemical meth-
ods only identify a segment or epitope of ER pro-
tein that is immunologically-reactive with the used
antibody. Hence, as it is, an immunohistochemical
technique  gives  no  information  about  the  func-
tional status of the ER molecule, and/or that of the
complex  downstream  ER  pathways.  This  may  be
one of the reasons why one-third of patients with
ER-positive  breast  cancers  initially,  and  another
one-third  eventually,  do  not  respond  to  endocrine
treatment modalities [148].

Those  comments  also  raise  an  important
 secondary topic in this context. Puristically, one
should  only  use  cross-validated,  ERP-positive
breast cancer specimens from patients who were
proven  to  benefit  from  endocrine  therapy,  as
biological controls in clinical PPIHC. That pro-
vision is virtually never heeded today by most
laboratories,  but  clearly  was  by  others  [117,
147, 149].

Ideally, one should employ antibody reagents
whose binding to tissue targets is known to cor-
relate with in vivo activity of the substrate. With
regard  to  breast  cancer,  the  active  isoform  of
ERP  appears  to  be  phosphorylated  ERP-alpha
(PERPA), which reflects the presence of an intact
intracellular  signaling  pathway  [150,  151].
Nevertheless,  the  great  majority  of  laboratories
doing PPIHC for ERP do not utilize anti-PERPA
antibodies.

Whereas  an  increasing  number  of  ERP  anti-
bodies have entered the commercial market, and
some  of  them  show  suboptimal  specificity  for
functional ERP epitopes [149], attention to appro-
priate clinical validation using tenets proposed by
McGuire  may  provide  a  basis  for  reproducible
and meaningful PPIHC and address, at least, some
of  Nadji’s  concerns  [148].  Practically  speaking,

without  careful  and  appropriate  validation,  the
true relationship between antibody specificity for
functional  ERP  epitopes  and  treatment  failure
alluded to by Nadji cannot be understood.

Alternatives to Immunohistology
for Prognostication and Prediction

In light of the deficiencies and distortions attached
to  PPIHC  as  a  reflection  of  actual  tumor-related
“biopredictor”  distributions,  other   methods  have
been  evaluated  as  alternatives
traditional
paraffin- section immunohistology. Four of them –
the automated quantitative analysis (AQUA) sys-
tem,  polymerase  chain  reaction  (PCR)-based
assays, fluorescent and chromogenic in situ hybrid-
ization (FISH), and gene-chip arrays – show par-
ticular utility.

to

The AQUA Technique

Developed at Yale University, the AQUA method
is, in a way, a return to a technique of the past,
but with new dimensions [152–154]. This proce-
dure  uses  immunofluorescence  as  its  principal
antigen-detection system with paraffin sections,
and can vary antibody concentrations over a pre-
defined  range  in  the  study  of  each  test  sample.
Fluorescent emission data are recorded by image
acquisition and software-mediated analysis, and
matched to a subcellular compartment of interest
(e.g.,  nucleus,  cytoplasm,  cell  membrane,  etc.)
[152].  Results  of  the  AQUA  technique  parallel
those  of  enzyme-linked  immunosorbent  assays
(ELISAs)  in  clinical  chemistry,  and  are  much
better  than  traditional  IHC  at  portraying  linear
biomarker  activities  over  a  continuous  range  of
values [102].

Recent  publications  on  AQUA  by  the  Yale
group of investigators have shown that ERP den-
sity in breast cancer does indeed maintain a lin-
ear  association  with  the  biological  response  to
tamoxifen,  just  as  it  did  in  the  past  [152,  154].
Interestingly,  they  also  demonstrate  that  both
low and high levels of HER-2 protein in breast

286

M.R. Wick et al.

carcinomas  are  linked  with  adverse  clinical
behavior. p53 protein had neither of those char-
acteristics [152].

Polymerase Chain Reaction-Based
Analyses

PCR-based  analyses,  which  have  been  avail-
able for over 15 years, have two potential uses
in  the  setting  being  discussed.  First,  if  it  is
known that one or more particular mutations in
a specified gene have a prognostic significance,
they  can  be  relatively  easily  demonstrated  by
conventional PCR or “real-time” PCR (RT-PCR)
[155]. In the latter case, the presence of the tar-
get nucleic acid sequence(s) is “reported” as it is

detected [156]. Examples of prognostic analytes
that can be  studied in this way are represented
by  p53  mutations  and  activating  mutations  in
the EGFR and CD117 genes [157]. Only a small
amount  of  tissue  is  required  for  PCR,  and  the
study  can  even  be  done  on  fine  needle  aspira-
tion biopsies (Fig.  16.25) or effusion cytology
specimens.

Secondly,

in  RT-PCR,

complementary
 deoxyribonucleic acid (cDNA) derived from the
clinical  sample  is  analyzed  in  parallel  with
“housekeeping”  genes.  Comparison  of  the  two,
and standardization of final results, allows for a
quantitative  measurement  of  specific  patient-
related  nucleic  acid  sequences  [158].  Thus,
potential gene amplification can be detected with
this method [159, 160].

Fig. 16.25  Polymerase chain reaction-based assays for mutations in the p53 gene can be performed on specimens of
limited volume, as true of this fine-needle aspiration biopsy of a nonsmall-cell lung carcinoma

16  Evidence-Based Practices in Applied Immunohistochemistry

287

Fig. 16.27  The tumor shown in Fig. 16.26 also manifests
several  intranuclear  signals  in  a  chromogenic  in  situ
hybridization study for HER-2

from  the  specimen.  Detection  methods  depend
on the nature of the probe label; it can be a radio-
nuclide,  a  heavy-metal  complex,  a  fluorophore
(FISH)  (Fig.  16.26),  or  a  chromogenic  dye
(CISH) [159, 161, 164] (Fig. 16.27). Once again,
the  advantage  of  in  situ  hybridization  over
PPIHC  is  that  the  former  method  is  direct,
whereas the latter is indirect. In situ hybridiza-
tion  can  be  employed  to  assess  the  number  of
gene  copies,  the  presence  of  mutant  gene
sequences,  or  the  amount  of  intracellular  mes-
senger RNA related to a particular gene.

Fig. 16.26  The cells in this ductal mammary adenocarci-
noma  (a)  show  multiple  copies  of  the  HER-2  gene,  as
depicted by the fluorescent in situ hybridization method (b)

Fluorescent and Chromogenic In Situ
Hybridization (FISH and CISH)

Nucleic Acid Microarrays

FISH methods for studying the number of gene
copies or the presence of specific gene mutations
in  human  cells  are  now  widespread  [161].
Indeed, many laboratories have foregone “surro-
gate” testing (usually with IHC) for prognostic-
predictive  gene  alterations  and  moved
to
exclusive use of in situ hybridization [162]. That
is certainly true for HER-2 in the current prac-
tice  of  surgical  pathology  [163].  In  this  tech-
nique, labeled nucleic acid probes are hybridized
with  native  single-stranded  DNA  (following  a
denaturation  step)  or  ribonucleic  acid  (RNA)

Nucleic acid microarrays are “multiplex” (multi-
ple simultaneous test-capable) platforms that are
used in the analysis of gene copies in a clinical
sample,  relative  to  integrated  reference  controls
[165–169]. They comprise arrayed series of thou-
sands  of  microscopic  oligonucleotide  spots,
called  “features.”  Each  feature  contains  pico-
moles of specific nucleic acid sequences, known
as  probes  or  reporters  (Fig.  16.28).  Hybridized
probe-target complexes are detected and quanti-
fied with fluorophores, heavy metals, or chemilu-
minescence labels, to assess the relative numbers

288

M.R. Wick et al.

Fig. 16.28  Gene chip structure is based on the presence
of many predetermined oligonucleotide spots (“features”),
with which controlled and labeled nucleic acid probes can

be hybridized. The results present a multifaceted picture
that  can  show  increased,  decreased,  or  unchanged  gene
copy numbers as compared with integrated controls

of  nucleic  acid  sequences  in  the  target  tissue.
Because  each  array  can  contain  >104  probes,  it
allows  for  many  assays  to  be  done  in  parallel.
Probes are attached to a solid surface by covalent
bonds; the solid component can be glass or a sili-
con chip [168, 169].

After  hybridization  with  reporter  probes,  the
chip  is  scanned  with  an  appropriate  detector
device,  and  the  signals  are  quantified.  A  “heat
map”  is  then  generated  by  associated  computer
software that shows which nucleic acid sequences
are  increased  in  number,  which  are  unchanged,
and  which  are  decreased,  relative  to  controls
[169, 170] (Fig. 16.29). Depending on whether a
chip  comprises  DNA  or  RNA  sequences,  the
presence of either gene amplification or overex-
pression in the clinical sample can be determined
with this technology.

Gene chips are powerful tools in prognostica-
tion  and  prediction  because  of  their  multiplex
capabilities.  Rather  than  providing  information
on only one gene or gene product, chips paint a
broad  picture  of  nucleic  acid  composition  or
expression in any given sample [171, 172].

Should PPIHC Have a Future, Based
on EBM Principles?

This discussion has focused on the good and the
not-so-good  aspects  of  applied  IHC.  Casting
diplomacy aside, we conclude that more of the
latter  elements  exist  than  the  former  in  refer-
ence to PPIHC. The principal reasons account-
ing  for  the  persistence  of  “forecast”-oriented
immunohistology  seem  to  relate  to  its  general

16  Evidence-Based Practices in Applied Immunohistochemistry

289

Fig. 16.29  A “heat map” of gene chip results, showing increased (green), unchanged (black), or decreased (red) indi-
vidual gene-related signals as compared with controls

availability  and  relatively  low  cost,  and  not  to
its  superior  performance  in  relation  to  other
testing methods.

Ultimately,  whether  the  healthcare  system  in
the United States or other countries continues to
use PPIHC will depend on a comprehensive anal-
ysis  of  its  cost-effectiveness.  Other  specialties
have  begun  to  use  that  approach  with  beneficial
results [173, 174] (Fig. 16.30). If a test is inexpen-
sive but it is mediocre, overused, and may produce

misleading  information,  it  logically  should  be
abandoned. On the other hand, alternative assays
that cost more, but have excellent predictive val-
ues and low rates of error, are those that best serve
patients and the system at large. In that context,
we believe that the future of PPIHC is, or should
be, in doubt on evidence-based scientific grounds.
At this stage in its evolution, medical economists
are  very  likely  better  judges  of  its  eventual  fate
than are pathologists or other physicians.

290

M.R. Wick et al.

Fig.  16.30  Graphic  comparison  of  cost-effectiveness
analyses of fecal occult blood testing (a) and systematic
colonoscopic examination over time (b), for the detection
of colorectal carcinoma. The area under the curve of (b) is

less than that seen in (a), indicating greater cost-effectiveness
of  colonoscopy.  This  same  technique  can  be  applied  to
immunohistochemical evaluations

References

  1.  Taylor  CR.  Immunohistological  approach  to  tumor

diagnosis. Oncology. 1978;35:187–97.

  2.  Taylor CR. Immunoperoxidase techniques: practical
and  theoretical  aspects.  Arch  Pathol  Lab  Med.
1978;102:113–21.

  3.  Pearse  AGE.  A  review  of  modern  methods  in  his-

tochemistry. J Clin Pathol. 1951;4:1–36.

  4.  Pearse  AGE.  Histochemistry  and  its  application  to
the  basic  sciences.  Lect  Sci  Basis  Med.  1955;4:
358–86.

  5.  Lillie  RD.  Problems  of  fixation  in  histochemistry.

J Histochem Cytochem. 1958;6:301–2.

  6.  Bennett  HS.  A  perception  of  histochemistry.
J Histochem Cytochem. 1983;31(Suppl):127–30.
  7.  Rosai  J,  Rodriguez  HA.  Application  of  electron
microscopy  to  the  differential  diagnosis  of  tumors.
Am J Clin Pathol. 1968;50:555–62.

  8.  Kuzela DC, True LD, Eiseman B. The role of elec-
tron  microscopy  in  the  management  of  surgical
patients. Ann Surg. 1982;195:1–11.

  9.  Fisher C, Ramsay AD, Griffiths M, McDougall J. An
assessment  of  the  value  of  electron  microscopy  in
tumor diagnosis. J Clin Pathol. 1985;38:403–8.
  10. Ordonez  NG,  Mackay  B.  Electron  microscopy  in
tumor diagnosis: indications for its use in the immu-
nohistochemical era. Hum Pathol. 1998;29:1403–11.
  11.  Ruska  E.  Ernst  Ruska:  autobiography.  Stockholm:

Nobel Foundation Press; 1986.

  12.  McDevitt  HO.  Albert  Hewett  Coons,  1912–1978.

New York: National Academies Press; 1979.

  13.  Coons  AH,  Creech  HJ,  Jones  R.  Immunological
properties  of  an  antibody  containing  a  fluorescent
group. Proc Soc Exp Biol. 1941;47:200–2.

  14.  Coons AH. The beginnings of immunofluorescence.

J Immunol. 1961;87:499–503.

  15.  Lasker  Foundation.  1959  Winners  –  Albert  Lasker
basic  medical  research  award.  http://www.lasker-
foundation.org/awards/1959basic.htm.  Accessed  7
Dec 2010.

  16.  Beutner EH, Jordon RE. Demonstration of skin anti-
bodies  in  sera  of  pemphigus  vulgaris  patients  by
indirect  immunofluorescent  staining.  Proc  Soc  Exp
Biol Med. 1964;117:505–10.

16  Evidence-Based Practices in Applied Immunohistochemistry

291

  17.  Tan EM, Kunkel HG. An immunofluorescence study
of the skin lesions in systemic lupus erythematosus.
Arthritis Rheum. 1966;9:37–46.

  18.  Nagasawa T, Miyakawa Y, Shibata S. New fluores-
cent staining method for renal biopsy: introduction
of anti-glomerular basement membrane labeled anti-
body.  Saishin  Igaku  (Modern  Medicine).  1968;23:
2656–63.

  19.  Wilson  CB,  Dixon  FJ,  Fortner  JG,  Cerilli  GJ.
Glomerular  basement  membrane-reactive  antibody
in anti-lymphocyte globulin. J Clin Invest. 1971;50:
1525–35.

  20.  Hall CE, Nisonoff A, Slayter HS. Electron micros-
copy  observations  of  rabbit  antibodies.  J  Biophys
Biochem Cytol. 1959;6:407–12.

  21.  Singer  SJ,  Schick  AF.  The  properties  of  specific
stains  for  electron  microscopy  prepared  by  the
 conjugation  of  antibody  molecules  with  ferritin.
J Biophys Biochem Cytol. 1961;9:519–37.

  22.  Slot JW, Posthuma G, Chang LY, Crapo JD, Geuze
HJ. Quantitative aspects of immunogold labeling in
embedded and in nonembedded sections. Am J Anat.
1989;185:271–81.

  23.  Sternberger  LA.  Electron  microscopic  immunocy-
tochemistry:  a  review.  J  Histochem  Cytochem.
1967;15:139–59.

  24.  Roth J, Heitz PU. Immunolabeling with the protein
A-gold  technique:  an  overview.  Ultrastruct  Pathol.
1989;13:467–84.

  25.  Sternberger  LA,  Cuculis  JJ.  Method  for  enzymatic
intensification of the immunocytochemical reaction
without  use  of  labeled  antibodies.  J  Histochem
Cytochem. 1969;17:190.

  26.  Sternberger  LA,  Hardy  Jr  PH,  Cuculis  JJ,  Meyer
HG.  The  unlabeled  antibody-enzyme  method  of
immunohistochemistry:  preparation  and  properties
of  soluble  antigen-antibody  complex  (horseradish
peroxidase-anti-horseradish peroxidase)  and its use
in
identification  of  spirochetes.  J  Histochem
Cytochem. 1970;18:315–33.

  27.  Tornehave  D,  Folkersen  J,  Teisner  B,  Chemnitz  J.
Immunohistochemical  aspects  of  immunological
cross-reaction and masking of epitopes for localiza-
tion studies on pregnancy-associated plasma protein A.
Histochem J. 1986;18:184–8.

  28.  DeLellis  RA,  Kwan  P.  Technical  considerations  in
the immunohistochemical demonstration of interme-
diate filaments. Am J Surg Pathol. 1988;12(Suppl):
17–23.

  29.  Drier  JK,  Swanson  PE,  Cherwitz  DL,  Wick  MR.
S100 protein immunoreactivity in poorly-differenti-
ated carcinomas: immunohistochemical comparison
with  malignant  melanoma.  Arch  Pathol  Lab  Med.
1987;111:447–52.

  30. Anonymous. Signal-to-nose ratio. http://en.wikipedia.
org/wiki/signal-to-noise_ratio. Accessed 7 Dec 2010.
  31.  Hsu SM, Raine L, Fanger H. The use of antiavidin
antibody  and  avidin-biotin-peroxidase  complex  in
immunoperoxidase  techniques.  Am  J  Clin  Pathol.
1981;75:816–21.

  32.  Hsu SM, Raine L, Fanger H. Use of avidin-biotin-
peroxidase  complex  (ABC)  in  immunoperoxidase
techniques:  a  comparison  between  ABC  and  unla-
beled  antibody  (PAP)  procedures.  J  Histochem
Cytochem. 1981;29:577–80.

  33.  Guesdon  JL,  Ternynck  T,  Avrameas  S.  The  use  of
avidin-biotin  interaction  in  immunoenzyme  tech-
niques. J Histochem Cytochem. 1979;27:1131–6.
  34.  Bratthauer  GL.  The  avidin-biotin  complex  (ABC)
method  and  other  avidin-biotin  bindings  methods.
Methods Mol Biol. 2010;588:257–70.

  35.  Miller  RT,  Groothuis  CL.  Improved  avidin-biotin
immunoperoxidase method for terminal deoxyribo-
nucleotidyl transferase and immunophenotypic char-
acterization  of  blood  cells.  Am  J  Clin  Pathol.
1990;93:670–4.

  36.  Elias  JM,  Margiotta  M,  Gaborc  D.  Sensitivity  and
detection efficiency of the peroxidase-antiperoxidase
(PAP),  avidin-biotin-peroxidase  complex  (ABC),
and  peroxidase-labeled  avidin-biotin  (LAB)  meth-
ods. Am J Clin Pathol. 1989;92:62–7.

  37.  Mokry J. Versatility of immunohistochemical reac-
tions:  comprehensive  survey  of  detection  systems.
Acta Medica. 1996;39:129–40.

  38.  Swanson PE, Kagen KA, Wick MR. Avidin-biotin-
peroxidase-antiperoxidase  (ABPAP)  complex:  an
immunocytochemical method with enhanced sensi-
tivity. Am J Clin Pathol. 1987;88:162–76.

  39.  Kammerer  U,  Kapp  M,  Gassel  AM,  et  al.  A  new
rapid immunohistochemical staining technique using
the  Envision  antibody  complex.  J  Histochem
Cytochem. 2001;49:623–30.

  40.  Masouredis  SP,  Sudora  E,  Mahan  L,  Victoria  EJ.
Quantitative  immunoferritin  microscopy  of  Fya,
Fyb, Jka, U, and Dib antigen site numbers on human
red cells. Blood. 1980;56:969–77.

  41.  Ripoche  J,  Sim  RB.  Loss  of  complement  receptor
type 1(CR1) on aging of erythrocytes: studies of pro-
teolytic  release  of  the  receptor.  Biochem  J.  1986;
235:815–21.

  42.  Andrade  RE,  Hagen  KA,  Swanson  PE,  Wick  MR.
The use of proteolysis with ficin for immunostaining
of paraffin sections: a study of lymphoid, mesenchy-
mal,  and  epithelial  determinants  in  human  tissues.
Am J Clin Pathol. 1988;90:33–9.

  43.  Hajdu  I.  The  immunohistochemical  detection  of
J-chain  in  lymphoid  cells  in  tissue  sections:  the
necessity  of
trypsin  digestion.  Cell  Immunol.
1983;79:157–63.

  44.  Dell’Orto P, Viale G, Colombi R, Braidotti P, Coggi
G.  Immunohistochemical  localization  of  human
immunoglobulins and lysozyme in epoxy-embedded
lymph nodes: effect of different fixatives and of pro-
teolytic digestion. J Histochem Cytochem. 1982;30:
630–6.

  45.  Miller  RT,  Swanson  PE,  Wick  MR.  Fixation  &
epitope retrieval in diagnostic immunohistochemis-
try:  a  concise  review  with  practical  considerations.
Appl  Immunohistochem  Mol  Morphol.  2000;8:
228–35.

292

M.R. Wick et al.

  46.  Hiort  O,  Lwan  PW,  DeLellis  RA.  Immuno-
histochemistry of estrogen receptor protein in paraf-
fin  sections:  effects  of  enzymatic  pretreatment  and
cobalt  chloride  intensification.  Am  J  Clin  Pathol.
1988;90:559–63.

  47.  Pileri S, Serra  L, Martinelli G. The  use of  pronase
enhances sensitivity of the PAP method in the detec-
tion  of  intracytoplasmic  immunoglobulins.  Basic
Appl Histochem. 1980;24:203–7.

  48. Taylor CR, Shi SR, Chaiwun B, et al. Strategies for
improving the immunohistochemical staining of vari-
ous intranuclear prognostic markers in formalin-paraf-
fin  sections:  androgen  receptor,  estrogen  receptor,
progesterone  receptor,  p53  protein,  proliferating  cell
nuclear antigen, and Ki-67 antigen revealed by antigen
retrieval techniques. Hum Pathol. 1994;25:263–70.
  49.  Shi  SR,  Key  ME,  Kalra  KL.  Antigen  retrieval  in
an
formalin-fixed  paraffin-embedded
enhancement  method  for
immunohistochemical
staining based on microwave oven heating of tissue
sections. J Histochem Cytochem. 1991;39:741–8.
  50.  Suurmeijer  AJH.  Microwave-stimulated  antigen
retrieval:  a  new  method  facilitating  immunohis-
tochemistry  of  formalin-fixed,  paraffin-embedded
tissue. Histochem J. 1992;24:597.

tissues:

  51.  Gown  AM,  deWever  N,  Battifora  H.  Microwave-
based  antigenic  unmasking:  a  revolutionary  new
technique  for  routine  immunohistochemistry.  Appl
Immunohistochem. 1993;1:256–66.

  52.  Norton  AJ,  Jordan  S,  Yeomans  P.  Brief  high  tem-
perature  heat  denaturation  (pressure  cooking):  a
simple and effective method of antigen retrieval for
routinely-processed  tissues.  J  Pathol.  1994;173:
371–9.

  53.  Cattoretti  G,  Suurmeijer  AJH.  Antigen  unmasking
on  formalin-fixed  paraffin-embedded  tissues  using
microwaves:  a  review.  Adv  Anat  Pathol.  1995;2:
2–9.

  54.  Miller RT, Estran C. Heat-induced epitope retrieval
with a pressure cooker: suggestions for optimal use.
Appl Immunohistochem. 1995;3:190–3.

  55.  Pileri  S,  Roncador  G,  Ceccarelli  C,  et  al.  Antigen
retrieval techniques in immunohistochemistry: com-
parison  among  different  methods.  J  Pathol.  1997;
183:116–23.

  56.  Bogen SA, Vani K, Sompuram SR. Molecular mech-
anisms of antigen retrieval: antigen retrieval reverses
steric interference caused by formalin-induced cross-
links. Biotech Histochem. 2009;84:207–15.

  57.  Boenisch T. Heat-induced antigen retrieval: what are
we  retrieving?  J  Histochem  Cytochem.  2006;54:
961–4.

  58.  Leong TY, Leong ASY. How does antigen retrieval

work? Adv Anat Pathol. 2007;14:129–31.

  59.  Werner  M,  Von  Wasielewski  R,  Komminoth  P.
Antigen retrieval, signal amplification, and intensifi-
cation  in  immunohistochemistry.  Histochem  Cell
Biol. 1996;105:253–60.

  60.  Puchtler H, Meloan SN. On the chemistry of formal-
dehyde fixation and its effects on immunohistochem-
ical reactions. Histochemistry. 1985;82:201–4.

  61.  Paterson DA, Reid CP, Anderson TJ, Hawkins RA.
Assessment  of  estrogen  receptor  content  of  breast
carcinoma  by  immunohistochemical  techniques  on
fixed  and  frozen  tissue  and  by  biochemical  ligand-
binding assay. J Clin Pathol. 1990;43:46–51.

  62.  Fisher CJ, Gillett CE, Vojtesek G, Barnes DM, Millis
RR. Problems with p53 immunohistochemical stain-
ing: the effect of fixation and variation in the meth-
ods of evaluation. Br J Cancer. 1994;69:26–31.
  63. Battifora  H,  Kopinski  M.  The  influence  of  protease
digestion and duration of fixation on the immunostain-
ing of keratins: a comparison of formalin and ethanol
fixation. J Histochem Cytochem. 1986;34:1095–100.

  64.  Elias JM, Gown AM, Nakamura RM, et al. Quality
control in immunohistochemistry: report of a work-
shop sponsored by the Biological Stain Commission.
Am J Clin Pathol. 1989;92:836–43.

  65. Werner M, Chott A, Fabiano A, Battifora H. Effect of
formalin  tissue  fixation  and  processing  on  immuno-
histochemistry. Am J Surg Pathol. 2000;24:1016–9.

  66.  Torlakovic EE, Riddell R, Banerjee D, et al. Canadian
Association  of  Pathologists-Associatio  canadienne
des  pathologists  National  Standards  Committee/
Immunohistochemistry:  best  practice  recommenda-
tions  for  standardization  of  immunohistochemistry
tests. Am J Clin Pathol. 2010;133:354–65.

  67.  Reynolds GJ. External quality assurance and assess-
immunocytochemistry.  Histopathology.

ment
in
1989;15:627–33.

  68.  Wick MR. Technologic anarchy? Am J Clin Pathol.

1989;91(Suppl):S1.

  69.  Swanson  PE.  HIERanarchy:  the  state  of  the  art  in
immunohistochemistry. Am J Clin Pathol. 1997;107:
139–40.

  70.  Seidal T, Balaton A, Battifora H. Interpretation and
quantification  of  immunostains.  Am  J  Surg  Pathol.
2001;25:1204–7.

  71.  Wick MR, Mills SE. Consensual interpretive guide-
lines  for  diagnostic  immunohistochemistry.  Am  J
Surg Pathol. 2001;26:1208–10.

  72.  Wick MR, Swanson PE. Targeted controls in clinical
immunohistochemistry: a useful approach to quality
assurance. Am J Clin Pathol. 2002;117:7–8.

  73.  Shi SR, Liu C, Pootrakul L, et al. Evaluation of the
value  of  frozen  tissue  sections  used  as  “gold  stan-
dards” for immunohistochemistry. Am J Clin Pathol.
2008;129:358–66.

  74.  Battifora H. The multitumor (sausage) tissue block:
novel  method  for  immunohistochemical  antibody
testing. Lab Invest. 1986;55:244–8.

  75.  Bubendorf L, Nocito A, Moch H, Sauter G. Tissue
microarray (TMA) technology: miniaturized pathol-
ogy  archives  for  high-throughput  in-situ  studies.
J Pathol. 2001;195:72–9.

  76.  Horvath  L,  Henshall  S.  The  application  of  tissue
microarrays to cancer research. Pathology. 2001;33:
125–9.

  77.  Rimm DL, Camp RL, Charette LA, Costa J, Olsen
DA, Reiss M. Tissue microarrays: a new technology
for  amplification  of  tissue  resources.  Cancer  J.
2001;7:24–31.

16  Evidence-Based Practices in Applied Immunohistochemistry

293

  78.  Wick MR. Quality assurance in diagnostic immuno-
histochemistry:  a  discipline  coming  of  age.  Am  J
Clin Pathol. 1989;92:844.

  79.  Friedland DJ, Go AS, Davoren JB, et al. Evidence-
based  medicine:  a  framework  for  clinical  practice.
Stamford: Appleton & Lange; 1998. p. 1–246.
  80.  Marchevsky AM, Wick MR. Evidence-based medi-
cine, medical decision-analysis, and pathology. Hum
Pathol. 2004;35:1179–88.

  81.  Da Silva L, Parry S, Reid L, et al. Aberrant expres-
sion  of  E-cadherin  in  lobular  carcinomas  of  the
breast. Am J Surg Pathol. 2008;32:773–83.

  82.  Tavassoli  FA,  Eusebi  V.  Tumor  of  the  mammary
glands. AFIP atlas of tumors series 4. Washington:
Armed Forces Institute of Pathology; 2009.

  83.  Ellis GL, Auclair PL. Tumors of the salivary glands.
AFIP atlas of tumors series 4. Washington:  Armed
Forces Institute of Pathology; 2008.

  84.  Fletcher  CDM,  Unni  KK,  Mertens  F,  editors.
Pathology and genetics. World Health Organization
classification of tumours. Tumours of soft tissue and
bone. Lyon: IARC Press; 2002.

  85.  Mills SE, Carter D, Greenson JK, Reuter VE, Stoer
MH.  Sternberg’s  diagnostic  surgical  pathology.  5th
ed.  Philadelphia:  Lippincott  Williams  &  Wilkins;
2009.

  86. Dabbs DJ. Diagnostic immunohistochemistry: thera-
nostic  and  genomic  applications,  expert  consults:
online and print. Philadelphia: W.B. Saunders; 2010.
  87.  Swerdlow  SH,  Campo  E,  Harris  NL,  et  al.  World
Health  Organization  classification  of  tumours  of
haematopoietic and lymphoid tissues. 4th ed. Lyon:
IARC Press; 2008.

  88.  Louis DN, Ohgaki H, Wiestlier OD, Cavenoo WK.
World Health Organization classification of tumours
of the central nervous system. 4th ed. Lyon: IARC
Press; 2007.

  89.  Montgomery  K.  How  doctors  think:  clinical  judg-
ment and the practice of medicine. Oxford: Oxford
University Press; 2005.

  90.  Downie RS. Clinical judgment: evidence in practice.

Oxford: Oxford University Press; 2000.

  91.  Cai  Y-C,  Banner  B,  Glickman  J,  Ooze  RD.
Cytokeratins 7 and 20 and thyroid transcription fac-
tor 1 can help distinguish pulmonary from gastroin-
testinal  carcinoid  and  pancreatic  endocrine  tumors.
Hum Pathol. 2001;32:1087–93.

  92.  Brambilla E, Travis WD, Colby TV, et al. The new
World  Health  Organization  classification  of  lung
tumours. Eur Respir J. 2001;18:1059–68.

  93.  Granberg  D,  Wilander  E,  Oberg  K,  Skogseid  B.
Prognostic markers in patients with typical bronchial
carcinoid  tumors.  J  Clin  Endocrinol  Metab.  2000;
85:3425–30.

  94.  Edge SB, Byrd DR, Compton CC, et al. AJCC can-
cer staging handbook from the AJCC cancer staging
manual. 7th ed. New York: Springer; 2010. p. 234.

  95.  Evans AJ. Alpha-metylacyl CoA racemase (P504S):
overview and potential uses in diagnostic pathology
as applied to prostate needle biopsies. J Clin Pathol.
2003;56:892–7.

  96.  Reis-Filho  JS,  Milanezi  F,  Amendoeira  I,  et  al.
Distribution of p63, a novel myoepithelial marker, in
fine-needle aspiration biopsies of the breast: an anal-
ysis of 82 samples. Cancer. 2003;99(3):172–9.
  97.  Saad RS, Liu Y, Han H, et al. Prognostic significant
of HER2/neu, p53 and vascular endothelial growth
factor expression in early stage conventional adeno-
carcinoma and bronchioloalveolar carcinoma of the
lung. Mod Pathol. 2004;17:1235–42.

  98.  Cagle PT, Brown RW, Lebovitz RM. p53 immunos-
taining  in  the  differentiation  of  reactive  processes
from malignancy in pleural biopsy specimens. Hum
Pathol. 1994;25:443–8.

  99.  Mukhopadhyay  N,  Zhang  S,  Katzenstein  AL.
Immunohistochemical  markers
in  diagnosis  of
 papillary  thyroid  carcinoma:  utility  of  HBME1
 combined with CK19 immunostaining. Mod Pathol.
2006;19(112):1631–7.

 100.  Westfall DE, Fan X, Marchevsky AM. Evidence-based
guidelines to optimize the selection of antibody panels
in  cytopathology:  pleural  effusions  with  malignant
 epithelioid cells. Diagn Cytopathol. 2010;38:9–14.
 101.  Marchevsky AM, Wick MR. Evidence-based guide-
lines for the utilization of immunostains in diagnos-
tic  pathology:  pulmonary  adenocarcinoma  versus
Immunohistochem  Mol
mesothelioma.  Appl
Morphol. 2007;15(2):140–4.

 102.  Rimm  DL.  What  brown  stains  cannot  do  for  you.

Nature Biotechnol. 2006;24:914–6.

 103.  Taylor CR, Levenson RM. Quantification of immu-
nohistochemistry – issues concerning methods, util-
ity, and semiquantitative assessment. Histopathology.
2006;49:411–24.

 104.  Ellis  CM,  Dyson  MJ,  Stephenson  TJ,  Maltby  EL.
HER-2 amplification status in breast cancer: a com-
parison between immunohistochemical staining and
fluorescence  in-situ  hybridization  using  manual
techniques. J Clin Pathol. 2005;58:710–4.

 105.  Cuadros M, Villegas R. Systematic review of HER-2
breast  cancer  testing.  Appl  Immunohistochem  Mol
Morphol. 2009;17:1–7.

 106.  Prives  C,  Hall  PA.  The  p53  pathway.  J  Pathol.

1999;187:112–26.

 107.  Hall PA, McCluggage WG. Assessing p53 in clinical
contexts: unlearned lessons and new perspectives. J
Pathol. 2006;208:1–6.

 108.  Gusterson BA, Hunter KD. Should we be surprised
at  the  paucity  of  response  to  EGFR  inhibitors?
Lancet Oncol. 2009;10:522–7.

 109.  Cregger  M,  Berger  AJ,  Rimm  DL.  Immuno-
histochemistry  and  quantitative  analysis  of  protein
expression.  Arch  Pathol  Lab  Med.  2006;130:
1026–30.

 110.  Fritz P, Wu X, Tuczek H, Multhaupt H, Schwarz mann
P. Quantitation in immunohistochemistry: a research
method  or  a  diagnostic  tool  in  surgical  pathology?
Pathologica. 1995;87:300–9.

 111.  Fritz  P,  Multhaupt  H,  Hoenes  J,  et  al.  Quantitative
immunohistochemistry: theoretical background and
its  application  in  biology  and  surgical  pathology.
Prog Histochem Cytochem. 1992;24:1–53.

294

M.R. Wick et al.

 112.  Bahr GF. Frontiers of quantitative cytochemistry: a
review of recent developments and potentials. Anal
Quant Cytol. 1979;1:1–19.

 113.  Anonymous.  Dynamic  range.  http://en.wikipedia.
org/wiki/dynamic-range. Accessed 7 Dec 2010.
 114.  Faratian  D,  Clyde  RG,  Crawford  JW,  Harrison  DJ.
Systems  pathology  –  taking  molecular  pathology
into  a  new  dimension.  Natl  Rev  Clin  Oncol.  2009;
6:455–64.

 115.  De Alava E. Molecular pathology in sarcomas. Clin

Transl Oncol. 2007;9:130–44.

 116.  He YD. Genomic approach to biomarker identifica-
tion and its recent applications. Cancer Biomarkers.
2006;2:103–33.

 117. McGuire WL. Breast cancer prognostic factors: eval-
uation guidelines. J Natl Cancer Inst. 1991;83:154–5.
 118.  Krug LM, Crapanzano JP, Azzoli CG, et al. Imatinib
mesylate lacks activity in small-cell lung carcinoma
expressing  c-kit  protein:  a  phase-II  clinical  trial.
Cancer. 2005;103:2128–31.

 119.  Bezwoda WR. c-erbB-2 expression and response to
treatment  in  metastatic  breast  cancer.  Med  Oncol.
2000;17:22–8.

 120. Rogers SJ, Box C, Chambers P, et al. Determinants of
response to epidermal growth factor receptor tyrosine
kinase inhibition in squamous cell carcinoma of the
head and neck. J Pathol. 2009;218:122–30.

 121.  Atkins  D,  Reiffen  KA,  Tegtmeier  CL,  Winther  H,
Bonato MS, Storkel S. Immunohistochemical detec-
tion  of  EGFR  in  paraffin-embedded  tumor  tissues:
variation in staining intensity due to choice of fixa-
tive and storage time of tissue sections. J Histochem
Cytochem. 2004;52:893–901.

 122.  Ponz-Sarvise  M,  Rodriguez  J,  Viudez  A,  et  al.
Epidermal  growth  factor  receptor  inhibitors  in  col-
orectal  cancer  treatment:  what’s  new?  World  J
Gastroenterol. 2007;13:5877–87.

 123.  Heist RS, Christiani D. EGFR-targeted therapies in
lung  cancer:  predictors  of  response  and  toxicity.
http://www.medscape.com/viewarticle/589343.
Accessed 7 Dec 2010.

 124.  Saltz  L.  Epidermal  growth  factor  receptor-negative
colorectal cancer: is there truly such an entity? Clin
Colorectal Cancer. 2005;5 Suppl 2:S98–100.

 125.  Mathieu  A,  Weynand  B,  Verbeken  E,  et  al.
Comparison  of  four  antibodies  for  immunohis-
tochemical  evaluation  of  epidermal  growth  factor
receptor  expression  in  non-small-cell  lung  cancer.
Lung Cancer. 2010;69:46–50.

 126.  Buffet  W,  Geboes  KP,  Dehertogh  G,  Geboes  K.
EGFR-immunohistochemistry  in  colorectal  cancer
and  non-small-cell  lung  cancer:  comparison  of  3
commercially-available  EGFR  antibodies.  Acta
Gastroenterol Belg. 2008;71:213–8.

 127.  Yamatodani  T,  Ekblad  L,  Kjellen  E,  Johnsson  A,
Mineta  H,  Wennerberg  J.  Epidermal  growth  factor
receptor  status  and  persistent  activation  of  Akt  and
p44/42 MAPK pathways correlate with the effect of
cetuximab  in  head  and  neck  and  colon  cancer  cell
lines. J Cancer Res Clin Oncol. 2009;135:395–402.

 128.  Khambata-Ford  S,  Harbison  CT,  Hart  LL,  et  al.
Analysis  of  potential  predictive  markers  of  cetux-
imab benefit in BMS099, a phase-III study of cetux-
imab  and  first-line  taxane/carboplatin  in  advanced
non-small-cell
J  Clin  Oncol.
2010;28:918–27.

cancer.

lung

 129.  Dacic  S,  Yousem  SA.  Molecular  testing  in  lung
 carcinoma:  quo  vadis?  Am  J  Clin  Pathol.  2010;
134:7–9.

 130.  Hirota  S,  Isozaki  K,  Moriyama  Y,  et  al.  Gain-of-
function mutations of c-kit in human gastrointestinal
stromal tumors. Science. 1998;279:577–80.

 131.  Sarlomo-Rikala  M,  Kovatich  AJ,  Barusevicius  A,
Miettinen M. CD117: a sensitive marker for gastro-
intestinal stromal tumors that is more specific than
CD34. Mod Pathol. 1998;11:728–34.

 132.  Tazawa K, Tsukada K, Makuuchi H, Tsutsumi Y. An
immunohistochemical and clinicopathological study
of  gastrointestinal  stromal  tumors.  Pathol  Int.
1999;49:786–98.

 133.  Wisniewski  D,  Lambek  CL,  Liu  C,  et  al.
Characterization  of  potent  inhibitors  of  the  bcr-abl
and the c-kit receptor tyrosine kinases. Cancer Res.
2002;62:4244–55.

 134.  Ben-Neriah  Y,  Daley  GQ,  Mes-Masson  AM,  Witte
ON, Baltimore D. The chronic myelogenous leuke-
mia-specific P210 protein is the product of the bcr/
abl hybrid gene. Science. 1986;233:212–4.

 135.  Gibson PC, Cooper K. CD117 (c-kit): a diverse pro-
tein with selective applications in surgical pathology.
Adv Anat Pathol. 2002;9:65–9.

 136.  De  Silva  CM,  Reid  R.  Gastrointestinal  stromal
tumors (GISTs): c-kit mutations, CD117 expression,
differential  diagnosis,  and  targeted  cancer  therapy
with imatinib. Pathol Oncol Res. 2003;9:13–9.
 137.  Knight III WA, Livingston RB, Gregory EJ, McGuire
WL. Estrogen receptor as an independent prognostic
factor for early recurrence in breast cancer. Cancer
Res. 1977;37:4669–71.

 138.  McGuire WL. Hormone receptors: their role in pre-
dicting prognosis and response to endocrine therapy.
Semin Oncol. 1978;5:428–33.

 139.  Hull III DF, Glark GM, Osborne CK, Chamness GC,
Knight  III  WA,  McGuire  WL.  Multiple  estrogen
receptor assays in human breast cancer. Cancer Res.
1983;43:413–6.

 140.  Barbanel  G,  Borgna  JL,  Bonnafous  JC,  Mani  JC.
Development of a microassay for estradiol receptors.
Eur J Biochem. 1977;80:411–23.

 141.  Bezwoda WR, Esser JD, Dansey R, Kessel I, Lange
M. The value of estrogen and progesterone receptor
determinations  in  advanced  breast  cancer:  estrogen
receptor  level  but  not  progesterone  receptor  level
correlates  with  response  to  tamoxifen.  Cancer.
1991;68:867–72.

 142.  Pertschuk  LP,  Tobin  EH,  Gaetjens  E,  et  al.
Histochemical  assay  of  estrogen  and  progesterone
receptors in breast cancer: correlation with biochem-
ical assays and patients’ response to endocrine thera-
pies. Cancer. 1980;46(Suppl):2896–901.

16  Evidence-Based Practices in Applied Immunohistochemistry

295

 143.  Seymour L, Meyer K, Esser J, MacPhail AP, Behr A,
Bezwoda WR. Estimation of PR and ER by immu-
nocytochemistry in breast cancer: comparison with
radioligand  binding  methods.  Am  J  Clin  Pathol.
1990;94(Suppl):S35–40.

 144.  Tesch  M,  Shawwa  A,  Henderson  R.  Immuno-
histochemical determination of estrogen and proges-
terone  receptor  status  in  breast  cancer.  Am  J  Clin
Pathol. 1993;99:8–12.

 145.  Nadji  M,  Gomez-Fernandez  C,  Ganjei-Azar  P,
Morales AR. Immunohistochemistry of estrogen and
progesterone  receptors  reconsidered:  experience
with  5,993  breast  cancers.  Am  J  Clin  Pathol.
2005;123:21–7.

 146.  Collins  LC,  Botero  ML,  Schnitt  SJ.  Bimodal  fre-
quency distribution of estrogen receptor immunohis-
tochemical  staining  results  in  breast  cancer:  an
analysis  of  825  cases.  Am  J  Clin  Pathol.
2005;123:16–20.

147.   Harvey  JM,  Clark  GM,  Osborne  CK,  Allred  DC.
Estrogen  receptor  status  by  immunohistochemistry
is superior to the ligand-binding assay for predicting
response  to  adjuvant  endocrine  therapy  in  breast
cancer. J Clin Oncol. 1999;17:1474–81.

 148.  Nadji  M.  Quantitative  immunohistochemistry  of
estrogen receptors in breast cancer: “much ado about
nothing!”.  Appl  Immunohistochem  Mol  Morphol.
2008;16:105–7.

 149.  Gomez-Fernandez C, Mejias A, Walker G, Nadji M.
Immunohistochemical expression of estrogen recep-
tor in adenocarcinomas of the lung: the antibody fac-
tor. Appl Immunohistochem Mol Morphol. 2010;18:
137–41.

 150.  Fuqua  SA,  Chamness  GC,  McGuire  WL.  Estrogen
receptor mutations in breast cancer. J Cell Biochem.
1993;51:135–9.

 151.  Murphy LC, Skliris GP, Rowan BG, et al. The rele-
vance of phosphorylated forms of estrogen receptor
in human breast cancer in vivo. J Steroid Biochem
Mol Biol. 2009;114:90–5.

 152.  McCabe A, Dolled-Filhart M, Camp RL, Rimm DL.
Automated  quantitative  analysis  (AQUA)  of  in-situ
protein expression, antibody concentration, and prog-
nosis. J Natl Cancer Inst U S A. 2005;97:1808–15.

 153.  Moeder  CB,  Giltnane  JM,  Moulis  SP,  Rimm  DL.
Quantitative,  fluorescence-based  in-situ  assessment
of  protein  expression.  Methods  Mol  Biol.
2009;520:163–75.

 154.  Harigopal  M,  Barlow  WE,  Tedeschi  G,  et  al.
Multiplexed assessment of the Southwest Oncology
Group-directed  Intergroup  Breast  Cancer  Trial
S9313 by AQUA shows that both high and low levels
of HER-2 are associated with poor outcome. Am J
Pathol. 2010;176:1639–47.

 155.  Steel  JH,  Poulson  R.  Making  sense  out  of  in-situ

PCR. J Pathol. 1997;182:11–2.

 156.  Lambros MB, Natrajan R, Geyer FC, et al. PPM1D
gene amplification and overexpression in breast can-
cer: a qRT-PCR and chromogenic in-situ hybridiza-
tion study. Mod Pathol. 2010;23:1334–45.

 157.  Ross  JS.  Multigene  classifiers,  prognostic  factors,
and  predictors  of  breast  cancer  clinical  outcome.
Adv Anat Pathol. 2009;16:204–15.

 158.  Mocellin  S,  Rossi  CR,  Pilati  P,  Nitti  D,  Marincola
FM. Quantitative real-time PCR: a powerful ally in
cancer research. Trends Mol Med. 2003;9:189–95.

 159.  Van de Vijver M. Emerging technologies for HER-2

testing. Oncology. 2002;63(Suppl):33–8.

 160.  Susini  T,  Bussani  C,  Marini  G,  et  al.  Preoperative
assessment of HER-2/neu status in breast carcinoma:
the role of quantitative real-time PCR on core-biopsy
specimens. Gynecol Oncol. 2010;116:234–9.
 161.  Wilcox JN. Fundamental principles of in-situ hybrid-

ization. J Histochem Cytochem. 1993;41:1725–33.

 162.  Naber SP, Tsutsumi Y, Yin S, et al. Strategies for the
analysis of oncogene overexpression: studies of the
neu oncogene in breast carcinoma. Am J Clin Pathol.
1990;94:125–36.

 163.  Ross JS. Saving lives with accurate HER-2 testing.

Am J Clin Pathol. 2010;134:183–4.

 164.  Scartozzi M, Bearzi I, Mandolesi A, et al. Epidermal
growth  factor  receptor  (EGFR)  gene  copy  number
(GCN) correlates with clinical activity of irinotecan-
cetuximab  in  k-ras  wild-type  colorectal  cancer:  a
fluorescence in-situ (FISH) and chromogenic in-situ
(CISH) analysis. BMC Cancer. 2009;9:303.

 165.  Guo QM. DNA microarrays and cancer. Curr Opin

Oncol. 2003;15:36–43.

 166. Churchill GA. Fundamentals of experimental design
for cDNA microarrays. Nat Genet. 2002;32(Suppl):
490–5.

 167.  Murphy D. Gene expression studies using microar-
rays:  principles,  problems,  and  prospects.  Adv
Physiol Educ. 2002;26:256–70.

 168.  Iida  K,  Nishimura  I.  Gene  expression  profiling  by
DNA  microarray  technology.  Crit  Rev  Oral  Biol
Med. 2002;13:35–50.

 169.  Ross JS, Mazumder A. Tissue microarrays and gene
chips. In: Wick MR, editor. Metastatic carcinomas of
unknown  origin.  New  York:  Demos  Publishing;
2008. p. 177–90.

 170.  Wu  W,  Noble  WS.  Genomic  data  visualization  on

the Web. Bioinformatics. 2004;20:1804–5.

 171. Roepman  P,  Horlings  HM,  Krijgsman  O,  et  al.
Microarray-based determination of estrogen receptor,
progesterone receptor, and HER-2 receptor status in
breast cancer. Clin Cancer Res. 2009;15:7003–11.
 172.  Idikio HA. Immunohistochemistry in diagnostic sur-
gical pathology: contributions of protein life-cycle,
use of evidence-based methods, and data normaliza-
immunohistochemical
tion  on
stains. Int J Clin Exp Pathol. 2010;3:169–76.

interpretation  of

 173.  Khandker  RK,  Dulski  JD,  Kilpatrick  JB,  Ellis  RP,
Mitchell JB, Baine WB. A decision model and cost-
effectiveness analysis of colorectal cancer screening
and  surveillance  guidelines  for  average-risk  adults.
Int J Technol Assess Health Care. 2000;16:799–810.
 174.  Sonnenberg  A,  Delco  F,  Inadomi  JM.  Cost-
effectiveness  of  colonoscopy  in  screening  for  col-
orectal cancer. Ann Intern Med. 2000;133:573–84.

Evidence-Based Pathology
and Laboratory Medicine
in the Molecular Pathology Era:
Transition of Tests from the
Research Bench into Practice

Jia-Perng Jennifer Wei and Wayne W. Grody

17

Keywords
Evidence-based pathology • Molecular pathology • Genomics in pathology
• ACCE test evaluation • Evidence levels analysis

As our knowledge of genomics expands, medical
practice  is  slowly  moving  toward  a  new  era  of
genomic  medicine,  with  promises  of  personal-
ized  care  and  disease  prevention  based  on
genomic  tools  and  technologies.  As  a  result,
molecular pathology has become fundamental to
almost  every  aspect  of  healthcare  delivery.
Molecular pathology, a rapidly evolving discipline
within  pathology,  incorporates  the  principles,
techniques,  and  tools  of  molecular  biology  into
diagnostic medicine in the clinical laboratory. To
provide additional information for various clini-
cal inquiries, molecular pathology integrates and
applies  knowledge  from  anatomic  and  clinical
pathology, molecular biology, biochemistry, pro-
teomics,  and  genetics.  Therefore,  it  is  strategi-
cally  positioned  at  the  interface  between  basic
science and medicine.

With  the  completion  of  the  human  genome
sequence  and  the  advent  of  the  “postgenomic
era,”  there  is  increasing  demand  to  translate
testing
genomic  knowledge

into  clinical

J.-P.J. Wei ()
Ambry Genetics, 100 Columbia #200,
Aliso Viejo, CA 92656, USA
e-mail: jwei@ambrygen.com

 applications  that  have  the  potential  to  improve
 healthcare.  However,  a  major  hurdle  in  this
 process is the fact that standards for evaluating
the clinical utility of a genetic test are not well
developed. For the majority of emerging genetic
tests, the goals of testing are often poorly defined
or  understood  due  to  uncertain  penetrance  of
causative mutations, prolonged time-lag between
diagnosis and onset of symptoms, lack of knowl-
edge about the natural history of newly discov-
ered or rare disorders, and absence of effective
therapeutic interventions. In addition, the under-
lying technologies are elaborate and constantly
evolving.  The  newer  whole-genome  technolo-
gies  produce  such  huge  masses  of  data  that
interpretation  of  test  results  becomes  highly
complex  and  time  consuming.  Distinguishing
between  novel  mutations  and  benign  sequence
variants is difficult and often highly speculative,
depending  upon  a  priori  assumptions  that  may
or  may  not  be  correct.  Most  importantly,  the
number and quality of studies addressing these
issues are limited. As a result, test applications
are  being  proposed  and  marketed  based  on
descriptive evidence and pathophysiologic rea-
soning,  without  the  appropriate  data  provided
by well-designed clinical trials or observational
studies to back them up [1].

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_17, © Springer Science+Business Media, LLC 2011

297

298

J.-P.J. Wei and W.W. Grody

Structured Approaches for Test
Evaluation: ACCE and EGAPP

To address the need for evidentiary standards in
genomic  medicine,  the  Centers  for  Disease
Control  and  Prevention  (CDC)  sponsored  the
ACCE  project  (http://www.cdc.gov/genomics/
gtesting/ACCE/acce_proj.htm#T1).  The  ACCE
acronym denotes the four aspects of evaluation:
Analytic  validity,  Clinical  validity,  Clinical  util-
ity,  and  associated  Ethical,  legal,  and  social
implications [2, 3]. The ACCE project strives to
establish  a  framework  for  assessing  data  on
emerging  genetic  tests.  To  refine  and  test  this
review process for applications of genomic tech-
nology  that  are  in  transition  from  research  to
clinical  practice,  the  National  Office  of  Public
Health Genomics (NOPHG) at the CDC launched
the  Evaluation  of  Genomic  Applications  in
Practice  and  Prevention  (EGAPP)  initiative  in
2004. The EGAPP effort has applied the ACCE
framework  to  five  genetic  testing  applications,
providing  evidence  reports  for  others  to  use  in
formulating recommendations [4–8]. The EGAPP
initiative continues to support timely and efficient
translation of genomic applications into clinical
practice by developing data collection, synthesis,
and review capacity. Since strict adherence to the
ACCE methodology is expensive and time con-
suming,  we  recommend  using  it  as  a  general
guide for decision-makers to appraise the readi-
ness of a new genetic test for transition from dis-
covery into clinical practice.

Formulation of the Central Question
that the Test Is Supposed to Answer

According to the ACCE framework, the problem
of  interest  needs  to  be  carefully  defined  before
the evidentiary review process. Depending on the
purpose of the genetic test, the problem of inter-
est may pertain to a specific medical disorder or a
desired outcome. In the case of a medical disor-
der,  it  should  be  defined  based  on  its  clinical
manifestations,  rather  than  the  laboratory  tests
employed for its detection. For pharmacogenomic
testing, the clinical problem relates to the  outcome

of interest. It may be a reduction of adverse drug
events,  treatment  optimization,  or  identification
of patients most likely to benefit from a specific
drug. The next step involves the characterization
of  test  properties.  It  may  entail  specifying  the
genetic  variant,  the  assay  chosen  to  detect  this
genetic variant, reliability of the assay,  consistency
from laboratory to laboratory, and the complexity
of test interpretation. Since the performance char-
acteristics of a given test may vary depending on
the  intended  use  of  the  test,  it  is  imperative  to
delineate  the  clinical  scenario.  Aspects  of  the
clinical scenario that need to be addressed include
the  clinical  setting  (e.g.,  primary  or  specialty
care;  presence  or  absence  of  pre-  and  posttest
genetic  counseling),  test  application  (e.g.,  diag-
nosis  or  screening),  and  test  subjects  (e.g.,  the
general  healthy  population,  selected  high-risk
individuals,  or  patients  who  are  already
symptomatic).

Systematic Literature Review
of Available Evidence

After  establishing  the  central  question  that  the
test is supposed to answer, one can proceed to a
systematic,  comprehensive  search  for  relevant
information  in  the  scientific  literature.  This
approach  is  considered  acceptable,  and  indeed
most often imperative, since it is recognized that
ascertainment  of  clinical  predictive  value  and
genotype-phenotype correlations for most molec-
ular tests is beyond the scope and capabilities of
any  one  laboratory  or  center  [9].  The  strategy
employed  in  the  systematic  literature  review  to
identify  relevant  papers  should  be  stipulated.
Prior to the evaluation process, data sources, cri-
teria for the inclusion or exclusion of a study, and
criteria for quality assessment of a study need to
be established as well.

Evaluation of the Quality of Available
Evidence: Evidence Levels

The  United  States  Preventive  Services  Task
Force’s Guide to Community Preventive Services
has  developed  a  sound  basis  for  evaluating  the

17  Evidence-Based Pathology and Laboratory Medicine in the Molecular Pathology Era

299

quality of relevant studies [10]. In general, stud-
ies are characterized in terms of their design and
execution.  Suitability  of  design  depends  on  the
degree  to  which  study  design  characteristics
affect  the  validity  of  the  results  of  a  study.  For
example,  prospective  studies  with  concurrent
comparison  groups  such  as  randomized  con-
trolled  trials  are  considered  of  superior  quality
relative to observational studies without concur-
rent  comparison  groups.  However,  for  many
genetic tests, especially those for rare disorders,
the  prevalence  of  a  specific  genotype  is  so  low
that randomized trials are not feasible, and obser-
vational  studies  may  provide  better  evidence.
Assessment of study execution for a genetic test
depends  on  several  features:  descriptions  of  the
study population, sampling of the study popula-
tion, measurement of genotype(s) and associated
phenotype(s), data analysis, interpretation of results,
and other confounding factors. If a study fails to
adequately address specific aspects of these char-
acteristics, it is considered a limitation.

Analytic Validity: Sensitivity,
Specificity, Quality Control,
and Assay Robustness

Subsequent  to  formulating  the  clinical  problem
and  collecting  the  available  best  evidence  from
the literature, assessment of the four ACCE com-
ponents  can  begin.  EGAPP  defines  the  analytic
validity of a genetic test as its ability to accurately
and reliably measure the genotype of interest [2].
Integral  elements  of  analytic  validity  include
analytic  sensitivity,  analytic  specificity,  quality
control, and assay robustness. Analytic sensitiv-
ity reflects the detection rate, and it is the proba-
bility  that  a  test  will  be  positive  when  a  target
DNA sequence is present. Alternatively, analytic
sensitivity can be defined as the limit of detection
of an assay, namely the lowest amount of target
sequence that can be detected in a specimen with
confidence. The likelihood that a test will be neg-
ative  in  the  absence  of  a  target  DNA  sequence
establishes  the  analytic  specificity.  Quality  con-
trol encompasses a set of procedures designed to
ensure the appropriate performance of a method
and the quality of the resulting data. To assess the

precision of a method within a laboratory, quality
control usually involves the inclusion of positive
and negative controls, reagent blanks, and dupli-
cates in analytical runs. Proficiency testing (PT)
is another essential component of quality assur-
ance.  For  example,  the  College  of  American
Pathologists (CAP) and the American College of
Medical Genetics (ACMG) jointly administer PT
programs for the more widely performed genetic
tests. These surveys provide information regard-
ing the consistency and accuracy of a specific test
among  laboratories.  Finally,  assay  robustness
examines  magnitude  of  changes  in  test  results
secondary  to  small  changes  in  preanalytic  and
analytic variables.

Since the technologies employed in molecular
diagnostic  testing  are  complex  and  constantly
evolving,  it  is  necessary  to  conduct  a  formal
evaluation of analytic validity. To accomplish this
task, a variety of data sources need to be used to
obtain  objective  and  reliable  information.  The
best information derives from collaborative stud-
ies using a large panel of well-characterized sam-
ples (both cases and controls) that are exchanged
between laboratories, blindly tested and reported,
with  the  results  independently  analyzed  [11].
Unfortunately,  such  optimal  studies  rarely  exist
for  any  genetic  test,  especially  for  a  rare
disease,  prior  to  its  introduction  into  clinical
practice.  Less  optimal  sources  of  data  include
well-designed method comparison and validation
studies, data from PT programs, and FDA sum-
maries  of  test  kits  or  reagents  that  have  been
reviewed and approved by that agency.

Evaluation of the Analytic Sensitivity
and Specificity of Different
Molecular Tests

Most genetic variants can be tested by a variety
of  protocols.  For  example,  accumulating  evi-
dence  indicates  that  specific  mutations  in  the
tyrosine  kinase  domain  of  EGFR  confer  an
improved  response  to  EGFR  inhibitors,  such  as
gefitinib and erlotinib, in patients with non-small
cell lung cancers [12]. The methodologies  utilized
to  detect  these  mutations  range  from  traditional
Sanger  sequencing  to  high-resolution  melting

300

J.-P.J. Wei and W.W. Grody

analysis  to  allele  specific  real-time  PCR  to
pyrosequencing. The test performance character-
istics may differ greatly depending on the instru-
ments and methodologies employed. The limit of
detection,  or  analytic  sensitivity,  for  traditional
Sanger sequencing is approximately 20%, since
minority nucleotide signals below that level (rel-
ative  to  the  major  nucleotide  at  that  position)
begin  to  blend  in  with  the  general  background
“noise” of the technique [13]. In practical terms,
this  translates  into  a  limit  of  detection  of  the
mutation  of  interest  in  a  specimen  no  less  than
20% of tumor cells carrying the specific mutation
in  a  background  of  wild  type  cells.  In  contrast,
utilizing  allele  specific  real-time  PCR,  the  ana-
lytic  sensitivity  may  approach  1%.  However,
there  is  a  paucity  of  studies  one  can  rely  on  to
determine the degree of impact (if any) on clini-
cal outcomes caused by these differences in ana-
lytic  sensitivity.  Needless  to  say,  this  dearth  of
published  studies  on  analytic  validity  limits  the
strength  of  conclusions  regarding  the  clinical
validity and utility of the test.

Clinical Performance Characteristics
of Molecular Tests: Sensitivity
and Predictive Value

The clinical validity of a genetic test establishes
its accuracy at predicting a phenotype of interest
or a clinical outcome. According to the ACCE
evaluation  process,  clinical  validity  builds  on
analytic  validity  by  examining  five  more  ele-
ments:  clinical  sensitivity,  clinical  specificity,
prevalence,  positive  and  negative  predictive
values,  and  penetrance  [2].  In  contrast  to  ana-
lytic  sensitivity,  where  the  goal  is  to  correctly
identify  a  genotype,  clinical  sensitivity  mea-
sures the proportion of individuals who have (or
will develop) the disorder of interest and whose
test results are positive; these results are consid-
ered true-positive (TP). If an individual with the
phenotype of interest renders a negative result,
it  is  considered  false-negative  (FN).  Thus,
 clinical  sensitivity  is  defined  as  the  number  of
TP  results  divided  by  the  sum  of  the  TP  and
FN  results  [TP/(TP + FN)].  Clinical  specificity

determines  the  proportion  of  subjects  with
 negative test values and who do not have (or will
not  develop)  the  phenotype;  these  results  are
considered true-negative (TN). If a subject lacks
the  phenotype  of  interest  but  yields  a  positive
result,  it  is  considered  a  false-positive  (FP)
result. Mathematically, clinical specificity is the
quotient between the number of TN results and
the sum of the TN and FP results [TN/(TN + FP)].
Prevalence  refers  to  the  number  of  individuals
within the specified testing population who have
(or  will  develop)  the  phenotype.  As  a  result,
prevalence  can  affect  the  positive  and  negative
predictive values of a molecular test. For exam-
ple, if a given mutation is extremely rare in the
tested population, there is a greater chance that
a positive test result may be due to a technical
(analytic) FP rather than a TP.

The clinical performance characteristics of a
molecular  test  are  intricately  related  to  one
another.  When  the  test  renders  a  clinically  FN
result,  it  is  usually  not  caused  by  laboratory
errors. Instead, it indicates the presence of other
causal factors that may contribute to the develop-
ment of the interested phenotype, in addition to
the  specific  mutation(s)  being  tested.  When  a
genetic test produces a FP result, there are two
possible  explanations.  The  positive  test  result
may be due to analytic error(s), or it may indi-
cate  incomplete  penetrance.  For  instance,  a
genetic  test  may  correctly  identify  individuals
homozygous for the C282Y mutation in the HFE
gene; however, they may never develop serious
clinical manifestations of iron overload in their
lifetime,  due  to  the  low  clinical  penetrance  of
HFE  mutations  [14].  In  addition  to  penetrance,
another relationship between genotype and phe-
notype needs to be considered. Sometimes, dif-
ferent mutations in the same gene cause different
phenotypic effects. In the DMD gene, one series
of  deletions  cause  Becker  muscular  dystrophy,
while other deletions in the same gene manifest
as  Duchenne  muscular  dystrophy.  Since  this
genotype  to  phenotype  relationship  (depending
on  whether  the  deletion  is  in-frame  or  out-of-
frame) is highly consistent, some clinicians use
this  information  for  prognostic  and  counseling
purposes [15].

17  Evidence-Based Pathology and Laboratory Medicine in the Molecular Pathology Era

301

Clinical Utility

Clinical utility of a genetic test refers to its ability
to  influence  health  outcomes  through  the  adop-
tion  of  therapeutic  or  preventive  interventions
based on test results. Both the risks and benefits
of a test’s introduction into clinical practice need
to be considered. The ACCE framework has for-
mulated  a  series  of  questions,  namely  questions
26  through  41,  to  facilitate  the  organization  of
information regarding clinical utility [2]. Of the
four main aspects in the ACCE evaluation process,
clinical utility may be considered the most com-
plex component to examine. To properly analyze
the clinical utility of a genetic test, one needs to
delineate the natural history of the specific clini-
cal disorder, potential risks and benefits, quality
assurance  of  test  performance,  and  associated
economic,  ethical,  legal,  social,  and  policy.
Accurate information concerning the natural his-
tory of a clinical disorder is important. If the dis-
order has serious health consequences, the typical
age of onset can be utilized to determine the opti-
mal  age  for  either  screening  or  early  diagnostic
testing.  Unfortunately,  however,  many  genetic
and  neoplastic  diseases  show  wide  variation  in
age of onset. It is important to evaluate the avail-
ability  and  effectiveness  of  interventions.  In  the
absence of effective interventions, other measur-
able effects, such as psychological and emotional
impact of the information provided by the testing
results  on  the  patients,  should  be  considered.
When balancing the pros and cons of implement-
ing a new DNA test, health risks need to be con-
sidered.  Health  risks  might  represent  morbidity
and mortality associated with subsequent proce-
dures for diagnosis or treatment. They might also
encompass less quantifiable risks, such as anxiety
and stigmatization. Economic evaluation (includ-
ing test cost, available CPT codes, insurance cov-
erage, etc.) and resource allocation should also be
included in the appraisal of clinical utility.

In  order  to  evaluate  the  clinical  utility  of  a
genetic test, it is necessary to examine the merit
and suitability of existing evidence. Similar to the
appraisal of analytic and clinical validity, impor-
tant characteristics that affect the quality of data
on  clinical  utility  include  the  size  and  selection

criteria  for  the  study  population,  the  type  of
 laboratory assay and interventions employed, and
the study design [16]. The quantity of data refers
to the number of studies and the number of total
subjects in the studies. A study that addresses the
clinical  utility  of  a  molecular  test  needs  to  pro-
vide a detailed description of the intervention and
the  context  in  which  the  intervention  was  con-
ducted.  The  quality  of  studies  depends  on  their
methodology  and  execution.  Randomized  con-
trolled trials, cohort, or case-control studies may
be employed to evaluate the impact of a molecu-
lar test on health outcomes. Of these study meth-
ods, randomized controlled trials are believed to
offer  the  most  reliable  evidence.  If  the  sample
size  is  adequate,  randomization  ensures  equal
distribution of both known and unsuspected con-
founding  factors.  For  instance,  cohort  studies
allow participants to select the desired therapeu-
tic  option,  and  this  choice  may  reflect  the  test
subjects’
introducing
 confounding  factors.  Blinding  of  participants,
providers,  and  investigators  further  minimizes
the  likelihood  of  placebo  effects  and  observa-
tional bias. As in studies of clinical validity, meta-
analysis of similar studies may be used to estimate
the overall consistency of clinical utility.

characteristics,

thus

Formal Assessment of the Clinical
Validity of Molecular Tests: Selection
of Best Available Evidence
and Meta-Analysis

To  conduct  a  formal  assessment  of  the  clinical
validity of a genetic test, it is imperative to criti-
cally appraise the quality and appropriateness of
available evidence. Important variables that influ-
ence  the  overall  quality  of  evidence  for  clinical
validity include the number and quality of studies,
the size and selection criteria for the study popu-
lation, the type of assay employed (as well as its
analytic validity), and the endpoints  measured [16].
The  quantity  of  data  pertains  to  the  number  of
studies  and  the  number  of  total  subjects  in  the
studies. The quality of studies is usually dictated
by  their  designs.  For  instance,  well-designed
 longitudinal  cohort  studies  usually  provide  the

302

J.-P.J. Wei and W.W. Grody

information necessary to evaluate the strength of
association between a genotype or biomarker and
a  specific  phenotype  or  disorder.  Furthermore,
meta-analysis of similar studies may be employed
to  estimate  the  overall  consistency  of  clinical
validity, and to compensate for the small size of
individual  studies.  Since  the  majority  of  genetic
tests  are  designed  to  detect  events  of  relatively
low  frequency,  longitudinal  cohort  studies  are
usually not feasible. Thus, case-control and cross-
sectional studies can serve as alternative sources
of evidence. For currently available genetic tests,
their  clinical  validity  may  remain  uncertain  and
evolve as evidence accumulates.

Pilot Studies

Often, there is limited or no available information
in  the  literature  regarding  the  potential  clinical
validity and applicability of a molecular test, and
pilot  studies  are  needed  to  collect  data.  Even
though  evidence  gathered  in  pilot  studies  is  not
sufficient  for  clinical  application,  pilot  studies
provide valuable information regarding the readi-
ness of a novel molecular test for transition from
the  lab  bench  to  routine  care.  They  subject  the
DNA  testing  process  to  the  daily  pressures  of
clinical testing, thus determining its analytic per-
formance characteristics under real-world condi-
tions. They offer the opportunity to observe and
document the test subjects’ responses to the test-
ing process. It was through just such a process of
pilot studies that the now-accepted universal cystic
fibrosis carrier screening program was developed
and  assessed  [17].  Additional  information  that
may be acquired in pilot trials includes patterns of
decision-making, economic information, and accep-
tance rates at various stages of the testing process.
Consequently,  pilot  studies  provide  the  foun-
dation necessary for subsequent clinical trials.

Ethical Issues

In  addition  to  providing  diagnostic,  prognostic,
and  therapeutic  information,  molecular  genetics
and oncology tests have ethical, legal, and social

implications. One of the greatest concerns about
genetic  testing,  especially  when  performed
presymptomatically,  involves  the  potential  for
insurance or employment discrimination, stigma-
tization, and long-term psychological harms from
testing. Unfortunately, these effects are difficult to
study. Since the beginning of the Human Genome
Project, genetic discrimination has been a concern
of  policy-makers,  legal  scholars,  and  patients  at
risk  for  genetic  disorders  [18].  In  an  attempt  to
prevent  genetic  discrimination  and  the  misuse  of
genetic  information  in  employment  and  health
insurance,
Information  Nondis-
crimination Act (GINA) was finally passed by the
U.S.  Congress  and  signed  into  law  by  President
G.W. Bush on May 21, 2008 [19]. However, it may
take several years before the impact of GINA on
the  incidence  of  reported  genetic  discrimination
can be properly evaluated.

the  Genetic

Even  if  the  results  of  genetic  testing  do  not
affect clinical management or lead to a measur-
able  effect  on  health,  genetic  information  can
help individual and family decision-making. For
highly penetrant, single-gene disorders that lack
effective  therapy,  genetic  information  provides
assistance  to  inform  reproductive  or  other  life
decisions.  For  example,  testing  for  Huntington
disease cannot alter the course of this lethal con-
dition, but it allows mutation carriers and noncar-
riers to prepare for the future with that prognosis
in  mind.  For  complex  multifactorial  illnesses,
genetic  testing  provides  information  regarding
association between genotypic variations and risk
of disease. Even though predictive genetic testing
can identify individuals at increased risk, it may
also cause increased distress and anxiety. Several
studies  have  examined  the  effects  of  BRCA1/2
testing  on  individuals  and  their  families.  The
majority  of  studies  have  reported  no  significant
change in psychological outcomes among asymp-
tomatic mutation carriers relative to baseline [20].
However, there appear to be short-term increases
in  anxiety  among  asymptomatic  mutation  carri-
ers  [21].  It  is  also  important  to  understand  the
factors  that  determine  interest  in  predictive
genetic testing. The usefulness and personal value
attached  to  knowledge  about  genetic  disease  or
cancer  risk  may  vary  by  age  or  other  personal

17  Evidence-Based Pathology and Laboratory Medicine in the Molecular Pathology Era

303

characteristics. For example, the implications of
a positive test for a BRCA1 or BRCA2 mutation
differ considerably for a woman of child-bearing
age  compared  with  a  perimenopausal  woman,
because  oophorectomy  is  an  important  preven-
tion  option  for  such  women.  In  addition,  some
testing decisions may be motivated by the desire
to  help  offspring.  A  female  patient  with  cancer
may be more interested in BRCA1/2 testing if she
has daughters who might in turn benefit from the
information in terms of their own inherited risk
and  by  making  presymptomatic  testing  in  them
easier and less expensive since the family’s BRCA
mutation will then be known.

Real-World Considerations

The  experience  of  predictive  testing  for  muta-
tions in the BRCA1 and BRCA2 genes is instruc-
tive for our more general discussion of transition
of research assays to clinical tests. When those
two genes were first discovered in the mid-1990s
and  their  penetrance  shown  to  be  appreciably
less  than  100%,  there  were  some  medical  ethi-
cists  and  others  who  argued  that  the  unknowns
were too great to justify clinical mutation testing
at  that  point.  Yet,  it  is  only  by  embarking  on
widespread testing, even before all the answers
were in, that we were able to further refine the
genotype–phenotype  correlations,  predictive
value, and clinical penetrance of these mutations.
Similarly,  testing  for  K-ras  mutations  in  colon
cancers,  while  only  modestly  predictive  of
response to anti-EGFR inhibitor therapies, soon
led  to  the  revelation  that  mutation  in  another
gene  involved  in  the  same  signaling  pathway,
BRAF,  could  help  explain  a  proportion  of  the
K-ras-negative nonresponders. It can be expected
that  continued  testing  in  the  actual  clinical
setting  will  reveal  other  genes  and  mutations
that will steadily raise the predictive value of the
molecular  tests.  Yet  another  example  is  the
almost overnight and universal adoption of array-
comparative  genomic  hybridization  in  place  of
standard karyotype analysis for diagnostic work-up
of  patients  with  nonspecific  developmental
delay, autism, or  congenital malformations [22].

Table 17.1  Critical parameters for determining  transition
of a research molecular test to a clinical test

Analytic validity
Clinical validity
Clinical utility
Literature review
Meta-analyses
ACCE approach
EGAPP recommendations
Pilot studies
Randomized controlled trials
Professional practice guidelines
Cost and reimbursement policies
Ethical and psychosocial considerations

Although  the  approach  suffers  from  the  uncer-
tainty  produced  by  the  large  number  of  novel
deletions  and  duplications  revealed  in  every
human  genome,  it  is  only  through  continued
clinical  testing,  with  reporting  of  such  findings
in a centralized database, that the true genotype–
phenotype relationships will become known and
established.  Thus,  while  we  have  attempted  in
this chapter to delineate the various parameters
and approaches that should be followed in order
to  make  the  determination  of  when  a  research
test  is  ready  for  transition  to  a  clinical  test
(Table 17.1), it is also important to allow some
latitude  during  the  phase  at  which  the  biology
and  molecular  pathology  of  these  disease  pro-
cesses are still being worked out. And that notion
will be even more true in the coming years as we
move  beyond  single-gene  molecular  tests  into
whole-genome  arrays  and  next-generation
whole-genome sequencing [23].

Conclusions

Molecular  pathology  presents  a  particularly
 difficult challenge to the systematic methodol-
ogy proposed by evidence-based pathology for
the  gathering  of  evidence  and  classification  of
such  evidence  using  various  evidence  level
schemes.  The  rapid  developments  in  the  field,
complexity of molecular tests, massive quantity
of data accrued, and the almost infinite number

304

J.-P.J. Wei and W.W. Grody

of   analytes  addressed  by  these  new  technolo-
gies render many of the requirements delineated
in this chapter – for positive mutation controls,
clinical validation, etc. – almost moot. Clearly
the genie is out of the bottle, and we have little
choice but to move forward thoughtfully, incor-
porating  the  approaches  we  have  described
when possible, but not adhering to them so rig-
idly  that  patients  are  deprived  for  too  long  of
their  access  to  these  potentially  life-saving
technologies.

References

  1.  Khoury MJ, Berg A, Coates R, Evans J, Teutsch SM,
Bradley  LA.  The  evidence  dilemma  in  genomic
medicine.  Health  Aff  (Millwood).  2008;27(6):
1600–11.

  2.  Haddow  JE,  Palomaki  GE.  ACCE:  a  model  process
for  evaluating  data  on  emerging  genetic  tests.  In:
Khoury  MJ,  Little  J,  Burke  W,  editors.  Human
genome  epidemiology:  a  scientific  foundation  for
using genetic information to improve health and pre-
vent disease. Oxford: Oxford University Press; 2004.
p. 217–33.

  3.  Eagle A. Seal of approval ACCE rolls out a new certi-
fication for clinical engineers. Health Facil Manage.
2004;17(5):30–2, 34.

  4.  Palomaki GE, Haddow JE, Bradley LA, FitzSimmons
SC.  Updated  assessment  of  cystic  fibrosis  mutation
frequencies in non-Hispanic Caucasians. Genet Med.
2002;4(2):90–4.

  5.  Palomaki GE, Bradley LA, Richards CS, Haddow JE.
Analytic validity of cystic fibrosis testing: a prelimi-
nary estimate. Genet Med. 2003;5(1):15–20.

  6.  Palomaki GE, Haddow JE, Bradley LA, Richards CS,
Stenzel TT, Grody WW. Estimated analytic validity of
HFE C282Y mutation testing in population screening:
the  potential  value  of  confirmatory  testing.  Genet
Med. 2003;5(6):440–3.

  7.  Gudgeon JM, McClain MR, Palomaki GE, Williams
MS. Rapid ACCE: experience with a rapid and struc-
tured  approach  for  evaluating  gene-based  testing.
Genet Med. 2007;9(7):473–8.

  8.  McClain  MR,  Palomaki  GE,  Piper  M,  Haddow  JE.
A  rapid-ACCE  review  of  CYP2C9  and  VKORC1
alleles testing to inform warfarin dosing in adults at
elevated  risk  for  thrombotic  events  to  avoid  serious
bleeding. Genet Med. 2008;10(2):89–98.

  9.  Maddalena A, Bale S, Das S, Grody W, Richards S.
Technical standards and guidelines: molecular genetic

testing  for  ultra-rare  disorders.  Genet  Med.  2005;
7(8):571–83.

 10. Briss  PA,  Brownson  RC,  Fielding  JE,  Zaza  S.
Developing  and  using  the  guide  to  community
preventive  services:  lessons  learned  about  evidence-
based  public  health.  Annu  Rev  Public  Health.
2004;25:281–302.

 11. Teutsch  SM,  Bradley  LA,  Palomaki  GE,  et  al.  The
Evaluation  of  Genomic  Applications  in  Practice  and
Prevention (EGAPP) initiative: methods of the EGAPP
Working Group. Genet Med. 2009;11(1):3–14.

 12. Sequist LV, Lynch TJ. EGFR tyrosine kinase inhibi-
tors in lung cancer: an evolving story. Annu Rev Med.
2008;59:429–42.

 13. Whitehall  V,  Tran  K,  Umapathy  A,  et  al.  A  multi-
center blinded study to evaluate KRAS mutation test-
ing methodologies in the clinical setting. J Mol Diagn.
2009;11(6):543–52.

 14. Rossi  E,  Jeffrey  GP.  Clinical  penetrance  of  C282Y
homozygous HFE haemochromatosis. Clin Biochem
Rev. 2004;25(3):183–90.

 15. Bushby  KM.  Genetic  and  clinical  correlations  of
Xp21  muscular  dystrophy.  J  Inherit  Metab  Dis.
1992;15(4):551–64.

 16. Little J, Bradley L, Bray MS, et al. Reporting, apprais-
ing, and integrating data on genotype prevalence and
gene-disease  associations.  Am  J  Epidemiol.  2002;
156(4):300–10.

 17. Richards CS, Grody WW. Prenatal screening for cys-
tic fibrosis: past, present and future. Expert Rev Mol
Diagn. 2004;4(1):49–62.

 18. Billings  PR,  Kohn  MA,  de  Cuevas  M,  Beckwith  J,
Alper JS, Natowicz MR. Discrimination as a conse-
quence  of  genetic  testing.  Am  J  Hum  Genet.  1992;
50(3):476–82.

 19. Erwin  C.  Legal  update:  living  with  the  Genetic
Information  Nondiscrimination  Act.  Genet  Med.
2008;10(12):869–73.

 20. Schlich-Bakker  KJ,  Ausems  MG,  Schipper  M,
Ten  Kroode  HF,  Warlam-Rodenhuis  CC,  van  den
Bout  J.  BRCA1/2  mutation  testing  in  breast  cancer
patients:  a  prospective  study  of
long-term
psychological  impact  of  approach  during  adjuvant
radiotherapy. Breast Cancer Res Treat. 2008;109(3):
507–14.

the

 21. Meiser B. Psychological impact of genetic testing for
cancer  susceptibility:  an  update  of  the  literature.
Psychooncology. 2005;14(12):1060–74.

 22. Miller  DT,  Adam  MP,  Aradhya  S,  et  al.  Consensus
statement: chromosomal microarray is a first-tier clin-
ical diagnostic test for individuals with developmental
disabilities or congenital anomalies. Am J Hum Genet.
2010;86(5):749–64.

 23. ten Bosch JR, Grody WW. Keeping up with the next
generation: massively parallel sequencing in clinical
diagnostics. J Mol Diagn. 2008;10(6):484–92.

The Use of Decision Analysis Tools
for the Selection of Clinical
Laboratory Tests: Developing
Diagnostic and Forecasting Models
Using Laboratory Evidence

18

Ji Yeon Kim, Elizabeth M. Van Cott, and Kent B. Lewandrowski

Keywords
Decision support • Medical order entry systems • Laboratory utilization
• Evidence-based medicine

Archie Cochrane in his seminal book Effectiveness
and Efficiency (1972) argued that “health services
should be evaluated on the basis of scientific evi-
dence rather than on clinical impression, anecdotal
experience, ‘expert’ opinion or tradition” [1]. This
tenet  of  evidence-based  medicine  (EBM)  [2,  3]
has resonated strongly in the ethos of contempo-
rary  practice,  fueling  growth  in  the  number  of
clinical guidelines and changes in healthcare pol-
icy  and  financing.  Further  driving  the  EBM-
movement are preventable adverse events related
to  medical  errors,  now  recognized  as  a  cause  of
more deaths than those from breast cancer or motor
vehicle  accidents  [4–6].  Recently,  the  American
Recovery and Reinvestment Act of 2009 (ARRA)
allocated approximately $19 billion to promote the
adoption of electronic health records (EHR), with
the idea that such technology can make health care
more evidence-based and less error-prone [7, 8].

Despite public enthusiasm for EBM, there has
been relatively little change in physician behavior,
and in fact, the data shows most physicians have a

K.B. Lewandrowski ()
Department of Pathology, Massachusetts General
Hospital and Harvard Medical School,
Gray 5–536 Chemistry Massachusetts General Hospital,
55 Fruit Street, Boston, MA 02114, USA
e-mail: Klewandrowski@partners.org

difficult time following guidelines [9]. Supporting
these observations is the striking regional variation
in  the  use  of  healthcare  resources,  measured  by
rates of physician visits, hospitalizations,  specialist
referrals,  laboratory  testing,  and  interventions,
which does not correlate with improved quality or
access to healthcare, better outcomes, or increased
patient satisfaction [10, 11]. In fact, for some mea-
sures  related  to  health  prevention,  such  as  influ-
enza  vaccination  rates,  increased  spending  is
associated with worse care [10]. This is concern-
ing given that national health spending in the U.S.
reached $2.3 trillion in 2008, or 16.2% of the gross
domestic product [12].

Laboratory services are particularly vulnerable
to  potential  misuse  and  overuse  [13,  14].  Use  of
laboratory services can be inflated by public expec-
tations for frequent testing [15] and the practice of
“defensive medicine” [16, 17]. Meanwhile, labora-
tory tests are subject to systemic and random errors,
and a “shot-gun” approach to testing increases the
potential for false-positive and false-negative results
[18].  Operational  efficiency  in  the  laboratory  and
 clinical areas can be adversely affected by higher
testing volumes from inappropriate and unnecessary
orders, compromising turnaround times for labora-
tory tests with clinical urgency [19]. Downstream,
this  can  directly  impact  the  length  of  stay  for
patients,  as  in  the  emergency  department  [20].

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_18, © Springer Science+Business Media, LLC 2011

305

306

J.Y. Kim et al.

In terms of financial impact, it has been estimated
that eliminating redundant  laboratory tests alone
would save about $8  billion a year in the U.S. [21],
and the burden of ensuring medical necessity for
testing  is  gradually  being  shifted  to  the  clinical
laboratories themselves [22].

With  technological  advances  in  laboratory
automation and instrumentation greatly reducing
analytical errors, more errors now take place out-
side the laboratory in both test ordering and result
interpretation  [23–25],  with  the  majority  made
before  the  patient  specimen  reaches  the  labora-
tory [26, 27]. At the same time, the information-
oriented  agenda  of  EBM  has  given  pathology
data an increasingly central role in initiating and
coordinating patient care, from diagnosis to treat-
ment  decisions  to  disease  monitoring.  It  is
believed that more than half of all medical deci-
sions are influenced by laboratory data [28, 29],
with  one  study  demonstrating  that  94%  of
requests to the electronic medical record were for
laboratory  results  alone  [30].  Thus,  diagnostic
errors are of particular concern to both physicians
and patients, which is highlighted by the fact that
diagnostic errors are the most common reason for
medical malpractice claims [31–33].

It has been observed that many physicians have
testing- and diagnosis-related questions as they see
patients, but are unable to find answers because of
lack of time and poor organization of information
sources  [34,  35].  Hayward  has  said  that  “physi-
cians suffer from information hunger in the midst
of plenty” [36], which rings particularly true in our
internet-based era [37], where online tools such as
PubMed currently hosts more than 19 million cita-
tions  for  the  biomedical  literature  (http://www.
ncbi.nlm.nih.gov/pubmed;  last  accessed  12  May
2010). A meta-analysis of various studies has found
that diagnosis-related questions comprise, on aver-
age, 24% of information need, and another 49% are
related to therapy and drug information [38].

In several areas, diagnostic tests are becoming
synonymous with targeted therapeutics, inspiring
the  term  “theragnostics”  [39].  For  example,  the
Food  and  Drug  Administration  (FDA)  has  rec-
ommended that maraviroc, part of a new class of
HIV/AIDS drugs that are chemokine coreceptor
5 (CCR5) antagonists, only be used after testing

confirms  that  a  patient  is  infected  with  a
CCR5-tropic  viral  strain  [40,  41].  In  cancer,
 testing for newly identified molecular drivers for
lung adenocarcinomas, colorecetal cancer, breast
cancer,  and  oligodendrogliomas,  among  others,
has been critical for establishing patient eligibil-
ity  for  targeted  chemotherapy,  and  for  guiding
patient management [42–45].

As  part  of  this  trend,  the  number  of  tests  in
molecular diagnostics is rapidly expanding, particu-
larly  for  genetic  diseases,  infectious  diseases,  and
cancer.  In  our  institution,  for  tumor  diagnostics
alone, we currently have 17 distinct assays which
cover gene mutations, DNA methylation alterations,
microsatellite instability, and diagnostic and prog-
nostic FISH assays, and we are planning on adding
five new tests over the next 6 months. In the coagu-
lation laboratory, we have added four new tests over
the past 4 months, for a current total of 52 different
tests. Our current main reference laboratory catalog
lists  approximately  7,140  different  tests.  Both  the
rate of test menu growth and the sheer number of
test options present challenges for clinicians.

Although most specialists are able to stay rea-
sonably current in their narrow area of expertise,
the  situation  for  the  typical  general  internist  or
surgeon  is  becoming  increasingly  untenable.
Invariably,  this  leads  to  an  increase  in  subspe-
cialty consults, with a resulting increase in cost.
But these days, even specialists may find it diffi-
cult  to  select  or  interpret  the  correct  test.
Unperceived  or  unexpressed  information  needs
are difficult to identify [46], but some inappropri-
ate test requests may be due to unacknowledged
knowledge gaps. In one example of a routine test,
25-hydroxyvitamin  D  is  the  best  test  to  assess
vitamin D status under most clinical situations, as
opposed  to  1-25  dihydroxyvitamin  D.  At  our
institution, we noticed that displaying test selec-
tion  guidelines  for  vitamin  D  in  a  “pop-up
reminder” every time a physician requested 1-25
dihydroxyvitamin  D  caused  a  71%  reduction  in
1,25-(OH)2  vitamin  D  orders  [47].  In  virtually
every case where the 1,25-(OH)2 vitamin D was
not ordered, the user ordered the more appropri-
ate 25-OH vitamin D test.

Laboratory  knowledge  is  specialized  and
local, and can be difficult to acquire. A laboratory’s

18  The Use of Decision Analysis Tools for the Selection of Clinical Laboratory Tests

307

unique menu of tests and policies may differ from
other laboratories. This information may not be
readily  accessible  on  global  search  engines  or
online  references,  and  is  most  often  maintained
internally  by  the  laboratory  itself.  Thus,  local
pathologists have an opportunity to play a more
proactive role in the total testing process [48].

laboratory

The  entire  process  of  laboratory  testing,
starting with the decision to order a test, should
be evidence-based, and ideally, cost-conscious
[49,  50].  There  is  a  need  for  decision  support
tools  to  aid  physicians  in  the  selection  and
interpretation  of
tests.  Clinical
pathologists  must  also  know  how  to  evaluate
the  clinical  context  and  usefulness  of  tests  in
order  to  make  recommendations  to  hospital
administrators  and  clinicians  about  adding,
replacing,  or  eliminating  tests  from  the  test
menu, as well as guiding what defines appropri-
ate  testing  for  specific  clinical  situations.
Pathologists must also consider whether or not
studies were carried out in the appropriate pop-
ulations suspected of the target disease, and not
just  those  with  obvious  disease  compared  to
healthy  controls  [51].  With  these  consider-
ations, a number of statistical tools exist to help
evaluate  and  compare  test   performance,  and
some of these will be discussed briefly below.

Statistics

The  assumption  behind  EBM  is  that  there  are
clinically meaningful subgroups of patients, fol-
lowing a similar disease progression and sharing
comparable  risks  for  morbidity  and  mortality.
Identifying a patient as belonging to one of these
subgroups  allows  the  clinician  to  extrapolate
treatment  and  management  decisions  from  the
published  literature.  The  ability  of  a  particular
test  to  successfully  distinguish  subjects  in  the
appropriate diagnostic and prognostic subcatego-
ries can be quantified in a number of ways.

With a binary test result (i.e., positive or nega-
tive), a 2-by-2 table comparing the test results to
true disease status via an independent gold stan-
dard (i.e., biopsy) can be used to assess test accu-
racy (Fig. 18.1). Accuracy can be represented as
diagnostic  sensitivity  (probability  of  a  positive
test, given the patient has the disease) and diag-
nostic specificity (probability of a negative test,
given the patient does not have the disease) [52].
The  likelihood  ratio  (LR)  combines  these  two
measures, and represents the probability of a test
result in the presence vs. absence of disease. For
example, the LR for a positive test result (LR(+))
compares the sensitivity, or probability of a posi-
tive test result in a disease-positive  population, to

Fig. 18.1  Effect of disease prevalence on test performance

308

J.Y. Kim et al.

the false-positive rate, or probability  of a positive
test  result  in  a  disease-negative   population.  A
LR(+)  of  four  means  that  the  positive  test  is  4
times as likely to occur in patients with disease as
in patients without disease, which is not the same
as saying that patients with  disease are 4 times as
likely  to  have  a  positive  test  result  when  com-
pared to those without disease.

Usually the question is whether or not the patient
has the disease, and not about the likelihood of a
particular test result. Bayes’ theorem allows us to
make  a  statement  about  the  inverse  conditional
probability, in this case, the probability of disease
given  a  test  finding  (see  Fig.  18.2).  This  can  be
reported  as  the  positive  predictive  value  (PPV;
probability of disease, given a positive test result)
and the negative predictive value (NPV; probabil-

ity of no disease, given a negative test result), and
calculated from the same 2-by-2 table, as long as
the  total  numbers  are  adjusted  to  reflect  the
 preexisting  disease  prevalence  in  the  population
(see Fig. 18.1). Both the PPV and NPV are affected
by  disease  prevalence,  whereas  sensitivity  and
specificity remain independent of prevalence.

Bayes’  theorem  can  be  used  to  evaluate  the
clinical utility of a new test. For example, a result
of  >13  pg/ml  for  high-sensitivity
troponin
T (hsTnT) has been reported as a superior marker
for acute coronary syndromes (ACS) when com-
pared to a result of >0.03 ng/ml by conventional
cardiac  troponin  T  (cTnT),  based  on  improved
sensitivity (from 35 to 62%) and a small increase
in  the  area  under  the  ROC  curve  (see  below),
although the latter was statistically insignificant

Fig. 18.2  Application of Bayes’ theorem. P(X) means probability of event X. P(X|Y) means probability of event X,
given event Y

18  The Use of Decision Analysis Tools for the Selection of Clinical Laboratory Tests

309

and the improvement in sensitivity was at the cost
of  decreased  specificity  (from  99  to  89%)  [53].
When using Bayes’ theorem to look at the predic-
tive  power  of  a  positive  test  for  ACS,  it  was
 demonstrated that even very high values of hsTnT
do not establish a diagnosis of ACS if the pretest
probability is low [54]. In fact, with a pretest ACS
probability of 10%, a positive cTnT is better for
predicting  ACS  (PPV  of  80%)  than  a  positive
hsTnT (PPV of 39%).

Even  so,  many  test  evaluations  are  reported
using  sensitivity  and  specificity  assessments,  as
predictive  value  estimates  require  prior  knowl-
edge  of  disease  prevalence  and  outcomes  data,
which may be difficult to obtain for target patient
populations. At the same time, because many tests
have  results  that  are  on  a  continuous  spectrum,
they  can  exhibit  all  sensitivity  and  specificity
 values (from 0 to 100%) depending on the partic-
ular value chosen to represent the “positive” result
cutoff (e.g., hsTnT of >13, >14, >15 pg/ml, etc.).
Receiver-operating characteristic (ROC) plots are
often used to provide a more global view of test
performance,  as  these  plots  depict  all  possible
sensitivity and specificity pairs for a test [55]. The
area under the curve (AUC), also called the c-sta-
tistic or c index, can range from 0.5 (no discrimi-
natory ability) to 1.0 (perfect discrimination), and
is  a  summary  measure  that  can  also  be  used  to
compare  whole  ROC  curves  to  one  another.  An
alternative is to restrict the area comparisons to a
relevant portion of the curve at a desired sensitiv-
ity or specificity. The tangent to the ROC curve is
equivalent to the LR(+), when that particular test
result (with its given sensitivity and specificity) is
chosen as the decision threshold.

Given that multiple test values are represented in
a  ROC  plot,  a  decision  threshold  must  be  chosen
that  incorporates  considerations  about  the  relative
costs of false-positive and false-negative results, as
well as the prevalence of disease in the population
being tested. A simplified approach is to calculate a
slope  using  the  following  equation:  m = (false-
positive  cost/false-negative  cost) × ((1  –  preva-
lence)/prevalence), and then choose the point on the
ROC plot where the tangent has this slope [55].

In  practice,  calculating  the  true  “cost”  of
false-positive and false-negative results requires

thoughtful conversations between the laboratory
and clinicians, and thresholds may change over
time. In our hospital, setting notification alarms
for critical laboratory values is one such exam-
ple of this ongoing process, which is negotiated
through  hospital  committee  meetings  consist-
ing of representatives from the laboratories, cli-
nicians, and hospital administration. Performing
callbacks  for  critical  lab  values  are  labor-  and
time-intensive for the laboratory, and should be
limited to those values that are truly dangerous.
Using  the  published  literature,  consultations
with  clinicians,  and  our  own  internal  data  on
the volume of critical callbacks for each analyte
per result value, we made the case for changing
the lower limit for glucose callbacks from less
than 60 mg/dl (<3.3 mmol/l) to less than 45 mg/
dl  (<2.5  mmol/l),  which  has  resulted  in  2,136
fewer calls per year (reduction of 5.7% for all
callbacks) [56].

As  previously  mentioned,  ROC  curves  clas-
sify patients by their likelihood of having a posi-
tive  test  result,  but  prognostic  models  evaluate
tests for their ability to predict future risk of dis-
ease,  and  may  be  of  greater  interest  to  patients
and clinicians [57]. Calibration examines the pre-
dictive  value  of  a  test  result,  and  compares  the
observed  vs.  predicted  probabilities  of  disease
within predetermined subgroups of patients shar-
ing similar risks of disease. Risk stratification is
initially  performed  using  an  existing  disease
model  based  on  traditional  assessment  factors,
and is compared to a revised stratification scheme
including the independent test result. The percent
of  reclassified  patients  after  the  addition  of  the
new  test  can  be  used  as  an  indicator  of  clinical
impact.  As  a  result,  disease  prevalence  and  the
way subgroups are modeled have a major effect
on  assessments  of  calibration.  The  Hosmer-
Lemeshow  test  [58]  and  the  net  reclassification
index  (NRI)  [59]  are  two  examples  of  formal
calibration measures.

The ROC curve and the AUC are less sensitive
than the NRI when evaluating the addition of new
tests/predictors  for  disease.  In  particular,  the
impact on AUC by a new test is blunted when the
preexisting  model  already  strongly  predicts  dis-
ease, even if the test is independent from the other

310

J.Y. Kim et al.

predictor  variables  [57].  For  example,  among
patients with ACS, early stratification helps assign
high-risk  patients  to  more  aggressive  and  costly
therapies. Patients can be assessed using the clini-
cally  robust  GRACE  (Global  Registry  of  Acute
Coronary Events) risk scores [60] to evaluate their
risk for mortality and acute myocardial infarction
(AMI)  events.  NT-proBNP  (N-terminal  pro-B-
type natriuretic peptide) has been identified as a
biomarker that is useful in patients with AMI [61],
and also seems to provide prognostic information
for short- and long-term mortality and future MI
[62–64]. Investigators examined the effect of add-
ing admission NT-proBNP levels to the GRACE
risk score in predicting early and late deaths fol-
lowing ACS [65]. The AUC for 30-day mortality
was  0.79  for  NT-proBNP,  0.84  for  the  GRACE
risk score, and 0.85 for the combination. The dif-
ference between AUC for the combination of pre-
dictors  vs.  the  GRACE  score  alone  was  not
statistically different (p = 0.20), but the impact on
NRI of adding NT-proBNP to the GRACE model
was a 24.4% overall improvement (p < 0.001).

At the same time, when looking at subgroups,
the  combination  of  NT-proBNP  and  GRACE
score was better for predicting survivors at 30-days
(NRI  of  41.4%),  and  did  worse  at  predicting

 nonsurvivors  [65].  Thus,  the  clinical  utility  of
NT-proBNP  is  likely  to  be  poor  if  it  is  used  to
identify patients at high-risk of early events [66].
In considering the NRI, it is therefore important
to consider the changes in risk categories for spe-
cific  outcomes  of  interest,  rather  than  just  the
overall score.

Decision Support

EBM attempts to quantify the probabilities associ-
ated with medical decisions, and encourages clini-
cians to face these uncertainties explicitly, rather
than relying on personal intuition or expert opin-
ion alone. While laboratory tests are rarely used in
isolation, the results are often integrated with med-
ical  history,  physical  examination  findings,  and
imaging studies to assess the likelihood of disease.
This  complex  decision-making  process  has  been
difficult to capture, model, and analyze.

Still,  a  variety  of  decision  support  tools  are
available to assist physicians in the selection and
interpretation  of  laboratory  tests  (Table  18.1).
Specific examples of these tools will be described
later  in  the  chapter.  In  most  cases,  these  tools
must  be  reviewed  regularly  to  ensure  the  most

Table 18.1  Decision support tools for the selection and interpretation of laboratory tests

Tool
Diagnostic algorithms
Published practice standards
Disease or condition-
specific templates

Interpretive guidelines

Consultative interpretive
services
Computerized provider
order entry
Computer-based decision
support
Computerized reminder
alert systems
Online or text handbooks

Comment(s)
Available in books or online
Available in books or online
Includes admission, procedure and chemotherapy templates for specific conditions (e.g.,
heart failure) that specify appropriate tests, medications, and nursing orders. Templates
ensure that the correct interventions are accomplished at the appropriate time and
standardize care for a specific condition
Available in books or online. Particularly useful are institution-specific online
laboratory handbooks
Consultative services provided by clinical pathologists to aid in the selection and
interpretation of laboratory tests
Permits real time decision support at the time the test is ordered. Can be used to redirect
physicians away from unnecessary tests and suggest more appropriate alternatives
Includes query functions for specific signs, symptoms, or diseases and recommend
appropriate tests and their interpretation
Can automatically alert physicians to flagged values or missing/delayed screening or
monitoring tests
Describes test performance characteristics, interferences, false-positive and negative
results, drug effects and other information

18  The Use of Decision Analysis Tools for the Selection of Clinical Laboratory Tests

311

up-to-date  content.  This  is  most  easily  accom-
plished  with  online  formats,  as  decision  support
tools available only in print media usually become
obsolete a short time after publication.

To be effective, decision support tools must be
used  by  physicians  and  the  suggested  advice
acted  upon.  Physicians  are  notoriously  finicky
when it comes to using technologies that will pre-
sumably improve their practice. Careful consid-
eration must therefore be given to fitting the tool
directly into the physician’s regular workflow, as
well as to making these systems extremely easy
to  access  and  to  use  [67].  In  a  study  by  Bates
et al. [68], the authors highlight their “Ten com-
mandments  for  effective  decision  support.”  We
strongly recommend this paper for anyone who is
designing or planning implementation of a deci-
sion support system. The ten key points included
the following:
   1.  Speed is everything
   2.  Anticipate needs and deliver in real time
   3.  Fit into the user’s workflow
   4.  Little things can make a big difference
   5.  Recognize that physicians will strongly resist

stopping

   6.  Changing direction is easier than stopping
   7.  Simple interventions work best
   8.  Ask  for  additional  information  only  when

you really need it

   9.  Monitor impact, get feedback, and respond
 10.  Manage and maintain your knowledge based

systems
An important caveat concerning decision sup-
port tools concerns their clinical and scientific reli-
ability.  The  availability  of  numerous  lay  or
quasi-professional  websites  that  provide  medical
advice underscores this growing problem [69–71].
It can be difficult even for experienced physicians
to  assess  the  quality  of  information  offered  by
these  websites,  especially  considering  that  the
physicians may access the site for the very reason
that  they  are  unsure  about  the  most  appropriate
way  to  proceed.  Furthermore,  information  pro-
vided by medical organizations and societies is not
prima fascia reliable. Contradictions can be easily
found  when  searching  different  professional
 websites  or  publications  about  the  same  clinical

problem.  For  these  reasons,  the  physician  must
approach decision support tools with a degree of
healthy  skepticism.  Careful  consideration  should
be given to the source of the information and how
current the information provided is.

Examples of Decision Support Tools

the  clinician  and

Order Form Design
A  simple  example  of  a  decision  support  tool  is
the laboratory order form. In most cases, whether
on  paper  or  a  computer  screen,  the  requisition
form  is  the  primary  and  obligatory  interface
between
laboratory.
Requisition design has been shown to have a sig-
nificant  impact  on  ordering  practices.  Simple
changes, such as grouping or separating tests on
paper forms or adding or deleting tests from the
first-view of a test menu in a computerized order
entry system, can change test ordering behaviors
dramatically [72–75].

the

Unfortunately, in many current configurations
of  order  entry  systems,  user  interfaces  are  not
designed  to  be  flexible,  and  cannot  be  easily
modified  or  rapidly  updated  by  the  laboratory.
Most are not designed to interface directly with
the  laboratory  information  system  (LIS),  and
their administration is often outside the purview
of the laboratory. Going forward, innovative mid-
dleware  solutions  may  be  able  to  enhance  the
flow of information between the local laboratory,
LIS, and provider order entry systems [47].

Diagnostic Algorithms
Diagnostic  algorithms  have  been  employed  for
decades in the clinical laboratory to control test
utilization and to ensure that the most appropri-
ate  tests  are  selected  in  the  correct  sequence.
Many algorithms also provide interpretive infor-
mation  to  confirm  or  rule  out  a  specific  condi-
tion.  The  classic  example  is  the  thyroid  reflex
algorithm  in  which  the  physician  requests  a
“thyroid screen” which enables the laboratory to
perform a predetermined sequence of tests (i.e.,
thyroid-stimulating  hormone,  if  low  or  high
result,  follow  with  thyroid  hormone  testing).

312

J.Y. Kim et al.

Fig. 18.3  Reflex screening algorithm for celiac disease at the Massachusetts General Hospital tTG, tissue transglutaminase

Table 18.2  List of tests for celiac disease that appear on the online laboratory handbook of the Massachusetts general
hospital

Test name
Antitissue transgluta minase IgA

Celiac disease panel
Endomysial IgA antibody

Gliadin IgG and IgA antibodies

Comment(s)
The IgA tissue transglutaminase test is the single most efficient serologic
screening test for the diagnosis of celiac disease. The use of antigliadin
antibodies as a screening test is no longer recommended
Includes Tissue Transglutaminase IgA and a total IgA level
Note: For initial screening of celiac disease please do not order Endomysial IgA
but instead order Tissue Transglutaminase IgA. It is currently recommended in
most screening algorithms for celiac disease
Note: This test is performed at an external reference lab

Careful  consideration  should  be  given  to  the
reflex and diagnostic thresholds chosen, so as to
achieve the maximum number of diagnoses for
the number of tests performed [76]. As a further
example,  Fig.  18.3  shows  a  reflex   testing  strat-
egy for celiac disease screening that is soon to be
in  use  at  the  Massachusetts  General  Hospital.
This  algorithm  was  developed  because  many
physicians in our institution were confused about
the correct tests to order to evaluate a patient for
celiac  disease.  Available  tests  on  our  menu
include antitissue transglutaminase IgA and IgG,
a celiac disease panel, anti-endomysial IgA and
gliadin IgG and IgA. If a physician types “celiac
disease”  in  our  online  laboratory  handbook,  a

list of tests with recommendations for the most
appropriate  screening  test(s)  is  displayed,  as
shown in Table 18.2. If the physician selects the
“celiac  disease  panel,”  this  order  will  trigger
automatic  performance  of  the  reflex  test  panel,
shown in Fig. 18.3. Decision support in the case
of celiac disease testing therefore occurs on two
different levels, one when the physician types in
the test request in the online handbook, and the
other when they request an approved reflex test-
ing  algorithm.  Collectively,  these  interventions
assist  the  physician  in  test  selection,  help  to
eliminate  unnecessary  tests,  and  encourage
use  of  the  algorithm  approved  for  use  in  our
institution.

18  The Use of Decision Analysis Tools for the Selection of Clinical Laboratory Tests

313

Published Practice Standards

Consultative Interpretive Services

Practice standards for the diagnosis and manage-
ment  of  various  conditions  are  available  from
many sources, including textbooks, online publi-
cations, and various media produced by medical
professional  organizations.  These  standards  are
intended to give physicians general approaches to
specific  clinical  conditions,  although  the  physi-
cian may need to adjust the overall approach to
suit the needs of individual patients. For example,
the American Diabetes Association (ADA) pro-
vides  online  up-to-date  clinical  practice  recom-
mendations  available  from  the  ADA  website
(http://www.diabetes.org).  Taking  this  approach
one  step  further,  some  institutions  have  devel-
oped locally approved practice guidelines based
on  expert  review  of  the  available  evidence.  In
some  cases,  these  guidelines  are  incorporated
into  the  admission  or  order  entry  system  of  the
hospital  as  disease-  or  condition-specific  tem-
plates.  These  templates  provide  standard  orders
for  pharmacy,  nursing  care,  consultations,  and
laboratory testing. All that is required is that the
ordering  physician  selects  the  template,  and  a
standard set of orders is automatically performed
efficiently,  in  a  predetermined  sequence.  In  our
institution, we employ a large number of admis-
sion templates. For example, we have developed
a “rule out acute myocardial infarction” template
that specifies (among other things) the following
set of cardiac marker tests:
 1.  CPK+CK-MB  and  Troponin  T  at  0  h  from

presentation

 2.  Troponin T at 8 h from presentation
 3.  Troponin T at 16 h from presentation
 4.  ECG at 0, 8 and 16 h from presentation

This  testing  strategy  was  developed  to  stan-
dardize  test  ordering  for  myocardial  infarction
and to reduce excess utilization of cardiac mark-
ers, including redundant orders, and unnecessary
repeat testing for total creatine kinase (CK) and
its  isoenzyme  CK-MB.  The  template  is  supple-
mented  with  online  decision  support  treatment
strategies (Fig. 18.4). Soon we plan to eliminate
CK-MB entirely, again reflecting the need to keep
decision support tools up-to-date.

Interpretations of laboratory tests, provided by a
laboratory pathologist or other qualified expert,
can  be  valuable  tools  in  assisting  clinicians.
Without  an  accompanying  interpretation,  labo-
ratory  tests  can  often  be  misinterpreted.  For
example,  we  encountered  a  patient  who  had
been  misdiagnosed  with  protein  S  deficiency,
which had led her to abort her pregnancy due to
fears  of  recurrent  venous  thromboembolism.
Neither she nor her physician realized that pro-
tein  S  typically  decreases  during  normal  preg-
nancy.  In  another  case,  the  diagnosis  of  von
Willebrand  disease  was  missed  in  a  newborn,
because  the  clinicians  did  not  know  that  von
Willebrand  factor  is  typically  elevated  above  a
patient’s  baseline  at  birth,  which  can  mask  the
diagnosis. In addition, the newborn was ill from
infection  and  internal  bleeding  at  the  time  of
testing,  and  acute  illness  also  elevates  von
Willebrand factor above a patient’s baseline. In
a  third  case,  an  experienced  hematologist
thought that slightly elevated hemoglobin A2 in
a patient with sickle cell trait indicated coexist-
ing beta thalassemia. Fortunately, this third case
example occurred at our institution, which pro-
vides interpretations by pathologists for hemo-
globin  electrophoresis  and  other  complex
laboratory  tests.  The  interpretation  for  this
patient stated that the results are consistent with
sickle  cell  trait  and  concomitant  alpha  thalas-
semia trait, based on the relatively low percent-
age  of  hemoglobin  S  and  the  low  MCV.
Hemoglobin  S  can  falsely  elevate  hemoglobin
A2 due to co-elution, without beta thalassemia.
Thus, a misdiagnosis was avoided.

Surveys  of  physicians  receiving  pathologist
interpretations with their specialized coagulation
test results showed that 98% find the interpreta-
tions  “useful  or
informative.”  In  addition,
responses  indicated  that  72%  of  interpretations
reduced  the  number  of  tests  needed  to  make  a
diagnosis, 72% helped avoid a misdiagnosis, and
59% shortened the time to diagnosis [77].

Interpretations  can  also  improve  physicians’
abilities  to  select  the  appropriate  test(s)  needed
to  reach  a  diagnosis.  Laboratory  test  ordering

314

J.Y. Kim et al.

Fig.  18.4  Example  of  a  decision  support  strategy  for
non-ST  segment  elevation  acute  coronary  syndrome  at
the  Massachusetts  General  Hospital.  Hx  history;  PE
physical  examination;  ECG  electrocardiogram;  TIMI
thrombolysis  in  myocardial  infarction;  CHF  congestive

heart failure; Tnt troponin T; ASA aspirin; UFH unfrac-
tionated heparin; LMWH low molecular weight heparin;
IIb/IIIa, glycoprotein IIb/IIIa inhibitor; GP blycoprotein;
D/C, discharge; cath, catheterization; PCI percutaneous
coronary intervention

 patterns  were  studied  immediately  after  we
implemented a coagulation interpretation service
for  a  group  of  outside  hospitals,  and  the  results
were  compared  to  ordering  patterns  after  the
interpretation  service  had  been  in  place  for  2.5
years.  The  number  of  coagulation  test  ordering
errors decreased by nearly two errors per requisi-
tion  during  the  study  period  [77].  Furthermore,
initially,  over  63%  of  requisitions  had  4  errors,
but at the end of the study period, this was reduced
to  only  10%.  For  example,  clinicians  had  fre-
quently  ordered  antigen  assays  (immunoassays)
to assess for protein C, protein S or antithrombin
III deficiency, but after receiving interpretations
for 2.5 years, they more frequently ordered func-
tional  assays,  which  are  the  appropriate  tests  to
order.  The  interpretations  include  mention  that
antigen  assays  are  inadequate  because  they  are

not  able  to  detect  type  II  (qualitative)  deficien-
cies,  as  they  do  not  assess  protein  function.  In
contrast, functional assays are able to detect both
type I (quantitative) and type II deficiencies. The
results of this study provided evidence that inter-
pretations  can  successfully  modify  physicians’
ability to order tests appropriately.

Interpretations are most informative if all the
relevant  results  for  a  specimen  are  interpreted
together,  while  also  taking  into  account  the
patient’s medical history. That is to say, patient-
specific  interpretations  are  more  valuable  than
generic  interpretations.  If  a  patient  has  low
 protein C, low protein S, and normal antithrom-
bin III, it is most useful for the interpretation to
indicate that the most likely explanation for this
combination  of  findings  is  warfarin  or  vitamin
K  deficiency,  rather  than  list  all  the  possible

18  The Use of Decision Analysis Tools for the Selection of Clinical Laboratory Tests

315

causes of low protein C, and then separately list
all  the  possible  causes  of  low  protein  S.
Incorporating the normal antithrombin III result
into  the  interpretation  allows  the  exclusion  of
some other possible causes of low protein C and
low  protein  S,  or  at  least  renders  them  much
less  likely.  The  interpretation  can  also  give
 suggestions  for  follow-up  testing,  if  appropri-
ate.  In  the  current  example,  the  interpretation
would indicate that testing can be repeated any
time when the patient has not had warfarin for at
least 20 days, because it can take that long for
protein  S  to  recover  to  normal  after  warfarin
discontinuation.

In  another  example,  if  a  patient  on  warfarin
tests positive for a lupus anticoagulant, the inter-
pretation can notify clinicians that lupus antico-
agulants  are  capable  of  artifactually  prolonging
the  prothrombin  time  and  international  normal-
ized  ratio  (PT-INR),  potentially  overestimating
the  patient’s  level  of  warfarin  anticoagulation.
The  interpretation  can  note  that  a  chromogenic
factor X assay can be performed on this specimen
if requested, to help determine whether or not the
lupus  anticoagulant  is  artifactually  prolonging
the  PT-INR.  In  an  additional  example,  for  a
patient with low antithrombin III and 3+ protei-
nuria on a urinalysis, the interpretation can note
that  proteinuria  can  cause  an  acquired  loss  of
antithrombin III, but other possible causes of low
antithrombin  III  can  also  be  included  for  com-
pleteness.  Table  18.3  shows  some  additional
example interpretations.

An interpretive service is even more efficient
when combined with strategic testing algorithms
that simplify the diagnostic process for clinicians.
Test  requisitions  or  order  entry  systems  can  be
simplified  to  offer  the  appropriate  algorithms.
For example, for a patient undergoing evaluation
because  of  a  bleeding  history,  the  clinician  can
order a “prolonged PT and PTT evaluation,” and
the laboratory will follow an algorithm (Fig. 18.5)
to reach the diagnosis on one specimen, without
performing  any  unnecessary  tests.  The  alterna-
tive  is  cumbersome  and  inefficient,  as  well  as
inconvenient  for  the  patient:  the  clinician  waits
for the PT or PTT results to come back abnormal,
collects another specimen, and tries to remember

Table 18.3  Examples of interpretations for the coagula-
tion service at MGH

Scenario
Normal von
Willebrand
results in the
presence
of an acute
phase reaction

Mildly low
antithrombin III
result in a
patient on
heparin

Interpretation
The von Willebrand panel values are
normal, however, fibrinogen is
elevated at 590 mg/dl. Both fibrinogen
and von Willebrand factor are acute
phase reactants. Therefore, it is
possible that von Willebrand factor is
elevated above the patient’s true
baseline. Taken together, it is not
possible to exclude von Willebrand
disease with certainty at this time. If a
second study has not been performed,
a repeat study when the patient is not
likely to be in an acute phase reaction
(normal value for fibrinogen) may be
informative
The patient is blood type O. Normal
blood type O adults have a mean von
Willebrand factor level of approxi-
mately 75%
Antithrombin III is slightly low.
Heparin administration can cause
slight decreases in antithrombin
within several days, secondary to
increased clearance. If hereditary
antithrombin deficiency is strongly
suspected, the assay may be repeated
once the patient has been off heparin
for at least 1–2 weeks
The specimen submitted has
prolonged PTT. When the sample was
treated with an enzyme that degrades
heparin, the PTT corrected into the
normal range indicating the prolonga-
tion is due to the presence of heparin
in the sample

which  coagulation  factors  to  order  for  which
 prolongation,  and  subsequently  would  need  to
collect  yet  another  specimen  if  it  turns  out  that
lupus  anticoagulant  or  inhibitor  tests  are  indi-
cated.  The  clinicians  can  also  order  all  of  these
tests up front, but this wastes healthcare resources
if the tests turn out to be unnecessary.

Test requisitions or order entry systems can be
designed  to  encourage  appropriate  test  ordering
of  complex  tests  by  offering  these  as  test  algo-
rithms or panels, rather than simply listing all test
names individually. For example, most clinicians
do  not  realize  that  “ristocetin  cofactor”  is  the
name of the test for von Willebrand factor activ-
ity, so they order “von Willebrand factor antigen”

316

J.Y. Kim et al.

Fig. 18.5  Simplified algorithm for a prolonged PTT evalu-
ation at the Massachusetts General Hospital. An unexplained
prolonged PTT with a normal PT is likely to be clinically
significant, and an evaluation to determine the etiology of
the prolongation is warranted. Even if the evaluation leads
to  the  diagnosis  of  factor  XII  deficiency,  which  does  not
cause bleeding, the knowledge will spare the patient unnec-
essary  transfusions  of  fresh  frozen  plasma.  The  algorithm
shown is highly simplified and the nuances followed by the
laboratory are more complex than what is shown. The first
step determines whether or not the prolongation is due to

heparin, low molecular weight heparin, or fondaparinux anti-
coagulation. If any of these anticoagulants explains the pro-
longation, no further testing is performed. If none of these
anticoagulants  are  detected,  a  mixing  study  is  performed,
where the patient’s plasma is mixed 1:1 with normal plasma,
and the PTT is performed on the mixture both at time 0 and
after a 2-hour incubation. Depending on the results, factor
assays,  lupus  anticoagulant  tests,  and/or  Bethesda  assays
(for  a  factor  inhibitor)  may  be  subsequently  performed,
until the etiology of the prolongation is determined. PTT,
partial thromboplastin time; PT, prothrombin time

when they see it listed on the requisition or order
entry system. By ordering the antigen test with-
out the activity test, type 2 von Willebrand dis-
ease  could  be  misdiagnosed  as  normal.  In
contrast, if a von Willebrand panel is offered on
the requisition or order entry system, the appro-
priate laboratory tests can be ordered. If panels or
algorithms  are  offered,  the  requisition  or  order
entry system should explain what tests are or may
be included (for example, on the back of the req-
uisition).  For  hospital  laboratories,  it  is  recom-
mended  to  initially  obtain  approval  from  the
hospital’s medical policy committee for the algo-
rithm  (reflex  test)  protocols  that  the  laboratory
would like to use. Clinicians still should have the
ability to order a test individually.

Computer-Based Decision Support

Computer-based  decision  support  has  been  rap-
idly evolving over the past 10 years. This support
can take various forms, such as publicly available
online  search-capable  websites,  customized

 computer  programs  to  aid  in  specific  medical
decisions (e.g., pulmonary function tests, electro-
cardiograms,  acid–base  disorders)  and  institu-
tion-specific  decision  support  tools  provided  on
the intranet. A number of websites open to physi-
cians and the general public are available to assist
in test selection and to provide interpretive infor-
mation on laboratory testing.

A  typical  example  is  “WebMD  symptom
checker”  (http://symptoms.webmd.com).  This
website  allows  the  user  to  search  by  disease,
symptom,  laboratory  test,  or  other  parameters.
The site provides general information about labo-
ratory test selection and interpretation within the
overall context of the disease or symptom-based
search. Other similar web-based tools are avail-
able, with the majority of these being not limited
to test selection alone.

Another example is “Lab Tests Online” (http://
www.labtestsonline.org).  On  this  site,  the  user
can search by test, condition, disease, or screen-
ing  to  access  peer-reviewed  information  on  a
number of medical topics. Most of these online
sites essentially take the place of standard texts in

18  The Use of Decision Analysis Tools for the Selection of Clinical Laboratory Tests

317

print, where the reader must search the table of
contents  or  index.  Online  sources  are  easier  to
use, are mobile, can be accessed from any com-
puter, and can be updated on a continual basis. To
counter  this  online  threat,  publishers  of  estab-
lished medical textbooks often offer online access
and  search  functions  with  purchase  of  the  text-
book. For example, Cecil Medicine 23rd edition
offers an online Expert Consultant with purchase
of the book.

Online (or Text) Interpretive Guidelines

Many laboratories provide physicians with online
or  printed  test  interpretation  guidelines.  For
example,  Mayo  Medical  Laboratories  publishes
an annual interpretive handbook. The 2009–2010
edition  contains  over  800  pages  describing  the
use,  interpretation  and  appropriate  cautionary
comments for a number of tests on the menu. As
one  example,  under  the  listing  for  plasma  free
metanephrines, the utility of the test is explained,
stating that this test is the most sensitive (nearly
100%) test to screen for elevated catecholamines,
recommending  fractionated  24-h  urinary  cate-
cholamines as a confirmatory test, and cautioning
about  specific  drugs  that  may  elevate  cate-
cholamine  levels,  producing  borderline  elevated
plasma  metanephrine  levels.  Printed  references
are very useful, but are not as readily available as
online formats. Importantly, many generic online
references  provide  similar  information,  but  the
interpretative  data  is  not  specific  to  any  labora-
tory.  These  generic  online  sources  may  yield
erroneous  recommendations  when  tests  have
substantial differences in performance from one
laboratory to another. This is particularly true for
genetic testing, where different laboratories may
test for a varying number of mutations for a given
genetic disorder.

Computerized Alerts and Reminders

The  amount  of  individual  patient  information
that  the  typical  physician  must  be  aware  of  is
constantly  expanding.  Most  organizations  are

attempting  to  assist  the  physician  in  organizing
and storing this information in the form of EHR.
The EHR is slowly replacing paper-based medi-
cal  records  in  hospital  and  outpatient  settings.
Beyond  simply  storing  and  displaying  clinical
information in an organized user friendly format,
the EHR also permits various alerts and remind-
ers to be incorporated into the physician’s regular
workflow. This may include reminders to perform
screening  tests  on  selected  patients,  abnormal
and critical value alerts and other features, such
as  disease  management  protocols  to  ensure  that
important tests have been ordered and abnormal
results acted upon appropriately.

For  example,  patients  receiving  long-term
anticoagulation therapy with Coumadin must be
monitored  at  regular  intervals  using  a  PT-INR
test to ensure adequate anticoagulation therapy. If
the patient’s PT-INR becomes subtherapeutic, the
patient  may  develop  a  fatal  clot  or  embolism.
Excessive anticoagulation may result in bleeding
or hemorrhage. Usually the physician schedules
office visits for these patients at various intervals
and provides a prescription for outpatient PT-INR
testing  at  more  frequent  intervals.  Once  the
patient  has  left  the  office,  the  physician  has  no
way to be certain that the patient actually went to
the  laboratory  for  the  regular  PT-INR  testing,
unless  the  office  staff  periodically  reviews  the
patient’s records to check for the results of recent
testing. Noncompliance on the part of the patient
can  have  potentially  catastrophic  consequences.
On the other hand, if the test orders for PT-INR
have been recorded in an order entry system, it is
possible to implement an alert system such that
the physician is made aware if the patient did not
show  up  for  testing  within  an  appropriate  time
interval. Furthermore, the system could alert the
physician of nontherapeutic PT-INR values, thus
permitting more timely adjustments to therapy.

Another example is the use of electronic dis-
ease management protocols to ensure that impor-
tant  tests  and  procedures  have  been  performed
according  to  accepted  standards  of  care.  For
example,  patients  with  diabetes  mellitus  require
regular  monitoring  of  hemoglobin  A1c,  urinary
microalbumnin,  lipids,  and  other  parameters.
Given the large number of diabetic patients in a

318

J.Y. Kim et al.

typical primary care practice, it is relatively easy
for important screening and monitoring tests to be
overlooked. Some insurance plans have mandated
that testing be performed regularly as part of pay-
for-performance incentives. The simplest approach
to ensuring that appropriate testing is performed
is to use an electronic diabetic patient monitoring
template, with automatic reminders when patients
have not received recommended testing. A meta-
analysis  of  the  effectiveness  of  computerized
decision support systems has found that automatic
prompts  are  associated  with  improved  provider
adherence,  when  compared  to  systems  that
required users to activate the system [78].

Decision Analysis and Forecasting
Models in the Laboratory

While decision support analysis can be valuable
to assist physicians caring for individual patients,
these tools can also be applied to populations of
patients  and  to  aid  in  forecasting  trends  in  the
clinical  laboratory.  The  capability  to  aggregate
laboratory  data  across  populations  of  patients
presents opportunities to systematically improve
medical  care  on  a  population-based
level.
Implementation  of  electronic  medical  records
facilitates  this  process  by  incorporating  labora-
tory data into electronic formats that can be ana-
lyzed  to  detect  correlations  and  trends.  The
analysis can be performed by the LIS, specialized
middleware,  or  by  computer  programs  that  can
access  the  electronic  medical  record.  Examples
of  specific  applications  include  surveillance  for
infection control, safety and adverse event analy-
sis,  operations  and  workflow  analysis,  quality
improvement,  and  forecasting  trends  in  clinical
laboratory services.

Surveillance

Figure 18.6 shows quarterly rates of MRSA infec-
tion in our hospital tracked over time. As a histori-
cal  baseline,  we  had  1.21  infections  per  1,000
patients  in  2005.  The  rate  has  steadily  declined,
reaching 0.45 infections per 1,000 patients in the

Fig. 18.6  Quarterly rate of methicillin-resistant Staphy-
lococcus aureus (MRSA) infection per 1,000 patients over
time. Q quarter

most  recent  quarter.  This  data  is  derived  by
 integrating data supplied by the microbiology LIS
with clinical electronic medical records. The data
could  also  be  analyzed  by  location,  underlying
disease or other factors, permitting infection con-
trol  officers  to  target  specific  high-risk  areas  for
further reductions in these infections.

Another  example  of  using  laboratory  data  for
surveillance in our hospital involves the use of anti-
microbial sensitivity data. The microbiology labo-
ratory  has  implemented  a  special  program  that
collates sensitivity data for various organisms that is
analyzed annually to produce an antibiotic sensitiv-
ity  profile.  This  information  assists  our  infectious
disease  department  to  produce  a  list  of  recom-
mended antibiotics for various infections based on
our local antimicrobial sensitivity patterns.

Safety and Adverse Event Analysis

Our  hospital  utilizes  a  real  time  electronic  safety
reporting  system.  This  system  replaced  our  older
paper-based  “incident  reporting  system,”  that  was
inefficient and often reported events too long after
the incident to permit successful investigation and
corrective  action.  Any  employee  can  file  an  elec-
tronic safety report. The reports are reviewed by the
hospital Office of Quality and Safety, and, depend-
ing on the location and severity, by the departmental
Quality Chair and Quality Manager. Potential out-
comes include investigation and/or follow-up of the
event, recommendations,  implementation of correc-
tive actions, and  aggregation of the event data into

18  The Use of Decision Analysis Tools for the Selection of Clinical Laboratory Tests

319

the quality and safety database. We receive  quarterly
aggregated reports listing the number of event types
(e.g.,  blood/blood  products,  diagnosis/treatment,
specimen issues), severity levels (e.g., near miss, no
harm,  temporary  minor  harm,  permanent  harm),
and aggregated details about events (e.g., number of
delayed tests, wrong patient, wrong test, mislabeled
specimens,  ABO  complications).  The  aggregated
data  allows  us  to  target  quality  improvement
 activities  around  the  most  common  or  high-risk
events, and to track the success of our interventions
over time.

Forecasting Trends in Clinical
Laboratory Services

The  clinical  laboratory  is  a  service-oriented
department  that  must  anticipate  and  respond  to
the  needs  of  clinical  services.  Most  laboratory
trends occur fairly slowly, and can be monitored
by  projecting  testing  requirements  and  volumes
using  historical  data.  However,  in  some  cases
hospitals undergo abrupt changes in clinical ser-
vices  that  cannot  be  understood  by  historical
trend  data  alone.  For  example,  a  hospital  may
open a new oncology or transplant center, merge
and consolidate with other area hospitals or start
a  regional  outreach  program.  These  types  of
events  may  substantially  change  the  test  menu,
test volumes, and influence the types of services
that  must  be  provided.  In  some  cases,  decision
support  tools  can  be  applied  to  these  types  of
challenges. For example, predictions of outreach
test volumes can be obtained from the expected
number of physicians and types of clinical prac-
tices,  using  decision  support  tools  that  contain
databases of physician test ordering behavior.

Operations and Workflow Analysis

Some consulting companies and vendors of labo-
ratory instrumentation have developed proprietary
computer-based systems to aid in operations and
workflow  analysis.  Most  of  these  are  based  on
process flow and lean principles, and are linked to
benchmarking databases that allow the laboratory

to  compare  themselves  to  similar  operations.
These  services  are  available  for  a  fee  or  are
included  as  part  of  a  large  instrument  purchase.
There are also inexpensive off-the-shelf decision
support programs to aid the laboratory in perform-
ing their own operations and workflow analysis.
These  systems  include  the  basic  tools  that  are
required  to  map  operational  processes  and  per-
form basic analyses based on lean principles.

Decision Support Incorporated
into ARRA Legislation and Other
Considerations

Beginning  in  2011,  Medicare  physicians  who
implement  and  report  “meaningful  use”  of  EHR
will be eligible for substantial financial incentives
approved in the recent ARRA (American Recovery
and Reinvestment Act) legislation. The Centers for
Medicare & Medicaid Services (CMS) has recently
proposed that the “meaningful use” criteria should
include the use of “five clinical decision support
rules relevant to [each] specialty” [79]. In response,
the  Meaningful  Use  Workgroup  of  the  HITPC
(Health IT Policy Committee) has recommended
that the wording be amended to explicitly require
that  one  of  these  five  clinical  decision  support
rules address efficient diagnostic test ordering [80].
Thus,  decision  support  tools  for  laboratory  test
ordering  are  likely  to  become  a  major  issue  for
providers and hospitals going forward.

At the same time, liability issues regarding the
dissemination and future use of such tools remain
murky [81]. Programs using a “closed-loop” sys-
tem  to  make  decisions  directly  controlling  a
patient’s treatment are viewed as medical devices
under control of the FDA, but physicians may be
held liable when they are making the final assess-
ment for care.

Finally, demonstrations of clinical decision sup-
port systems have primarily focused on their effects
on  practitioner  performance  [78],  and  not  patient
outcomes.  Those  that  have  included  patient  out-
comes in evaluations of decision support tools have
found  inconsistent  results;  many  were  hampered
by  inadequate  numbers  of  patients,  and  failed
to  have  the  statistical  power  to  demonstrate

320

J.Y. Kim et al.

improvements  [78]. Particularly when it comes to
patient  outcomes  related  to  preventative  care,
 studies  may  have  to  rely  on  multicenter  cluster-
randomized controlled trials [82]. However, given
the substantial time and resources required for such
collaborations, it is unclear how many such trials
will be feasible. Furthermore, it is difficult to imag-
ine having multiple repeated evaluations of a deci-
sion support system every time new knowledge is
added to the system. The long track record and near
ubiquitous use of computers for supporting safety,
efficiency,  and  quality  in  other  nonhealthcare
related commercial, industrial, and scientific enter-
prises suggests that using reasonable proof of effec-
tiveness, rather than imposing onerous requirements,
may be the way to move forward.

Conclusion

The number and complexity of available labora-
tory  tests  continues  to  increase  at  a  rapid  pace.
Staying current with accepted standards for labo-
ratory testing for diagnosis, monitoring and prog-
nosis  is  extremely  challenging,  particularly  for
nonspecialists who see a diverse patient popula-
tion. Decision support tools to aid physicians in
appropriate  test  selection  and  interpretation  are
widely  available  and  will  become  increasingly
important. The most effective and practical deci-
sion  support  tools  are  developed  or  selected
locally at the institutional level and embedded in
the regular workflow of the physician. Many of
these tools can be incorporated into the electronic
medical record system where they can be easily
accessed by any physician while caring for their
patients.  Careful
“Ten
Commandments  for  effective  clinical  decision
support,”  described  by  Bates  et  al.,  [68]  will
enhance the chances for success in the design and
implementation of new decision support tools.

attention

the

to

References

  1.  Cochrane  AL.  Effectiveness  and  efficiency;  random
reflections  on  health  services.  London]:  Nuffield
Provincial Hospitals Trust; 1972.

  2.  Sackett  DL,  Rosenberg  WM,  Gray  JA,  Haynes  RB,
Richardson WS. Evidence based medicine: what it is
and what it isn’t. BMJ. 1996;312(7023):71–2.

  3.  Rosenberg  W,  Donald  A.  Evidence  based  medicine:
an  approach  to  clinical  problem-solving.  BMJ.
1995;310(6987):1122–6.

  4.  Kohn  LT,  Corrigan  J,  Donaldson  MS,  Institute  of
Medicine  (U.S.).  Committee  on  Quality  of  Health
Care  in  America.:  To  err  is  human:  building  a  safer
health system. Washington, D.C: National Academy
Press; 2000.

  5.  Brennan TA, Leape LL, Laird NM, Hebert L, Localio
AR, Lawthers AG, et al. Incidence of adverse events
and negligence in hospitalized patients. Results of the
Harvard  Medical  Practice  Study  I.  N  Engl  J  Med.
1991;324(6):370–6.

  6.  Leape LL, Brennan TA, Laird N, Lawthers AG, Localio
AR, Barnes BA, et al. The nature of adverse events in
hospitalized  patients.  Results  of  the  Harvard  Medical
Practice Study II. N Engl J Med. 1991;324(6):377–84.
  7.  Blumenthal  D.  Stimulating  the  adoption  of  health
information technology. N Engl J Med. 2009;360(15):
1477–9.

  8.  Wilson JF. Making electronic health records meaning-

ful. Ann Intern Med. 2009;151(4):293–6.

  9.  Timmermans S, Mauck A. The promises and pitfalls
of evidence-based medicine. Health Aff (Millwood).
2005;24(1):18–28.

 10. Fisher  ES,  Wennberg  DE,  Stukel  TA,  Gottlieb  DJ,
Lucas  FL,  Pinder  EL.  The  implications  of  regional
variations in Medicare spending. Part 1: the content,
quality,  and  accessibility  of  care.  Ann  Intern  Med.
2003;138(4):273–87.

 11. Fisher  ES,  Wennberg  DE,  Stukel  TA,  Gottlieb  DJ,
Lucas  FL,  Pinder  EL.  The  implications  of  regional
variations  in  Medicare  spending.  Part  2:  health  out-
comes  and  satisfaction  with  care.  Ann  Intern  Med.
2003;138(4):288–98.

 12. Hartman  M,  Martin  A,  Nuccio  O,  Catlin  A.  Health
spending growth at a historic low in 2008. Health Aff
(Millwood). 2010;29(1):147–55.

 13. Benson ES. Initiatives toward effective decision making
and laboratory use. Hum Pathol. 1980;11(5):440–8.
 14. Robinson  A.  Rationale  for  cost-effective  laboratory
medicine. Clin Microbiol Rev. 1994;7(2):185–99.
 15. van Bokhoven MA, Pleunis-van Empel MC, Koch H,
Grol  RP,  Dinant  GJ,  van  der  Weijden  T.  Why  do
patients want to have their blood tested? A qualitative
study of patient expectations in general practice. BMC
Fam Pract. 2006;7:75.

 16. Kessler DP, Summerton N, Graham JR. Effects of the
medical liability system in Australia, the UK, and the
USA. Lancet. 2006;368(9531):240–6.

 17. Oboler  SK,  Prochazka  AV,  Gonzales  R,  Xu  S,
Anderson  RJ.  Public  expectations  and  attitudes  for
annual physical examinations and testing. Ann Intern
Med. 2002;136(9):652–9.

 18. Koch  H,  van  Bokhoven  MA,  ter  Riet  G,  van  Alphen-
Jager JT, van der Weijden T, Dinant GJ, et al. Ordering
blood  tests  for  patients  with  unexplained  fatigue  in

18  The Use of Decision Analysis Tools for the Selection of Clinical Laboratory Tests

321

 general  practice:  what  does  it  yield?  Results  of  the
VAMPIRE
trial.  Br  J  Gen  Pract.  2009;59(561):
e93–100.

 19. Fernandes CM, Worster A, Hill S, McCallum C, Eva K.
Root  cause  analysis  of  laboratory  turnaround  times
for  patients  in  the  emergency  department.  CJEM.
2004;6(2):116–22.

 20. Francis  AJ,  Ray  MJ,  Marshall  MC.  Pathology  pro-
cesses and emergency department length of stay: the
impact of change. Med J Aust. 2009;190(12):665–9.
 21. Jha AK, Chan DC, Ridgway AB, Franz C, Bates DW.
Improving safety and eliminating redundant tests: cut-
ting  costs  in  U.S.  hospitals.  Health  Aff  (Millwood).
2009;28(5):1475–84.

 22. OIG  Compliance  Program  Guidance  for  Clinical
Laboratories. Fed Regist. 1998;63(163):45076–87.
 23. Plebani M. Exploring the iceberg of errors in labora-

tory medicine. Clin Chim Acta. 2009;404(1):16–23.
 24. Howanitz PJ. Errors in laboratory medicine: practical
lessons  to  improve  patient  safety.  Arch  Pathol  Lab
Med. 2005;129(10):1252–61.

 25. Plebani M. Errors in clinical laboratories or errors in
laboratory medicine? Clin Chem Lab Med. 2006;44(6):
750–9.

 26. Plebani M, Carraro P. Mistakes in a stat laboratory: types
and frequency. Clin Chem. 1997;43(8 Pt 1):1348–51.
 27. Carraro P, Plebani M. Errors in a stat laboratory: types
and frequencies 10 years later. Clin Chem. 2007;53(7):
1338–42.

 28. Becich  MJ.  Information  management:  moving  from
test  results  to  clinical  information.  Clin  Leadersh
Manag Rev. 2000;14(6):296–300.

 29. Forsman RW. Why is the laboratory an afterthought
for  managed  care  organizations?  Clin  Chem.
1996;42(5):813–6.

 30. Forsman  R.  The  electronic  medical  record:  implica-
tions  for  the  laboratory.  Clin  Leadersh  Manag  Rev.
2000;14(6):292–5.

 31.  Holohan TV, Colestro J, Grippi J, Converse J, Hughes M.
Analysis of diagnostic error in paid malpractice claims
with  substandard  care  in  a  large  healthcare  system.
South Med J. 2005;98(11):1083–7.

 32.  Gandhi TK, Kachalia A, Thomas EJ, Puopolo AL, Yoon
C, Brennan TA, et al. Missed and delayed diagnoses in
the  ambulatory  setting:  a  study  of  closed  malpractice
claims. Ann Intern Med. 2006;145(7):488–96.

 33. Kachalia  A,  Gandhi  TK,  Puopolo  AL,  Yoon  C,
Thomas EJ, Griffey R, et al. Missed and delayed diag-
noses in the emergency department: a study of closed
malpractice  claims  from  4  liability  insurers.  Ann
Emerg Med. 2007;49(2):196–205.

 34. Covell  DG,  Uman  GC,  Manning  PR.  Information
needs  in  office  practice:  are  they  being  met?  Ann
Intern Med. 1985;103(4):596–9.

 35. Ely  JW,  Osheroff  JA,  Gorman  PN,  Ebell  MH,
Chambliss ML, Pifer EA, et al. A taxonomy of generic
clinical  questions:  classification  study.  BMJ.  2000;
321(7258):429–32.

 36. Hayward  R.  Clinical  decision  support  tools:  do
they  support  clinicians?  Fut  Pract.  2004:66–68.

Available at: http://www.cche.net/about/files/clinical_
decision_support_tools.pdf.

 37. Hughes  B,  Joshi  I,  Lemonde  H,  Wareham  J.  Junior
physician’s  use  of  Web  2.0  for  information  seeking
and medical education: a qualitative study. Int J Med
Inform. 2009;78(10):645–55.

 38. Davies K. The information-seeking behaviour of doc-
tors:  a  review  of  the  evidence.  Health  Info  Libr  J.
2007;24(2):78–94.

 39. Pene F, Courtine E, Cariou A, Mira JP. Toward therag-
nostics. Crit Care Med. 2009;37(1 Suppl):S50–8.
 40. Jones J, Taylor B, Wilkin TJ, Hammer SM. Advances in
antiretroviral therapy. Top HIV Med. 2007;15(2):48–82.
 41. Mueller  MC,  Bogner  JR.  Treatment  with  CCR5
antagonists: which patient may have a benefit? Eur J
Med Res. 2007;12(9):441–52.

 42. Ladanyi  M,  Pao  W.  Lung  adenocarcinoma:  guiding
EGFR-targeted  therapy  and  beyond.  Mod  Pathol.
2008;21 Suppl 2:S16–22.

 43. Lievre A, Bachet JB, Le Corre D, Boige V, Landi B,
Emile JF, et al. KRAS mutation status is predictive of
response  to  cetuximab  therapy  in  colorectal  cancer.
Cancer Res. 2006;66(8):3992–5.

 44. Slamon DJ, Leyland-Jones B, Shak S, Fuchs H, Paton
V,  Bajamonde  A,  et  al.  Use  of  chemotherapy  plus  a
monoclonal  antibody  against  HER2  for  metastatic
breast  cancer  that  overexpresses  HER2.  N  Engl  J
Med. 2001;344(11):783–92.

 45. Reifenberger G, Louis DN. Oligodendroglioma: toward
molecular  definitions  in  diagnostic  neuro-oncology.
J Neuropathol Exp Neurol. 2003;62(2):111–26.

 46. Buckland MK. Library services in theory and context.
2nd ed. Oxford, New York: Pergamon Press; 1988.
 47.  Grisson  R,  Kim  JY,  Brodsky  V,  Kamis  IK,  Singh  B,
Belkziz SM, et al. A novel class of laboratory middle-
ware.  Promoting  information  flow  and  improving
computerized provider order entry. Am J Clin Pathol.
2010;133(6):860–9.

 48. Lewandrowski K. Managing utilization of new diagnostic
tests. Clin Leadersh Manag Rev. 2003;17(6):318–24.
 49. Mark DB. Decision-making in clinical medicine. In:
Fauci  AS,  Braunwald  E,  Kasper  DL,  Hauser  SL,
Longo DL, Jameson JL, Loscalzo J, editors. Harrison’s
principles of internal medicine. 17tth ed. New York:
McGraw-Hill; 2008. p. 16–23.

 50. Cronje RJ, Freeman JR, Williamson OD, Gutsch CJ.
Evidence-based medicine: recognizing and managing
clinical uncertainty. Lab Med. 2004;35:723–9.

 51. Lumbreras B, Parker LA, Porta M, Pollan M, Ioannidis
JP, Hernandez-Aguado I. Overinterpretation of clini-
cal  applicability  in  molecular  diagnostic  research.
Clin Chem. 2009;55(4):786–94.

 52. Galen RS, Gambino SR. Beyond normality: the pre-
dictive  value  and  efficiency  of  medical  diagnoses.
New York: Wiley; 1975.

 53. Januzzi Jr JL, Bamberg F, Lee H, Truong QA, Nichols
JH,  Karakas  M,  et  al.  High-sensitivity  troponin
T   concentrations  in  acute  chest  pain  patients  evalu-
ated with cardiac computed tomography. Circulation.
2010;121(10):1227–34.

322

J.Y. Kim et al.

 54. Diamond  GA,  Kaul  S.  How  would  the  Reverend
Bayes interpret high-sensitivity troponin? Circulation.
2010;121(10):1172–5.

 55. Zweig MH, Campbell G. Receiver-operating charac-
teristic (ROC) plots: a fundamental evaluation tool in
clinical medicine. Clin Chem. 1993;39(4):561–77.
 56. Dighe AS, Rao A, Coakley AB, Lewandrowski KB.
Analysis  of  laboratory  critical  value  reporting  at  a
large  academic  medical  center.  Am  J  Clin  Pathol.
2006;125(5):758–64.

 57. Cook NR. Statistical evaluation of prognostic versus
diagnostic  models:  beyond  the  ROC  curve.  Clin
Chem. 2008;54(1):17–23.

 58. Hosmer DW, Hosmer T, Le Cessie S, Lemeshow S. A
comparison  of  goodness-of-fit  tests  for  the  logistic
regression model. Stat Med. 1997;16(9):965–80.
 59. Pencina  MJ,  D’Agostino  Sr  RB,  D’Agostino  Jr  RB,
Vasan RS. Evaluating the added predictive ability of a
new marker: from area under the ROC curve to reclas-
sification and beyond. Stat Med. 2008;27(2):157–72.
discussion 207–212.

 60. de Araujo Goncalves P, Ferreira J, Aguiar C, Seabra-
Gomes R. TIMI, PURSUIT, and GRACE risk scores:
sustained prognostic value and interaction with revas-
cularization in NSTE-ACS. Eur Heart J. 2005;26(9):
865–72.

 61. Gill  D,  Seidler  T,  Troughton  RW,  Yandle  TG,
Frampton CM, Richards M, et al. Vigorous response
in  plasma  N-terminal  pro-brain  natriuretic  peptide
(NT-BNP)  to  acute  myocardial  infarction.  Clin  Sci
(Lond). 2004;106(2):135–9.

 62.  Omland T, Aakvaag A, Bonarjee VV, Caidahl K, Lie RT,
Nilsen DW, et al. Plasma brain natriuretic peptide as an
indicator of left ventricular systolic function and long-
term  survival  after  acute  myocardial
infarction.
Comparison  with  plasma  atrial  natriuretic  peptide  and
N-terminal  proatrial  natriuretic  peptide.  Circulation.
1996;93(11):1963–69.

 63. Arakawa  N,  Nakamura  M,  Aoki  H,  Hiramori  K.
Plasma  brain  natriuretic  peptide  concentrations  pre-
dict survival after acute myocardial infarction. J Am
Coll Cardiol. 1996;27(7):1656–61.

 64.  de  Lemos  JA,  Morrow  DA,  Bentley  JH,  Omland  T,
Sabatine MS, McCabe CH, et al. The prognostic value of
B-type natriuretic peptide in patients with acute coronary
syndromes. N Engl J Med. 2001;345(14):1014–21.
 65. Khan SQ, Narayan H, Ng KH, Dhillon OS, Kelly D,
Quinn P, et al. N-terminal pro-B-type natriuretic pep-
tide complements the GRACE risk score in predicting
early and late mortality following acute coronary syn-
drome. Clin Sci (Lond). 2009;117(1):31–9.

 66. Rosjo  H,  Omland  T.  New  statistical  methods  for  the
evaluation of cardiovascular risk markers: what the clini-
cian should know. Clin Sci (Lond). 2009;117(1):13–5.
 67. Shortliffe EH. Computer programs to support clinical

decision making. JAMA. 1987;258(1):61–6.

 68. Bates DW, Kuperman GJ, Wang S, Gandhi T, Kittler
A,  Volk  L,  et  al.  Ten  commandments  for  effective

clinical  decision  support:  making  the  practice  of
evidence-based medicine a reality. J Am Med Inform
Assoc. 2003;10(6):523–30.

 69. Gottlieb S. Health information on the internet is often

unreliable. BMJ. 2000;321(7254):136.

 70. Eysenbach  G,  Powell  J,  Kuss  O,  Sa  ER.  Empirical
studies assessing the quality of health information for
consumers  on  the  world  wide  web:  a  systematic
review. JAMA. 2002;287(20):2691–700.

 71. Adams SA. Revisiting the online health information
reliability debate in the wake of “web 2.0”: An inter-
disciplinary literature and website review. Int J Med
Inform. 2010;79:391–400.

 72. Emerson JF, Emerson SS. The impact of requisition
design  on  laboratory  utilization.  Am  J  Clin  Pathol.
2001;116(6):879–84.

 73. Kahan NR, Waitman DA, Vardy DA. Curtailing labo-
ratory test ordering in a managed care setting through
redesign of a computerized order form. Am J Manag
Care. 2009;15(3):173–6.

 74. Shalev V, Chodick G, Heymann AD. Format change
of  a  laboratory  test  order  form  affects  physician
behavior. Int J Med Inform. 2009;78(10):639–44.
 75. Westbrook  JI,  Georgiou  A,  Dimos  A,  Germanos  T.
Computerised pathology test order entry reduces lab-
oratory turnaround times and influences tests ordered
by  hospital  clinicians:  a  controlled  before  and  after
study. J Clin Pathol. 2006;59(5):533–6.

 76. Srivastava  R,  Bartlett  WA,  Kennedy  IM,  Hiney  A,
Fletcher C, Murphy MJ. Reflex and reflective testing:
efficiency  and  effectiveness  of  adding  on  laboratory
tests. Ann Clin Biochem. 2010;47:223–7.

 77. Laposata ME, Laposata M, Van Cott EM, Buchner DS,
Kashalo MS, Dighe AS. Physician survey of a labora-
tory medicine interpretive service and evaluation of the
influence of interpretations on laboratory test ordering.
Arch Pathol Lab Med. 2004;128(12):1424–7.

 78. Garg AX, Adhikari NK, McDonald H, Rosas-Arellano
MP, Devereaux PJ, Beyene J, et al. Effects of comput-
erized clinical decision support systems on practitio-
ner  performance  and  patient  outcomes:  a  systematic
review. JAMA. 2005;293(10):1223–38.

 79. CMS defines ‘meaningful use’. Proposed rule outlines
requirements  for  EHR  incentive  payments.  MGMA
Connex. 2010;10(3):10–3.

 80. Letter  to  David  Blumenthal,  MD,  MPP,  National
Coordinator  for  Health  Information  Technology
[http://healthit.hhs.gov/portal/server.pt/gateway/
PTARGS_0_11113_911075_0_0_18/MU%20
NPRM%20Recommendations%20Final%20PT_
clean.pdf].

 81. Miller RA, Schaffner KF, Meisel A. Ethical and legal
issues related to the use of computer programs in clin-
ical medicine. Ann Intern Med. 1985;102(4):529–37.
 82. Chuang JH, Hripcsak G, Heitjan DF. Design and anal-
ysis of controlled trials in naturally clustered environ-
ments:  implications  for  medical  informatics.  J  Am
Med Inform Assoc. 2002;9(3):230–8.

Implementation and Benefits
of Computerized Physician Order
Entry and Evidence-Based Clinical
Decision Support Systems

Stacy E.F.  Melanson, Aileen P. Morrison,
David W. Bates, and Milenko J. Tanasijevic

19

Keywords
Evidence-based  clinical  decision  support  systems  •  Computerized
 physician  order  entry  •  Evidence-based  medicine  •  Clinical  decision
 support systems

In the field of clinical pathology and laboratory
medicine,  test  complexity  and  test  menus  con-
tinue  to  expand,  necessitating  that  clinicians
obtain domain expertise to make the appropriate
testing  decisions  for  patients.  The  efficiency
focus  is  increasingly  central  because  healthcare
costs continue to rise and diagnostic testing rep-
resents  a  significant  portion  of  the  incremental
cost increase [1]. Institutions can no longer afford
to diagnose and manage patients without consid-
ering the overall cost-benefit impact of laboratory
tests.  Computerized  physician  order  entry
(CPOE)  and  clinical  decision  support  systems
(CDSSs)  are  one  modality  through  which  evi-
dence-based  medicine  and  practice  guidelines
can  be  deployed  to  assist  clinicians  at  the  time
orders are being placed with the goals of improv-
ing  the  quality  of  care,  decreasing  errors,  and

reducing  costs.  After  a  brief  introduction  to
CDSSs,  this  chapter  uses  specific  examples  to
illustrate how evidence-based clinical pathology
can  be  used  to  implement  CDSSs  and  monitor
their success through cost-benefit evaluation.

Introduction to Clinical Decision
Support Systems

CPOE,  which  allows  physicians  to  enter  orders
electronically  rather  than  using  paper  requisi-
tions,  provides  the  backbone  for  CDSSs.  While
the implementation of CPOE requires significant
analysis, planning, and resources, the benefits are
numerous, including standardization of practice,
improved  communication,  automatic  recording
of  auditing  data,  and  prevention  of  medication
misuse [2].

M.J. Tanasijevic (*)
Department of Pathology,
Brigham and Women’s Hospital,
Harvard Medical School, Boston, MA, USA
e-mail: mtanasijevic@partners.org

to

influence

CDSSs within CPOE are powerful tools with
which
test  ordering  behavior.
Evidence shows that a substantial portion of diag-
nostic testing may be unnecessary, that clinicians
may be ordering testing inappropriately and that

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_19, © Springer Science+Business Media, LLC 2011

323

324

S.E.F. Melanson et al.

the cost associated with diagnostic testing is high;
making  these  areas  obvious  targets  for  CDSSs.
Common strategies include implementing rules-
based order entry with reminders, offering testing
guidelines and displaying test-specific (e.g., test
charges)  or  patient-specific  (e.g.,  past  results,
patient medications) information, and using order
sentences to promote use of desired tests. In con-
trast  to  retrospective  reminders  and  educational
sessions, these strategies have generally produced
successful results.

Most  CDSSs  use  electronic  reminders  at  the
time of ordering, but, importantly, do not dictate
how  clinical  care  should  be  delivered.  CDSSs
should  be  designed  such  that  clinicians  view
these  systems  as  helpful  tools  instead  of  nui-
sances. Moreover, appropriate interventions and
guidelines must reach all users of the laboratory,
be  introduced  at  the  very  level  of  individual
 decision-making, and be nonintrusive [1].

group  that  is  exposed  to  the  intervention  and  a
control group that is exposed to the current state,
which  is  facilitated  by  use  of  the  computer
 system. A reasonable amount of time should also
be allowed to measure outcomes in both groups
consistent with the learning curve and volume of
testing.

Although initial development of the CDSS is
important, a committee structure responsible for
maintaining  and  updating  the  CDSS  based  on
external and internal evidence, literature review,
and  frequent  audits  is  critical  for  success,  and
depending on the size of the institution more than
one may be needed. For example, one may  handle
medication-related issues and another may tackle
laboratory issues.

Our  group  has  developed  a  number  of  such
interventions  and  randomized  studies.  Their
design and outcomes are described in the follow-
ing  sections  to  illustrate  the  general  principles
described above.

Basics of CDSS Implementation

The implementation of CDSSs should involve a
multidisciplinary  team  of  clinicians,  patholo-
gists,  hospital  administration,  and  information
technology.  Pathologists  are  integral  to  the
 process  because  they  understand  the  technical
and  clinical  aspects  of  laboratory  testing,  have
multispecialty  medical  knowledge,  are  data-
oriented,  commonly  work  on  multidisciplinary
teams,  and  understand  the  underlying  cost-
benefit implications.

Each institution must choose its own strategy
and appropriate test(s) to target based on discus-
sion with the multidisciplinary team and audit of
current  practices.  As  an  example,  chart  reviews
can  be  performed  to  assess  the  degree  of  inap-
propriate utilization of laboratory tests based on
established or internally derived clinical criteria.
Once a target test or group of tests is chosen, a
careful  design  of  the  intervention  is  critical  for
success. It is particularly important that the inter-
vention makes it quick and easy for clinicians to
make the correct decision. The effectiveness of a
particular intervention should be assessed through
a  randomized  study,  including  an  experimental

Optimization of Laboratory Test
Utilization

Reports have shown that as many as 10% of com-
monly ordered tests are redundant [3] and at least
30% of arterial blood gases may be unnecessary
[4].  Possible  explanations  for  the  excessive  test
ordering  include  clinicians’  difficulty  in  deter-
mining when the most recent test was performed
or lacking the knowledge regarding the appropri-
ate testing interval. Redundant testing is not only
costly but can also lead to unnecessary interven-
tions  or  treatments  if  false-positive  results  are
produced. However, managing test utilization has
been difficult and interventions such as feedback,
education,  rationing,  and  financial  incentives,
have shown limited and/or transient reductions in
utilization [5–9].

CDSS  within  CPOE  can  reduce  redundant
testing by providing utilization reminders at the
time  of  clinical  decision  making.  Furthermore,
CDSSs link the ordering clinicians with the par-
ticular order, simplifying utilization audits. These
electronic systems also allow outdated tests to be
removed from the system.

19

Implementation and Benefits of Computerized Physician Order Entry

325

Fig. 19.1  An order for a test can be overridden by providing justification (from Bates et al. [10], with permission of
Elsevier)

The published literature shows that  well-designed
electronic  reminders  can  decrease  the  number  of
redundant laboratory tests and decrease the overall
number  of  inappropriate  tests  performed,  thus
resulting in improved patient care and cost savings
[10–13].  Some  successful   interventions  include
displaying the date and the results from the most
proximal  previous
[13],  computerized
test
 prediction  of  abnormal  results  based  on  previous
results [12], and display of length of stay informa-
tion  based  on  diagnosis  [11].  The  selection  of
appropriate  targets  for  intervention  is  critical,  as
either  high  volume,  commonly  ordered  tests  or
those  with  the  highest  variable  cost   typically
have
impact.
Importantly, these interventions seek to increase
the proportion of tests ordered which are appro-
priate, not merely reduce overall test volume.

the  highest  postinterventional

In  one  randomized  study  at  our  institution,
redundancy  checks  were  triggered  when  clini-
cians  ordered  metabolic  profiles,  urinalysis,
therapeutic  drugs,  urine  cultures,  sputum  cul-
tures,  stool  cultures,  Clostridium  difficile  cul-
tures,  and  fibrin  split  products  [10].  In  most
cases,  the  interval  defining  redundancy  was
<20  h,  although  the  intervals  were  selected
through a review of the available evidence. Tests
ordered within the first 24 h of admission were
exempted. The default was set to cancel the test
order, but the clinician could override the deci-

sion support by providing a clinical justification
for the override (Fig. 19.1).

Urinalyses, chemistry profiles and urine  cultures
accounted  for  a  high  percentage  of  redundant
orders [10]. Redundancy alerts for these tests were
also  the  most  likely  to  be  overridden  by  the
 clinician. Some common reasons for a clinician to
override  a  reminder  were:  (1)  condition  warrants
more  frequent  testing,  (2)  clinical  condition  has
changed, (3) last result requires confirmation,  (4)
previous specimen unsatisfactory, and (5) different
site  or  testing  conditions  [10].  However,  upon
reviewing the medical records the override reasons
were justified in less than 50% of cases. It was also
discovered that many clinicians were never exposed
to the electronic reminders because laboratory tests
could have been ordered through sets or templates
independent  to  the  CDSS  [10].  Specimens  were
also  sometimes  sent  to  the  laboratory  directly
 without an order being placed, and lab policy at the
time required processing such specimens.

Overall,  the  study  found  that  the  CDSS  was
effective at reducing redundant tests. In the inter-
vention  group  only  27%  of  redundant  tests  were
ultimately ordered, while in the control group 51%
were ordered (Table 19.1). Importantly, the CDSS
in this instance did not have any adverse impact on
the quality of patient care, indicating that imple-
mentation of similar electronic reminders may be
warranted in targeted areas.

326

S.E.F. Melanson et al.

Table 19.1  CDSS effectiveness at reducing redundant tests

Intervention (n = 437)
Number ordered
136
113
110
  39
  15
  24
437

Test
Number performed
  85 (46%)
Urinalysis
  81 (57%)
Chemistry 20 profile
  50 (55%)
Urine culture
  18 (64%)
Sputum culture
    3 (21%)
Stool culture
  20 (49%)
Other
Total
257 (51%)
a The reminders were delivered in the intervention group and triggered, but not delivered, in the control group
From Bates et al. [10], with permission of Elsevier

Number performed
  35 (26%)
  37 (33%)
  22 (20%)
  14 (36%)
    3 (20%)
    6 (25%)
117 (27%)

Control (n = 502)
Number ordered
185
143
  91
  28
  14
  41
502

Antiepileptic Drug Monitoring

Antiepileptic drug monitoring accounts for almost
20% of the therapeutic drug testing  performed in
clinical  laboratories  [14].  Our  group  developed
appropriateness  criteria  for  antiepileptic  drug
monitoring  based  on  evidence-based  medicine
and expert opinion [14, 15]. These criteria were
not developed as extensive guidelines for clinical
appropriateness,  but  instead  to  provide  simple
rules with which to evaluate levels. The appropri-
ate indications included suspicion for toxicity or
noncompliance,  baseline  measurement  once  the
patient  has  reached  steady  state  or  a  change  in
dose or clinical condition (Table 19.2). Based on
these  criteria,  a  high  percentage  of  antiepileptic
drug levels were found to be ordered inappropri-
ately, usually due to routine daily ordering [14].
Furthermore, the inappropriate levels were rarely
clinically important.

We next implemented a CDSS to improve the
appropriateness of antiepileptic drug level moni-
toring  [16].  For  orders  which  appeared  redun-
dant,  an  automated  redundancy  reminder  was
provided (Fig. 19.2a), while nonredundant orders
prompted  an  educational  screen  with  common
indications for monitoring and pharmacokinetic
parameters of each antiepileptic drug (Fig. 19.2b).
These two interventions led to a 27 and 4% order
cancellation rate, respectively. Inappropriate test
ordering decreased from 54 to 15%. Furthermore,
the results were sustainable over a 4-year follow-
up  period,  suggesting  that  CDSSs  can  durably
affect clinician behavior.

Table  19.2  Appropriateness  criteria  for  antiepileptic
drug monitoring

Measuring a serum level is always appropriate

Within 6 h after a seizure recurrence
In the event of suspected dose-related drug toxicity a
In the event of suspected patient noncompliance
Measuring a serum level is appropriate only if the
blood sample is drawn in steady state conditions, i.e.,
after 4 half-lives on an unchanged dose regimenb

As a baseline measurement after starting antiepileptic
drug therapy
As a control measurement after a change in the dose
regimen
After adding a second drug with a potential for
interaction with the antiepileptic drugc
After a change in the patient’s liver or gastrointestinal
tract function

a For  phenytoin,  nystagmus,  ataxia,  and  drowsiness;  for
carbamazepine, gastrointestinal symptoms, diplopia, and
dizziness;  for  phenobarbital,  sedation,  depression,  and
cognitive decline; and for valproic acide, hepatic dysfunc-
tion and tremor
b Steady  state  is  assumed  to  be  reached  after  6  days  for
phenytoin,  after  3  days  for  carbamazepine  and  valproic
acid, and after 20 days for phenobarbital
c Another  antiepileptic  drug,  warfarin,
rifampicin
From Schoenenberger et al. [14], with permission

isoniazid,  or

Appropriateness of Digoxin Levels

Digoxin  levels  are  commonly  performed  to
assess  therapeutic  efficacy  and  compliance  as
well  as  evaluate  for  toxicity.  Appropriate
 timing  of  levels  depends  upon  many  factors
including  clinical  condition,  patient  status,

19

Implementation and Benefits of Computerized Physician Order Entry

327

Fig. 19.2 ( a and b) Examples of notes on phenobarbital drug level (from Chen et al. [16], © 2003–2010 American
Society for Clinical Pathology; © 2003–2010 American Journal of Clinical Pathology)

pharmacokinetics, and patient location. Clinical
criteria  to  evaluate  the  appropriateness  of
digoxin  levels  are  available  [17]  and  can  be
useful  to  guide  clinicians  and  reduce  the  cost
and time associated with inappropriate digoxin
levels.

Canas et al. [18] used a combination of litera-
ture review and expert opinion to develop appro-
priateness  criteria  for  monitoring  digoxin  levels
(Table  19.3).  Appropriate  indications  included
suspected  toxicity,  high-risk  patients,  dosage
adjustment  or  monitoring  after  steady  state  was

328

S.E.F. Melanson et al.

Table  19.3  Appropriateness  of  serum  digoxin  level
requests

Appropriate if
For both inpatients and outpatients:
1.  Subtherapeutic response (either A, B, C, or D)

A.   No improvement or worsening of congestive heart

failure or atrial fibrillation or flutter

B.  Suspected noncompliance
C.   Concomitant use of an interacting drug (antacids,
a kaolin and pectin combination [Kaopectate],
neomycin, quinidine, spironolactone, nifedipine,
cholestramine, verapamil)
D.  Suspected malabsorption
2.  Suspected toxicity (either A or B)

A.  Appearance of arrhythmias suspected to be caused

by digoxin (supraventricular tachycardia,
atrioventricular conduction defects, multifocal
premature ventricular contractions)

B.  Noncardiac signs or symptoms of digoxin toxicity
(visual changes, anorexia, nausea, vomiting,
diarrhea, abdominal pain, confusion, headache)

3.  High-risk patient (unstable or declining renal

function, low serum potassium level, hypoxia, recent
increase in diuretic dose)

4.  Initiation of digoxin therapy or dosage adjustment
after steady state reached (5 half-lives, 10 days)a

For inpatients:
5.  Admission level for inpatients if no previous digoxin

level within last 9 monthsb is available

For outpatients:
6.  Routine monitoring annually in outpatients on stable
dose of digoxin (inappropriate if level drawn less
than every 10 months)b

a Ten days was chosen in this study as a conservative esti-
mate of the interval required to reach steady state, although
some patients may reach steady state in 8 days
b Time intervals chosen by consensus of expert opinions
From  Canas  et  al.  [18].  ©  1999  American  Medical
Association. All rights reserved

achieved, admission levels, serial determinations
at least 10 days apart, and yearly routine monitor-
ing  in  outpatients.  The  authors  determined  the
number of digoxin levels on both inpatients and
outpatients that were drawn appropriately at their
institution based on these criteria.

They  found  that  many  as  84%  of  inpatient
digoxin  levels  had  no  appropriate  indication
[18]. Most frequently digoxin levels were drawn
more often than every 10 days for routine moni-
toring.  In  fact,  it  was  common  to  measure
digoxin levels daily in inpatients. The percent-
age  of  appropriate  levels  was  higher  (52%)  in

the  outpatient  population.  However,  similar  to
inpatients,  the  main  reason  for  inappropriate
levels was routine or too frequent monitoring. In
both  the  inpatient  and  outpatient  setting  the
number of toxic levels was low and most likely
misleading due to inappropriate ordering of the
levels.  In  one  patient,  the  dose  of  digoxin  was
decreased,  otherwise  no  other  interventions
were done as a result of high levels.

This study demonstrated that introducing CDSSs
with simple criteria for appropriate digoxin indica-
tions could improve the utilization of digoxin  levels
without compromising clinical care. Cost savings
associated  with  the  CDSS  were  deemed  to  be
 significant. As with antiepileptic drug monitoring,
this approach could be taken for other therapeutic
drugs, presumably with similar outcomes.

Appropriateness of Prostate-Specific
Antigen

Prostate-specific  antigen  (PSA)  testing  is  rela-
tively  costly.  Various  guidelines  are  available
that indicate clinical scenarios and patient popu-
lations in which PSA testing is deemed appro-
priate  [19].  The  American  Cancer  Society
recommends  providing  information  about  PSA
testing to men at average risk for prostate cancer
starting at age 50 [20,  21]. PSA testing is also
warranted  to  monitor  disease  progression  and
recurrence [22].

Poteat et al. [19] reviewed the available litera-
ture at the time of the study and developed a set of
appropriateness  criteria  for  PSA  (Table  19.4).
Testing  indications  included  screening,  prostate
cancer workup, monitoring for cancer recurrence,
and assessing treatment efficacy. Similar to other
studies referenced above, the criteria were devel-
oped using evidence-based medicine which con-
siders both benefit and cost. Tests with marginal
benefits,  such  as  screening  patients  with  a  less
than 10 year life expectancy, were not considered
appropriate. The authors then examined the appro-
priateness of PSA testing at their institution based
on these criteria [19] and developed an algorithm
for  examining  whether  clinically  relevant  new
information was obtained from the testing.

19

Implementation and Benefits of Computerized Physician Order Entry

329

Table 19.4  Appropriateness criteria for measuring serum prostate-specific antigen concentration

Appropriateness
Appropriate

Appropriate but debated

Inappropriate

Criteria
Assessing prostate cancer progression after therapy
Evaluating treatment efficacy during therapy
Monitoring for prostate cancer recurrence 2–4 times per year: patients 1, 2, and
3 years or more after treatment with curative intent receive a PSA assay every 3,
4, and 6 months, respectively
Diagnostic workup and staging in men with signs or symptoms associated with
prostate cancer
For men with carcinoma of unknown primary site
Establishing a baseline value before beginning therapy for benign prostatic
hypertrophy with a 5 alpha-reductase inhibitor, such as finasteride
As once yearly screening of asymptomatic men aged 50–75 years; screening
with PSA should be accompanied by rectal examination
Screening men with a family history of African-American men aged 40–75 years
As a staging modality to replace bone scan in selected cases of prostate cancer
Screening asymptomatic men older than 75 years or asymptomatic patients with
less than 10 years of life expectancy
Screening asymptomatic men with no risk factors younger than 50 years or those
with risk factors before age 40 years

From  Poteat  et al. [19]. © 2003–2010 American Society  for Clinical Pathology; ©2003–2010 American Journal of
Clinical Pathology

The  study  concluded  that  most  PSA  testing
was performed on outpatients and approximately
one fifth of the orders were considered inappro-
priate. A CDSS using simple age- and frequency-
based  criteria  could  have  eliminated  most
inappropriate  test  orders  without  compromising
clinical  information  leading  to  substantial  cost
savings in the laboratory.

Effect of Displaying Test Charges

Clinicians  are  typically  unaware  of  the  cost  of
tests  and  evidence  suggests  that  displaying  lab
charges  affects  clinician  behavior  and  might
reduce  cost  and  unnecessary  test  utilization
[1,  23].  Feedback  to  the  clinician  regarding
charges after they have placed the order, in an
attempt  to  curb  future  unnecessary  orders  for
expensive  tests,  has  had  variable  affects  [1].
However, displaying charges electronically using
CPOE  offers  the  advantage  of  communicating
the  information  in  real  time.  Furthermore,  it  is
easy,  nonintrusive  and  does  not  affect  quality.
Electronic  display  also  provides  ongoing  rein-
forcement by displaying the charge each time the

clinician attempts to order a test. Previous stud-
ies have been performed in the outpatient setting
and  have  shown  success  using  CPOE  displays
[23, 24].

Our institution performed a randomized con-
trolled trial with over 7,000 patients to determine
whether  the  display  of  charges  for  inpatients  at
the time of ordering affected test utilization and
cost, similar to that seen in studies on outpatients
[1].  In  the  intervention  group,  charges  were
 displayed for nineteen clinical laboratory tests at
the time of ordering and the total cost was tallied.
The  clinical  laboratory  tests  were  grouped  in
two  categories:  commonly  and  less  commonly
ordered.  There  was  no  significant  difference
between groups in the number of tests ordered in
either category. In addition, there was no signifi-
cant decrease in charges or potential cost savings
associated with the intervention.

The authors were surprised by the lack of impact
from displaying associated inpatient laboratory
charges  [1].  Possible  explanations  include  the
percentage  (53%)  of  orders  placed  through
CPOE,  resulting  in  the  number  of  clinicians
exposed  to  the  intervention  being  smaller  than
expected.  In  addition,  we  displayed  laboratory

330

S.E.F. Melanson et al.

charges as opposed to laboratory costs. The cli-
nicians may have been less sensitive to the for-
mer category.

Critical Results

Accrediting  organizations  such  as  the  Joint
Commission  and  the  College  of  American
Pathologists require that the laboratory commu-
nicate critical results to a licensed care provider
in  a  timely  manner  [25,  26].  Critical  results,
particularly  those  associated  with  adminis-
tering  certain  medications  can  also  signify
worsening  clinical  conditions.  For  example,
declining platelet counts in the setting of hep-
arin  therapy  raise  the  possibility  of  heparin-
induced
laboratory
thrombocytopenia.  Many
information systems are not sophisticated enough
to  flag  trends  in  test  results,  such  as  declining
values  over  time  or  lab  test–drug  interactions.
CDSSs can be designed to alert clinicians when
more complex scenarios regarding critical labo-
ratory  results  or  changes  in  laboratory  results
are obtained. Furthermore, immediate notifica-
tion of clinicians can ensure that intervention is
performed  in  a  timely  manner.  Time  to  inter-
vention  is  critical  as  some  studies  have  illus-
trated that delay in treatment can be significant
[27, 28]. Several institutions have implemented
CDSSs which page clinicians with results that
meet their critical criteria and warrant immediate
intervention [29–32].

In a study at our institution, the authors gathered
baseline data and investigated the number of critical
laboratory results each day, the time it took for a
clinician to act on these results and the time it took
for the patient’s clinical condition to resolve [33].
We  evaluated  high  and  low  sodium,  potassium,
and  glucose  levels,  and  falling  hematocrit.  An
average of 0.44 of these critical results per patient-
day was identified. The median time to treatment
was 2.3 h and the median time until the condition
was resolved was 14.3 h.

Potential treatment delays associated with the
standard,  telephone-based  critical  result  report-
ing prompted the designing, and implementation

of a CDSS to help improve the clinical response
time [34]. CDSS rules were designed to individu-
alize critical results by accounting for changes in
laboratory  results  over  time,  and  patient–drug
interactions  (Table  19.5).  For  example,  physi-
cians were paged when the patient’s serum potas-
sium was less than 3.3 mEq/L and the patient had
an active order for digoxin. Our group also devel-
oped  criteria  to  identify  appropriate  treatments
ordered after the critical result, and measured the
time to treatment ordered as well as time to criti-
cal  condition  resolved  in  the  control  and  inter-
vention group.  The  median time until treatment
ordered was significantly shorter for the interven-
tion  group  vs.  control  group  (1.0  h  vs.  1.6  h,
P = 0.003; mean, 4.1 h vs. 4.6 h, P = 0.003). The
time  until  the  critical  condition  resolved  also
decreased  (median,  8.4  h  vs.  8.9  h,  P = 0.11;
mean,  14.4  h  vs.  20.2  h,  P = 0.11).  The  studies
illustrate  a  decrease  in  time  to  notify  the  clini-
cians  as  well  as  a  decrease  in  time  to  take  the
appropriate action. Physicians were also very sat-
isfied  to  be  paged  about  these  values  –  95%  of
physicians  reported  a  high  level  of   satisfaction
with  the  approach.  A  key  to  success  was  being
highly selective regarding which tests physicians
were paged directly about.

Cost Benefits

Despite  studies  that  indicate  a  reduction  in
 medication  error  rates  and  improved  workflow
and  test  utilization  using  CPOE  and  CDSSs
[6, 35–37], relatively high costs and limited data
on financial benefits may limit their implementa-
tion.  Appropriate  assessment  of  cost-benefit  is
difficult to perform since it involves various cat-
egories  of  cost  across  different  hospital  depart-
ments. Moreover, the cost benefit associated with
reduction in high volume automated tests is lim-
ited, since a 50% reduction in test utilization may
only  translate  into  a  disproportionately  much
smaller savings in the laboratory, which might be
only 10–20% [38].

Total  system-wide  savings  may  be  affected
by  decreased  adverse  drug  events,  improved

19

Implementation and Benefits of Computerized Physician Order Entry

331

Table 19.5  Frequency distribution of alerts

Rule
1
2
3

4
5

6
7
8
9
10
11
12

Alerting criterion
Hematocrit has fallen 10% or more since last result and is now less than 26% b
Serum glucose is greater than or equal to 400 mg/dL
Hematocrit has fallen 6% or more since previous result, and has fallen faster than 0.4% per
hour since last result, and is now less than 26% and the patient is not on the cardiac surgery
service b
Serum potassium is greater than or equal to 6.0 mEq/L
Serum potassium has fallen 1.0 mEq/L or more over the last 24 h and is now less than
3.2 mEq/L c
Serum potassium less than 3.3 mEq/L and patient has an active order for digoxin c
Serum sodium is greater than 160 mEq/L
Serum sodium has fallen 15 mEq/L or more in last 24 h and is now less than 130 mEq/L d
Serum glucose is less than or equal to 40 mg/dL
Hematocrit is less than or equal to 15% b
Serum potassium is less than or equal to 2.4 mEq/L c
Serum sodium is less than or equal to 115 mEq/L d
Total

No. (%) a
38 (19.8)
34 (17.7)
32 (16.7)

  32 (16.7)
  29 (15.1)

  15 (7.8)
  5 (2.6)
  4 (2.1)
  3 (1.6)
  0 (0)
  0 (0)
  0 (0)
192 (100)

a Combined number of occurrences in control and intervention groups, after exclusions
b For low or falling hematocrit, rule 1 takes precedence over rule 3, which takes precedence over rule 10
c For low or falling potassium, rule 5 takes precedence over rule 6, which takes precedence over rule 11
d For low or falling sodium, rule 8 takes precedence over rule 12
From Kuperman et al. [34], with permission from BMJ Publishing Group Ltd

 workflow  and  efficiency,  decreased  drug  costs,
and  decreased  laboratory  and  radiological  test
 utilization  [35].  Further  interventions  aimed  at
reducing hospital length of stay can translate into
significant cost savings. For example, a CDSS that
provided  renal  dosing  guidance  and  recommend
dose adjustments based on a patient’s renal func-
tion was shown to decrease length of stay [39].

At our institution, Kaushal et al. [35] demon-
strated cumulative savings of $16.7 million over
a  10-year  period  ($2.2  million  annualized)  fol-
lowing  implementation  of  CPOE  and  CDSSs.
The greatest cumulative savings were renal dos-
ing  guidance,  nursing  time  utilization,  specific
drug  guidance,  and  adverse  drug  event  preven-
tion (Table 19.6).

Key Success Factors

Using experience with CDSSs at our institution,
certain patterns emerged that determined the suc-
cess of our CDSS interventions (Table 19.7) [40].
Most  importantly,  the  applications  must  not

slow  down  the  end  user.  Even  extremely  well-
documented decision support will fail if it takes
too long to place the order. Our end users rated
speed as much more important to them than either
quality  or  cost  [40].  Next,  due  to  time  pressure
and performance demands, the information must
be available readily when the clinician needs it.
If  too  many  interventions  are  implemented  the
overall speed of the system can be compromised
negating the potential benefits.

CDSSs, rather than simply providing electronic
information, should integrate data components such
as drug level and abnormal lab and present data that
clinicians  may  miss.  In  this   context,  particularly
useful are systems which remind clinicians to alter
a  drug  dose  based  on  declining  renal  function  or
which  suggest  a  clinical  action  such  as  order  a
trough level based on a medication order for vanco-
mycin;  so-called  “corollary  orders”  [40].  These
tools  should  be  integrated  into  clinical  workflow
such that they are displayed at the time of clinical
decision-making. Clinicians should not be able to
easily ignore reminders, but in turn, the reminders
should  be  informative  and  limited  in  volume.

S.E.F. Melanson et al.

332

a
s
t
fi
e
n
e
b

l
a
t
o
T

3
.
6

0
.
6

9
.
4

7
.
3

9
.
1

8
.
1

1
.
1

0
.
1

6
.
0

6
.
0

4
.
0

1
.
0

1
.
0

s
e
t
a
d

e
v
i
L

7
9
/
2
1

d
n
a

,
s
E
D
A
d
e
s
a
e
r
c
e
d

,
y
a
t
s

f
o

h
t
g
n
e
l

d
e
s
a
e
r
c
e
d

:
s
E
D
A
d
e
s
a
e
r
c
e
D

r
a
e
y
r
e
p

s
n
o
i
t
n
e
v
r
e
t
n
i

0
7
4
,
6
1

;
s
n
o
i
t
p
i
r
c
s
e
r
p

e
t
a
i
r
p
o
r
p
p
a

d
e
s
a
e
r
c
n
i

s
g
n
i
v
a
s

t
s
o
c

f
o
d
o
h
t
e

M

e
c
n
a
d
i
u
g

g
n
i
s
o
d

l
a
n
e
R

t
n
e
m
e
l
e
S
S
D
C

l
a
t
i
p
s
o
H
s
’
n
e
m
o
W
d
n
a
m
a
h
g
i
r

B

t
a

s
t
n
e
m
e
l
e
m
e
t
s
y
s

t
r
o
p
p
u
s
n
o
i
s
i
c
e
d

l
a
c
i
n
i
l
c

r
o
f

s
t
fi
e
n
e
b
e
v
i
t
a
l
u
m
u
C

.

6
9
1
e
l
b
a
T

3
9
/
7

3
9
/
1
1

4
9
/
0
1

8
9
/
4

7
9
/
2
1

5
9
/
7

4
9
/
1
1

4
9
.
5

4
9
/
7

0
0
/
2

0
0
/
5

3
9
/
7

3
9
/
7

7
9
/
7

8
9
/
8

7
9
/
2
1

6
9
/
0
1

5
9
/
3

d
r
o
c
e
r

n
o
i
t
a
r
t
s
i
n
i
m
d
a

n
o
i
t
a
c
i
d
e
m
a

e
t
a
r
e
n
e
g

o
t

e
m

i
t

g
n
i
s
a
e
r
c
e
d

y
b

y
l
r
a
l
u
c
i
t
r
a
p
s
e
s
r
u
n

r
o
f

w
o
fl
k
r
o
w
d
e
n
i
l

m
a
e
r
t
s

:
y
c
n
e
i
c
fi
f
e

d
n
a
w
o
fl
k
r
o
w
d
e
v
o
r
p
m

I

n
o
i
t
a
z
i
l
i
t
u

e
m

i
t

e
s
r
u
N

r
a
e
y

r
e
p

s
n
o
i
t
n
e
v
r
e
t
n
i

n
i
c
y
m
o
c
n
a
v

6
3
5
,
5
;
y
a
d

r
e
p
s
e
o
d

5
1
.
3

o
t

2
9
.
3
m
o
r
f

y
c
n
e
u
q
e
r
f

n
i

e
s
a
e
r
c
e
d

l
l
a
r
e
v
o

n
a

n
i

g
n
i
t
l
u
s
e
r

,
y
a
d

r
e
p
s
e
m

i
t

3

o
t

4
m
o
r
f

e
s
u

n
o
r
t
e
s
n
a
d
n
o

f
o

y
c
n
e
u
q
e
r
f

g
n
i
s
a
e
r
c
e
d
t
s
e
g
g
u
s

r
a
e
y

r
e
p
s
n
o
i
t
n
e
v
r
e
t
n
i

5
7
9

,
e
l
p
m
a
x
e

r
o
F

.
s
e
s
o
d

d
e
s
a
e
r
c
e
d

n
i

g
n
i
t
l
u
s
e
r

y
c
n
e
u
q
e
r
f

r
o

e
s
u

d
e
s
a
e
r
c
e
d

:
s
t
s
o
c

g
u
r
d

d
e
s
a
e
r
c
e
D

,
n
i
c
y
m
o
c
n
a
v

,
e
n
o
m
r
o
h

h
t
w
o
r
g

n
a
m
u
h
(

2
-
e
n
i
m
a
t
s
i
h

,
n
o
r
t
e
s
n
a
d
n
o

,
e
n
o
x
a
i
r
t
f
e
c

e
c
n
a
d
i
u
g

g
u
r
d

e
v
i
s
n
e
p
x
e

r
o

c
fi
i
c
e
p
S

)
s
r
e
k
c
o
l
b

r
o
t
p
e
c
e
r

,
y
g
r
e
l
l
a

,
y
c
n
e
u
q
e
r
f

,
e
t
u
o
r

,
e
s
o
d

g
u
r
d

h
g
u
o
r
h
t

s
E
D
A
d
e
s
a
e
r
c
e
d

:
s
E
D
A
d
e
s
a
e
r
c
e
D

n
o
i
t
n
e
v
e
r
p

t
n
e
v
e

g
u
r
d

e
s
r
e
v
d
A

s
g
n
i
n
r
a
w
y
r
o
t
a
r
o
b
a
l

d
n
a

,
n
o
i
t
c
a
r
e
t
n
i

g
u
r
d

e
r
a

s
e
g
r
a
h
C

.
s
t
s
e
t

y
r
o
t
a
r
o
b
a
l

f
o

g
n
i
r
e
d
r
o

d
e
s
a
e
r
c
e
d

:
s
t
s
e
t

y
r
o
t
a
r
o
b
a
l

d
e
s
a
e
r
c
e
D

y
a
l
p
s
i
d

e
g
r
a
h
c

y
r
o
t
a
r
o
b
a
L

.
s
t
s
e
t

d
e
r
e
d
r
o

r
e
w
e
f

%
5
.
4

n
i
g
n
i
t
l
u
s
e
r

r
a
e
y

r
e
p

s
e
m

i
t

8
0
6
,
0
1

d
e
y
a
l
p
s
i
d

s
g
n
i
n
r
a
w
y
r
o
t
a
r
o
b
a
l

t
n
a
d
n
u
d
e
r

d
n
a

;
n
o
i
t
a
c
i
n
u
m
m
o
c

d
e
v
o
r
p
m

i

h
g
u
o
r
h
t

s
E
D
A

t
a
e
r
t

o
t

e
m

i
t

d
e
s
a
e
r
c
e
d

:
s
E
D
A
d
e
s
a
e
r
c
e
D

s
e
i
t
i
l
a
m
r
o
n
b
a

y
r
o
t
a
r
o
b
a
l

l
a
c
i
t
i
r
c

g
n
i
d
r
a
g
e
r

r
a
e
y

h
c
a
e

d
e
t
a
r
e
n
e
g

e
r
a

s
t
r
e
l
a

0
2
7
,
6

d
e
z
i
r
e
t
u
p
m
o
c

a

y
b
s
n
o
i
t
a
c
i
d
e
m
s
u
o
n
e
v
a
r
t
n
i

f
o

e
s
u

d
e
s
a
e
r
c
e
d

:
s
t
s
o
c

g
u
r
d

d
e
s
a
e
r
c
e
D

g
n
i
k
a
t

e
r
a

o
h
w
s
n
o
i
t
a
c
i
d
e
m
s
u
o
n
e
v
a
r
t
n
i

e
v
i
s
n
e
p
x
e

n
o
s
t
n
e
i
t
a
p

s
e
fi
i
t
n
e
d
i

t
a
h
t

t
r
o
p
e
r

r
a
e
y
r
e
p

d
e
t
a
r
e
n
e
g

e
r
a

s
t
r
e
l
a

5
9
6
,
5
1

;
d
o
o
f

r
o
s
n
o
i
t
a
c
i
d
e
m

l
a
r
o

r
e
h
t
i
e

l
a
i
t
n
e
t
o
p

f
o

n
o
i
t
a
c
fi
i
t
o
n

n
a
i
c
i
s
y
h
p

y
l
r
a
e

h
g
u
o
r
h
t

s
E
D
A
d
e
s
a
e
r
c
e
d

:
s
E
D
A
d
e
s
a
e
r
c
e
D

n
i

g
n
i
t
l
u
s
e
r

r
a
e
y

r
e
p

s
e
m

i
t

7
1
8
,
2

d
e
u
s
s
i

e
r
a

s
g
n
i
n
r
a
w
y
r
o
t
a
r
o
b
a
l

t
n
a
d
n
u
d
e
R

s
t
s
e
t

d
e
t
s
e
g
g
u
s

f
o
%
9
6

f
o

n
o
i
t
a
l
l
e
c
n
a
c

r
a
e
y

r
e
p
s
n
o
i
t
n
e
v
r
e
t
n
i

0
3
2

y
l
l
a
r
e
n
e
g

;
s
E
D
A

e
c
n
a
d
i
u
g
l
a
r
o

o
t

s
u
o
n
e
v
a
r
t
n
I

g
n
i
t
r
e
l
a

y
r
o
t
a
r
o
b
a
l

c
i
n
a
P

r
o
t
i
n
o
m
E
D
A

e
m

i
t

t
a

s
t
n
e
i
t
a
p

r
o
f

s
s
e
c
c
a

n
o
i
t
a
m
r
o
f
n
i

d
e
v
o
r
p
m

i

:
y
c
n
e
i
c
fi
f
e

d
n
a
w
o
fl
k
r
o
w
d
e
v
o
r
p
m

I

y
r
a
m
m
u
s

n
o
i
t
a
c
i
d
e
m
d
e
t
a
m
o
t
u
A

t
s
i
l

n
o
i
t
a
c
i
d
e
m
a

e
t
a
r
e
n
e
g

o
t

d
e
d
e
e
n
e
s
i
w
r
e
h
t
o

e
m

i
t

f
f
a
t
s

s
e
s
a
e
r
c
e
d

;
e
g
r
a
h
c
s
i
d

f
o

s
h
p
a
r
g
o
i
d
a
r

B
U
K

f
o

e
s
u
r
e
v
o
e
c
u
d
e
r

o
t

r
a
e
y

r
e
p

s
n
o
i
t
n
e
v
r
e
t
n
i

8
8
4
,
2
s
e
t
a
r
e
n
e
g

d
n
a

g
n
i
t
s
e
t

y
r
a
s
s
e
c
e
n
n
u

d
e
s
a
e
r
c
e
d

:
n
o
i
t
a
z
i
l
i
t
u

l
a
c
i
g
o
l
o
i
d
a
r

d
e
s
a
e
r
c
e
D

t
n
a
t
s
i
s
s
a

h
p
a
r
g
o
i
d
a
r

)

B
U
K

(

l
a
n
i
m
o
d
b
a

n
a

;
n
o
i
t
a
t
n
e
m
u
c
o
d

d
e
v
o
r
p
m

i

s
n
a
i
c
i
s
y
h
p

r
o
f

w
o
fl
k
r
o
w
d
e
n
i
l

m
a
e
r
t
s

:
y
c
n
e
i
c
fi
f
e

d
n
a
w
o
fl
k
r
o
w
d
e
v
o
r
p
m

I

)
s
t
s
i
c
a
m
r
a
h
p

h
t
i

w
k
r
o
w
e
r

e
c
u
d
e
r

r
o

t
r
a
h
c

g
n
i
d
n
fi
e
m

i
t

d
e
c
u
d
e
r

,
.
g
.
e
(

,
t
u
o
-
e
l
u
r

,
s
n
o
i
t
a
c
i
d
n
i

y
g
o
l
o
i
d
a
R

t
n
a
t
s
i
s
s
a

d
n
a

n
o
i
t
a
z
i
l
i
t
u

e
m

i
t

n
a
i
c
i
s
y
h
P

e
g
r
a
h
c
s
i
d

l
a
t
i
p
s
o
h

t
a

s
n
o
i
t
a
d
n
e
m
m
o
c
e
r

t
s
e
t

c
i
g
o
l
o
t
a
m
u
e
h
r

0
2
1

y
l
e
t
a
m
i
x
o
r
p
p
a

:
s
t
s
e
t

y
r
o
t
a
r
o
b
a
l

d
e
s
a
e
r
c
e
D

e
c
n
a
d
i
u
g

l
e
v
e
l

g
u
r
d

c
fi
i
c
e
p
S

n
o
i
t
c
u
d
e
r

e
s
o
d

g
u
r
d

g
n
i
d
n
e
m
m
o
c
e
r

y
b

s
E
D
A
d
e
s
a
e
r
c
e
d

:
s
E
D
A
d
e
s
a
e
r
c
e
D

e
c
n
a
d
i
u
g

g
n
i
s
o
d

y
l
r
e
d
l
E

s
t
n
e
i
t
a
p

c
i
r
t
a
i
r
e
g

n
i

s
t
s
e
t

r
e
w
e
f

n
i

t
l
u
s
e
r

r
a
e
y

r
e
p

)
s
t
s
e
t

c
i
g
o
l
o
t
a
m
u
e
h
r

,
s
c
i
t
p
e
l
i
p
e
i
t
n
a
(

r
e
d
d
a
l
b

d
n
a

,
r
e
t
e
r
u

,
y
e
n
d
i
k
B
U
K

;
t
n
e
v
e

g
u
r
d

e
s
r
e
v
d
a
E
D
A

%
0
8

n
a

n
e
v
i
g

l
a
t
i
p
s
o
H
s
’
n
e
m
o
W
d
n
a
m
a
h
g
i
r

B

t
a

S
S
D
C

f
o

t
n
e
m
e
l
e

h
c
a
e

r
o
f

2
0
0
2

o
t

2
9
9
1
m
o
r
f

)
s
r
a
l
l
o
d

f
o

s
n
o
i
l
l
i

m
2
0
0
2

n
i
(

s
t
fi
e
n
e
b

e
v
i
t
a
l
u
m
u
c

e
h
t

s
t
c
i
p
e
d

e
l
b
a
t

s
i
h
T

a

d
t
L
p
u
o
r
G
g
n
i
h
s
i
l
b
u
P
J

M
B
m
o
r
f

n
o
i
s
s
i
m
r
e
p

h
t
i

w

,
]
5
3
[

.
l
a

t
e

l
a
h
s
u
a
K
m
o
r
F

e
t
a
r

t
n
e
m
e
s
r
u
b
m
i
e
r

e
v
i
t
c
e
p
s
o
r
 p

19

Implementation and Benefits of Computerized Physician Order Entry

333

Table 19.7  Ten commandments for effective clinical decision support

  1.  Speed is Everything
  2.   Anticipate Needs and Deliver

in Real Time

  3.  Fit into User’s Workflow

  4.  Little Things Can Make a Big Difference

  5.   Recognize that Physicians Will Strongly

Resist Stopping

  6.  Changing Direction is Easier than Stopping

  7.  Simple Interventions Work Best
  8.  Ask for Additional Information Only When

You Really Need It

  9.  Monitor Impact, Get Feedback, and Respond

10.   Manage and Maintain Your

Knowledge-based Systems

User satisfaction depends largely on the speed of the application
Information should be brought to the clinician at the time it is
needed
Guidelines which are available for passive consultation are less
effective than those which are built in to the ordering process
Screen design and usability can have a big impact and should be
carefully attended to
Clinicians often override suggestions to cancel an order

Changing defaults within the ordering screen or providing
alternate suggestions may be an effective way to change
physician behavior
Reminders should be simplified and fit onto one screen
Requiring physicians to input extra data elements may decrease
the success of a computerized guideline
Recording auditing data and gathering user feedback may help
to improve the intervention
Systems should be monitored for frequency of alerts, reminders,
responses, and overrides

Adapted from Bates et al. [40], with permission from BMJ Publishing Group Ltd

Some systems require clinicians to input a reason
why the reminder was overridden. A mechanism
should be in place to allow overriding reasons to
be tracked and audited.

The  CDSS  should  also  be  user-friendly  by
defaulting  to  the  most  common  decision  or  by
providing drop down menus instead of free text.
Importantly,  usability  testing  should  be  per-
formed  by  the  end  users,  not  the  developers  or
pathologists.

CDSSs  that  stop  clinicians  from  performing
an  action,  such  as  ordering  a  test,  should  be
avoided. Whenever possible an acceptable alter-
native should be provided. The decision support
interventions should be simple and fit on a single
screen without extraneous information that may
result in clinicians’ quitting the ordering session
before they reach the intended guideline.

Auditing the impact of the CDSS is also criti-
cal  as  many  interventions  do  not  produce  the
intended  results.  Feedback  from  end  users  is
important to determine users’ satisfaction and col-
lect valuable suggestions for improvement [40].

Lastly,  unanticipated  problems  should  be
expected. Our institution implemented a decision
support  system  linked  to  a  specific  test,  only  to
find that that test was ordered primarily through
order sets and the majority of clinicians were not

presented  with  the  support  [40].  Therefore,  a
troubleshooting team should be an integral com-
ponent of the process.

Future Directions

Review  of  the  literature  and  data  for  our  own
institution  illustrate  that  CDSSs  designed  using
evidence-based  medicine  are  effective  at  reduc-
ing the number of inappropriate laboratory tests
and  controlling  cost.  Each  institution  should
determine  appropriate  target  areas  for  CDSSs
that  promise  to  provide  the  highest  impact.
Internal audits and evidence-based guidelines are
helpful  tools  in  that  respect.  In  our  experience,
CDSSs targeting test utilization, therapeutic drug
monitoring,  and  critical  test  result  communica-
tion are highly effective.

As  technology  expands  and  many  institu-
tions implement CPOE that communicates bidi-
rectionally  with  the  laboratory  and  handheld
computers  to  guide  specimen  collection,  the
benefits of CDSSs can be magnified. Decision
support  may  be  implemented  not  only  at  the
time of order entry, but also at the time of speci-
men collection. Some potential benefits include
a  reduction  in  the  number  of  “no  sample

334

S.E.F. Melanson et al.

received”  and  “wrong  sample  type”  errors,
which  occur  when  a  test  is  ordered  but  no
 sample is drawn, or the wrong type of sample is
drawn. Furthermore, such  systems in conjunc-
tion  with  barcode  technology  can  prevent
 specimen  labeling  errors  [41–43].  Automated
systems  can  also  be  put  into  place  to  allow
orders to be added to existing specimens in the
laboratory, when appropriate, reducing the need
for  additional  phlebotomy.  Ultimately,  evi-
dence-based practice and CDSSs can capitalize
on  advances  in  information  technology  to
improve  workflow  and  quality  and  safety  in
healthcare,  with  the  net  being  substantial
improvement in all these areas.

References

  1.  Bates  DW  et  al.  Does  the  computerized  display  of
charges affect inpatient ancillary test utilization? Arch
Intern Med. 1997;157(21):2501–8.

  2.  Kuperman GJ, Gibson RF. Computer physician order
entry:  benefits,  costs,  and  issues.  Ann  Intern  Med.
2003;139(1):31–9.

  3.  Bates DW et al. What proportion of common diagnos-
tic tests appear redundant? Am J Med. 1998;104(4):
361–8.

  4.  Melanson  SE  et  al.  Utilization  of  arterial  blood  gas
measurements in a large tertiary care hospital. Am J
Clin Pathol. 2007;127(4):604–9.

  5.  Axt-Adam  P,  van  der  Wouden  JC,  van  der  Does  E.
Influencing  behavior  of  physicians  ordering  labora-
tory  tests:  a  literature  study.  Med  Care.  1993;31(9):
784–94.

  6.  Bates DW et al. Strategies for physician education in
drug  monitoring.  Clin  Chem.

therapeutic
1998;44(2):401–7.

  7.  Harpole LH et al. Automated evidence-based critiqu-
ing  of  orders  for  abdominal  radiographs:  impact  on
utilization  and  appropriateness.  J  Am  Med  Inform
Assoc. 1997;4(6):511–21.

  8.  Solomon DH et al. Techniques to improve physicians’
use of diagnostic tests: a new conceptual framework.
JAMA. 1998;280(23):2020–7.

  9.  Lyon AW, Greenway DC, Hindmarsh JT. A strategy to
promote  rational  clinical  chemistry  test  utilization.
Am J Clin Pathol. 1995;103(6):718–24.

 10. Bates  DW  et  al.  A  randomized  trial  of  a  computer-
based intervention to reduce utilization of redundant
laboratory tests. Am J Med. 1999;106(2):144–50.
 11. Shea S et al. Computer-generated informational mes-
sages directed to physicians: effect on length of hospi-
tal stay. J Am Med Inform Assoc. 1995;2(1):58–64.

 12. Tierney WM et al. Computer predictions of abnormal
test  results.  Effects  on  outpatient  testing.  JAMA.
1988;259(8):1194–8.

 13. Tierney WM et al. Computerized display of past test
results. Effect on outpatient testing. Ann Intern Med.
1987;107(4):569–74.

 14. Schoenenberger RA et al. Appropriateness of antiepi-
leptic  drug  level  monitoring.  JAMA.  1995;274(20)
:1622–6.

 15. Tanasijevic MJ, Bates DW. Criteria for appropriate
therapeutic  monitoring  of  antiepileptic  drugs.  In:
Therapeutic  drug  monitoring  and
toxicology,
American Association for Clinical Chemistry, editor.
Washington, D.C.; 1997. p. 13–19.

 16. Chen P et al. A computer-based intervention for improv-
ing the appropriateness of antiepileptic drug level moni-
toring. Am J Clin Pathol. 2003;119(3):432–8.

 17. Michalko  KJ,  Blain  L.  An  evaluation  of  a  clinical
pharmacokinetic  service  for  serum  digoxin  levels.
Ther Drug Monit. 1987;9(3):311–9.

 18. Canas  F  et  al.  Evaluating  the  appropriateness  of
Intern  Med.
level  monitoring.  Arch

digoxin
1999;159(4):363–8.

 19. Poteat HT, et al. Appropriateness of prostate-specific
antigen  testing.  Am  J  Clin  Pathol.  2000;113(3):
421–8.

 20. Wolf AM, et al. American Cancer Society guideline
for the early detection of prostate cancer: update 2010.
CA Cancer J Clin. 2010;60(2):70–98.

 21. Smith RA et al. Cancer screening in the United States,
2010:  a  review  of  current  American  Cancer  Society
guidelines and issues in cancer screening. CA Cancer
J Clin. 2010;60(2):99–119.

 22. Lieberman  R.  Evidence-based  medical  perspectives:
the evolving role of PSA for early detection, monitor-
ing of treatment response, and as a surrogate end point
of  efficacy  for  interventions  in  men  with  different
clinical risk states for the prevention and progression
of prostate cancer. Am J Ther. 2004;11(6):501–6.
 23. Tierney WM, Miller ME, McDonald CJ. The effect on
test  ordering  of  informing  physicians  of  the  charges
for  outpatient  diagnostic  tests.  N  Engl  J  Med.
1990;322(21):1499–504.

 24. Hampers LC et al. The effect of price information on
test-ordering  behavior  and  patient  outcomes  in  a
 pediatric emergency department. Pediatrics. 1999;103
(4 Pt 2):877–82.

 25. College  of  American  Pathologists.  Laboratory
Accreditation  Checklist.  [cited  2010  May  28];
Available from: http://www.cap.org/apps/cap.portal.
 26. The  Joint  Commission.  2010  National  Patient
Safety  Goals.  [cited  2010  May  28];  Available  from:
http://www.jointcommission.org/patientsafety/
nationalpatientsafetygoals/.

 27. Rind DM et al. Effect of computer-based alerts on the
treatment and outcomes of hospitalized patients. Arch
Intern Med. 1994;154(13):1511–7.

 28. Tate KE, Gardner RM, Weaver LK. A computerized
laboratory  alerting  system.  MD  Comput.  1990;7(5):
296–301.

19

Implementation and Benefits of Computerized Physician Order Entry

335

 29. Park  HI  et  al.  Evaluating  the  short  message  service
alerting system for critical value notification via PDA
telephones. Ann Clin Lab Sci. 2008;38(2):149–56.
 30. Piva E et al. Evaluation of effectiveness of a comput-
erized notification system for reporting critical values.
Am J Clin Pathol. 2009;131(3):432–41.

 31. Etchells E et al. Real-time clinical alerting: effect of
an automated paging system on response time to criti-
cal  laboratory  values–a  randomised  controlled  trial.
Qual Saf Health Care. 2010;19(2):99–102.

 32. Parl FF et al. Implementation of a closed-loop report-
ing system for critical values and clinical communica-
tion in compliance with goals of the joint commission.
Clin Chem. 2010;56(3):417–23.

 33. Kuperman  GJ  et  al.  How  promptly  are  inpatients
treated  for  critical  laboratory  results?  J  Am  Med
Inform Assoc. 1998;5(1):112–9.

 34. Kuperman  GJ  et  al.  Improving  response  to  critical
laboratory  results  with  automation:  results  of  a  ran-
domized  controlled  trial.  J  Am  Med  Inform  Assoc.
1999;6(6):512–22.

 35. Kaushal R et al. Return on investment for a computer-
ized physician order entry system. J Am Med Inform
Assoc. 2006;13(3):261–6.

 36. Dexter  PR  et  al.  A  computerized  reminder  system
to increase the use of preventive care for hospital-

ized  patients.  N  Engl  J  Med.  2001;345(13):
965–70.

 37. Overhage JM et al. A randomized trial of “corollary
orders”  to  prevent  errors  of  omission.  J  Am  Med
Inform Assoc. 1997;4(5):364–75.

 38. Winkelman JW. Less utilization of the clinical labora-
tory  produces  disproportionately  small  true  cost
reductions. Hum Pathol. 1984;15(6):499–501.

 39. Chertow  GM  et  al.  Guided  medication  dosing  for
inpatients  with  renal  insufficiency.  JAMA.  2001;
286(22):2839–44.

 40. Bates  DW  et  al.  Ten  commandments  for  effective
clinical decision support: making the practice of evi-
dence-based  medicine  a  reality.  J  Am  Med  Inform
Assoc. 2003;10(6):523–30.

 41. Hayden  RT  et  al.  Computer-assisted  bar-coding
 system  significantly  reduces  clinical
laboratory
 specimen identification errors in a pediatric oncology
hospital. J Pediatr. 2008;152(2):219–24.

 42. Morrison  AP  et  al.  Reduction  in  specimen  labeling
errors after implementation of a positive patient iden-
tification  system  in  phlebotomy.  Am  J  Clin  Pathol.
2010;133(6):870–7.

 43. Bologna  LJ,  Lind  C,  Riggs  RC.  Reducing  major
 identification  errors  within  a  deployed  phlebotomy
process. Clin Leadersh Manag Rev. 2002;16(1):22–6.

Evidence-Based Pathology and Tort
Law: How Do They Compare?

20

Mark R. Wick and Elliott Foucar

Keywords
Evidence-based pathology • Tort law in medicine • Evidence-based pathology
in the legal system • Medical malpractice and evidence-based medicine

“Medical malpractice reform has long been the graveyard for high hopes and good
intentions.”

– (D. Hyman, JD, Professor, University of Maryland School of Law, 2002) [1]

“Tort reform …hurts the hapless patients who  suffer grievous harm at the hands
of incompetent doctors.”

– (Editorial opinion, The New York Times, January 2005) [2]

“The justice system in America works, and it works very well.”

– (Mark Lanier, JD, plaintiffs’ attorney, commenting on a $253.5 million jury
award  based  on  less  than  one  hour  of  jury  deliberations  devoted  to  the
 pathogenesis of the cardiac death of the plaintiff’s husband. August 2005) [3]

“Jury awards can be … inexplicable on any basis but caprice or passion.”

– (Justice Sandra Day O’Connor, US Supreme Court, commenting on a 1993 9th Circuit
Court ruling) [4]

“Tears have always been considered legitimate arguments before a jury. Indeed, if counsel
has them at his command, it may be seriously questioned whether it is not his professional
duty to shed them whenever proper occasion arises.”

– (Proceedings, Tennessee Supreme Court, 1897) [5]

“Tort” (from middle-English, essentially meaning
“injury”)  law  is  a  complex  set  of  procedures  for
decision-making, the purported goal of which is to
use facts to resolve disputes. The problems at issue
reflect an allegation – made by the plaintiff(s) – that
carelessness has led to a personal injury. The careless,

M.R. Wick (*)
Department of Pathology, University of Virginia Medical
School, Charlottesville, VA, USA
e-mail: mrw9c@virginia.edu

or “tortious” act (in legal parlance, “negligence”),
can either be a wrongful deed or the failure to do
something.  Malpractice  cases  are  specialized  tort
actions  that  are  based  on  a  plaintiff’s  claim  of
 professional  negligence  (by  physicians,  dentists,
lawyers, architects, engineers, etc.).

The  quality  of  legal  decisions  in  tort  law
depends on the soundness of the rules on which
its procedures are based; the quality of available
information;  and  the  skill  of  ultimate  decision-
makers  in  understanding  and  integrating  those

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1_20, © Springer Science+Business Media, LLC 2011

337

338

M.R. Wick and E. Foucar

factors  [6,  7].  In  medical  malpractice  cases  as
well as other kinds of “civil” (noncriminal) suits,
“expert” witnesses are often the principal source
of technical information that is introduced to the
court [8]. Those individuals are usually – but not
always  –  physicians  themselves  [9].  Lay  jurors
listen  to  presentations  by  the  “experts”  and  the
attorneys and are ultimately charged as the “find-
ers  of  fact.”  They  mix  their  community  values
with  the  evidence  presented  at  trial,  under  the
 direction of the trial judge, to reach a legal deci-
sion for either the plaintiff or the defendant.

The  traditions  of  English  common  law  have
been  refined  over  several  centuries,  and  they  are
the foundation for administration of the tort sys-
tem, both conceptually and procedurally. Although
many  cases  that  enter  the  system  are  “resolved”
and  never  come  to  trial,  negotiations  leading  to
that  outcome  are  dominated  by  the  opponents’
opinions of what the outcome would be if the case
were to go to a jury. Early on, it became increas-
ingly apparent that lay jurors would likely require
“expert”  input  to  come  to  rational   conclusions
concerning  technically  complicated  issues  (such
as those in the area of medical  malpractice). For
example, as early as the 1700s, judges opined that
medical  standards  should  be  “testified-to  by  the
surgeons themselves” [10]. That led to the custom
for each party to engage its own “experts.” Sheila
Jasanoff (John F. Kennedy School of Government,
Harvard University) has stated that societies expect
“experts” “…to have thought more carefully and
responsibly than any of us, as individual citizens,
could possibly hope to do” [11]. Whether or not
that is always true is open to debate.

Great import has been attached to the ability of
juries to deliberate in “good faith” and “good con-
science,” but much less concern has been expended
over  the  quality  of  objective  data  provided  to
them. Because of the growing technological com-
plexity  of  society  at  large,  good  information  is
every  bit  as  important  as  “good  faith”  or  “good
conscience.” Indeed, there is a real risk that, when
presented with conflicting opinions on unfamiliar
subjects,  well-meaning  jurors  may,  in  “good
faith,” be influenced by testimony in the realm of
so-called “junk-science” [12].

Recent legal statutes and decisions have aimed
to  better  the  quality  of  tort  law  decisions  by

to

improve  “expert”

attempting
testimony.
However,  in  analogy  to  the  experience  of  many
physicians with some aspects of “evidence-based
medicine,” lawyers have found it easier to describe
ideal  scientific  evidence  than  to  effectualize  it.
This is particularly true because the Law has tra-
ditionally  not  been  very  discerning  about  scien-
tific  rigor.  It  has  instead  focused  on  procedural
priorities  that  are  often  incompatible  with  strict
scientific  standards.  In  other  words,  the  practice
of scientifically-based medicine and the practice
of Law can be, and often are, very dissimilar indeed.
This overview examines the American tort sys-
tem  from  an  evidence-based  perspective,  with  a
particular  orientation  towards  medical  malprac-
tice actions. It includes a discussion of standards
for “outcomes analysis” in the Law; recognition
and classification of errors made by the courts; the
relationship between medical errors, “negligence,”
and “standard of care”; and the issue of reconcil-
ing plaintiffs’ rights with medical–scientific facts.
We also consider selected obstacles to developing
a  system  that  is  capable  of  reaching  evidence-
based  decisions  on  complex  scientific  topics,
including  the  interpretation  of  tissue  specimens
by pathologists.

High-Quality Decisions in Tort Law

It is impossible to discuss the importance of “good
information” in the courts without first considering
how one recognizes a high-quality, evidence-based
outcome in a tort action. Because the legal process
can be said to produce binary results (for or against
a plaintiff ), an evaluation of its performance can be
accomplished  using  measures  that  are  familiar  to
pathologists. One such tool is the use of the familiar
four-cell table, which compares given test results to
accepted  standards.  This  presentation  allows  for
classification  of  results  as  “true  positives,”  “true
negatives,” “false positives,” and “false negatives.”
However, in reference to jury decisions, one could
ask what standard of comparison should be used in
that  process.  Many  lawyers  would  say  that  the
jurors’ judgment is itself that standard, and there-
fore only “true-positive” and “true-negative” results
are  operative  in  the  courts.  This  viewpoint  was
apparent in an extreme form when the US Supreme

20  Evidence-Based Pathology and Tort Law

339

Court ruled that even the decisions of jurors who
were  actively  using  mind-altering  drugs  during  a
trial were valid [13].

From  a  scientific  perspective,  the  approach
just described is patently unsound. It is a closed
circle that mechanistically compares a result with
itself. Moreover, it represents a barrier to improve-
ment  of  the  “test.”  In  other  words,  why  study
 performance  when  the  test  is  already  known  to
provide the best possible answer?

How does one rectify the problem? The authors
believe  that  when  a  trial  is  centered  on  a  puta-
tively  erroneous  pathologic  interpretation,  the
final jury decision should be compared with the
consensus conclusion of a group of knowledge-
able  and  unbiased  pathologists  (KUPs).  Those
individuals would not be engaged by lawyers for
the plaintiff or defense, but rather by the court in
general. As such, they would truly constitute a
“peer”  group  with  regard  to  the  status  of  the
defendant. A similar paradigm could apply to all
malpractice  actions  concerning  any  professional
vocation. Jury pronouncements that departed from
the consensus “standard” could then be classified
scientifically as “false-positive” or “false-negative”
results.

However,  pathologists  –  and  physicians  in
general  –  must  acknowledge  that  the  identifica-
tion of legal “test”-malfunction is more compli-
cated than finding problems in medical validity.
Although the courts do make technical informa-
tion available to jurors, an important basic con-
ceptual difference between the Law and Medicine
must be realized – it is not the primary goal of the
tort  system  to  achieve  a  scientifically  correct
conclusion,  but  rather  to  assure  the  legal  and
social  rights  of  the  plaintiff.  Because  attorneys
and  judges  are  educated  people,  they  could
certainly design a system intended to mirror the
opinions of knowledgeable and unbiased profes-
sional “experts.” Instead, the existing model sim-
ply guarantees the plaintiff a right to bring his or
her complaint to the court, and to be adjudicated
by a jury of the plaintiff’s “peers.” As alluded-to
earlier,  such  “peers”  are  not  really  “equals”  of
defendants in proceedings that concern profes-
sional and scientific issues. That is especially so
because attorneys actually aim to exclude jurors
who fit that description.

When  a  jury  trial  occurs  in  the  present  legal
schema,  sociopolitical  aspects  of  “peer  review”
of the plaintiff’s complaint are generally met but
scientific  ones  are  not  [14].  If  one  accepts  the
premise  that  lay  jurors  will  continue  to  decide
the  results  of  professional  malpractice  cases,
efforts at reforming the system must aim to remove
personal bias from the  testimony of “experts” and
assure the validity and strength of their scientific
credentials.  Panels  of  court-appointed,  unbiased
peer-professionals could also be used to provide
appropriate counsel to judges.

Trial decisions that differ from the results of
scientific  analyses  can  be  best  understood  by
dividing  them  into  two  categories  –  (1)  techni-
cally dissonant and politically consonant, and (2)
technically dissonant and politically dissonant.

Type 1 Jury Errors: Technically
Dissonant But Politically Consonant

When  a  type  1  error  occurs,  the  scientific  and
medical  information  (SMI)  provided  to  jurors
was accurate and complete, but the jurors were
unable  to  understand  that  information  or  chose
to ignore it. For example, they may have based
their  group- decision  on  “community  values”
(such as feeling sorry for the plaintiff or “liking”
the defendant). In other words, the jury members
felt  that  their  decision  was  the  “right”  (politi-
cally consonant) thing to do.

This  type  of  error  predictably  results  from
 lay-person  juries  having  to  make  decisions  on
problems  involving  complicated  scientific  issues,
or when clinical outcomes produce juror sympathy.
Efforts to eliminate this category of jury “malfunc-
tion”  would  require  radical  changes  in  the  legal
system,  e.g.,  removing  juries  completely  from
malpractice litigation by invoking the “complexity
exemption” in the seventh Amendment to the U.S.
Constitution [15]. Realistically, such attempts would
undoubtedly  face  daunting   political  opposition.
Indeed, there is instead an existing social trend in
the  opposite  direction,  i.e.,  challenging  opinions
of the “educated elite” by “democratizing” decisions
that concern science and technology [16, 17].

It  is  discouraging  to  most  physicians  and
pathologists  in  particular  when  valid  SMI  is

340

M.R. Wick and E. Foucar

laboratory

ignored, because our focus is on the validity and
performance  of  objective
tests.
However, we also accept that “wrong” diagnoses
are  unavoidable.  For  example,  if  one  feels  that
patient  welfare  is  best  served  by  an  assay  that
preferentially produces false-positive results, the
test in question is intentionally designed to favor
sensitivity over specificity [18].

Even though it is “mixing metaphors,” the court
consciously weighs “politically-correct” decisions
against  scientifically  valid  ones.  Furthermore,
because tort law is specifically aimed at achieving
social–political objectives, the objective scientific
integrity of its processes can suffer.

Type 2 Jury Errors: Technically
Dissonant and (Therefore) Politically
Dissonant

In  type  2  errors,  at  least  some  SMI  provided  to
the  jury  did  not  accurately  reflect  the  reality  of
medical  practice  or  scientific  fact,  and  the  jury
apparently relied on that inaccurate testimony to
reach a final decision. This form of legal dysfunc-
tion is actually amenable to reform.

Accrued evidence supports the idea that most
citizens value lay-person “peer” juries, inevitably
making  the  courts  vulnerable  to  type  1  error
[19–21].  However,  there  is  no  indication  that
people  want  to  be  misled  scientifically  while
serving as jurors. Hence, one can conclude  that
when  inaccurate  medical  testimony  produces  a
verdict that a properly informed jury would not
have  reached,  the  legal  outcome  is  both  techni-
cally flawed and sociopolitically incorrect.

that  of  their  medical  peers.  Indeed,  one  could
justifiably argue that the rights of the defendant
had been violated in that context.

Doctors – being nonlawyers – must recognize
the fact that individual “rights” can  conflict with
each  other  and  are  pragmatically  unequal.  The
courts are focused on deciding which rights take
precedence over others. The seventh Amendment
assures any defendant the “right” to have a jury
of peers in a tort case [22]. Conversely, one has
no  constitutional  right  to  a  jury  decision  that
matches  the  opinion  of  unbiased  and  optimally
qualified “experts.” No existing statute or judicial
decision mandates that professional malpractice
defendants  have  a  right  to  be  judged  by  voca-
tional and educational equals.

The rights of citizen jurors can also be com-
promised in the courtroom. In their role as con-
sumers,  they  benefit  from  legislation  aimed  to
information  on
inaccurate
protect  against
 medications,  food,  investments,  and  other  tan-
gible life elements [23, 24]. However, as jurors,
those  same  people  encounter  the  rights  of
plaintiff’s  and  defendant’s  attorneys  to  present
the “strongest possible case,” including the opin-
ions  of  “experts”  for  both  sides.  Lawyers  who
knowingly  use  “experts”  to  misinform  juries  –
a  practice  that  unfortunately  is  real  –  are  like
companies who seek to deceive their consumers
[25]. Tort reformers feel that the present system
of  “expert”  testimony  is  an  anachronism  that
exploits  the  naivete  of  lay  jurors.  Contrarily,
defenders of the status quo believe that nonpro-
fessional  “peer”  juries  are  effective  even  when
presented with “junk” expert testimony.

Adversarial “Experts”

Rights Are Not Equal

Physicians  think  of  laboratory  test  design  in
terms  of  precision  and  accuracy.  They  accord-
ingly  have  trouble  understanding  a  “test”  ( jury
trial)  that  prioritizes  a  social  goal;  namely,  the
preservation of the plaintiff’s rights. As a result,
doctors  who  are  sued  usually  are  incensed  if  a
jury reaches a decision that would be contrary to

Using  physicians  to  explain  pathology-related
issues to juries is certainly preferable to using no
experts at all, or such “experts” as architects who
would likely have no familiarity with the topic at
issue. However, one would be incorrect in assum-
ing  that  every  physician  specialist  reflects  the
prevailing view of his or her specialty group
as a whole. Indeed, some “experts” may hold to

20  Evidence-Based Pathology and Tort Law

341

opinions that are highly idiosyncratic or even bla-
tantly incorrect. Experience attests to the reality
that a medical education does not protect a person
against intellectual or ethical failures [26, 27].

have been exposed in public forums, such as the
case of a single physician-“expert” who person-
ally certified a diagnosis of asbestosis in >50,000
cases [32].

It is obvious that the courts do want to know
what pathologists think, but it is troubling to see
how  that  information  is  sometimes  obtained.
Science holds that in order to draw rational con-
clusions from a sample of any given population,
the  sampling  must  be  done  systematically  [28].
That principle applies to everything from presi-
dential  polls  to  taste-tests  of  potato  chips.  One
cannot  simply  have  opposing  factions  report
highly select opinions that favor a certain candi-
date or product. Unfortunately, that basic concept
does not have traction in the legal world; the right
of  lawyers  to  find  “experts”  who  support  their
 clients’  positions  supersedes  the  need  for  accu-
rate  SMI  in  the  courtroom.  The  selection  of
“experts”  sometimes  even  ignores  the  need  for
specialized training or experience in the pertinent
topic.  Unlike  scientific  sampling,  legal  searches
for  “experts”  are  comparable  to  “comparison-
shopping”  for  predefined  items  at  particular
prices.  Thus,  the  jury  may  hear  diametrically
opposed  “expert”  opinions,  effectively  forcing
jurors to rely on factors such as the experts’ cha-
risma or lack thereof [29].

Biased  selection  of  “experts”  by  lawyers  is
further complicated by the inability of judges to
weigh and digest SMI and by personal idiosyn-
cracies of jurists that can be prejudicial [30]. In
fact, attorneys recognize that “…the trial judge is
hardly  a  more  qualified  assessor  of  scientific
credibility  than  the  jury  itself”  [31].  When  the
judge fails as a gatekeeper of accurate informa-
tion,  jurors  will  be  faced  with  apparent  uncer-
tainty and disagreement among “experts.” In fact,
no valid disagreement may exist in the proffered
SMI, if testimony were to be evaluated by scien-
tifically adept parties instead of by lay persons.

A quantification of the level of flawed SMI in
the courtroom would be helpful in understanding
how science affects jurisprudence. However, such
data are currently unavailable and will probably
continue to be so. Only the most egregious exam-
ples of fraudulent (? criminal) scientific testimony

Scientific Information and Juries

Although the civil court system values social con-
cerns at least as highly as scientific validity, SMI
is  still  a  part  of  malpractice  lawsuits.  It  is  ger-
mane to ask whether lay jurors can properly digest
technical details in cases that involve pathologists
or other professional defendants. The process of
teaching the intricacies of pathology to residents-
in-training  takes  several  years  beyond  medical
school. Hence, as expected, public records reflect
the fact that lay juries are often baffled by pathol-
ogists’ testimony. One juror, who was interviewed
after a trial that included pathologic information
on  coronary  arterial  thrombosis  and  myocardial
infarction,  compared  the  SMI  presented  in  the
courtroom to the inchoate sounds coming from a
faceless teacher in a “Charlie Brown” cartoon on
television  [33].  Nevertheless,  most  lawyers
continue  to  aver  that  “expert”  testimony  can  be
assimilated successfully by lay jurors. Believing
assertions  such  as  that  may  demand  substantial
credulity [34].

It would be relatively easy to convene “mock”
juries,  give  them  conflicting  “expert”  medical
opinions, and test them to see how much scientific
information  had  been  absorbed  correctly.  That
process  would  provide  at  least  some  tangible
information on the ability of lay people to digest
SMI. Nonetheless, because the legal system lacks
even  the  rudimentary  features  of  an  objective,
evidence-based  mechanism,  such  tests  of  proce-
dural  validity  have  never  been  performed.  The
effects of professional inertia and self-interest are
probably also operative in this problem [35].

Sporadic  assertions  have  been  made  in  the
legal  literature  that  a  “statistically  significant”
correlation  exists  between  jury  verdicts  and
“expert”  opinions  [14].  However,  this  would
only  show  that  jurors  do  not  ignore  “expert”
opinion.  Moreover,  physicians  who  attempt  to

342

M.R. Wick and E. Foucar

improve  “expert”  testimony  in  the  courtroom
by  publicly challenging the opinions of profes-
sional  mavericks  are  in  danger  of  being  sued
for  offenses  such  as  “defamation  of  character”
[36].  Sadly,  most  physicians  and  medical  spe-
cialty  organizations  have  ignored  this  problem
altogether [37].

Two Cultures: Lawyers and Doctors

Lawyers and judges control the legal system, and
it  would  seem  rational  for  physicians  to  work
with  these  individuals  to  reduce  jury  error.
Unfortunately, the two parties typically adhere to
irreconcilable  paradigms  in  their  evaluation  of
the  Law.  Physicist  and  novelist  C.P.  Snow  was
famous,  in  part,  for  his  Cambridge  University
lectures in 1959, which noted that science and the
humanities are populated by people who do not
understand each other because they live in different
cultures [38–40]. Similarly, the philosophical gap
between lawyers and physicians is a sizable one.
Each group has its own modes of training, spe-
cialized  terminologies,  professional  objectives,
ways of evaluating the results of their work, and
forums for publication and discussion of profes-
sional thought. In the main, these are only margin-
ally related to one another. Consequently, several
observers  have  noted  a  “rawness”  of  physician-
based  antipathy  toward  attorneys,  as  well  as  a
“searing distrust” of the courts [41].

Sadly,  effective  criticism  of  the  use  of
“experts”  who  sometimes  misinform  jurors
depends to some extent on the inherently weak
foundation  of  what  is  known  as  “argument  by
incredulity,”  that  is,  my  view-point  is  true
because  I  can’t  imagine  it  to  be  false  [42].
Physicians  may  consider  it  to  be  simply  unac-
ceptable to ever allow the delivery of misinfor-
mation  to  jurors.  In  contrast,  a  critical  mass  of
lawyers  believes  that  the  opportunity  for  cross
examination of “experts,” truthful opposing tes-
timony,  and  the  option  to  appeal  unfavorable
verdicts  effectively  compensates  for  f laws  in
“expert” testimony [43, 44]. Physicians are free
to consider the latter opinion overtly wrong, but
that does not prove that physicians are correct.

Medical Error and “Standard
of Care”

There  has  been  widespread,  intense  pressure  to
reduce medical error – a laudable goal. With that
fact as a background, one might assume that the
tort system could be a valuable asset in prevent-
ing iatrogenic harm to patients.

In  principle,  that  premise  could  be  true;  in
actuality,  however,  it  is  not.  An  allegation  of
medical “negligence” always attends malpractice
lawsuits, but many plaintiffs’ lawyers try to blur
the distinction between true negligence – that is,
the willful or careless commission of a wrongful
act – and simple human or system-based error, or
adverse outcomes not due to error. That situation
stifles  any  meaningful  input  from  the  courts  in
estimating the relative weight of those elements
as causes of adverse clinical outcome.

In  contrast  to  medical  error,  which  has  been
studied assiduously over the past decade, depar-
ture  from  “the  standard  of  [medical]  care”  is  a
vestigial  legalism  that  has  only  weak  links  to
medical error analysis. For example, in the Law,
it  does  not  matter  whether  a  misdiagnosis
stemmed from ambient disturbances in the labo-
ratory, technical problems in the histology labo-
ratory, transposition of specimens before receipt
in the pathology suite, or misinterpretation of the
disease  process  by  a  pathologist.  Laboratory
directors and practicing pathologists are held per-
sonally and globally responsible for all of those
factors;  they  are  all  subsumed  by  the  phrase
“standard of care.”

Tort cases involving pathologists depend upon
fellow pathologists’ perception of standard of care
three potential dispositions;

 1.  No expert can be found to assert that a diagno-
sis or interpretation fell below the “standard of
care,” and the plaintiff has no case

 2.  The mistake is blatantly the result of substan-
dard  practices  or  professional  incompetence,
as judged by unbiased peer-evaluators, and no
expert can be found who will testify in favor
of the pathologist. The particular details of the
error are important only in regard to the award

20  Evidence-Based Pathology and Tort Law

343

of damages. The error is so blatant that prin-
ciples of prevention analysis do not apply
 3.  “Expert”  opinions  differ  over  whether  there
was  a  negligent  departure  from  the  “standard
of care.” If the case is not removed from the
system  by  settlement  or  summary  judgment,
the conflicting “experts” will address a jury in
court. The jury will have to decide which expert
opinion reflects practice reality, and then how
to integrate this conclusion into a final verdict.
The  most  valuable  information  on  medical
error coming from the courts has been collected
by  insurance  companies,  not  lawyers,  and  ana-
lyzed  by  other  physicians  [45].  However,  such
data are very incomplete, because many malprac-
tice  cases  are  settled  under  private  terms  [46],
and  details  of  jury  deliberations  are  not  often
made available as public information.

Plaintiffs’ attorneys aver that they are “fighting
medical error” by threatening tort actions against
physicians  who  deliver  substandard  care  [47].
Nonetheless,  no  credible  objective  proof  has
appeared showing that this approach does produce
improvement in medical practice or patient wel-
fare.  Undeniably,  however,  it  does  measurably
discourage doctors from practicing in geographic
locales where torts are rife. In addition, it has been
proven  beyond  doubt  that  perceived  malpractice
risks prompt physicians to over-order tests, medi-
cations,  and  procedures  in  a  defensive  posture,
elevating the cost and complexity of medical care
[48].  The  vacuous  concept  called  “standard  of
care”  leads  doctors  to  think  increasingly  about
how lay jurors might respond to each of the many
professional decisions that comprise patient care
[49]. Typically, that type of rumination is scien-
tifically unproductive, expensive for the medical
system, and inefficient.

Is the Professional “Standard
of Care” a Valid Concept?

At their extremes, the ideas underlying “standard
of  care”  are  straightforward.  Everything  in-
between is a muddle.

One can attempt to resolve this confusion by
consulting a legal dictionary, which says that the
“standard of [professional] care” is “the average
degree of skill, care, and diligence exercised by
members  of  the  same  profession,  practicing  in
the  same  or  a  similar  locality,  in  light  of  the
 present state of… science” [50]. That definition
is  inherently  nebulous.  In  the  same  dictionary,
“average”  is  defined  as  “ordinary”  or  “usual.”
A  meaningful  understanding  of  those  words,  in
turn, requires additional information:

 1.  Data would have to be gathered on the perfor-
mance  of  a  representative  sample  of  qualified
“local”  professionals  in  a  given  vocation,
regarding the type of case under discussion, plus
 2.  A  reproducible  and  logical  threshold  would
need to be established to separate “ordinary”
from “non-ordinary” performance.

As an example, one might find that unbiased
evaluation  of  a  melanocytic  lesion  by  several
experienced  “local”  pathologists  resulted  in  the
following  diagnoses  –  Spitz  nevus  (25%);
melanoma (72%); and other lesions (3%). With
this  information  in  hand,  one  could  attempt  to
identify  professional  conclusions  that  were
“substandard.”  The  process  might  result  in  the
conclusion that a minority opinion (in this example,
“Spitz  nevus”)  did  still  comport  with  the  “stan-
dard of care” [51]. That is particularly true if the
recommended  treatment  attached  to  the  latter
interpretation  did  not  differ  substantially  from
that used for the preeminent diagnosis. However,
the  question  remains:  Is  the  diagnosis  of  spitz
nevus “average”?

Another challenge to “average” or “ordinary”
skill  is  its  unclear  relationship  to  subspecialty
training or certification. Indeed, when the latter
is  required,  the  advanced  certificate-holder
would, by definition, be “special” and not “ordi-
nary!” Furthermore, the threshold of “ordinary
practice” is not a constant of nature such as the
speed  of  light,  and  it  cannot  be  identified  with
precision.  In  the  existing  legal  system,  “ordi-
nary”  and  “non-ordinary”  are  completely
changeable terms, definitions of which could be

344

M.R. Wick and E. Foucar

chosen  arbitrarily  to  favor  a  plaintiff,  a  defen-
dant, or neither party.

As  the  percentage  of  pathologists  who
agree  with  a  given  diagnosis  becomes  lower,
the  corresponding  claim
to  “ordinariness”
becomes  less  credible.  On  the  other  hand,  it
might  theoretically  be  decided  that  “ordinary”
skill was defined by agreement among ³95% of
reviewing  pathologists;  that  high  threshold
would  inherently  produce  cases  with  no  stan-
dard of care.

Yet another approach to the “ordinary vs. non-
ordinary”  issue  would  be  to  define  the  most
 commonly made diagnosis as the proper  “standard
of  care,”  and  all  others  as  “non-ordinary.”  That
model would be regarded as highly illogical and
flawed by persons with a scientific background,
because the majority of observers can agree on a
decision  or  interpretation  that  is  entirely  wrong
on objective grounds. Nonetheless, most lay per-
sons  are  accustomed  to  separating  “winners”
from “losers” through majority voting. A review
of US Supreme Court decisions shows that 5 to 4
votes are relatively common, but the majority is
clearly  determinative  [52].  On  the  other  hand,
majority  scientific  opinion  has  definite  conse-
quences, but it does not change scientific reality.
Even  if  most  scientists  in  the  world  decided  to
again assert that the earth was flat, it would still,
in truth, be spherical.

Some  experienced  pathologists  who  partici-
pate in malpractice litigation use self-determined
thresholds to identify cases in which they cannot
support a defendant as complying with “standard
of  care”  [53].  If,  for  example,  it  is  concluded
that the defendant pathologist has erred, but at
least 20% of all pathologists would have made
the same error, an expert could decide to support
the actions of the defendant physician. To date,
however,  a  testable  rationale  for  that  approach
has  not  been  advanced,  nor  is  there  any  pub-
lished proof that a valid method exists for deter-
mining  how  many  pathologists  would  make  a
certain error. Nevertheless, these issues must be
discussed if “standard of care” is to move from
the  shadows  into  the  realm  of  evidence-based
error analysis.

Applying “Standard of Care”
to Features of a Given Case

There  is  no  current  administrative  legal  mecha-
nism  for  routinely  providing  judges  and  juries
with information from court-appointed and unbi-
ased  “experts.”  Also,  as  just  discussed,  defini-
tions  of  “ordinary”  and  “non-ordinary”  practice
are ethereal.

Peer-reviewed medical publications may proffer
general  conclusions  on  diagnostic   accuracy  and
precision,  and  one  might  further  assume  that
such information could be used legally to define
“standard  of  practice.”  For  example,  published
information  is  available  concerning  the  rate  of
interpretation  of  Papanicolaou
false-negative
(Pap)  tests  that  actually  show  invasive  cervical
squamous  carcinoma  [54].  Nonetheless,  those
data can be completely  irrelevant to allegations of
malpractice  in  a   specific  single  case  of  “missed
cancer”  using  the  Pap  smear.  That  is  because
morphologic findings in a specific case at issue may
be (and often are) markedly different from those
on which published conclusions were drawn.

The  Law  invokes  the  “expert”  paradigm  to
explain  how  the  complex  literature  should  be
applied  to  the  case  at  hand.  Theoretically,  the
“experts” give their views of how the handling of
a  particular  case  complied  with  real-life  “stan-
dard” practice, and their conflicting conclusions
are,  in  a  legal  sense,  each  supposed  to  be  dis-
positive. However, that construction is ultimately
invalid.  First  of  all,  none  of  the  “experts”  were
present  in  the  specific  hospital  or  laboratory  on
the  day  a  diagnosis  was  made,  and  they  typi-
cally have no detailed knowledge of the circum-
stances under which the case was evaluated [55].
For example, verbal interchanges of information
between pathologists and clinicians are extremely
common and very important to patient care, but
memories of such communications can be lost or
altered with the passage of time if they are not
written  down  in  the  medical  record.  The  latter
fact by no means detracts from the weight they
carried in the “here and now.” Finally hindsight
bias can be almost impossible to eliminate.

20  Evidence-Based Pathology and Tort Law

345

Credibility of “Expert” Witnesses

Jurors weigh many factors when coming to their
conclusion. In criminal trials, juries have some-
times delivered a verdict of “innocent” when it is
obvious that the law has, in fact, been breached
[56, 57]. In those instances, one might view the
process as defensible because the juries wished
to bring their community-based standards to bear
against  laws  that  they  collectively  felt  to  be
unjust. With regard to medical malpractice cases,
juries also have determinative latitude. They may
base a judgment on the opinion of one “expert,”
attempt  to  integrate  the  assertions  of  several
“experts,”  or  ignore  all  of  them.  However,  in
actual practice, the impact of “expert” testimony
on  the  jury  depends  on  now  jurors  perceive  its
credibility, which, of course, derives from jurors
perceptions of the people who are offering it [58].
Credibility  or  “worthiness  of  belief ”  is  so
important  that  it  deserves  further  evaluation.  If
trial topics are mainstream and the jury is famil-
iar with them, “credible” testimony must simply
meet  the  test  of  plausibility.  For  example,  if  a
witness insists that he saw an accused murderer
from  a  mile  away  in  a  dark  street  on  a  cloudy
night, the jurors’ experience and common sense
would  tell  them  otherwise.  On  the  other  hand,
most  lay  persons  charged  with  assessing  the
credibility  of  “expert”  testimony  in  pathology
have no familiarity with that topic. They usually
search  for  surrogate  indicators  of  believability,
such  as  the  manner  of  speech  and  choice  of
words, style of dress and grooming, respect for
the jurors and other people in court, and the per-
ceived strength of the professional credentials of
the witness [59].

Problematically, the credibility of an “expert”
might be unquestioned in the eyes of a lay jury,
whereas  medical–scientific  authorities  would
universally judge him or her to be a charlatan. The
problem  of  “pseudo-credible”  testimony  has,  in
the past, led to some trial outcomes which departed
markedly from those that established SMI would
have dictated. This situation threatened the courts
with  a  loss  of  face  and  public  confidence  in  the

past,  and  the  Supreme  Court  felt  compelled  to
attempt remedial action in an attempt to salvage
the credibility of the legal system [60].

The Daubert Case and Standard
of Care

During the 1990s, the US Supreme Court issued
several rulings which provided new criteria that
judges  could  apply  in  performing  their  “gate-
keeper”  function  regarding  “expert”  testimony
[60–64].  However,  one  would  be  mistaken  in
believing that these rulings are relevant to most
current tort cases that involve pathologist defen-
dants [65, 66]. Nevertheless, there are exceptions
to that statement. If an “expert” were to assert at
deposition that a single physiological mitosis in a
melanocytic skin lesion mandated a diagnosis of
melanoma on its own weight, that opinion could
not   possibly  be  supported  by  the  peer-reviewed
 literature  –  a  requirement  stemming  from  the
case of Daubert v. Merrell-Dow Pharmaceuticals
([92-102], 509 US 579 [1993]). In some jurisdic-
tions, opposing council would have the option to
file  a  “Daubert  challenge”  to  the  testimony.  If
after  reviewing  the  testimony  the  case’s  judge
agreed that the testimony was “junk,” there would
be no need for the attorney filing the challenge to
discredit that particular testimony at trial because
the  testimony  would  be  barred  from  the  court-
room by the judge.

life,

Sadly,  that  scenario  is  an  idealized  one.  In
real
things  are  more  complicated.
Obfuscating arguments can easily be presented
to  muddy  the  medicolegal  water  over  admissi-
bility of “expert” testimony, especially if no lit-
erature exists that exactly describes the particular
case  in  question.  Thus,  the  “Daubert  criteria”
are  functionally  irrelevant  in  many  instances
where  “expert”  testimony is not “expert.” The
question  for  the  court  is  not  whether  the  cited
literature  is  valid,  or  whether  it  is  applicable,
but rather whether the subjective interpretations
of that information by the respective “experts”
are valid [62].

346

M.R. Wick and E. Foucar

With regard to subjective expert opinion, most
judges who oversee the admissibility of “expert”
medical testimony could only function properly,
in a scientific sense, if they sought the advice of
unbiased  court-appointed  authorities.  How ever,
there are no mandates, or even procedural provi-
sions, for judges to seek such help, and medical
organizations  have  not  offered  it  to  the  court
spontaneously.  Rather  than  admitting  that  they
are  personally  unable  to  evaluate  the  probative
value of scientific-medical testimony, most judges
use  existing  statutes  and  decisions  concerning
“expert”  witnesses  to  provide  a  “preference  for
admissibility”  [67].  Jurors  who  may  have  less
education than judges are then required to deter-
mine  the  scientific  veracity  of  testimony  that  is
the  “admissible  truth”  rather  than  the  “whole
truth” [68].

“Finality” in the Courts vs. “Finality”
in Medicine

In addition to bringing values of the community
into the legal system, jurors are also charged with
ending conflicts between the specific parties at the
bar.  This  process  is  complicated  when  credible
“experts”  disagree,  when  both  the  plaintiff  and
the  defendant(s)  seem  to  be  worthy  people,  and
when  the  opposing  lawyers  present  their  cases
skillfully. Perhaps the testimony of the “experts”
indicates that there is genuine  disagreement over
the  crux  of  the  case,  but  the  ultimate  scientific
validity is not the dominent issue at that moment
in time; the needs of the court to resolve the issue
before it are more concrete.

Physicians may be offended by this perceived
“rush to judgment” in the face of factual uncer-
tainty. Artificial legal “finality” may be contrary
to the general principles of science and medicine.
Indeed, when one sees dogmatism in the face of
uncertain  or  conflicting  objective  data,  it  can
 generally be surmised that one is dealing with an
ingenue or a fraud as a witness.

However, even in the real world of patient care,
uncertainty must coexist to some extent with final-
ity. In pathology practice, a low level of disquietude
in difficult cases is relatively common, but substan-

tial uncertainty typically stimulates a consultation
with medical colleagues. Those helpers may trans-
form what are inherently ambiguous findings into
a final diagnosis and plan of action. Similarly, the
decisions  of  juries  function  to  transform  medical
and legal uncertainties into finalities.

Conclusions

In their role as diagnosticians, pathologists must
be able to identify and respond to areas of uncer-
tainty. An evidence-based approach to scientific
investigation  and  medical  practice  is  thought  to
optimize the specialty’s approach to uncertainty.
The  legal  system  also  must  resolve  uncertainty,
but in many cases, physicians consider the uncer-
tainty  surrounding  malpractice  cases  to  be  arti-
facts  arising  out  of  the  scientific  weaknesses  of
the legal system’s procedures.

Physicians  are  one  in  the  view  that  better
 scientific presentation in the courtroom is a laud-
able goal. Moreover, at least some judges would
like to improve the quality of SMI offered to lay
juries. However, efforts aimed at closing the space
between admissible testimony and scientific truth
have lacked  infrastructural support, and attempts
to exclude “outlier” testimony have been largely
ineffective [67, 69].

Some medical groups such as the American
Association  of  Neurological  Surgeons  and  the
American  Association  of  Radiologists  have
attempted  to  improve  the  quality  of  “expert”
testimony by doing their own peer reviews and
imposing  sanctions  on  physicians  whose  testi-
mony  was  obviously  erroneous  [70].  Those
efforts are commendable, and they have helped
to identify failures of the legal system to control
defective  “expert”  input.  Nevertheless,  this
approach  is  clearly  not  the  only  answer  to  the
problem. Until all judges and medical specialty
societies   cooperate  closely  to  assure  the  accu-
racy of “expert” presentations in the courtroom,
the  legal  system  will  continue  to  depend  on
jurors to separate fact from fiction.

Interestingly,  the  introduction  of  DNA-based
technology into criminal trials has prompted the
legal  profession  itself  to  question  the  quality  of

20  Evidence-Based Pathology and Tort Law

347

other  forensic  evidence  that  traditionally  had
been  considered  to  have  very  high  credibility
[71].  However,  there  is  currently  no  indication
that  the  lawyers  who  control  malpractice  litiga-
tion  have  identified  any  reason  to  re-evaluate
their systems or methods. This legal satisfaction
with the status quo can be discouraging to patho-
logists who have had first-hand experience with
malpractice  litigation,  but  should  not  prevent
individual  pathologists  from  taking  their  own
small  steps  to  improve  the  scientific  quality  of
malpractice litigation. First, one must be willing
to commit the time and emotional energy required
to  participate  in  malpractice  litigation  as  an
expert,  rather  than  just  complain  from  the  side-
lines about tort system deficiencies. As patholo-
gist Richard J. Zarbo observed, “the system only
works when good people get involved.” Secondly,
the pathologist must commit to providing honest,
clear, credible, and evidence-based testimony. At
that  point  the  pathologist  has  become  a  one
person force for tort reform.

References

  1.  Hyman DA. Medical malpractice and the tort system:
what do we know and what (if anything) should we do
about  it?  Published  as  part  of  a  symposium  on  civil
justice in the Texas Law Review, Vol. 80, No. 7, June
2002.  Available  at  http://www.law.umaryland.edu.
Accessed 2 Oct 2005.

  2.  Malpractice  mythology  (Editorial).  The  New  York
Times.  January  9,  2005.  Available  at  http://www.
nytimes.com. Accessed 9 Jan 2005.

  3.  Berenson  A.  Jury  finds  Merck  liable  in  the  Vioxx
death and awards $253 million. The New York Times.
August  19,  2005.  Available  at  http://www.nytimes.
com. Accessed 19 Aug 2005.

  4.  Olson  W.  The  next  Sandra  Day.  The  Wall  Street

Journal. July 7, 2005. p. A12.

  5.  Olson W. Justice served, sometimes. The Wall Street

Journal. September 8, 2005. p. D10.

  6.  Wick MR, Adams RK. Medical malpractice actions:
Pathol.

Semin  Diagn

elements.

procedural
2007;24:60–4.

  7.  Foucar  E,  Wick  MR.  Evidence-based  medicine  and

tort law. Semi Diagn Pathol. 2005;22:167–176.

  8.  Feld  AD,  Carey  WD.  Expert  witness  malfeasance:
how  should  specialty  societies  respond?  Am  J
Gastroenterol. 2005;100:991–5.

  9.  Cecil  JS.  Ten  years  of  judicial  gatekeeping  under

Daubert. Am J Public Health. 2005;95:S74–80.

 10. Rosenbaum  S.  The  impact  of  United  States  law  on
medicine as a profession. JAMA. 2003;289:1546–56.
 11. Steinbrook R. Science, politics, and federal advisory

committees. N Engl J Med. 2004;350:1454–60.

 12. Huber  PW.  Galileo’s  revenge:  junk  science  in  the

courtroom. New York: Basic Books; 1991.

 13. U.S. Supreme Court, Tanner v United States, 483 U.S.
107  (1987).  Available  at  http://caselaw.lp.findlaw.
com. Accessed 12 Aug 2004.

 14. Vidmar  N.  Expert  evidence,  the  adversarial  system,
and the jury. Am J Public Health. 2005;95:S137–43.
 15. Miller  JF.  Should  juries  hear  complex  cases?  Duke
Law & Technical Review. April 2, 2004. Available at
http://www.law.duke.edu. Accessed 11 Jan 2005.
 16. Bal  R,  Bijker  WE,  Hendriks  R.  Democratisation  of

scientific advice. BMJ. 2004;329:1339–41.

 17. Ezrahi Y. Nature as dogma. Book review of: politics
of nature: how to bring the sciences into democracy.
Bruno  Latour.  Harvard  University  Press;  2004.  Am
Sci. 2005;93:89–90.

 18. Foucar  E.  Diagnostic  decision  making  in  anatomic
pathology.  Am  J  Clin  Pathol.  2001;116(Suppl):
S21–33.

 19. McDougal L. I trust juries – and Americans like you.

Newsweek. December 22, 2003. p. 16.

 20. Olson  W.  Stop  the  shakedown.  The  Wall  Street

Journal. October 29, 2004. p. A14.

 21. Mohr JC. American medical malpractice litigation in
historical perspective. JAMA. 2000;283:1731–7.
 22. Murray  I.  The  malpractice  economist:  liable  to
suffer.  The  American  Enterprise.  September  2003.
p. 50–1.

 23. Zhang J. How much soy lecithin is in that cookie? The

Wall Street Journal. October 13, 2005. p. D1.

 24. Simon  R.  Payback  time  for  dot-com  investors.  The

Wall Street Journal. February 1, 2005. p. D1.

 25. Crossen C. A thirties revelation: rich people who steal
are  criminals,  too.  The  Wall  Street  Journal.  October
15, 2003. p. B1.

 26. Martinson BC, Anderson MS, de Vries R. Scientists

behaving badly. Nature. 2005;435:737–8.

 27. Saks  MJ,  Koehler  JJ.  The  coming  paradigm  shift  in
forensic  identification  science.  Science.  2005;309:
892–5.

 28. Junghans  C,  Feder  G,  Hemingway  H,  Timmis  A,
Jones  M.  Recruiting  patients  to  medical  research:
double  blind  randomized  trial  of  “opt-in”  and
 “opt-out” strategies. BMJ. 2005;331:940–4.

 29. Judge  declares  mistrial  in  case  of  Ohio  highway
shootings.  The  Associated  Press.  May  9,  2005.
Available  at  http://www.nytimes.com.  Accessed  9
May 2005.

 30. Fridman DS, Janoe JS. Judicial gatekeeping in New
Mexico.  From  The  Judicial  Gatekeeping  Project.
1999.  Available  at  http://cyber.law.harvard.edu/
daubert/nm.htm. Accessed 15 June 2005.

 31. Michaels  D.  Scientific  evidence  and  public  policy.

Am J Public Health. 2005;95:S5–7.

 32. Silicosis,  Inc.  (Editorial).  The  Wall  Street  Journal.

October 27, 2005. p. A20.

348

M.R. Wick and E. Foucar

 33. Tesoriero HW, Brat I, McWilliams G, Martinez B.
Merck  loss  jolts  drug  giant,  industry.  In  landmark
Vioxx  case,  jury  tuned  out  science,  explored
coverup angle. The Wall Street Journal, August 22,
2005. p. A1.

 34. Halpern SD. Towards evidence based bioethics. BMJ.

2005;331:901–3.

 35. Lipton  P.  Testing  hypotheses:  prediction  and  preju-

dice. Science. 2005;307:219–21.

 53. Epstein JI. Pathologists and the judicial system: how

to avoid it. Am J Surg Pathol. 2001;25:527–37.

 54. Rylander  E.  Negative  smears  in  women  developing
invasive cervical cancer. Acta Obstet Gynecol Scand.
1977;56:115–8.

 55.  Wick MR. Medicolegal liability in surgical pathology:
a consideration of underlying causes and selected per-
tinent concepts. Semin Diagn Pathol. 2007;24:89–97.
 56. Dalrymple T. Trial by human beings. The jury system

 36. Albert  T.  Expert  witness  sues  critics.  American

and its discontents. Natl Rev. 2005;25:30–1.

Medical News. June 28, 2004. p. 1.

 37. Milunsky A. Lies, damned lies, and medical experts: the
abrogation of responsibility by specialty organizations
and a call for action. J Child Neurol. 2003;18:413–9.
 38. Petroski H. Technology and the humanities. American

Scientist. 2005;93:304–7.

 39. Mawer S. Science in literature. Nature. 2005;434:297–9.
 40. Byatt  AS.  Fiction  informed  by  science.  Nature.

2005;434:294–6.

 41. Jacobson  PD,  Bloche  MG.  Improving  relations
between attorneys and physicians. JAMA. 2005;294:
2083–5.

 42. Andrews  M.  Making  malpractice  harder  to  prove.
The New York Times. December 21, 2003. Available
at: http://www.nytimes.com. Accessed 21 Dec 2003.
 43. Victoroff  MS.  Peer  review  of  the  inexpert  witness,
or…Do  you  trust  the  chickens  to  guard  the  coop?
Managed  Care,  September  2002.  Available  at  http://
managedcaremag.com. Accessed 7 Aug 2003.

 44. Begley  S.  Ban  on  “junk  science”  also  keeps  jurors
from  sound  evidence.  The  Wall  Street  Journal.  June
27, 2003. p. B1.

 45. Troxel  DB.  Error  in  surgical  pathology.  Am  J  Surg

Pathol. 2004;28:1092–5.

 46. Sandlin S. Unser malpractice lawsuit is settled. ABQ
Journal.com online edition, October 4, 2005. Available
at http://abqjournal.com. Accessed 25 Oct 2005.
 47. Hupert N, Lawthers AG, Brennen TA, Peterson LM.
Processing  the  tort  deterrent  signal:  a  qualitative
study. Soc Sci Med. 1996;43:1–11.

 48.  Budetti PP. Tort reform and the patient safety movement.
Seeking a common ground. JAMA. 2005;293:2660–2.
 49. Gold JA. Malpractice. Book review of: Medical mal-
practice:  a  physician’s  source-book.  Anderson  RE,
editors. Humana Press; 2005. JAMA. 2005;293:1393.
 50. The  Publisher’s  Editorial  Staff,  Nolan  JR,  Nolan-
law  dictionary  (Centennial

Haley  JM.  Black’s
Edition). 1990. St. Paul: West Group.

 51. Reay DT, Davis GJ, and the members of the CAP Forensic
Pathology  Committee.  Legal  basis  for  civil  claims
(Chapter 6). In: The pathologist in court. A Publication of
the College of American Pathologists; 2003. p. 27–33.
 52. Sunstein CR. Courting division. The New York Times.
October  6,  2005.  Available  at  http://www.nytimes.
com. Accessed 6 Oct 2005.

 57. Balko  R.  Justice  often  served  by  jury  nullification.
July 28, 2005. Fox News Channel. Available at http://
foxnews.com. Accessed 28 July 2005.

 58. Be  prepared  (Professional  Issues).  Interview  with
Sara  C.  Charles  and  Paul  Frisch.  American  Medical
News. July 11, 2005. p. 14–5.

 59. Reay  DT,  Davis  GJ,  and  the  members  of  the  CAP
Forensic  Pathology  Committee.  Courtroom  etiquette
(Chapter 11). In: The pathologist in court. College of
American Pathologists; 2003. p. 56–9.

 60. Petroski H. Daubert and Kumho. American Scientist.

1999;87:402–6.

 61. US Supreme Court, 509 U.S. 579. Daubert v Merrell
Dow Pharmaceuticals, Inc. 1993. Available at http://
supct.law.cornell.edu. Accessed 10 Sep 2003.

 62. US  Supreme  Court,  522  U.S.  136.  General  Electric
Co  v  Joiner.  1997.  Available  at  http://supct.law.cor-
nell.edu. Accessed 10 Sep 2003.

 63. U.S.  Supreme  Court,  526  U.S.  137.  Kumho  Tire
Company  v  Patrick  Carmichael.  1999.  Available  at
http://supct.law.cornell.edu. Accessed 10 Sept 2003.
 64. Jasanoff  S.  Law’s  knowledge:  science  for  justice
in  legal  settings.  Am  J  Public  Health.  2005;95:
S49–58.

 65. Faigman DL. Is science different for lawyers? Science.

2002;297:339–40.

 66. Foucar  E.  Pathology  expert  witness  testimony  and
pathology  practice:  a  tale  of  two  standards.  Arch
Pathol Lab Med. 2005;129:1268–76.

 67. Kassirer  JP,  Cecil  JS.  Inconsistency  in  evidentiary
standards  for  medical  testimony.  Disorder  in  the
courts. JAMA. 2002;288:1382–7.

 68. Gutheil  TG,  Hauser  M,  White  MS,  Spruiell  G,
Strasburger LH. The “whole truth” versus the “admis-
sible truth”: an ethics dilemma for expert witnesses.
J Am Acad Psychiatry Law. 2003;31:422–7.

 69. Appelbaum PS. Law and psychiatry: policing expert
testimony:  the  role  of  professional  organizations.
Psychiatr Serv. 2002;53:389–99.

 70. “Expert” witness gets booted from ACR. Diagnostic
imaging  online.  July  8,  2004.  Available  at  http://
diagnosticimaging.com. Accessed 31 Oct 2004.
 71. Neufeld PJ. The (near) irrelevance of Daubert to crim-
inal  justice  and  some  suggestions  for  reform.  Am
J Public Health. 2005;95:S107–20.

Index

A
ACCE test evaluation
analytic validity

assay robustness, 299
defined, 299
integral elements, 299
proficiency testing (PT), 299
quality control, 299

clinical utility

assay and interventions, 301
economic evaluation, 301
meta-analysis, 301

EGAPP, effort, 298
evidence evaluation, 298–299
formal assessment, 301–302
formulation, 298
genetic testing applications, 298
literature review, 298
molecular tests, 299–300
pilot studies, 302
sensitivity and predictive value
HFE and DMD gene, 300
true-negative (TN) and false-positive (FP),

300

ACS. See Acute coronary syndromes
Acute coronary syndromes (ACS), 308–310
American Recovery and Reinvestment Act (ARRA),

305, 319

Analysis of variance (AOV), 49–50
Anatomic pathology, decision support systems.
See Decision support systems
Anatomic pathology, prognostication and prediction

axillary lymph nodes, 61–62
biological molecules, 62
goals, 62
histological grading, 61
hospice-care, 65
mammary carcinoma model

lymph node status, 72–74
MC (see McGuire criteria, prognostic test

evaluation)

methodological reproducibility and cross-validation,

78–79
prevalence, 66
prognosis forecasting, patients, 66
prognostic analytes, 75
surrogate, formal lymph node, 74–75
tissue sampling, 66–67
tumor size measurement, 70–71
usual ductal adenocarcinoma (UDA), 65
variants, histologic, 67–70

personalized medicine
cost, health care, 64
federal politicians, 65
health care spending, 64
human genome, 63
PPMT (see Prognostic/predictive medical test)
technological medical entrepreneurs,

64–65

U.S. Congressional Budget Office, 65

queries, illness, 61
risks

negative events/hazards, 62
term meaning, 62
uncertainty, 62–63

TNM system, 61
vicissitudes, health care, 65

Annotated dendrogram fingerprint (ADF), 101
AOV. See Analysis of variance
Applied immunohistochemistry

diagnostic (see Diagnostic immunohistochemistry)
EBM (see Evidence-based medicine (EBM))
PPIHC (see Prognostic-predictive

immunohistochemistry)

ARRA. See American Recovery and Reinvestment Act
Avidin-biotin-peroxidase complex (ABC), 265

clinical bias, “prognostic” markers, 80–81
cross-validation methods, omitting, 79–80
heterogeneous data types, 75
histologic grading, 71–72
incorrect categorical and binary data generation,

76–78

B
BAC. See Bronchioloalveolar carcinoma
Bayes theorem/rule, 43
BDNF. See Brain-derived neurotrophic factor

A.M. Marchevsky and M.R. Wick (eds.), Evidence Based Pathology and Laboratory Medicine,
DOI 10.1007/978-1-4419-1030-1, © Springer Science+Business Media, LLC 2011

349

350

Best evidence, pathology

case control study differentiation, 38
clinical correlation, 39
comprehensive tables, 36
definition, 28
evaluation, 32
“evidence pyramid”
description, 32
graphical representation, 33
human-based endeavors, 33
pathologic entity, 33

external validity
definition, 32
evaluation, evidence quality, 32

generation, study type, 35–36
internal validity, 28–29
linguistic/legal environment, 27
meta-regression analysis
clinical decision, 35
significance evaluation, 34
“wobble”, 34

modern evidentiary rules, 28
pathologist

case series and study, 38
expert opinion, 38
pathology literature, 36
pertaining diagnosis, 37
quality evaluation, 28
real-life constraints, 28
research applications, 37
ROC

inaccurate assessments, 31
phosphorescence, oscilloscope, 31

rule, 28
squamous differentiation, 38
statistics, data analysis, 29–31
stringent requirements, 38
study designs

quality ranking, 32
systematic review and meta analysis,

33–35
subgroup analysis, 34

Biostatistics 101

analysis of variance (AOV), 46, 49–50
Bayes theorem/rule, 43
chi-square test

degrees of freedom, 47
equality test, 47
Mantel-Haenszel, 48
McNemar variant, 47
positive vs. negative, 46–47
probabilities, 47
statistical independence, 48

conditional probabilities, 42
hypothesis testing, statistical,

45–46
probability, 41
random variables

distribution function, 45
independent samples, 45

Index

parametric tests, 48–49
probability distributions, 43–45

regression analyses, 50–55
ROC curves, 42–43
statistical independence, 42
survival analysis

binary failure event, 55
Cox model, 58–59
hazard function, 57
log-rank test, 56–57
PSA serum value, 55
survival plot, 55–56

t test, 48
type II errors, statistical power and sample

sizes, 46

Wilcoxon and Kruskal–Wallis tests, 50

Biostatistics, evidence-based medicine. See Biostatistics

101

Bloom–Scarff–Richardson (BSR) grading method

modified BSR (MBSR), 72
UDA, grade II, 79, 80

Brain-derived neurotrophic factor (BDNF), 144
Breast cancer, prognostication model

clinical bias, “prognostic” markers, 80–81
CVM, 79–80
heterogeneous data types, 75
histologic grading, invasive breast carcinoma

BSR method, 71
MBSR, 72
nosological tumor types, 72

histologic variants

group I and II, 67
group III, 68–70

incorrect categorical and binary data generation,

76–78
lymph node status

aggressive axillary lymphadenectomy, 73
host immunity, 74
implants, 73
isolated tumor cells, 74
neoplastic implants, 74
scirrhous breast cancers, 72
sentinel node technique, 74

MC (see McGuire criteria (MC), prognostic test

evaluation)

mutations/amplifications, 75
reproducibility and cross-validation

immunostaining, nuclear p53-reactivity,

78, 79

southern blot preparation, 79

substaging surrogate, lymph node, 74–75
tissue sampling, prognostication and prediction

biopsy needles, 66
spatial heterogeneity, 67
tumor size measurement, 70–71
Bronchioloalveolar carcinoma (BAC)

adenocarcinoma, 220
pulmonary, 221

BSR grading method. See Bloom–Scarff–Richardson

(BSR) grading method

Index

351

C
CART. See Classification and regression tree analysis
Case-based reasoning (CBR)

classification and identification, 184
k-nearest neighbor (kNN) search, 184–185
population based studies, 184
prognostic support systems, 184

CBR. See Case-based reasoning
CDSSs. See Clinical decision support systems
Cedar Sinai Medical Center experience, evidence based
diagnostic criteria. See Evidence-based diag-
nostic criteria, Cedar Sinai Medical Center

Cell pathology. See Evidence-based cell pathology
CER. See Comparative effectiveness research
Classification, anatomic pathology. See also

Classification and diagnosis principles,
anatomic pathology

EBM, 95–96
elements, 95
foundational problems, 96
oncopathological taxonomic models, 104
pluralism, 103
populations, 96
scientific and managerial, lesion

canonical, 97
histogenetic (HG), 97

Classification and diagnosis principles, anatomic

pathology

boyd kinds

KNeop synovial sarcoma, 114
natural kinds, 115
SYT-SSX fusion product, 114

EBP

CBR, 96
EBM, 95–96
foundational problems, 96
populations, 96
statistical reasoning, 96

elements, 95
HG-KNeop’s (see Histogenetic neoplastic kinds)
human element

fine-grained taxonomic instability, 114
macro-revisions, 110–113
oncopathological reality, 110
translation and transmission, 113–114
individual neoplasm (see Individual neoplasm

(INeop))

intrinsic heterogeneity, (KNeop’s)
classification pluralism, 103
phenospace, 103

in-between, hybrid and novel, 96–97
INJS conditions, 109–110
managerial gradient, 108
M-KNeop’s (see Managerial neoplastic kinds)
persistence, 97–98
stylized ADF, 109

scientific and managerial classifications, lesion

canonical, 97
histogenetic, 97

Classification and regression tree analysis (CART),

124, 129

Clinical decision support systems (CDSSs)
alerts, frequency distribution, 330, 331
antiepileptic drug monitoring, 326
benefits, 331, 332
corollary orders, 331
cost benefits, 330–331
critical test results, 330
digoxin levels appropriateness

inpatient and outpatient, 328
serum, 327–328
timing, 326–327
implementation, 324
laboratory test utilization

CPOE, 324
overridden, justification, 325
redundant testsa reduction, 325–326

PSA (see Prostate-specific antigen)
strategies, 323–324
test charges display
CPOE, 329
inpatients, 329–330

user-friendly and end users feedback, 331

Comparative effectiveness research (CER)

EBM, 23, 24
research, 23

Complexity, neoplasm classification

ADF, 101
clones, 100
INeop vieId, 99
levels, 98
phenotypic plasticity, 100
single cancer genes, 98
symbolism, 101
synchronic and diachronic intra-tumor

heterogeneity, 99

tumor progression models and lineages, 100

Computerized alerts and reminders

EHR, 317
electronic disease management protocols,

lack of expertise and incomplete information, 97
myths

317–318

PT-INR, 317

essentialism, 115
eventual disappearance, 117–118
monism, 116
naïve realism, 115
problem cases, 117–118

problem cases

de jour classification, 96
ExtnI-CoPeTI structure, 108–109

Computerized physician order entry (CPOE)

CDSSs, 323–324
implementation, 323, 331
utilization reminders, clinical decision making, 324

Conditional probabilities
Bayes’ theorem, 308
likelihood ratio, 42
relative risk, 42

352

Consultative interpretive services

hemoglobin electrophoresis, 313
hospital laboratories, 316
immunoassays, 314
normal antithrombin III, 314, 315
order-entry systems, 315
protein S deficiency, 313
PTT evaluation, 315, 316
“ristocetin cofactor”, 315–316
von Willebrand factor, 313, 315
warfarin, 314–315

Cox model

hazard score formation

graphical nomogram, 59
men, hormone refractory prostate cancer, 59

multiplicative factor, 58–59
PSA serum, performance, 58
semiparametric, 58
survival time, 58

Cross-validation of methods (CVM)

description, 78
omitting

immunostaining, 80
p53 immunostain and mutations, 80
seroma, 80
UDA, BSR grade II, 79–80

D
DA. See Decision analysis
Data collection, pathology

elements, 247
evidence, 249
histologic classification, WHO, 247–248
meta analysis, 246–247
protein expression, 247
secondary data, 247
Decision analysis (DA)

Bayesian updating, 179
burgeoning oncopathological zoo

molecular kinds, 178
named entities, 178

client decision, 177
clinician’s lament, 175–176
cost functions

complex atypical hyperplasia, 181
optimal threshold, 181
treatment recommendations, 181

elements, 174
guiding principles

benign and malignant KNeop, 178
phenotype, 178

independence, informational evaluations, 179
intuitions, diagnostic pathology

distinction, 176
HG-KNeops, 176
patient’s clinical management, 176

irresolvable uncertainty, diagnosis

claimed differences and credible evidence, 182
debulking, 182
problem cases, 182

Index

judgmental psychology, 183
mathematical probability interpretations

frequency, 175
subjectivists/personalists, 175
novel case and closest fit, 182–183
principles

good decisions and outcomes, 177
uncertainty, 177

rubber band paradox, 182
sensitivity analysis, 179–180
subjective expected utility (SEU) theory, 174
subjective probability, 173
vagueness vs. probabilistic uncertainty

clarity test, 175
Fuzzy theory, 175
lottery metaphor, 174

value, information

clinical/radiological, 179
discriminating tests, order, 179
resection specimen, 179

Decision support

ARRA legislation

“closed-loop” system, 319
EHR, 319
long track record, 320

commandments, 311
computer based

automatic prompts, 318
forms, 316
online sources, 317

diagnostic algorithms
celiac disease, 312
reflex testing strategy, 312
test utilization control, 311

EBM, 310
off-the-shelf, 319
order form design

laboratory information system (LIS), 311
requisition design, 311

selection and interpretation tools, 310

Decision support systems (DSS)

case-based reasoning (CBR), 183
classifiers, 174
computerized, 207
description, 173–174
rule-based expert systems, 3

Diagnostic immunohistochemistry (DIHC)

ABC procedure, 265
antibody titration, 267–268
antikeratin antibody, 266
avidin-biotin-peroxidase complex method, 265–266
chromophore, 262
cross-validating techniques, 268
development, 263
diagnostic importance, 265
direct immunofluorescence method, 262
ecumenical alternative technique, 264
effective immunohistological procedure, 263
electron donors and recipients, 263–264
evidence-based medicine principles
diagnostic interest, 269–270

Index

353

histopathologic diagnosis, 272
ILCs, 271
medical decision-making techniques, 270
morphological diagnostic impressions, 270
reproducible practical tool, 269
surgical pathologists and cytopathologists, 269

fixation-induced coupling, 267
fluorescent immunohistology, 262
formalin fixation, 266
formalin-fixed tissue, 264–265
light-microscopic preparation, 264
Mannich reactions, proteins, 267
medical publications, 268
metastatic melanoma, 265
multitumor tissue blocks, 268
peroxidase-antiperoxidase method, 263–264
quality control methods
biomarkers, 268
chronological validation, 268
intra-and inter-laboratory reproducibility, 268
procedural and extramural validation, 268
reactive and non-reactive tissues, 268
reagent selection and interpretation, 268

EGFR. See Epidermal growth factor receptor
EHR. See Electronic health records
Electronic health records (EHR), 305, 317, 319
Electronic reminders, 325
Epidemiology, study results. See Meta-analysis,

therapies evaluation

Epidermal growth factor receptor (EGFR), 254
Estrogen/progesterone receptor proteins (ERP/PRP),

69, 70, 76, 82

Evaluating information, pathology. See Pathology

literature evaluation
Evaluation of diagnostic errors, pathology

classification, 236
communication lack

causes and remedies/solution, 237–238
electronic medical records, 238–239

complexity, 239
criteria and staging, 241
hierarchical culture, 242
human intervention, 240–241
inconsistency

diagnostic criteria, 239–240
evidence-based and time-tested principles, 240

renal cell carcinoma, 267
signal maximization center, 266
standardization, 267
TEM, 263
use and abuse, 272–275
working environment, 268

Diagnostic pathology

EBM tenets, 19–20, 23
knowledge accumulation, 20
traditional vs. EBM practice, 23

Diagnostic principle, anatomic pathology. See also
Classification and diagnosis principles,
anatomic pathology

description, 103
EBM, 95–96

Diagnostic systems, 184–185
Diagnostic test accuracy

meta-analyses

intercept, model, 165
logarithmic transformation, 165
odds ratio, 164
ROC curve, 165
sensitivity and specificity, 163
true-positive and false-positive proportion, 164–165

RCTs, 151
sensitivity and specificity, 151

E
EBM. See Evidence-based medicine
EBP. See Evidence-based pathology
Effect sizes

estimation, 256
fixed models, 251
mathematical formula, 248
meta-analysis, 256
random model, 248, 251

reduction, 235–236
test cycle phases
analytic, 237
postanalytic, 237
preanalytic, 236–237

time constraints, 242

Evaluation of genomic applications in practice and

prevention (EGAPP) evaluation. See ACCE
test evaluation
Evaluation, oncopathological studies

bias

definition, 129
referral and spectrum, 129
short follow-up, 129
communicative component
cytological atypia, 131
failure, 132
translation and transmission, 131–132

confounding factors, 130
external validity, 130
genomics

bias/variance dilemma, 135
“bottom-up” and candidate-gene approach, 134
context dependency, neoplastic cell, 133
education, 134
epistemological concerns, 136–138
evidence-based pathology, 138
“forensic statistics” analysis, 132
I
Neop heterogeneity and evolution, 133
markers, 138
mathematical-statistical issues, HDB, 134–136
microarrays, 132
noisy data, 138
observation studies vs. experimental studies, 133–134
peer-reviewed studies, 133–134

managerial class, 122
missing data, 129

354

Index

Evaluation, oncopathological studies (Continued)

multivariate continuum, 122
multivariate statistical methods

classification schema, 226
probable quality, 225

Bayesian inference

exploratory data analysis, 123
ovarian serous low malignant potential tumor, 126
uterine smooth muscle charts, 123, 124

predictive components

anatomic surgical pathology, 123
clinical prognostic models, 124
GEA, 124
study design, 127

sampling issue

atypical polypoid adenomyofibroma study, 128
CART, 129
multiple hypotheses, 127
overfitting, 128
phenospace, 126
statistical hypothesis model, 126, 128
type I and II errors, 129
validation, 129
Venn diagrams, 127

supervised and unsupervised classification models
managerial/nonmanagerial distinction, 122
“natural” clustering, 123
training set, 123

validity

chance issues, 121
internal, 121
role, chance, 126

Evidence-based CDSSs. See Clinical decision support

systems
Evidence-based cell pathology
morphological diagnosis
relevance, 208–209
reproducibility, 207–208
report communication, 209–210
sampling

chronic viral hepatitis, 206
colorectal cancer (CRC), 204
extramural vascular invasion, 205
liver biopsy, portal tracts, 206
lymph nodes (LN), 205
malignancy, 204
mathematical modeling, 205
METAVIR scoring system, 206
retrospective analysis, 205
sentinel nodes, 206
serial sectioning, 204
standardized protocols, 206
tumor pathology, 205
whole-specimen mounting, 204

Evidence-based diagnostic criteria, Cedar Sinai Medical

Center
anatomic pathology

diagnostic classification schema, 227
neoplasms, 226
thymic epithelial neoplasms, 227
type B thymomas, 227

appraisal and integration

anatomic pathology, 226–228

Bayes’ theorem, 218
prior and posterior probabilities, 218–219
process, 219
utilization, 219

cost effective immunohistochemistry

antibody use, 222, 223
OR analysis, 222–223
post-test odds, 223, 224
sensitivity and specificity, 222
data trumps eminence and tradition

EBM, 215
EBP, 215

EBP, 213
experimental design studies, 217
field testing

comedonecrosis, 231
metastastic breast cancer, 230
QDMBA paradigm, 230

forecasting models, 225
formulating well-designed questions,

214–215

molecular classifications, multivariate data

DNA methylation, 224
linear discriminant analysis and neural networks,

224–225
test cases, 225
molecular pathology

FDA, 224
image analysis systems, 223–224

pathologists, 214
patient-centered problems

“foreground” and “background” questions, 214
pathologists, 214

probabilities, odds and various ratios use

BAC vs. well-differentiated adenocarcinomas, 220
diagnostic criteria, 219
histopathologic features, 220, 221
LR+, 221–222
RR and OR, 221
statistically significant diagnostic features analysis,

220–221
prognostic information

clinico-pathologic entities, 229
UIP and NSIP, 229–230
QDMBA paradigm, 213–214
size estimations and power analysis, 218
stages I and II thymoma, 216–217
thymomas studies, 215
tumors classification, 215–216
Evidence-based immunohistochemistry

DIHC (see Diagnostic immunohistochemistry)
PPIHC (see Prognostic-predictive

immunohistochemistry)
Evidence-based medicine (EBM)

aberrant immunoreactivity, 272
adverse events, 305
antidote to anecdote, 95–96

Index

355

Bayesian approach, data analysis
prior probability, 10–11
training/testing sets, 11
“best evidence” incorporation

evidence guidelines integration, 12–13
quality evaluation, medical literature, 12
CDSSs (see Clinical decision support systems)
cellular monomorphism, 271
clinical guidelines, 4
contemporary practice, 305
decision making, 3, 5
definition, 3
detractors, 96
diagnostic interest, 269–270
effectiveness and efficiency, evaluation, 13
elements, 270
eminence-based medicine, 215
environment

If-Then, logic, 4
medical practice, 3
“outcomes research”, 4
randomized clinical trials (RCT), 4

evolution, discipline

EBG, 4
report/technology assessment, 4

formulation and treatment, clinical problem, 5–6
histopathologic diagnosis, 272
ILCs, 271
information, scientific literature

best-evidence summaries, 7, 8
data mining, language texts, 8
Google Scholar, 6
MEDLINE/PubMed database, 6
scientific references, retrieval, 6–7

inter-observer variability
kappa statistics, 11
reexcision, 12
reproducibility, classification schema, 11
specimen-derived data, 12

medical decision-making techniques, 270
medical information, use, 5
morphological diagnostic impressions, 270
participatory medicine, 23
pathology

ADASP, 14
assurance/improvement, quality, 13
“authoritative” interpretation, 14
cancers, asymptomatic patients, 24
CEBM, 36
comprehensive tables, existence, 36
consensus conferences, 14
“cookbook medicine”, 23
EBG development, 15
meta-analysis, 33–34
patient care, 24–25
quality evaluation, 28–32
rigors, higher tiers, 38
steps, practitioner, 24
TNM system, 13–14
traditional style vs. practice style, 23–24

patient care coordination, 306
reproducible practical tool, 269
statistical reasoning, 96
statistical significance, type I and II errors

likelihood ratio (LR), 10
null hypothesis, 8, 10
power analysis, 10
type II error, 10

surgical pathologists and cytopathologists, 269
teaching, 4–5
use and validity, clinical practice
Bayesian approach, 10–11
inter-observer variability, 11–12
statistical significance, 8–12

Evidence-based pathology (EBP)

adversarial experts

biased selection, 341
experts, 340
sampling, 341

CBR, 96
cell, Evidence–based cell pathology
clinico-pathological-correlation, 19
credibility, “expert” witnesses, 345
Daubert case

“gate-keeper” function, 345
obfuscating arguments, 345
unbiased court-appointed authorities, 346

diagnostic errors (see Evaluation of diagnostic errors,

pathology)

diagnostic pathology, 19–20
DIHC, 269
EBM

epidemiology, 19
participatory medicine, 23
pathology, 23–25

“experts”, 338
finality, courts vs. medicine, 346
guide to readers

knowledge, 190
peer review system, 190
histopathologic features, 272
judgmental psychology, 183
lawyers and doctors, 342
legal system (see Tort law, medicine)
malpractice cases, 337–338
medical error and standard of care

negligence, 342
potential dispositions, 342–343
tort actions, 343

molecular pathology (see Molecular pathology)
peer-reviewed medical publications, 344
population, 96
precision to efficient medicine

CER, 23
cost, medical resources, 22
specificity and sensitivity, 22

principles, 273
prognostic classification rule, 138
QDMBA, 213
“repackaging”, information, 189

356

Index

Evidence-based pathology (EBP) (Continued)

reshaping forces
EBM core, 20
immunohistochemistry (IHC), 21
laboratory medicine, 20–21
molecular medicine, 21
“quantitative functional histopathology”, 22
signal transduction pathway, 21
translational engineering and intelligence,

21–22

rights, 340
scientific information and juries

mock juries, 341
social concerns, 341
sporadic assertions, 341–342

socio-economical context, 20
statistical reasoning, 96
tort law (see Tort law, medicine)
validity, standard of care

“average”/“ordinary” skill, 343
ordinary vs. non-ordinary issue, 344
self-determined thresholds, 344
unbiased evaluation, melanocytic lesion, 343

Evidence-based pathology and laboratory

medicine. See Evidence-based medicine
(EBM)

Evidence evaluation
EBM, quality, 28
external validity, 32
Evidence levels analysis
design suitability, 299
genetic test assessment, 299
randomized controlled trials, 299

Evidence levels (ELs) scheme

EL 3, 197
proposed scale, 198

Experimental design, pathology research

software packages, 142
statistical power analysis, 141

External validity
definition, 32
evidence quality evaluation, 32

F
FDA. See Food and drug administration
Fine needle aspiration (FNA), 226
FNA. See Fine needle aspiration
Food and Drug Administration (FDA), 224
Forest plots

immunohistochemistry, 255
integrated odds ratio, 250
preparation, 248
software computation, 251
stage III thymomas, 258

Funnel plots

heterogeneity, evaluation, 252
homogeneous data, 253
publication bias, 252

G
GEA. See Gene expression array
Gene expression array (GEA)
epistemological concerns
data mining, 136
hypothesis-free data exploration, 136
self-fulfilling prophesy, 137–138
mathematical-statistical problems, HDB

bias-variance dilemma, 131
case-based reasoning (CBR), 135
curse of dimensionality, 135, 136
genomic signal processing, 134
“small sample scenario” problem, 134–135

noisy data, 138
observation studies vs. experimental studies,

133–134

General linear model (GLM), 52
Genomics, pathology

ACCE and EGAPP, 298
array-comparative hybridization, 303
clinical testing, 297
tools and technologies, 297
GLM. See General linear model
Global Registry of Acute Coronary Events (GRACE),

310

GRACE. See Global Registry of Acute Coronary Events

H
Halsted procedure, 73
Hazard function, 57
Heat-induced epitope retrieval (HIER), 267
Histogenetic neoplastic kinds (HG-KNeops)

description, 104
ExtnI-CoPeTI

biological variability, 107
clusters, 105–106
constraints, 107–108
peaks, two dimensional phenospace, 107
phenospace clusters, 106
model, Gouldian re-runs, 105
phenospace, domain, 104–105
problem cases

INJS conditions, 109
M-KNeop’s (see Managerial neoplastic kinds)
stylized ADF, 109
splitters and lumpers

grid, 108
non-zero probability, 108

Human error, diagnostic pathology

automation, 240
cases review, 241
checklists, 241

I
Immunohistochemistry (IHC)

diagnostic (see Diagnostic immunohistochemistry

(DIHC))

Index

357

evidence-based medicine, 21
prognostic-predictive (see Prognostic-predictive
immunohistochemistry (PPIHC))

Individually necessary and jointly sufficient (INJS)

EBM, 310
order form design, 311
quasi-professional websites, 311
tools, selection and interpretation,

conditions, 109
Individual neoplasm (INeop)

complexity

annotated dendrogram fingerprint (ADF), 101
cellular, 133
characteristics, 98
clones, 100
INeop vieId, 99
Müllerian neoplasia, 100
neoplastic cells, cancers, 98
normal uterine cervix, 100
organization levels, 98
progression models and lineages, 100
synchronic and diachronic intratumor

heterogeneity, 99–100

uniqueness, 98–99

context dependency, 101–102
dynamic processes, 102
heterogeneity and evolution, 133
microenvironment, 133
uniqueness, 102

Interanalytical agreement. See Cross-validation of methods
Internal validity, evidence
criteria sets, 28–29
definition, 28
experimental design integrity, 28
ranking system, 28
recency and relevance, 29
statistics, data analyzation

Blackstone’s formulation, 29
Cohen’s kappa, 30
confidence intervals, 30
funnel plot, 31
OR, 29
positive likelihood ratio (+LR), 29
PPV, 29
relative and absolute risk, 29
ROC, 29–30
sensitivity and specificity, 29
Spearman’s rank correlation coefficient, 31
standard data set, 31

study

design appraisal, 29
types, 29

Invasive lobular carcinoma (ILC), 271

L
Labeled streptavidin-biotin-peroxidase (LSAB), 266
Laboratory utilization

chemokine coreceptor 5 (CCR5) antagonists, 306
clinical pathologists, 307
decision support

commandments, 311
diagnostic algorithms, 311–312

310–311

“defensive medicine”, 305
diagnostic errors, 306
EHR and ARRA, 305
forecasting models

adverse event analysis, 318
ARRA legislation, 319–320
clinical trends, 319
operations and workflow analysis, 319
surveillance, 318

1–25 hydroxyvitamin D, 306
practice standards

cardiac marker tests, 313
computer based decision support, 316–317
computerized reminders, 317–318
creatine kinase (CK), 313
interpretive services, 313–316
online guidelines, 317

statistics

ACS, 308, 309
area under the curve (AUC), 309
Bayes theorem, 308–309
biopsy, disease prevalence, 307
c index, 309
disease-negative population, 307–308
disease prevalence, 307
disease progression, 307
GRACE, 310
likelihood ratio (LR), 307, 308
NRI, 309, 310
NT-proBNP, 310
risk stratification, 309
ROC curve, 308, 309
sensitivity and specificity assessments, 309

“theragnostics”, 306
tools design and implementation, 320

Logistic regression model

aPL’s antibodies, acute coronary artery syndrome,

53–54

atypical epithelium, prostate, 54–55
HPV DNA testing, ASCUS women, 53
natural logarithm, odds, 52
null hypothesis, 52

Log-rank test

invasive ductal carcinoma, probability plot, 57
Kaplan–Meier plot, 56
pleomorphic liposarcoma, probability plot, 56

Lymph node analysis, 205

M
Managerial neoplastic kinds (M-KNeops)
extended grading systems, 110, 112
grading infiltrating ductal carcinoma, 112
lotteries, 111

358

Index

McGuire criteria (MC), prognostic test evaluation

description, 81
estrogen and progesterone receptor proteins, 82
HER–2 and Herceptin

benefits, 84
fluorescent in-situ hybridization, 85
Herceptest©, 84
heteroantisera, 84
immunohistologic staining, 84
therapeutic target, 82
trastuzumab, 83, 84

Ki–67, 83
mutant p53 protein, 83
PPMTs, 82
putative markers, 81

Medical malpractice and evidence-based medicine

cases, 338
juries, determinative latitude, 345
litigation, 344
medical “negligence”, lawsuits, 342
professional cases, 339
settlement, 343
SMI, 341

Medical order entry systems

appropriate algorithms, 315
computerized, 320
PT-INR, test orders, 317
von Willebrand panel, 316

Meta-analysis, EBP. See Meta-analysis 101, pathologists
Meta-analysis 101, pathologists

applications, anatomic pathology

eminence-based medicine, 246
immunohistochemical panels, 246
novel prognostic markers, 246

data analysis

cohort size evaluation, 248
effect size, 248
“event”, 250
evidence summary, 249
forest plots, 250
non-significant result, 248
software estimation, 248

data collection

“electronic appendices”, 247
elements, 247
histologic types, 248
odds ratios (OR), 246–247
secondary data, 247
thymoma patients, 247–248

data heterogeneity and publication bias, evaluation

definition, 252
file drawer problem, 252
funnel plot, 252–253
reliable, 251–252
statistical tests, 253

epidemiology, 245
epidermal growth factor receptor (EGFR), 254
evidence summary, 254
forest plot, 255
indolent clinical course, 256

non-small cell carcinoma, 254
optimal cohort size, 245
power analysis, 256
prognostic role, micrometastases, 253–254
statistical procedure, 245

Meta-analysis, therapies evaluation

applications, 149–150
combinability, assessment

homogeneity and heterogeneity, 158–159
immunomodulation theory, 160
partial regression coefficients, 161
postoperative infection, 159
Q test statistic, 159
regression techniques, 160–161
stratification, 159

description, 149
diagnostic-test accuracy

average true-positive and false-positive

proportion, 163

cardinal difference, 162–163
logarithmic transformation, 165
logodds ratio, 165
odds ratio, 164
receiver-operating characteristic (ROC) curve, 165
sensitivity and specificity, 163

inclusion reports, assessment

exclusions, 155
non-WBC-reduced vs. WBC-reduced ABT, 155–156
observational studies, 156

medical interpretation

multivariate analysis, 162
patient care, 161
prestorage-filtered WBC-reduced vs.
non-WBC-reduced RBCs, 162

selection bias, 162
validity, 162
observation unit

exposure/intervention effects, 152
gastrointestinal surgery, 155
odds ratio (OR), bacterial infection, 151, 152
postoperative infection, 155
RCTs, 151, 152
WBC-containing ABT and bacterial infection,

153, 154

obstacles

allocation, subjects, 167
cutoff point, 167
gold standard, 166–167
publication bias, 167
Q test statistic, 166
random-effects method, 167
suboptimal technical/scientific merits, 166

quantitative research synthesis

average effect, 158
confidence interval (CI), 158
design differences, 158
fixed-effects method, 157
random-effects method, 157
uncertainty, 158
within-studies and between-studies, 157–158

Index

359

randomized controlled trials, assessment

advantage, 157
average effect, 157
low total quality score, 156
maximum quality score, 156
traditional narrative reviews, 149

Molecular medicine and evidence-based medicine, 21
Molecular pathology
clinical trials, 297
ethical issues

O
Odds ratios (OR)

calculation, 248
forest plot, 251
funnel plot, 252
levels, 258

Oncopathological reality, 110
OR. See Odds ratios

BRCA mutation, 302–303
GINA, 302
oophorectomy, 303
single-gene disorders, 302
evidence-based pathology, 303
genomics, 297
life-saving technologies, 304
real-world considerations

genotype-phenotype correlations, 303
K-ras mutations, 303
parameters, transition determination, 303

science and medicine interface, 297
test evaluation

analytic validity, 299
clinical performance, 300
clinical utility, 301
evidence quality, 298–299
formal assessment, 301–302
literature review, 298
pilot studies, 302
question formulation, 298
sensitivity and specificity, 299–300

whole-genome technologies, 297

Morphological diagnosis
cell pathology, 207
relevance

clinical features and outcomes, 209
diagnosis, described, 208
evidence-based approach, 208
identification and susceptibility, 209
pathological features, 208

reproducibility
DSS, 207
inflammatory and fibrosis components, 207
intra-and inter-observer, 208
kappa value and measurement, 207
microscopic vs. on-screen image, 208
teaching tool, 208

N
Net reclassification index (NRI), 309–310
Nonspecific interstitial pneumonia (NSIP), 196

survival proportions, patients, 196
and UIP, 229–230

Nottingham prognostic index (NPI),

71–72

NPI. See Nottingham prognostic index
NRI. See Net reclassification index
NSIP. See Nonspecific interstitial pneumonia

P
Pathology and laboratory medicine

risks, 15
statistical calculations, 10

Pathology interpretation, data, 189, 190, 192
Pathology literature evaluation

clinico-pathologic studies, 194–195
EBP guide to readers
knowledge, 190
peer review system, 190

epistemology, 189
gastric foveolar-type dysplasia study

Materials and Methods section, 191–192
methodological structure, 192

narrative vs. systematic reviews

background information, 190–191
seven steps, Cochrane Collaboration, 191

study design

applicability, 198–199
Barrett’s esophagus and dysplasia, 193–194
ELs scheme, 197
experimental pathology, 192–193
internal validity, 195
observational studies, 193
RCTs, 197
results, 195–196
types, 193

Pathology research

follicular variant, papillary carcinoma, 147–148
hypothesis testing

chi-square test, 141, 142
less intuitive, 141
p values, 141–142
“underpowered”, 141–142

statistical error types

binary random variables, 142–143
continuous random variables, 143–144
logistic model, 144–146
null hypothesis, 142
power estimation, 142
survival analysis, 146–147

Patient-physician relationship, evidence-based medicine, 23
Peroxidase-antiperoxidase (PAP), 263
Personal experience, pathology, 23–24, 198–199
Power analysis. See Pathology research
PPMT. See Prognostic/predictive medical test
Prediction, anatomic pathology. See also Anatomic
pathology, prognostication and prediction

term meaning, 63
tissue sampling, 66–67

360

Index

Probability, 41
Problem cases, anatomic pathology

ExtnI-CoPeTI structure, 108–109
INJS conditions, 109–110
managerial gradient, 108
M-KNeop’s (see Managerial neoplastic kinds)
stylized ADF, 109

Prognostication, anatomic pathology. See also Breast
cancer, prognostication model

goals, 62
morphology-based observations, 75
PPMT (see Prognostic/predictive medical test)
term meaning, 63
tissue sampling, 66–67
variants, 67–70

Prognostic classification rules

clinical models, 125
heterologous elements, malignant mixed tumors, 127
managerial relevance, 138
Prognostic models, pathology

Bayesian belief networks, 225
EBP, 215
molecular classifications, 224

Prognostic-predictive immunohistochemistry (PPIHC)

AQUA technique, 285–286
biochemical moieties, 280
CD117, 280
clinical application, 275
discipline, 276
dynamic range

base-doublings (dB), 279
defined, 276
dichotomous outcomes, 279
glioblastoma multiforme, 277
micropapillary adenocarcinoma, 278
quantitative-continuous analysis, 279
residual signal power, 279
signal-to-noise ratios (SNR), 279

EBM principles

cost-effectiveness analyses, 290
forecast-oriented immunohistology, 288–289

EGFR

adenocarcinoma, 281
cell surfaces, 280
epidermal growth factor receptor protein, 281

FISH methods, 287
hormone receptors, breast carcinoma

biological variation, 285
chemical competitive assays, 282–283
ERP/PRP status, 282
estrogen receptor protein, 284
in vivo activity, 285
section-based immunoassays, 282

immunoreactivity

actual molecular assessments, 276
antibody testing, 276
cellular and intracellular protein concentrations, 276
nucleic acid blotting techniques, 276
preanalytic variables, 276
quantitative estimation, 276

marker analysis method, 280
nucleic acid microarrays

gene chip structure, 287–288
“heat map”, 288, 289
hybridization, 288
picomoles, 287

polymerase chain reaction (PCR)-based analyses,

286–287

polypeptide gene product, 280
salient intracellular process, 280

Prognostic/predictive medical test (PPMT)

financial and political factors, 64
HER–2, 84
MC, 82

Prostate-specific antigen (PSA)

appropriateness criteria, 328–329
disease progression and recurrence, 328

PSA. See Prostate-specific antigen
Publication bias

file drawer problem, 252
funnel plot, 252–253
heterogeneity, 252
meta analysis, 251–252

Q
QDMBA. See Question-Data-Method-Bayesian

inference-Appraisal

Question-Data-Method-Bayesian inference-Appraisal

(QDMBA)

clinico-pathologic problem evaluation, 214
elements, 215
primary lung adenocarcinomas, 230
UIP and NSIP, 229

R
Randomized controlled trials (RCT)

molecular test, 301
quality assessment, meta-analysis,

156–157

WBC-containing ABT and bacterial infection, 153

Random variables

Gleason score, 43
independent samples, 45
probability distributions

binomial and Poisson, 44
continuous, 44–45
discrete, 43–44

statistical independence, 45
Receiver operator curve (ROC)

plotting, 42
PSA serum value, 42–43
sensitivity vs. 1-specificity, 43
Regression analyses, biostatistics

general linear model (GLM), 52
linear

dependent variable, 51
frequency distribution, residual error values, 52
“likelihood ratio test”, 52

Index

361

residual histogram, 51–52
square root, mitotic rate, 51

logistic model (see Logistic regression model)
non-linearity degree, 51
variables, 50

Report communication

cell pathology report, 209
computerized reporting, 210
format, 209–210

ROC. See Receiver operator curve

S
Sample sizes, pathology research

estimation

continuous random variables, 143–144
logistic model, 144–146
survival analysis, 146–147
two binary random variables, 142–143

hypothesis testing

chi-square test, 141, 142
less intuitive, 141
p values, 141–142

survival analysis

estrogen receptor-b (ER-b), 147
hazard ratio (hr), 146, 147
information, 146
random variables, 146
survival probability plot, 147

Sampling, 204–207
Search engines, evidence-based medicine, 6
Statistical methodology, medical literature review. See
Meta-analysis, therapies evaluation

Statistical power, pathology research

binary random variables

immunohistochemical (IHC) stain, 142
power vs. patients number, 143

continuous random variables
t and b42 amyloid, 144
biomarkers, diseases, 143
cerebral spinal fluid (CSF), 143

logistic model

contingency tables, 146
Cox survival model, 146
explanatory variables, 144
nucleic acid microarrays, 146
odds ratio, OR, 145
PASS package, 146
positive outcome, 145

probability, 144–145
sample size vs. OR, 145

sizes and power, 142
survival analysis

estrogen receptor-b (ER-b), 147
hazard ratio (hr), 146, 147
molecular marker/stain, 146
survival probability plot, 147

Survival plot

confidence limits, 55
Kaplan–Meier plots, 55–56
probability, 55
survival vs. time, 56

T
Tort law, medicine
description, 337
experts, 338
good faith and good conscience, 338
high-quality decisions
four-cell table, 338
knowledgeable and unbiased pathologists

(KUPs), 339

legal test-malfunction, 339
“peer review”, 339
type 1 and 2 jury errors, 339–340

legal decisions, 337–338
malpractice cases, 337
outcomes analysis, 338
quality, 338

Tumor-lymph node-distant metastasis (TNM) system, 61

U
UIP. See Usual interstitial pneumonia
Urinalyses, 325
Usual interstitial pneumonia (UIP), 196, 229, 230

V
Validity, study results
evaluation, 126
external, 130
internal and external, predictive component

clinical prognostic models, 125
FDA, 123–124
treatment recommendations, 124

statistical techniques, 138

